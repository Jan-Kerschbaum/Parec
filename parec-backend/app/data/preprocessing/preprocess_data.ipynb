{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing:\n",
    "\n",
    "‚ÑπÔ∏è This file reads in data from a JSON file, preprocesses and stores it in a new JSON file. \n",
    "\n",
    "‚úÖ It filters the data to only keep papers that are categorized under topics related to *Computer Science*, which helps to reduce the amount of resources and processing capacity needed to index and search the papers. \n",
    "\n",
    "üöÄ The notebook is intended to prepare the data for use in the Parec project, particularly for indexing and searching in Elasticsearch.\n",
    "\n",
    "***\n",
    "‚ö†Ô∏è Please note that the original file (`arxiv-metadata-oai-snapshot.json`) could not be uploaded to this repository due to its large size. If you want to re-implement this code, you need to download it from the [kaggle](https://www.kaggle.com/datasets/Cornell-University/arxiv) website. ‚ö†Ô∏è\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.18.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True) \n",
    "import itertools\n",
    "import re\n",
    "import requests\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'arxiv-metadata-oai-snapshot.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for categories\n",
    "# see https://arxiv.org/help/api/user-manual --> only keep categories related to Computer Science\n",
    "\n",
    "category_map = {\n",
    "'cs.AI': 'Artificial Intelligence',\n",
    "'cs.AR': 'Hardware Architecture',\n",
    "'cs.CC': 'Computational Complexity',\n",
    "'cs.CE': 'Computational Engineering, Finance, and Science',\n",
    "'cs.CG': 'Computational Geometry',\n",
    "'cs.CL': 'Computation and Language',\n",
    "'cs.CR': 'Cryptography and Security',\n",
    "'cs.CV': 'Computer Vision and Pattern Recognition',\n",
    "'cs.CY': 'Computers and Society',\n",
    "'cs.DB': 'Databases',\n",
    "'cs.DC': 'Distributed, Parallel, and Cluster Computing',\n",
    "'cs.DL': 'Digital Libraries',\n",
    "'cs.DM': 'Discrete Mathematics',\n",
    "'cs.DS': 'Data Structures and Algorithms',\n",
    "'cs.ET': 'Emerging Technologies',\n",
    "'cs.FL': 'Formal Languages and Automata Theory',\n",
    "'cs.GL': 'General Literature',\n",
    "'cs.GR': 'Graphics',\n",
    "'cs.GT': 'Computer Science and Game Theory',\n",
    "'cs.HC': 'Human-Computer Interaction',\n",
    "'cs.IR': 'Information Retrieval',\n",
    "'cs.IT': 'Information Theory',\n",
    "'cs.LG': 'Machine Learning',\n",
    "'cs.LO': 'Logic in Computer Science',\n",
    "'cs.MA': 'Multiagent Systems',\n",
    "'cs.MM': 'Multimedia',\n",
    "'cs.MS': 'Mathematical Software',\n",
    "'cs.NA': 'Numerical Analysis',\n",
    "'cs.NE': 'Neural and Evolutionary Computing',\n",
    "'cs.NI': 'Networking and Internet Architecture',\n",
    "'cs.OH': 'Other Computer Science',\n",
    "'cs.OS': 'Operating Systems',\n",
    "'cs.PF': 'Performance',\n",
    "'cs.PL': 'Programming Languages',\n",
    "'cs.RO': 'Robotics',\n",
    "'cs.SC': 'Symbolic Computation',\n",
    "'cs.SD': 'Sound',\n",
    "'cs.SE': 'Software Engineering',\n",
    "'cs.SI': 'Social and Information Networks',\n",
    "'cs.SY': 'Systems and Control'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"cs_categories.json\", \"w\") as outfile:      #save categories as json\n",
    "#     json.dump(category_map, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(path_to_dataset):\n",
    "    \"\"\"Reads in a dataset file and returns an iterable generator.\n",
    "    \n",
    "    Args:\n",
    "        path_to_dataset (str): A string representing the path to the dataset file.\n",
    "    \n",
    "    Yields:\n",
    "        str: A string containing each line of the dataset file.\n",
    "    \"\"\"\n",
    "    with open(path_to_dataset, 'r') as f:     #load original data set\n",
    "        for line in f:\n",
    "            yield line\n",
    "\n",
    "# Strip trailing whitespaces and \\n-characters\n",
    "def clean_strings(strings):\n",
    "    cleaned = strings.strip() \n",
    "    return re.sub('\\s+',' ', cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(path_to_dataset, category_map):\n",
    "    \"\"\"\n",
    "    Filter the data set to include only papers in the Computer Science category published from 2017 to 2022.\n",
    "\n",
    "    Args:\n",
    "        path_to_dataset (str): path to the dataset file.\n",
    "        category_map (dict): mapping of category codes to names.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the filtered metadata of the papers, containing abstract, title, author, year, category, and paper_id.\n",
    "    \"\"\"\n",
    "\n",
    "    authors = []\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    years = []\n",
    "    categories = []\n",
    "    ids = []\n",
    "    metadata = get_metadata(path_to_dataset)\n",
    "\n",
    "    for paper in metadata:\n",
    "        paper_dict = json.loads(paper)\n",
    "        ref = paper_dict.get('journal-ref')\n",
    "        try:\n",
    "            year = int(ref[-4:]) \n",
    "            if 2003 < year <= 2023:\n",
    "                categories.append(category_map[paper_dict.get('categories').split(\" \")[0]])\n",
    "                authors.append(paper_dict.get('authors'))\n",
    "                years.append(year)\n",
    "                titles.append(paper_dict.get('title'))\n",
    "                abstracts.append(paper_dict.get('abstract'))\n",
    "                ids.append(paper_dict.get('id'))\n",
    "        except:\n",
    "            pass \n",
    "    #print(\"Check length: \", len(titles), len(abstracts), len(years), len(authors), len(categories))\n",
    "\n",
    "    cleaned_abstracts = [clean_strings(abstract) for abstract in abstracts]\n",
    "    cleaned_titles = [clean_strings(title) for title in titles]\n",
    "    cleaned_authors = [clean_strings(author) for author in authors]\n",
    "\n",
    "    reduced = []\n",
    "    for author, title, abstract, year, category, id in zip(cleaned_authors, cleaned_titles, cleaned_abstracts, years, categories, ids):\n",
    "        reduced.append({\"abstract\":abstract, \"title\":title, \"author\":author, \"year\":year, \"category\":category, \"paper_id\": id})\n",
    "    \n",
    "    return {\"root\": reduced}        #add root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filter_dataset(data_file, category_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'abstract': 'Given a multiple-input multiple-output (MIMO) channel, feedback from the receiver can be used to specify a transmit precoding matrix, which selectively activates the strongest channel modes. Here we analyze the performance of Random Vector Quantization (RVQ), in which the precoding matrix is selected from a random codebook containing independent, isotropically distributed entries. We assume that channel elements are i.i.d. and known to the receiver, which relays the optimal (rate-maximizing) precoder codebook index to the transmitter using B bits. We first derive the large system capacity of beamforming (rank-one precoding matrix) as a function of B, where large system refers to the limit as B and the number of transmit and receive antennas all go to infinity with fixed ratios. With beamforming RVQ is asymptotically optimal, i.e., no other quantization scheme can achieve a larger asymptotic rate. The performance of RVQ is also compared with that of a simpler reduced-rank scalar quantization scheme in which the beamformer is constrained to lie in a random subspace. We subsequently consider a precoding matrix with arbitrary rank, and approximate the asymptotic RVQ performance with optimal and linear receivers (matched filter and Minimum Mean Squared Error (MMSE)). Numerical examples show that these approximations accurately predict the performance of finite-size systems of interest. Given a target spectral efficiency, numerical examples show that the amount of feedback required by the linear MMSE receiver is only slightly more than that required by the optimal receiver, whereas the matched filter can require significantly more feedback.', 'title': 'Capacity of a Multiple-Antenna Fading Channel with a Quantized Precoding Matrix', 'author': 'Wiroonsak Santipach and Michael L. Honig', 'year': 2009, 'category': 'Information Theory', 'paper_id': '0704.0217'}]\n"
     ]
    }
   ],
   "source": [
    "print(data[\"root\"][0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../runtime/arxiv_large.json', 'w') as fp:    #save reduced data set\n",
    "    json.dump(data, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elasticsearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19fd5efcc5752a60788dce8a956613aa03887a2e6ee65780e0b1022021e9dc86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
