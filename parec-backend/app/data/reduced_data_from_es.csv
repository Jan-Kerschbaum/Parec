abstract,title,author,year,category,id
"  As one of the newest members in the field of artificial immune systems (AIS),
the Dendritic Cell Algorithm (DCA) is based on behavioural models of natural
dendritic cells (DCs). Unlike other AIS, the DCA does not rely on training
data, instead domain or expert knowledge is required to predetermine the
mapping between input signals from a particular instance to the three
categories used by the DCA. This data preprocessing phase has received the
criticism of having manually over-?tted the data to the algorithm, which is
undesirable. Therefore, in this paper we have attempted to ascertain if it is
possible to use principal component analysis (PCA) techniques to automatically
categorise input data while still generating useful and accurate classication
results. The integrated system is tested with a biometrics dataset for the
stress recognition of automobile drivers. The experimental results have shown
the application of PCA to the DCA for the purpose of automated data
preprocessing is successful.
","PCA 4 DCA: The Application Of Principal Component Analysis To The
  Dendritic Cell Algorithm","Feng Gu, Julie Greensmith, Robert Oates and Uwe Aickelin",2009,Artificial Intelligence,
"  We outline initial concepts for an immune inspired algorithm to evaluate and
predict oil price time series data. The proposed solution evolves a short term
pool of trackers dynamically, with each member attempting to map trends and
anticipate future price movements. Successful trackers feed into a long term
memory pool that can generalise across repeating trend patterns. The resulting
sequence of trackers, ordered in time, can be used as a forecasting tool.
Examination of the pool of evolving trackers also provides valuable insight
into the properties of the crude oil market.
",Oil Price Trackers Inspired by Immune Memory,"WIlliam Wilson, Phil Birkin, Uwe Aickelin",2006,Artificial Intelligence,
"  Malicious users try to compromise systems using new techniques. One of the
recent techniques used by the attacker is to perform complex distributed
attacks such as denial of service and to obtain sensitive data such as password
information. These compromised machines are said to be infected with malicious
software termed a ""bot"". In this paper, we investigate the correlation of
behavioural attributes such as keylogging and packet flooding behaviour to
detect the existence of a single bot on a compromised machine by applying (1)
Spearman's rank correlation (SRC) algorithm and (2) the Dendritic Cell
Algorithm (DCA). We also compare the output results generated from these two
methods to the detection of a single bot. The results show that the DCA has a
better performance in detecting malicious activities.
",Performance Evaluation of DCA and SRC on a Single Bot Detection,"Yousof Al-Hammadi, Uwe Aickelin, Julie Greensmith",2010,Artificial Intelligence,
"  Accurate immunological models offer the possibility of performing
highthroughput experiments in silico that can predict, or at least suggest, in
vivo phenomena. In this chapter, we compare various models of immunological
memory. We first validate an experimental immunological simulator, developed by
the authors, by simulating several theories of immunological memory with known
results. We then use the same system to evaluate the predicted effects of a
theory of immunological memory. The resulting model has not been explored
before in artificial immune systems research, and we compare the simulated in
silico output with in vivo measurements. Although the theory appears valid, we
suggest that there are a common set of reasons why immunological memory models
are a useful support tool; not conclusive in themselves.
",Modelling Immunological Memory,"Simon Garret, Martin Robbins, Joanne Walker, William Wilson, Uwe
  Aickelin",2006,Artificial Intelligence,
"  In this paper we outline initial concepts for an immune inspired algorithm to
evaluate price time series data. The proposed solution evolves a short term
pool of trackers dynamically through a process of proliferation and mutation,
with each member attempting to map to trends in price movements. Successful
trackers feed into a long term memory pool that can generalise across repeating
trend patterns. Tests are performed to examine the algorithm's ability to
successfully identify trends in a small data set. The influence of the long
term memory pool is then examined. We find the algorithm is able to identify
price trends presented successfully and efficiently.
",Price Trackers Inspired by Immune Memory,"William Wilson, Phil Birkin, Uwe Aickelin",2006,Artificial Intelligence,
"  The need for integration of ontologies with nonmonotonic rules has been
gaining importance in a number of areas, such as the Semantic Web. A number of
researchers addressed this problem by proposing a unified semantics for hybrid
knowledge bases composed of both an ontology (expressed in a fragment of
first-order logic) and nonmonotonic rules. These semantics have matured over
the years, but only provide solutions for the static case when knowledge does
not need to evolve. In this paper we take a first step towards addressing the
dynamics of hybrid knowledge bases. We focus on knowledge updates and,
considering the state of the art of belief update, ontology update and rule
update, we show that current solutions are only partial and difficult to
combine. Then we extend the existing work on ABox updates with rules, provide a
semantics for such evolving hybrid knowledge bases and study its basic
properties. To the best of our knowledge, this is the first time that an update
operator is proposed for hybrid knowledge bases.
",Towards Closed World Reasoning in Dynamic Open Worlds (Extended Version),Martin Slota and Jo\~ao Leite,2010,Artificial Intelligence,
"  The dendritic cell algorithm is an immune-inspired technique for processing
time-dependant data. Here we propose it as a possible solution for a robotic
classification problem. The dendritic cell algorithm is implemented on a real
robot and an investigation is performed into the effects of varying the
migration threshold median for the cell population. The algorithm performs well
on a classification task with very little tuning. Ways of extending the
implementation to allow it to be used as a classifier within the field of
robotic security are suggested.
",The Application of a Dendritic Cell Algorithm to a Robotic Classifier,"Robert Oates, Julie Greensmith, Uwe Aickelin, Jonathan M. Garibaldi,
  Graham Kendall",2007,Artificial Intelligence,
"  In this paper the author presents a kind of Soft Computing Technique, mainly
an application of fuzzy set theory of Prof. Zadeh [16], on a problem of Medical
Experts Systems. The choosen problem is on design of a physician's decision
model which can take crisp as well as fuzzy data as input, unlike the
traditional models. The author presents a mathematical model based on fuzzy set
theory for physician aided evaluation of a complete representation of
information emanating from the initial interview including patient past
history, present symptoms, and signs observed upon physical examination and
results of clinical and diagnostic tests.
",A Soft Computing Model for Physicians' Decision Process,Siddharths Sankar Biswas,2010,Artificial Intelligence,
"  A cellular automata (CA) configuration is constructed that exhibits emergent
failover. The configuration is based on standard Game of Life rules. Gliders
and glider-guns form the core messaging structure in the configuration. The
blinker is represented as the basic computational unit, and it is shown how it
can be recreated in case of a failure. Stateless failover using primary-backup
mechanism is demonstrated. The details of the CA components used in the
configuration and its working are described, and a simulation of the complete
configuration is also presented.
",Failover in cellular automata,Shailesh Kumar and Shrisha Rao,2011,Artificial Intelligence,
"  The basic unit of meaning on the Semantic Web is the RDF statement, or
triple, which combines a distinct subject, predicate and object to make a
definite assertion about the world. A set of triples constitutes a graph, to
which they give a collective meaning. It is upon this simple foundation that
the rich, complex knowledge structures of the Semantic Web are built. Yet the
very expressiveness of RDF, by inviting comparison with real-world knowledge,
highlights a fundamental shortcoming, in that RDF is limited to statements of
absolute fact, independent of the context in which a statement is asserted.
This is in stark contrast with the thoroughly context-sensitive nature of human
thought. The model presented here provides a particularly simple means of
contextualizing an RDF triple by associating it with related statements in the
same graph. This approach, in combination with a notion of graph similarity, is
sufficient to select only those statements from an RDF graph which are
subjectively most relevant to the context of the requesting process.
",The Dilated Triple,"Marko A. Rodriguez, Alberto Pepe, Joshua Shinavier",2010,Artificial Intelligence,
"  The Dendritic Cell Algorithm (DCA) is an immune-inspired algorithm, developed
for the purpose of anomaly detection. The algorithm performs multi-sensor data
fusion and correlation which results in a 'context aware' detection system.
Previous applications of the DCA have included the detection of potentially
malicious port scanning activity, where it has produced high rates of true
positives and low rates of false positives. In this work we aim to compare the
performance of the DCA and of a Self-Organizing Map (SOM) when applied to the
detection of SYN port scans, through experimental analysis. A SOM is an ideal
candidate for comparison as it shares similarities with the DCA in terms of the
data fusion method employed. It is shown that the results of the two systems
are comparable, and both produce false positives for the same processes. This
shows that the DCA can produce anomaly detection results to the same standard
as an established technique.
","The DCA:SOMe Comparison A comparative study between two
  biologically-inspired algorithms","Julie Greensmith, Jan Feyereisl, Uwe Aickelin",2008,Artificial Intelligence,
"  The search for patterns or motifs in data represents a problem area of key
interest to finance and economic researchers. In this paper we introduce the
Motif Tracking Algorithm, a novel immune inspired pattern identification tool
that is able to identify unknown motifs of a non specified length which repeat
within time series data. The power of the algorithm comes from the fact that it
uses a small number of parameters with minimal assumptions regarding the data
being examined or the underlying motifs. Our interest lies in applying the
algorithm to financial time series data to identify unknown patterns that
exist. The algorithm is tested using three separate data sets. Particular
suitability to financial data is shown by applying it to oil price data. In all
cases the algorithm identifies the presence of a motif population in a fast and
efficient manner due to the utilisation of an intuitive symbolic
representation. The resulting population of motifs is shown to have
considerable potential value for other applications such as forecasting and
algorithm seeding.
",The Motif Tracking Algorithm,"William Wilson, Philip Birkin, Uwe Aickelin",2008,Artificial Intelligence,
"  A new emerging paradigm of Uncertain Risk of Suspicion, Threat and Danger,
observed across the field of information security, is described. Based on this
paradigm a novel approach to anomaly detection is presented. Our approach is
based on a simple yet powerful analogy from the innate part of the human immune
system, the Toll-Like Receptors. We argue that such receptors incorporated as
part of an anomaly detector enhance the detector's ability to distinguish
normal and anomalous behaviour. In addition we propose that Toll-Like Receptors
enable the classification of detected anomalies based on the types of attacks
that perpetrate the anomalous behaviour. Classification of such type is either
missing in existing literature or is not fit for the purpose of reducing the
burden of an administrator of an intrusion detection system. For our model to
work, we propose the creation of a taxonomy of the digital Acytota, based on
which our receptors are created.
",ToLeRating UR-STD,"Jan Feyereisl, Uwe Aickelin",2008,Artificial Intelligence,
"  In order to get strategic positioning for competition in business
organization, the information system must be ahead in this information age
where the information as one of the weapons to win the competition and in the
right hand the information will become a right bullet. The information system
with the information technology support isn't enough if just only on internet
or implemented with internet technology. The growth of information technology
as tools for helping and making people easy to use must be accompanied by
wanting to make fun and happy when they make contact with the information
technology itself. Basically human like to play, since childhood human have
been playing, free and happy and when human grow up they can't play as much as
when human was in their childhood. We have to develop the information system
which is not perform information system itself but can help human to explore
their natural instinct for playing, making fun and happiness when they interact
with the information system. Virtual information system is the way to present
playing and having fun atmosphere on working area.
",Virtual information system on working area,Spits Warnars,2008,Artificial Intelligence,
"  Earthquake DSS is an information technology environment which can be used by
government to sharpen, make faster and better the earthquake mitigation
decision. Earthquake DSS can be delivered as E-government which is not only for
government itself but in order to guarantee each citizen's rights for
education, training and information about earthquake and how to overcome the
earthquake. Knowledge can be managed for future use and would become mining by
saving and maintain all the data and information about earthquake and
earthquake mitigation in Indonesia. Using Web technology will enhance global
access and easy to use. Datawarehouse as unNormalized database for
multidimensional analysis will speed the query process and increase reports
variation. Link with other Disaster DSS in one national disaster DSS, link with
other government information system and international will enhance the
knowledge and sharpen the reports.
",Indonesian Earthquake Decision Support System,Spits Warnars,2009,Artificial Intelligence,
"  A combined Short-Term Learning (STL) and Long-Term Learning (LTL) approach to
solving mobile-robot navigation problems is presented and tested in both the
real and virtual domains. The LTL phase consists of rapid simulations that use
a Genetic Algorithm to derive diverse sets of behaviours, encoded as variable
sets of attributes, and the STL phase is an idiotypic Artificial Immune System.
Results from the LTL phase show that sets of behaviours develop very rapidly,
and significantly greater diversity is obtained when multiple autonomous
populations are used, rather than a single one. The architecture is assessed
under various scenarios, including removal of the LTL phase and switching off
the idiotypic mechanism in the STL phase. The comparisons provide substantial
evidence that the best option is the inclusion of both the LTL phase and the
idiotypic system. In addition, this paper shows that structurally different
environments can be used for the two phases without compromising
transferability.
","Two-Timescale Learning Using Idiotypic Behaviour Mediation For A
  Navigating Mobile Robot","Amanda Whitbrook, Uwe Aickelin, Jonathan M. Garibaldi",2010,Artificial Intelligence,
"  Weighted logic programming, a generalization of bottom-up logic programming,
is a well-suited framework for specifying dynamic programming algorithms. In
this setting, proofs correspond to the algorithm's output space, such as a path
through a graph or a grammatical derivation, and are given a real-valued score
(often interpreted as a probability) that depends on the real weights of the
base axioms used in the proof. The desired output is a function over all
possible proofs, such as a sum of scores or an optimal score. We describe the
PRODUCT transformation, which can merge two weighted logic programs into a new
one. The resulting program optimizes a product of proof scores from the
original programs, constituting a scoring function known in machine learning as
a ``product of experts.'' Through the addition of intuitive constraining side
conditions, we show that several important dynamic programming algorithms can
be derived by applying PRODUCT to weighted logic programs corresponding to
simpler weighted logic programs. In addition, we show how the computation of
Kullback-Leibler divergence, an information-theoretic measure, can be
interpreted using PRODUCT.
",Products of Weighted Logic Programs,"Shay B. Cohen, Robert J. Simmons, Noah A. Smith",2011,Artificial Intelligence,
"  The Dendritic Cell Algorithm (DCA) is inspired by the function of the
dendritic cells of the human immune system. In nature, dendritic cells are the
intrusion detection agents of the human body, policing the tissue and organs
for potential invaders in the form of pathogens. In this research, and abstract
model of DC behaviour is developed and subsequently used to form an algorithm,
the DCA. The abstraction process was facilitated through close collaboration
with laboratory- based immunologists, who performed bespoke experiments, the
results of which are used as an integral part of this algorithm. The DCA is a
population based algorithm, with each agent in the system represented as an
'artificial DC'. Each DC has the ability to combine multiple data streams and
can add context to data suspected as anomalous. In this chapter the abstraction
process and details of the resultant algorithm are given. The algorithm is
applied to numerous intrusion detection problems in computer security including
the detection of port scans and botnets, where it has produced impressive
results with relatively low rates of false positives.
",Detecting Danger: The Dendritic Cell Algorithm,"Julie Greensmith, Uwe Aickelin, Steve Cayzer",2008,Artificial Intelligence,
"  Strategic Environmental Assessment is a procedure aimed at introducing
systematic assessment of the environmental effects of plans and programs. This
procedure is based on the so-called coaxial matrices that define dependencies
between plan activities (infrastructures, plants, resource extractions,
buildings, etc.) and positive and negative environmental impacts, and
dependencies between these impacts and environmental receptors. Up to now, this
procedure is manually implemented by environmental experts for checking the
environmental effects of a given plan or program, but it is never applied
during the plan/program construction. A decision support system, based on a
clear logic semantics, would be an invaluable tool not only in assessing a
single, already defined plan, but also during the planning process in order to
produce an optimized, environmentally assessed plan and to study possible
alternative scenarios. We propose two logic-based approaches to the problem,
one based on Constraint Logic Programming and one on Probabilistic Logic
Programming that could be, in the future, conveniently merged to exploit the
advantages of both. We test the proposed approaches on a real energy plan and
we discuss their limitations and advantages.
",Logic-Based Decision Support for Strategic Environmental Assessment,"Marco Gavanelli and Fabrizio Riguzzi and Michela Milano and Paolo
  Cagnoli",2010,Artificial Intelligence,
"  This paper develops automated testing and debugging techniques for answer set
solver development. We describe a flexible grammar-based black-box ASP fuzz
testing tool which is able to reveal various defects such as unsound and
incomplete behavior, i.e. invalid answer sets and inability to find existing
solutions, in state-of-the-art answer set solver implementations. Moreover, we
develop delta debugging techniques for shrinking failure-inducing inputs on
which solvers exhibit defective behavior. In particular, we develop a delta
debugging algorithm in the context of answer set solving, and evaluate two
different elimination strategies for the algorithm.
",Testing and Debugging Techniques for Answer Set Solver Development,"Robert Brummayer, Matti J\""arvisalo",2010,Artificial Intelligence,
"  In this paper we explore the use of Answer Set Programming (ASP) to
formalize, and reason about, psychological knowledge. In the field of
psychology, a considerable amount of knowledge is still expressed using only
natural language. This lack of a formalization complicates accurate studies,
comparisons, and verification of theories. We believe that ASP, a knowledge
representation formalism allowing for concise and simple representation of
defaults, uncertainty, and evolving domains, can be used successfully for the
formalization of psychological knowledge. To demonstrate the viability of ASP
for this task, in this paper we develop an ASP-based formalization of the
mechanics of Short-Term Memory. We also show that our approach can have rather
immediate practical uses by demonstrating an application of our formalization
to the task of predicting a user's interaction with a graphical interface.
","Formalization of Psychological Knowledge in Answer Set Programming and
  its Application",Marcello Balduccini and Sara Girotto,2010,Artificial Intelligence,
"  Constraint programming (CP) has been used with great success to tackle a wide
variety of constraint satisfaction problems which are computationally
intractable in general. Global constraints are one of the important factors
behind the success of CP. In this paper, we study a new global constraint, the
multiset ordering constraint, which is shown to be useful in symmetry breaking
and searching for leximin optimal solutions in CP. We propose efficient and
effective filtering algorithms for propagating this global constraint. We show
that the algorithms are sound and complete and we discuss possible extensions.
We also consider alternative propagation methods based on existing constraints
in CP toolkits. Our experimental results on a number of benchmark problems
demonstrate that propagating the multiset ordering constraint via a dedicated
algorithm can be very beneficial.
",Filtering Algorithms for the Multiset Ordering Constraint,"Alan Frisch, Brahim Hnich, Zeynep Kiziltan, Ian Miguel, Toby Walsh",2009,Artificial Intelligence,
"  We argue that parameterized complexity is a useful tool with which to study
global constraints. In particular, we show that many global constraints which
are intractable to propagate completely have natural parameters which make them
fixed-parameter tractable and which are easy to compute. This tractability
tends either to be the result of a simple dynamic program or of a decomposition
which has a strong backdoor of bounded size. This strong backdoor is often a
cycle cutset. We also show that parameterized complexity can be used to study
other aspects of constraint programming like symmetry breaking. For instance,
we prove that value symmetry is fixed-parameter tractable to break in the
number of symmetries. Finally, we argue that parameterized complexity can be
used to derive results about the approximability of constraint propagation.
",The Parameterized Complexity of Global Constraints,"Christian Bessiere and Emmanuel Hebrard and Brahim Hnich and Zeynep
  Kiziltan and Toby Walsh",2008,Artificial Intelligence,
"  A technique for speeding up reinforcement learning algorithms by using time
manipulation is proposed. It is applicable to failure-avoidance control
problems running in a computer simulation. Turning the time of the simulation
backwards on failure events is shown to speed up the learning by 260% and
improve the state space exploration by 12% on the cart-pole balancing task,
compared to the conventional Q-learning and Actor-Critic algorithms.
","Time manipulation technique for speeding up reinforcement learning in
  simulations","Petar Kormushev, Kohei Nomoto, Fangyan Dong, Kaoru Hirota",2008,Artificial Intelligence,
"  Making a decision in a changeable and dynamic environment is an arduous task
owing to the lack of information, their uncertainties and the unawareness of
planners about the future evolution of incidents. The use of a decision support
system is an efficient solution of this issue. Such a system can help emergency
planners and responders to detect possible emergencies, as well as to suggest
and evaluate possible courses of action to deal with the emergency. We are
interested in our work to the modeling of a monitoring preventive and emergency
management system, wherein we stress the generic aspect. In this paper we
propose an agent-based architecture of this system and we describe a first step
of our approach which is the modeling of information and their representation
using a multiagent system.
",Towards an Intelligent System for Risk Prevention and Management,Fahem Kebair and Frederic Serin,2008,Artificial Intelligence,
"  In this paper we analyse Belief Propagation over a Gaussian model in a
dynamic environment. Recently, this has been proposed as a method to average
local measurement values by a distributed protocol (""Consensus Propagation"",
Moallemi & Van Roy, 2006), where the average is available for read-out at every
single node. In the case that the underlying network is constant but the values
to be averaged fluctuate (""dynamic data""), convergence and accuracy are
determined by the spectral properties of an associated Ruelle-Perron-Frobenius
operator. For Gaussian models on Erdos-Renyi graphs, numerical computation
points to a spectral gap remaining in the large-size limit, implying
exceptionally good scalability. In a model where the underlying network also
fluctuates (""dynamic network""), averaging is more effective than in the dynamic
data case. Altogether, this implies very good performance of these methods in
very large systems, and opens a new field of statistical physics of large (and
dynamic) information systems.
",Gaussian Belief with dynamic data and in dynamic network,"Erik Aurell, Ren\'e Pfitzner",2009,Artificial Intelligence,
"  The emerging Web of Data utilizes the web infrastructure to represent and
interrelate data. The foundational standards of the Web of Data include the
Uniform Resource Identifier (URI) and the Resource Description Framework (RDF).
URIs are used to identify resources and RDF is used to relate resources. While
RDF has been posited as a logic language designed specifically for knowledge
representation and reasoning, it is more generally useful if it can
conveniently support other models of computing. In order to realize the Web of
Data as a general-purpose medium for storing and processing the world's data,
it is necessary to separate RDF from its logic language legacy and frame it
simply as a data model. Moreover, there is significant advantage in seeing the
Semantic Web as a particular interpretation of the Web of Data that is focused
specifically on knowledge representation and reasoning. By doing so, other
interpretations of the Web of Data are exposed that realize RDF in different
capacities and in support of different computing models.
",Interpretations of the Web of Data,Marko A. Rodriguez,2011,Artificial Intelligence,
"  Voting is a simple mechanism to aggregate the preferences of agents. Many
voting rules have been shown to be NP-hard to manipulate. However, a number of
recent theoretical results suggest that this complexity may only be in the
worst-case since manipulation is often easy in practice. In this paper, we show
that empirical studies are useful in improving our understanding of this issue.
We demonstrate that there is a smooth transition in the probability that a
coalition can elect a desired candidate using the veto rule as the size of the
manipulating coalition increases. We show that a rescaled probability curve
displays a simple and universal form independent of the size of the problem. We
argue that manipulation of the veto rule is asymptotically easy for many
independent and identically distributed votes even when the coalition of
manipulators is critical in size. Based on this argument, we identify a
situation in which manipulation is computationally hard. This is when votes are
highly correlated and the election is ""hung"". We show, however, that even a
single uncorrelated voter is enough to make manipulation easy again.
","Where are the really hard manipulation problems? The phase transition in
  manipulating the veto rule",Toby Walsh,2009,Artificial Intelligence,
"  We show that some common and important global constraints like ALL-DIFFERENT
and GCC can be decomposed into simple arithmetic constraints on which we
achieve bound or range consistency, and in some cases even greater pruning.
These decompositions can be easily added to new solvers. They also provide
other constraints with access to the state of the propagator by sharing of
variables. Such sharing can be used to improve propagation between constraints.
We report experiments with our decomposition in a pseudo-Boolean solver.
","Decompositions of All Different, Global Cardinality and Related
  Constraints","Christian Bessiere, George Katsirelos, Nina Narodytska, Claude-Guy
  Quimper and Toby Walsh",2009,Artificial Intelligence,
"  We show that tools from circuit complexity can be used to study
decompositions of global constraints. In particular, we study decompositions of
global constraints into conjunctive normal form with the property that unit
propagation on the decomposition enforces the same level of consistency as a
specialized propagation algorithm. We prove that a constraint propagator has a
a polynomial size decomposition if and only if it can be computed by a
polynomial size monotone Boolean circuit. Lower bounds on the size of monotone
Boolean circuits thus translate to lower bounds on the size of decompositions
of global constraints. For instance, we prove that there is no polynomial sized
decomposition of the domain consistency propagator for the ALLDIFFERENT
constraint.
",Circuit Complexity and Decompositions of Global Constraints,"Christian Bessiere, George Katsirelos, Nina Narodytska and Toby Walsh",2009,Artificial Intelligence,
"  We relate tag clouds to other forms of visualization, including planar or
reduced dimensionality mapping, and Kohonen self-organizing maps. Using a
modified tag cloud visualization, we incorporate other information into it,
including text sequence and most pertinent words. Our notion of word pertinence
goes beyond just word frequency and instead takes a word in a mathematical
sense as located at the average of all of its pairwise relationships. We
capture semantics through context, taken as all pairwise relationships. Our
domain of application is that of filmscript analysis. The analysis of
filmscripts, always important for cinema, is experiencing a major gain in
importance in the context of television. Our objective in this work is to
visualize the semantics of filmscript, and beyond filmscript any other
partially structured, time-ordered, sequence of text segments. In particular we
develop an innovative approach to plot characterization.
",Tag Clouds for Displaying Semantics: The Case of Filmscripts,"F. Murtagh, A. Ganz, S. McKie, J. Mothe and K. Englmeier",2010,Artificial Intelligence,
"  Short philosophical essay
",What Does Artificial Life Tell Us About Death?,Carlos Gershenson,2011,Artificial Intelligence,
"  In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass,
but also of masses of non-empty intersections, that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM), which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section.
","Extension of Inagaki General Weighted Operators and A New Fusion Rule
  Class of Proportional Redistribution of Intersection Masses",Florentin Smarandache,2009,Artificial Intelligence,
"  In this article we review standard null-move pruning and introduce our
extended version of it, which we call verified null-move pruning. In verified
null-move pruning, whenever the shallow null-move search indicates a fail-high,
instead of cutting off the search from the current node, the search is
continued with reduced depth.
  Our experiments with verified null-move pruning show that on average, it
constructs a smaller search tree with greater tactical strength in comparison
to standard null-move pruning. Moreover, unlike standard null-move pruning,
which fails badly in zugzwang positions, verified null-move pruning manages to
detect most zugzwangs and in such cases conducts a re-search to obtain the
correct result. In addition, verified null-move pruning is very easy to
implement, and any standard null-move pruning program can use verified
null-move pruning by modifying only a few lines of code.
",Verified Null-Move Pruning,Omid David-Tabibi and Nathan S. Netanyahu,2002,Artificial Intelligence,
"  We introduce an extended tableau calculus for answer set programming (ASP).
The proof system is based on the ASP tableaux defined in [Gebser&Schaub, ICLP
2006], with an added extension rule. We investigate the power of Extended ASP
Tableaux both theoretically and empirically. We study the relationship of
Extended ASP Tableaux with the Extended Resolution proof system defined by
Tseitin for sets of clauses, and separate Extended ASP Tableaux from ASP
Tableaux by giving a polynomial-length proof for a family of normal logic
programs P_n for which ASP Tableaux has exponential-length minimal proofs with
respect to n. Additionally, Extended ASP Tableaux imply interesting insight
into the effect of program simplification on the lengths of proofs in ASP.
Closely related to Extended ASP Tableaux, we empirically investigate the effect
of redundant rules on the efficiency of ASP solving.
  To appear in Theory and Practice of Logic Programming (TPLP).
",Extended ASP tableaux and rule redundancy in normal logic programs,"Matti J\""arvisalo and Emilia Oikarinen",2008,Artificial Intelligence,
"  I argue that data becomes temporarily interesting by itself to some
self-improving, but computationally limited, subjective observer once he learns
to predict or compress the data in a better way, thus making it subjectively
simpler and more beautiful. Curiosity is the desire to create or discover more
non-random, non-arbitrary, regular data that is novel and surprising not in the
traditional sense of Boltzmann and Shannon but in the sense that it allows for
compression progress because its regularity was not yet known. This drive
maximizes interestingness, the first derivative of subjective beauty or
compressibility, that is, the steepness of the learning curve. It motivates
exploring infants, pure mathematicians, composers, artists, dancers, comedians,
yourself, and (since 1990) artificial systems.
","Driven by Compression Progress: A Simple Principle Explains Essential
  Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention,
  Curiosity, Creativity, Art, Science, Music, Jokes",Juergen Schmidhuber,2009,Artificial Intelligence,
"  Disjunctive finitary programs are a class of logic programs admitting
function symbols and hence infinite domains. They have very good computational
properties, for example ground queries are decidable while in the general case
the stable model semantics is highly undecidable. In this paper we prove that a
larger class of programs, called finitely recursive programs, preserves most of
the good properties of finitary programs under the stable model semantics,
namely: (i) finitely recursive programs enjoy a compactness property; (ii)
inconsistency checking and skeptical reasoning are semidecidable; (iii)
skeptical resolution is complete for normal finitely recursive programs.
Moreover, we show how to check inconsistency and answer skeptical queries using
finite subsets of the ground program instantiation. We achieve this by
extending the splitting sequence theorem by Lifschitz and Turner: We prove that
if the input program P is finitely recursive, then the partial stable models
determined by any smooth splitting omega-sequence converge to a stable model of
P.
",On finitely recursive programs,"Sabrina Baselice, Piero A. Bonatti, Giovanni Criscuolo",2009,Artificial Intelligence,
"  In a previous paper the authors argued the case for incorporating ideas from
innate immunity into articficial immune systems (AISs) and presented an outline
for a conceptual framework for such systems. A number of key general properties
observed in the biological innate and adaptive immune systems were hughlighted,
and how such properties might be instantiated in artificial systems was
discussed in detail. The next logical step is to take these ideas and build a
software system with which AISs with these properties can be implemented and
experimentally evaluated. This paper reports on the results of that step - the
libtissue system.
",libtissue - implementing innate immunity,"Jamie Twycross, Uwe Aickelin",2006,Artificial Intelligence,
"  Dendritic cells are antigen presenting cells that provide a vital link
between the innate and adaptive immune system, providing the initial detection
of pathogenic invaders. Research into this family of cells has revealed that
they perform information fusion which directs immune responses. We have derived
a Dendritic Cell Algorithm based on the functionality of these cells, by
modelling the biological signals and differentiation pathways to build a
control mechanism for an artificial immune system. We present algorithmic
details in addition to experimental results, when the algorithm was applied to
anomaly detection for the detection of port scans. The results show the
Dendritic Cell Algorithm is sucessful at detecting port scans.
","Information Fusion for Anomaly Detection with the Dendritic Cell
  Algorithm","Julie Greensmith, Uwe Aickelin, Gianni Tedesco",2010,Artificial Intelligence,
"  Network Intrusion Detection Systems (NDIS) monitor a network with the aim of
discerning malicious from benign activity on that network. While a wide range
of approaches have met varying levels of success, most IDS's rely on having
access to a database of known attack signatures which are written by security
experts. Nowadays, in order to solve problems with false positive alters,
correlation algorithms are used to add additional structure to sequences of IDS
alerts. However, such techniques are of no help in discovering novel attacks or
variations of known attacks, something the human immune system (HIS) is capable
of doing in its own specialised domain. This paper presents a novel immune
algorithm for application to an intrusion detection problem. The goal is to
discover packets containing novel variations of attacks covered by an existing
signature base.
",Integrating Innate and Adaptive Immunity for Intrusion Detection,"Gianni Tedesco, Jamie Twycross, Uwe Aickelin",2006,Artificial Intelligence,
"  Biologically-inspired methods such as evolutionary algorithms and neural
networks are proving useful in the field of information fusion. Artificial
Immune Systems (AISs) are a biologically-inspired approach which take
inspiration from the biological immune system. Interestingly, recent research
has show how AISs which use multi-level information sources as input data can
be used to build effective algorithms for real time computer intrusion
detection. This research is based on biological information fusion mechanisms
used by the human immune system and as such might be of interest to the
information fusion community. The aim of this paper is to present a summary of
some of the biological information fusion mechanisms seen in the human immune
system, and of how these mechanisms have been implemented as AISs
",Information Fusion in the Immune System,"Jamie Twycross, Uwe Aickelin",2010,Artificial Intelligence,
"  Multi-agent systems offer a new and exciting way of understanding the world
of work. We apply agent-based modeling and simulation to investigate a set of
problems in a retail context. Specifically, we are working to understand the
relationship between people management practices on the shop-floor and retail
performance. Despite the fact we are working within a relatively novel and
complex domain, it is clear that using an agent-based approach offers great
potential for improving organizational capabilities in the future. Our
multi-disciplinary research team has worked closely with one of the UK's top
ten retailers to collect data and build an understanding of shop-floor
operations and the key actors in a department (customers, staff, and managers).
Based on this case study we have built and tested our first version of a retail
branch agent-based simulation model where we have focused on how we can
simulate the effects of people management practices on customer satisfaction
and sales. In our experiments we have looked at employee development and
cashier empowerment as two examples of shop floor management practices. In this
paper we describe the underlying conceptual ideas and the features of our
simulation model. We present a selection of experiments we have conducted in
order to validate our simulation model and to show its potential for answering
""what-if"" questions in a retail context. We also introduce a novel performance
measure which we have created to quantify customers' satisfaction with service,
based on their individual shopping experiences.
",Modelling and simulating retail management practices: a first approach,"Peer-Olaf Siebers, Uwe Aickelin, Helen Celia, Chris Clegg",2009,Artificial Intelligence,
"  Intelligent agents offer a new and exciting way of understanding the world of
work. Agent-Based Simulation (ABS), one way of using intelligent agents,
carries great potential for progressing our understanding of management
practices and how they link to retail performance. We have developed simulation
models based on research by a multi-disciplinary team of economists, work
psychologists and computer scientists. We will discuss our experiences of
implementing these concepts working with a well-known retail department store.
There is no doubt that management practices are linked to the performance of an
organisation (Reynolds et al., 2005; Wall & Wood, 2005). Best practices have
been developed, but when it comes down to the actual application of these
guidelines considerable ambiguity remains regarding their effectiveness within
particular contexts (Siebers et al., forthcoming a). Most Operational Research
(OR) methods can only be used as analysis tools once management practices have
been implemented. Often they are not very useful for giving answers to
speculative 'what-if' questions, particularly when one is interested in the
development of the system over time rather than just the state of the system at
a certain point in time. Simulation can be used to analyse the operation of
dynamic and stochastic systems. ABS is particularly useful when complex
interactions between system entities exist, such as autonomous decision making
or negotiation. In an ABS model the researcher explicitly describes the
decision process of simulated actors at the micro level. Structures emerge at
the macro level as a result of the actions of the agents and their interactions
with other agents and the environment. 3 We will show how ABS experiments can
deal with testing and optimising management practices such as training,
empowerment or teamwork. Hence, questions such as ""will staff setting their own
break times improve performance?"" can be investigated.
",Multi-Agent Simulation and Management Practices,"Peer-Olaf Siebers, Uwe Aickelin, Helen Celia, Chris Clegg",2008,Artificial Intelligence,
"  In a previous paper the authors argued the case for incorporating ideas from
innate immunity into artificial immune systems (AISs) and presented an outline
for a conceptual framework for such systems. A number of key general properties
observed in the biological innate and adaptive immune systems were highlighted,
and how such properties might be instantiated in artificial systems was
discussed in detail. The next logical step is to take these ideas and build a
software system with which AISs with these properties can be implemented and
experimentally evaluated. This paper reports on the results of that step - the
libtissue system.
",Experimenting with Innate Immunity,"Jamie Twycross, Uwe Aickelin",2006,Artificial Intelligence,
"  In the past few years, IRC bots, malicious programs which are remotely
controlled by the attacker through IRC servers, have become a major threat to
the Internet and users. These bots can be used in different malicious ways such
as issuing distributed denial of services attacks to shutdown other networks
and services, keystrokes logging, spamming, traffic sniffing cause serious
disruption on networks and users. New bots use peer to peer (P2P) protocols
start to appear as the upcoming threat to Internet security due to the fact
that P2P bots do not have a centralized point to shutdown or traceback, thus
making the detection of P2P bots is a real challenge. In response to these
threats, we present an algorithm to detect an individual P2P bot running on a
system by correlating its activities. Our evaluation shows that correlating
different activities generated by P2P bots within a specified time period can
detect these kind of bots.
",Behavioural Correlation for Detecting P2P Bots,"Yousof Al-Hammadi, Uwe Aickelin",2010,Artificial Intelligence,
"  Dendritic cells are antigen presenting cells that provide a vital link
between the innate and adaptive immune system. Research into this family of
cells has revealed that they perform the role of coordinating T-cell based
immune responses, both reactive and for generating tolerance. We have derived
an algorithm based on the functionality of these cells, and have used the
signals and differentiation pathways to build a control mechanism for an
artificial immune system. We present our algorithmic details in addition to
some preliminary results, where the algorithm was applied for the purpose of
anomaly detection. We hope that this algorithm will eventually become the key
component within a large, distributed immune system, based on sound
immunological concepts.
","Introducing Dendritic Cells as a Novel Immune-Inspired Algorithm for
  Anomoly Detection","Julie Greensmith, Uwe Aickelin, Steve Cayzer",2005,Artificial Intelligence,
"  The successful execution of a construction project is heavily impacted by
making the right decision during tendering processes. Managing tender
procedures is very complex and uncertain involving coordination of many tasks
and individuals with different priorities and objectives. Bias and inconsistent
decision are inevitable if the decision-making process is totally depends on
intuition, subjective judgement or emotion. In making transparent decision and
healthy competition tendering, there exists a need for flexible guidance tool
for decision support. Aim of this paper is to give a review on current
practices of Decision Support Systems (DSS) technology in construction
tendering processes. Current practices of general tendering processes as
applied to the most countries in different regions such as United States,
Europe, Middle East and Asia are comprehensively discussed. Applications of
Web-based tendering processes is also summarised in terms of its properties.
Besides that, a summary of Decision Support System (DSS) components is included
in the next section. Furthermore, prior researches on implementation of DSS
approaches in tendering processes are discussed in details. Current issues
arise from both of paper-based and Web-based tendering processes are outlined.
Finally, conclusion is included at the end of this paper.
",Decision Support Systems (DSS) in Construction Tendering Processes,"Rosmayati Mohemad, Abdul Razak Hamdan, Zulaiha Ali Othman, Noor
  Maizura Mohamad Noor",2010,Artificial Intelligence,
"  In this book we introduce a new procedure called \alpha-Discounting Method
for Multi-Criteria Decision Making (\alpha-D MCDM), which is as an alternative
and extension of Saaty Analytical Hierarchy Process (AHP). It works for any
number of preferences that can be transformed into a system of homogeneous
linear equations. A degree of consistency (and implicitly a degree of
inconsistency) of a decision-making problem are defined. \alpha-D MCDM is
afterwards generalized to a set of preferences that can be transformed into a
system of linear and or non-linear homogeneous and or non-homogeneous equations
and or inequalities. The general idea of \alpha-D MCDM is to assign non-null
positive parameters \alpha_1, \alpha_2, and so on \alpha_p to the coefficients
in the right-hand side of each preference that diminish or increase them in
order to transform the above linear homogeneous system of equations which has
only the null-solution, into a system having a particular non-null solution.
After finding the general solution of this system, the principles used to
assign particular values to all parameters \alpha is the second important part
of \alpha-D, yet to be deeper investigated in the future. In the current book
we propose the Fairness Principle, i.e. each coefficient should be discounted
with the same percentage (we think this is fair: not making any favoritism or
unfairness to any coefficient), but the reader can propose other principles.
For consistent decision-making problems with pairwise comparisons,
\alpha-Discounting Method together with the Fairness Principle give the same
result as AHP. But for weak inconsistent decision-making problem,
\alpha-Discounting together with the Fairness Principle give a different result
from AHP. Many consistent, weak inconsistent, and strong inconsistent examples
are given in this book.
",$\alpha$-Discounting Multi-Criteria Decision Making ($\alpha$-D MCDM),Florentin Smarandache,2010,Artificial Intelligence,
"  To study the communication between information systems, Wang et al. [C. Wang,
C. Wu, D. Chen, Q. Hu, and C. Wu, Communicating between information systems,
Information Sciences 178 (2008) 3228-3239] proposed two concepts of type-1 and
type-2 consistent functions. Some properties of such functions and induced
relation mappings have been investigated there. In this paper, we provide an
improvement of the aforementioned work by disclosing the symmetric relationship
between type-1 and type-2 consistent functions. We present more properties of
consistent functions and induced relation mappings and improve upon several
deficient assertions in the original work. In particular, we unify and extend
type-1 and type-2 consistent functions into the so-called
neighborhood-consistent functions. This provides a convenient means for
studying the communication between information systems based on various
neighborhoods.
",Some improved results on communication between information systems,Ping Zhu and Qiaoyan Wen,2010,Artificial Intelligence,
"  Recently, Wang et al. discussed the properties of fuzzy information systems
under homomorphisms in the paper [C. Wang, D. Chen, L. Zhu, Homomorphisms
between fuzzy information systems, Applied Mathematics Letters 22 (2009)
1045-1050], where homomorphisms are based upon the concepts of consistent
functions and fuzzy relation mappings. In this paper, we classify consistent
functions as predecessor-consistent and successor-consistent, and then proceed
to present more properties of consistent functions. In addition, we improve
some characterizations of fuzzy relation mappings provided by Wang et al.
",Homomorphisms between fuzzy information systems revisited,Ping Zhu and Qiaoyan Wen,2011,Artificial Intelligence,
"  Dendritic cells are the crime scene investigators of the human immune system.
Their function is to correlate potentially anomalous invading entities with
observed damage to the body. The detection of such invaders by dendritic cells
results in the activation of the adaptive immune system, eventually leading to
the removal of the invader from the host body. This mechanism has provided
inspiration for the development of a novel bio-inspired algorithm, the
Dendritic Cell Algorithm. This algorithm processes information at multiple
levels of resolution, resulting in the creation of information granules of
variable structure. In this chapter we examine the multi-faceted nature of
immunology and how research in this field has shaped the function of the
resulting Dendritic Cell Algorithm. A brief overview of the algorithm is given
in combination with the details of the processes used for its development. The
chapter is concluded with a discussion of the parallels between our
understanding of the human immune system and how such knowledge influences the
design of artificial immune systems.
",Artificial Dendritic Cells: Multi-faceted Perspectives,Julie Greensmith and Uwe Aickelin,2009,Artificial Intelligence,
"  The Web community has introduced a set of standards and technologies for
representing, querying, and manipulating a globally distributed data structure
known as the Web of Data. The proponents of the Web of Data envision much of
the world's data being interrelated and openly accessible to the general
public. This vision is analogous in many ways to the Web of Documents of common
knowledge, but instead of making documents and media openly accessible, the
focus is on making data openly accessible. In providing data for public use,
there has been a stimulated interest in a movement dubbed Open Data. Open Data
is analogous in many ways to the Open Source movement. However, instead of
focusing on software, Open Data is focused on the legal and licensing issues
around publicly exposed data. Together, various technological and legal tools
are laying the groundwork for the future of global-scale data management on the
Web. As of today, in its early form, the Web of Data hosts a variety of data
sets that include encyclopedic facts, drug and protein data, metadata on music,
books and scholarly articles, social network representations, geospatial
information, and many other types of information. The size and diversity of the
Web of Data is a demonstration of the flexibility of the underlying standards
and the overall feasibility of the project as a whole. The purpose of this
article is to provide a review of the technological underpinnings of the Web of
Data as well as some of the hurdles that need to be overcome if the Web of Data
is to emerge as the defacto medium for data representation, distribution, and
ultimately, processing.
",A Reflection on the Structure and Process of the Web of Data,Marko A. Rodriguez,2009,Artificial Intelligence,
"  Fault diagnosis has become a very important area of research during the last
decade due to the advancement of mechanical and electrical systems in
industries. The automobile is a crucial field where fault diagnosis is given a
special attention. Due to the increasing complexity and newly added features in
vehicles, a comprehensive study has to be performed in order to achieve an
appropriate diagnosis model. A diagnosis system is capable of identifying the
faults of a system by investigating the observable effects (or symptoms). The
system categorizes the fault into a diagnosis class and identifies a probable
cause based on the supplied fault symptoms. Fault categorization and
identification are done using similarity matching techniques. The development
of diagnosis classes is done by making use of previous experience, knowledge or
information within an application area. The necessary information used may come
from several sources of knowledge, such as from system analysis. In this paper
similarity matching techniques for fault diagnosis in automotive infotainment
applications are discussed.
","Similarity Matching Techniques for Fault Diagnosis in Automotive
  Infotainment Electronics",Mashud Kabir,2009,Artificial Intelligence,
"  Diverse recommendation techniques have been already proposed and encapsulated
into several e-business applications, aiming to perform a more accurate
evaluation of the existing information and accordingly augment the assistance
provided to the users involved. This paper reports on the development and
integration of a recommendation module in an agent-based transportation
transactions management system. The module is built according to a novel hybrid
recommendation technique, which combines the advantages of collaborative
filtering and knowledge-based approaches. The proposed technique and supporting
module assist customers in considering in detail alternative transportation
transactions that satisfy their requests, as well as in evaluating completed
transactions. The related services are invoked through a software agent that
constructs the appropriate knowledge rules and performs a synthesis of the
recommendation policy.
","Performing Hybrid Recommendation in Intermodal Transportation-the
  FTMarket System's Recommendation Module",Alexis Lazanas,2009,Artificial Intelligence,
"  LSCS is a satellite workshop of the international conference on principles
and practice of Constraint Programming (CP), since 2004. It is devoted to local
search techniques in constraint satisfaction, and focuses on all aspects of
local search techniques, including: design and implementation of new
algorithms, hybrid stochastic-systematic search, reactive search optimization,
adaptive search, modeling for local-search, global constraints, flexibility and
robustness, learning methods, and specific applications.
","Proceedings 6th International Workshop on Local Search Techniques in
  Constraint Satisfaction","Yves Deville, Christine Solnon",2009,Artificial Intelligence,
"  Many problems that arise in machine learning domain deal with nonlinearity
and quite often demand users to obtain global optimal solutions rather than
local optimal ones. Optimization problems are inherent in machine learning
algorithms and hence many methods in machine learning were inherited from the
optimization literature. Popularly known as the initialization problem, the
ideal set of parameters required will significantly depend on the given
initialization values. The recently developed TRUST-TECH (TRansformation Under
STability-reTaining Equilibria CHaracterization) methodology systematically
explores the subspace of the parameters to obtain a complete set of local
optimal solutions. In this thesis work, we propose TRUST-TECH based methods for
solving several optimization and machine learning problems. Two stages namely,
the local stage and the neighborhood-search stage, are repeated alternatively
in the solution space to achieve improvements in the quality of the solutions.
Our methods were tested on both synthetic and real datasets and the advantages
of using this novel framework are clearly manifested. This framework not only
reduces the sensitivity to initialization, but also allows the flexibility for
the practitioners to use various global and local methods that work well for a
particular problem of interest. Other hierarchical stochastic algorithms like
evolutionary algorithms and smoothing algorithms are also studied and
frameworks for combining these methods with TRUST-TECH have been proposed and
evaluated on several test systems.
",TRUST-TECH based Methods for Optimization and Learning,Chandan K. Reddy,2007,Artificial Intelligence,
"  In this paper, I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions, but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision, recall, and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large.
",Node discovery in a networked organization,Yoshiharu Maeno,2009,Artificial Intelligence,
"  Semantic networks qualify the meaning of an edge relating any two vertices.
Determining which vertices are most ""central"" in a semantic network is
difficult because one relationship type may be deemed subjectively more
important than another. For this reason, research into semantic network metrics
has focused primarily on context-based rankings (i.e. user prescribed
contexts). Moreover, many of the current semantic network metrics rank semantic
associations (i.e. directed paths between two vertices) and not the vertices
themselves. This article presents a framework for calculating semantically
meaningful primary eigenvector-based metrics such as eigenvector centrality and
PageRank in semantic networks using a modified version of the random walker
model of Markov chain analysis. Random walkers, in the context of this article,
are constrained by a grammar, where the grammar is a user defined data
structure that determines the meaning of the final vertex ranking. The ideas in
this article are presented within the context of the Resource Description
Framework (RDF) of the Semantic Web initiative.
",Grammar-Based Random Walkers in Semantic Networks,Marko A. Rodriguez,2008,Artificial Intelligence,
"  Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp.
","Geometric Data Analysis, From Correspondence Analysis to Structured Data
  Analysis (book review)",Fionn Murtagh,2008,Artificial Intelligence,
"  Affinity propagation clustering (AP) has two limitations: it is hard to know
what value of parameter 'preference' can yield an optimal clustering solution,
and oscillations cannot be eliminated automatically if occur. The adaptive AP
method is proposed to overcome these limitations, including adaptive scanning
of preferences to search space of the number of clusters for finding the
optimal clustering solution, adaptive adjustment of damping factors to
eliminate oscillations, and adaptive escaping from oscillations when the
damping adjustment technique fails. Experimental results on simulated and real
data sets show that the adaptive AP is effective and can outperform AP in
quality of clustering results.
",Adaptive Affinity Propagation Clustering,"Kaijun Wang, Junying Zhang, Dan Li, Xinna Zhang and Tao Guo",2007,Artificial Intelligence,
"  We analyze the style and structure of story narrative using the case of film
scripts. The practical importance of this is noted, especially the need to have
support tools for television movie writing. We use the Casablanca film script,
and scripts from six episodes of CSI (Crime Scene Investigation). For analysis
of style and structure, we quantify various central perspectives discussed in
McKee's book, ""Story: Substance, Structure, Style, and the Principles of
Screenwriting"". Film scripts offer a useful point of departure for exploration
of the analysis of more general narratives. Our methodology, using
Correspondence Analysis, and hierarchical clustering, is innovative in a range
of areas that we discuss. In particular this work is groundbreaking in taking
the qualitative analysis of McKee and grounding this analysis in a quantitative
and algorithmic framework.
",The Structure of Narrative: the Case of Film Scripts,"Fionn Murtagh, Adam Ganz and Stewart McKie",2009,Artificial Intelligence,
"  The remarkable results of Foster and Vohra was a starting point for a series
of papers which show that any sequence of outcomes can be learned (with no
prior knowledge) using some universal randomized forecasting algorithm and
forecast-dependent checking rules. We show that for the class of all
computationally efficient outcome-forecast-based checking rules, this property
is violated. Moreover, we present a probabilistic algorithm generating with
probability close to one a sequence with a subsequence which simultaneously
miscalibrates all partially weakly computable randomized forecasting
algorithms. %subsequences non-learnable by each randomized algorithm.
  According to the Dawid's prequential framework we consider partial recursive
randomized algorithms.
",On Sequences with Non-Learnable Subsequences,Vladimir V. V'yugin,2008,Artificial Intelligence,
"  We study two aspects of information semantics: (i) the collection of all
relationships, (ii) tracking and spotting anomaly and change. The first is
implemented by endowing all relevant information spaces with a Euclidean metric
in a common projected space. The second is modelled by an induced ultrametric.
A very general way to achieve a Euclidean embedding of different information
spaces based on cross-tabulation counts (and from other input data formats) is
provided by Correspondence Analysis. From there, the induced ultrametric that
we are particularly interested in takes a sequential - e.g. temporal - ordering
of the data into account. We employ such a perspective to look at narrative,
""the flow of thought and the flow of language"" (Chafe). In application to
policy decision making, we show how we can focus analysis in a small number of
dimensions.
","The Correspondence Analysis Platform for Uncovering Deep Structure in
  Data and Information",Fionn Murtagh,2010,Artificial Intelligence,
"  This article presents a model of general-purpose computing on a semantic
network substrate. The concepts presented are applicable to any semantic
network representation. However, due to the standards and technological
infrastructure devoted to the Semantic Web effort, this article is presented
from this point of view. In the proposed model of computing, the application
programming interface, the run-time program, and the state of the computing
virtual machine are all represented in the Resource Description Framework
(RDF). The implementation of the concepts presented provides a practical
computing paradigm that leverages the highly-distributed and standardized
representational-layer of the Semantic Web.
",General-Purpose Computing on a Semantic Network Substrate,Marko A. Rodriguez,2010,Artificial Intelligence,
"  The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.
",Soft constraint abstraction based on semiring homomorphism,Sanjiang Li and Mingsheng Ying,2008,Artificial Intelligence,
"  We live in the Information Age, and information has become a critically
important component of our life. The success of the Internet made huge amounts
of it easily available and accessible to everyone. To keep the flow of this
information manageable, means for its faultless circulation and effective
handling have become urgently required. Considerable research efforts are
dedicated today to address this necessity, but they are seriously hampered by
the lack of a common agreement about ""What is information?"" In particular, what
is ""visual information"" - human's primary input from the surrounding world. The
problem is further aggravated by a long-lasting stance borrowed from the
biological vision research that assumes human-like information processing as an
enigmatic mix of perceptual and cognitive vision faculties. I am trying to find
a remedy for this bizarre situation. Relying on a new definition of
""information"", which can be derived from Kolmogorov's compexity theory and
Chaitin's notion of algorithmic information, I propose a unifying framework for
visual information processing, which explicitly accounts for the perceptual and
cognitive image processing peculiarities. I believe that this framework will be
useful to overcome the difficulties that are impeding our attempts to develop
the right model of human-like intelligent image processing.
","Modeling Visual Information Processing in Brain: A Computer Vision Point
  of View and Approach",Emanuel Diamant,2007,Artificial Intelligence,
"  How best to quantify the information of an object, whether natural or
artifact, is a problem of wide interest. A related problem is the computability
of an object. We present practical examples of a new way to address this
problem. By giving an appropriate representation to our objects, based on a
hierarchical coding of information, we exemplify how it is remarkably easy to
compute complex objects. Our algorithmic complexity is related to the length of
the class of objects, rather than to the length of the object.
",On Ultrametric Algorithmic Information,Fionn Murtagh,2010,Artificial Intelligence,
"  In this paper we extend the new family of (quantitative) Belief Conditioning
Rules (BCR) recently developed in the Dezert-Smarandache Theory (DSmT) to their
qualitative counterpart for belief revision. Since the revision of quantitative
as well as qualitative belief assignment given the occurrence of a new event
(the conditioning constraint) can be done in many possible ways, we present
here only what we consider as the most appealing Qualitative Belief
Conditioning Rules (QBCR) which allow to revise the belief directly with words
and linguistic labels and thus avoids the introduction of ad-hoc translations
of quantitative beliefs into quantitative ones for solving the problem.
",Qualitative Belief Conditioning Rules (QBCR),"Florentin Smarandache, Jean Dezert",2007,Artificial Intelligence,
"  Many systems can be described in terms of networks of discrete elements and
their various relationships to one another. A semantic network, or
multi-relational network, is a directed labeled graph consisting of a
heterogeneous set of entities connected by a heterogeneous set of
relationships. Semantic networks serve as a promising general-purpose modeling
substrate for complex systems. Various standardized formats and tools are now
available to support practical, large-scale semantic network models. First, the
Resource Description Framework (RDF) offers a standardized semantic network
data model that can be further formalized by ontology modeling languages such
as RDF Schema (RDFS) and the Web Ontology Language (OWL). Second, the recent
introduction of highly performant triple-stores (i.e. semantic network
databases) allows semantic network models on the order of $10^9$ edges to be
efficiently stored and manipulated. RDF and its related technologies are
currently used extensively in the domains of computer science, digital library
science, and the biological sciences. This article will provide an introduction
to RDF/RDFS/OWL and an examination of its suitability to model discrete element
complex systems.
",Using RDF to Model the Structure and Process of Systems,"Marko A. Rodriguez, Jennifer H. Watkins, Johan Bollen, Carlos
  Gershenson",2008,Artificial Intelligence,
"  We try to perform geometrization of psychology by representing mental states,
<<ideas>>, by points of a metric space, <<mental space>>. Evolution of ideas is
described by dynamical systems in metric mental space. We apply the mental
space approach for modeling of flows of unconscious and conscious information
in the human brain. In a series of models, Models 1-4, we consider cognitive
systems with increasing complexity of psychological behavior determined by
structure of flows of ideas. Since our models are in fact models of the
AI-type, one immediately recognizes that they can be used for creation of
AI-systems, which we call psycho-robots, exhibiting important elements of human
psyche. Creation of such psycho-robots may be useful improvement of domestic
robots. At the moment domestic robots are merely simple working devices (e.g.
vacuum cleaners or lawn mowers) . However, in future one can expect demand in
systems which be able not only perform simple work tasks, but would have
elements of human self-developing psyche. Such AI-psyche could play an
important role both in relations between psycho-robots and their owners as well
as between psycho-robots. Since the presence of a huge numbers of
psycho-complexes is an essential characteristic of human psychology, it would
be interesting to model them in the AI-framework.
",Toward Psycho-robots,Andrei Khrennikov,2010,Artificial Intelligence,
"  Informledge System (ILS) is a knowledge network with autonomous nodes and
intelligent links that integrate and structure the pieces of knowledge. In this
paper, we aim to put forward the link dynamics involved in intelligent
processing of information in ILS. There has been advancement in knowledge
management field which involve managing information in databases from a single
domain. ILS works with information from multiple domains stored in distributed
way in the autonomous nodes termed as Knowledge Network Node (KNN). Along with
the concept under consideration, KNNs store the processed information linking
concepts and processors leading to the appropriate processing of information.
","Creating Intelligent Linking for Information Threading in Knowledge
  Networks","Dr T.R. Gopalakrishnan Nair, Meenakshi Malhotra",2011,Artificial Intelligence,
"  In this paper, we present a novel approach for Human Computer Interaction
(HCI) where, we control cursor movement using a real-time camera. Current
methods involve changing mouse parts such as adding more buttons or changing
the position of the tracking ball. Instead, our method is to use a camera and
computer vision technology, such as image segmentation and gesture recognition,
to control mouse tasks (left and right clicking, double-clicking, and
scrolling) and we show how it can perform everything as current mouse devices
can. The software will be developed in JAVA language. Recognition and pose
estimation in this system are user independent and robust as we will be using
colour tapes on our finger to perform actions. The software can be used as an
intuitive input interface to applications that require multi-dimensional
control e.g. computer games etc.
",Mouse Simulation Using Two Coloured Tapes,"Vikram Kumar, Kamran Niyazi, Swapnil Mahe, Swapnil Vyawahare",2012,Artificial Intelligence,
"  Bayesian networks (BN) are used in a big range of applications but they have
one issue concerning parameter learning. In real application, training data are
always incomplete or some nodes are hidden. To deal with this problem many
learning parameter algorithms are suggested foreground EM, Gibbs sampling and
RBE algorithms. In order to limit the search space and escape from local maxima
produced by executing EM algorithm, this paper presents a learning parameter
algorithm that is a fusion of EM and RBE algorithms. This algorithm
incorporates the range of a parameter into the EM algorithm. This range is
calculated by the first step of RBE algorithm allowing a regularization of each
parameter in bayesian network after the maximization step of the EM algorithm.
The threshold EM algorithm is applied in brain tumor diagnosis and show some
advantages and disadvantages over the EM algorithm.
","The threshold EM algorithm for parameter learning in bayesian network
  with incomplete data","Fradj Ben Lamine, Karim Kalti, Mohamed Ali Mahjoub",2011,Artificial Intelligence,
"  We mathematically model Ignacio Matte Blanco's principles of symmetric and
asymmetric being through use of an ultrametric topology. We use for this the
highly regarded 1975 book of this Chilean psychiatrist and pyschoanalyst (born
1908, died 1995). Such an ultrametric model corresponds to hierarchical
clustering in the empirical data, e.g. text. We show how an ultrametric
topology can be used as a mathematical model for the structure of the logic
that reflects or expresses Matte Blanco's symmetric being, and hence of the
reasoning and thought processes involved in conscious reasoning or in reasoning
that is lacking, perhaps entirely, in consciousness or awareness of itself. In
a companion paper we study how symmetric (in the sense of Matte Blanco's)
reasoning can be demarcated in a context of symmetric and asymmetric reasoning
provided by narrative text.
","Ultrametric Model of Mind, I: Review",Fionn Murtagh,2012,Artificial Intelligence,
"  In a companion paper, Murtagh (2012), we discussed how Matte Blanco's work
linked the unrepressed unconscious (in the human) to symmetric logic and
thought processes. We showed how ultrametric topology provides a most useful
representational and computational framework for this. Now we look at the
extent to which we can find ultrametricity in text. We use coherent and
meaningful collections of nearly 1000 texts to show how we can measure inherent
ultrametricity. On the basis of our findings we hypothesize that inherent
ultrametricty is a basis for further exploring unconscious thought processes.
","Ultrametric Model of Mind, II: Application to Text Content Analysis",Fionn Murtagh,2012,Artificial Intelligence,
"  A resistive memory network that has no crossover wiring is proposed to
overcome the hardware limitations to size and functional complexity that is
associated with conventional analogue neural networks. The proposed memory
network is based on simple network cells that are arranged in a hierarchical
modular architecture. Cognitive functionality of this network is demonstrated
by an example of character recognition. The network is trained by an
evolutionary process to completely recognise characters deformed by random
noise, rotation, scaling and shifting
",Cognitive Memory Network,Alex Pappachen James and Sima Dimitrijev,2010,Artificial Intelligence,
"  The holistic approach to sustainable urban planning implies using different
models in an integrated way that is capable of simulating the urban system. As
the interconnection of such models is not a trivial task, one of the key
elements that may be applied is the description of the urban geometric
properties in an ""interoperable"" way. Focusing on air quality as one of the
most pronounced urban problems, the geometric aspects of a city may be
described by objects such as those defined in CityGML, so that an appropriate
air quality model can be applied for estimating the quality of the urban air on
the basis of atmospheric flow and chemistry equations.
  In this paper we first present theoretical background and motivations for the
interconnection of 3D city models and other models related to sustainable
development and urban planning. Then we present a practical experiment based on
the interconnection of CityGML with an air quality model. Our approach is based
on the creation of an ontology of air quality models and on the extension of an
ontology of urban planning process (OUPP) that acts as an ontology mediator.
",Ontologies for the Integration of Air Quality Models and 3D City Models,"Claudine M\'etral, Gilles Falquet, Kostas Karatzas",2008,Artificial Intelligence,
"  This paper develops generalizations of empowerment to continuous states.
Empowerment is a recently introduced information-theoretic quantity motivated
by hypotheses about the efficiency of the sensorimotor loop in biological
organisms, but also from considerations stemming from curiosity-driven
learning. Empowemerment measures, for agent-environment systems with stochastic
transitions, how much influence an agent has on its environment, but only that
influence that can be sensed by the agent sensors. It is an
information-theoretic generalization of joint controllability (influence on
environment) and observability (measurement by sensors) of the environment by
the agent, both controllability and observability being usually defined in
control theory as the dimensionality of the control/observation spaces. Earlier
work has shown that empowerment has various interesting and relevant
properties, e.g., it allows us to identify salient states using only the
dynamics, and it can act as intrinsic reward without requiring an external
reward. However, in this previous work empowerment was limited to the case of
small-scale and discrete domains and furthermore state transition probabilities
were assumed to be known. The goal of this paper is to extend empowerment to
the significantly more important and relevant case of continuous vector-valued
state spaces and initially unknown state transition probabilities. The
continuous state space is addressed by Monte-Carlo approximation; the unknown
transitions are addressed by model learning and prediction for which we apply
Gaussian processes regression with iterated forecasting. In a number of
well-known continuous control tasks we examine the dynamics induced by
empowerment and include an application to exploration and online model
learning.
",Empowerment for Continuous Agent-Environment Systems,Tobias Jung and Daniel Polani and Peter Stone,2011,Artificial Intelligence,
"  The proposed feature selection method builds a histogram of the most stable
features from random subsets of a training set and ranks the features based on
a classifier based cross-validation. This approach reduces the instability of
features obtained by conventional feature selection methods that occur with
variation in training data and selection criteria. Classification results on
four microarray and three image datasets using three major feature selection
criteria and a naive Bayes classifier show considerable improvement over
benchmark results.
","Improving feature selection algorithms using normalised feature
  histograms",Alex Pappachen James and Akshay Maan,2011,Artificial Intelligence,
"  Data Mining techniques plays a vital role like extraction of required
knowledge, finding unsuspected information to make strategic decision in a
novel way which in term understandable by domain experts. A generalized frame
work is proposed by considering non - domain experts during mining process for
better understanding, making better decision and better finding new patters in
case of selecting suitable data mining techniques based on the user profile by
means of intelligent agents. KEYWORDS: Data Mining Techniques, Intelligent
Agents, User Profile, Multidimensional Visualization, Knowledge Discovery.
","A framework: Cluster detection and multidimensional visualization of
  automated data mining using intelligent agents","R. Jayabrabu, V. Saravanan, K. Vivekanandan",2012,Artificial Intelligence,
"  In this paper we propose two new algorithms based on biclustering analysis,
which can be used at the basis of a recommender system for educational
orientation of Russian School graduates. The first algorithm was designed to
help students make a choice between different university faculties when some of
their preferences are known. The second algorithm was developed for the special
situation when nothing is known about their preferences. The final version of
this recommender system will be used by Higher School of Economics.
",Recommender System Based on Algorithm of Bicluster Analysis RecBi,"Dmitry I. Ignatov, Jonas Poelmans, Vasily Zaharchuk",2011,Artificial Intelligence,
"  Concept Relation Discovery and Innovation Enabling Technology (CORDIET), is a
toolbox for gaining new knowledge from unstructured text data. At the core of
CORDIET is the C-K theory which captures the essential elements of innovation.
The tool uses Formal Concept Analysis (FCA), Emergent Self Organizing Maps
(ESOM) and Hidden Markov Models (HMM) as main artifacts in the analysis
process. The user can define temporal, text mining and compound attributes. The
text mining attributes are used to analyze the unstructured text in documents,
the temporal attributes use these document's timestamps for analysis. The
compound attributes are XML rules based on text mining and temporal attributes.
The user can cluster objects with object-cluster rules and can chop the data in
pieces with segmentation rules. The artifacts are optimized for efficient data
analysis; object labels in the FCA lattice and ESOM map contain an URL on which
the user can click to open the selected document.
",Concept Relation Discovery and Innovation Enabling Technology (CORDIET),"Jonas Poelmans, Paul Elzinga, Alexey Neznanov, Stijn Viaene, Sergei O.
  Kuznetsov, Dmitry Ignatov, Guido Dedene",2011,Artificial Intelligence,
"  This paper investigates a new method for improving the learning algorithm of
Mixture of Experts (ME) model using a hybrid of Modified Cuckoo Search (MCS)
and Conjugate Gradient (CG) as a second order optimization technique. The CG
technique is combined with Back-Propagation (BP) algorithm to yield a much more
efficient learning algorithm for ME structure. In addition, the experts and
gating networks in enhanced model are replaced by CG based Multi-Layer
Perceptrons (MLPs) to provide faster and more accurate learning. The CG is
considerably depends on initial weights of connections of Artificial Neural
Network (ANN), so, a metaheuristic algorithm, the so-called Modified Cuckoo
Search is applied in order to select the optimal weights. The performance of
proposed method is compared with Gradient Decent Based ME (GDME) and Conjugate
Gradient Based ME (CGME) in classification and regression problems. The
experimental results show that hybrid MSC and CG based ME (MCS-CGME) has faster
convergence and better performance in utilized benchmark data sets.
","Extended Mixture of MLP Experts by Hybrid of Conjugate Gradient Method
  and Modified Cuckoo Search","Hamid Salimi, Davar Giveki, Mohammad Ali Soltanshahi, Javad Hatami",2012,Artificial Intelligence,
"  Real-time search methods are suited for tasks in which the agent is
interacting with an initially unknown environment in real time. In such
simultaneous planning and learning problems, the agent has to select its
actions in a limited amount of time, while sensing only a local part of the
environment centered at the agents current location. Real-time heuristic search
agents select actions using a limited lookahead search and evaluating the
frontier states with a heuristic function. Over repeated experiences, they
refine heuristic values of states to avoid infinite loops and to converge to
better solutions. The wide spread of such settings in autonomous software and
hardware agents has led to an explosion of real-time search algorithms over the
last two decades. Not only is a potential user confronted with a hodgepodge of
algorithms, but he also faces the choice of control parameters they use. In
this paper we address both problems. The first contribution is an introduction
of a simple three-parameter framework (named LRTS) which extracts the core
ideas behind many existing algorithms. We then prove that LRTA*, epsilon-LRTA*,
SLA*, and gamma-Trap algorithms are special cases of our framework. Thus, they
are unified and extended with additional features. Second, we prove
completeness and convergence of any algorithm covered by the LRTS framework.
Third, we prove several upper-bounds relating the control parameters and
solution quality. Finally, we analyze the influence of the three control
parameters empirically in the realistic scalable domains of real-time
navigation on initially unknown maps from a commercial role-playing game as
well as routing in ad hoc sensor networks.
",Learning in Real-Time Search: A Unifying Framework,"V. Bulitko, G. Lee",2006,Artificial Intelligence,
"  Classification of targets by radar has proved to be notoriously difficult
with the best systems still yet to attain sufficiently high levels of
performance and reliability. In the current contribution we explore a new
design of radar based target recognition, where angular diversity is used in a
cognitive manner to attain better performance. Performance is bench- marked
against conventional classification schemes. The proposed scheme can easily be
extended to cognitive target recognition based on multiple diversity
strategies.
",A cognitive diversity framework for radar target classification,Amit K. Mishra and Chris Baker,2010,Artificial Intelligence,
"  It is widely recognized today that the management of imprecision and
vagueness will yield more intelligent and realistic knowledge-based
applications. Description Logics (DLs) are a family of knowledge representation
languages that have gained considerable attention the last decade, mainly due
to their decidability and the existence of empirically high performance of
reasoning algorithms. In this paper, we extend the well known fuzzy ALC DL to
the fuzzy SHIN DL, which extends the fuzzy ALC DL with transitive role axioms
(S), inverse roles (I), role hierarchies (H) and number restrictions (N). We
illustrate why transitive role axioms are difficult to handle in the presence
of fuzzy interpretations and how to handle them properly. Then we extend these
results by adding role hierarchies and finally number restrictions. The main
contributions of the paper are the decidability proof of the fuzzy DL languages
fuzzy-SI and fuzzy-SHIN, as well as decision procedures for the knowledge base
satisfiability problem of the fuzzy-SI and fuzzy-SHIN.
",Reasoning with Very Expressive Fuzzy Description Logics,"I. Horrocks, J. Z. Pan, G. Stamou, G. Stoilos, V. Tzouvaras",2007,Artificial Intelligence,
"  Exact Max-SAT solvers, compared with SAT solvers, apply little inference at
each node of the proof tree. Commonly used SAT inference rules like unit
propagation produce a simplified formula that preserves satisfiability but,
unfortunately, solving the Max-SAT problem for the simplified formula is not
equivalent to solving it for the original formula. In this paper, we define a
number of original inference rules that, besides being applied efficiently,
transform Max-SAT instances into equivalent Max-SAT instances which are easier
to solve. The soundness of the rules, that can be seen as refinements of unit
resolution adapted to Max-SAT, are proved in a novel and simple way via an
integer programming transformation. With the aim of finding out how powerful
the inference rules are in practice, we have developed a new Max-SAT solver,
called MaxSatz, which incorporates those rules, and performed an experimental
investigation. The results provide empirical evidence that MaxSatz is very
competitive, at least, on random Max-2SAT, random Max-3SAT, Max-Cut, and Graph
3-coloring instances, as well as on the benchmarks from the Max-SAT Evaluation
2006.
",New Inference Rules for Max-SAT,"C. M. Li, F. Manya, J. Planes",2007,Artificial Intelligence,
"  Research on agent communication languages has typically taken the speech acts
paradigm as its starting point. Despite their manifest attractions, speech-act
models of communication have several serious disadvantages as a foundation for
communication in artificial agent systems. In particular, it has proved to be
extremely difficult to give a satisfactory semantics to speech-act based agent
communication languages. In part, the problem is that speech-act semantics
typically make reference to the ""mental states"" of agents (their beliefs,
desires, and intentions), and there is in general no way to attribute such
attitudes to arbitrary computational agents. In addition, agent programming
languages have only had their semantics formalised for abstract, stand-alone
versions, neglecting aspects such as communication primitives. With respect to
communication, implemented agent programming languages have tended to be rather
ad hoc. This paper addresses both of these problems, by giving semantics to
speech-act based messages received by an AgentSpeak agent. AgentSpeak is a
logic-based agent programming language which incorporates the main features of
the PRS model of reactive planning systems. The paper builds upon a structural
operational semantics to AgentSpeak that we developed in previous work. The
main contributions of this paper are as follows: an extension of our earlier
work on the theoretical foundations of AgentSpeak interpreters; a
computationally grounded semantics for (the core) performatives used in
speech-act based agent communication languages; and a well-defined extension of
AgentSpeak that supports agent communication.
","On the Formal Semantics of Speech-Act Based Communication in an
  Agent-Oriented Programming Language","R. H. Bordini, A. F. Moreira, R. Vieira, M. Wooldridge",2007,Artificial Intelligence,
"  Reputation mechanisms offer an effective alternative to verification
authorities for building trust in electronic markets with moral hazard. Future
clients guide their business decisions by considering the feedback from past
transactions; if truthfully exposed, cheating behavior is sanctioned and thus
becomes irrational.
  It therefore becomes important to ensure that rational clients have the right
incentives to report honestly. As an alternative to side-payment schemes that
explicitly reward truthful reports, we show that honesty can emerge as a
rational behavior when clients have a repeated presence in the market. To this
end we describe a mechanism that supports an equilibrium where truthful
feedback is obtained. Then we characterize the set of pareto-optimal equilibria
of the mechanism, and derive an upper bound on the percentage of false reports
that can be recorded by the mechanism. An important role in the existence of
this bound is played by the fact that rational clients can establish a
reputation for reporting honestly.
",Obtaining Reliable Feedback for Sanctioning Reputation Mechanisms,"B. Faltings, R. Jurca",2007,Artificial Intelligence,
"  We present a new algorithm for probabilistic planning with no observability.
Our algorithm, called Probabilistic-FF, extends the heuristic forward-search
machinery of Conformant-FF to problems with probabilistic uncertainty about
both the initial state and action effects. Specifically, Probabilistic-FF
combines Conformant-FFs techniques with a powerful machinery for weighted model
counting in (weighted) CNFs, serving to elegantly define both the search space
and the heuristic function. Our evaluation of Probabilistic-FF shows its fine
scalability in a range of probabilistic domains, constituting a several orders
of magnitude improvement over previous results in this area. We use a
problematic case to point out the main open issue to be addressed by further
research.
","Probabilistic Planning via Heuristic Forward Search and Weighted Model
  Counting","C. Domshlak, J. Hoffmann",2007,Artificial Intelligence,
"  Conjunctive queries play an important role as an expressive query language
for Description Logics (DLs). Although modern DLs usually provide for
transitive roles, conjunctive query answering over DL knowledge bases is only
poorly understood if transitive roles are admitted in the query. In this paper,
we consider unions of conjunctive queries over knowledge bases formulated in
the prominent DL SHIQ and allow transitive roles in both the query and the
knowledge base. We show decidability of query answering in this setting and
establish two tight complexity bounds: regarding combined complexity, we prove
that there is a deterministic algorithm for query answering that needs time
single exponential in the size of the KB and double exponential in the size of
the query, which is optimal. Regarding data complexity, we prove containment in
co-NP.
",Conjunctive Query Answering for the Description Logic SHIQ,"Birte Glimm, Ian Horrocks, Carsten Lutz, Ulrike Sattler",2008,Artificial Intelligence,
"  Experience in the physical sciences suggests that the only realistic means of
understanding complex systems is through the use of mathematical models.
Typically, this has come to mean the identification of quantitative models
expressed as differential equations. Quantitative modelling works best when the
structure of the model (i.e., the form of the equations) is known; and the
primary concern is one of estimating the values of the parameters in the model.
For complex biological systems, the model-structure is rarely known and the
modeler has to deal with both model-identification and parameter-estimation. In
this paper we are concerned with providing automated assistance to the first of
these problems. Specifically, we examine the identification by machine of the
structural relationships between experimentally observed variables. These
relationship will be expressed in the form of qualitative abstractions of a
quantitative model. Such qualitative models may not only provide clues to the
precise quantitative model, but also assist in understanding the essence of
that model. Our position in this paper is that background knowledge
incorporating system modelling principles can be used to constrain effectively
the set of good qualitative models. Utilising the model-identification
framework provided by Inductive Logic Programming (ILP) we present empirical
support for this position using a series of increasingly complex artificial
datasets. The results are obtained with qualitative and quantitative data
subject to varying amounts of noise and different degrees of sparsity. The
results also point to the presence of a set of qualitative states, which we
term kernel subsets, that may be necessary for a qualitative model-learner to
learn correct models. We demonstrate scalability of the method to biological
system modelling by identification of the glycolysis metabolic pathway from
data.
",Qualitative System Identification from Imperfect Data,"George M. Coghill, Ross D. King, Ashwin Srinivasan",2008,Artificial Intelligence,
"  Multi-robot path planning is difficult due to the combinatorial explosion of
the search space with every new robot added. Complete search of the combined
state-space soon becomes intractable. In this paper we present a novel form of
abstraction that allows us to plan much more efficiently. The key to this
abstraction is the partitioning of the map into subgraphs of known structure
with entry and exit restrictions which we can represent compactly. Planning
then becomes a search in the much smaller space of subgraph configurations.
Once an abstract plan is found, it can be quickly resolved into a correct (but
possibly sub-optimal) concrete plan without the need for further search. We
prove that this technique is sound and complete and demonstrate its practical
effectiveness on a real map.
  A contending solution, prioritised planning, is also evaluated and shown to
have similar performance albeit at the cost of completeness. The two approaches
are not necessarily conflicting; we demonstrate how they can be combined into a
single algorithm which outperforms either approach alone.
",Exploiting Subgraph Structure in Multi-Robot Path Planning,Malcolm Ross Kinsella Ryan,2008,Artificial Intelligence,
"  Model checking is a promising technology, which has been applied for
verification of many hardware and software systems. In this paper, we introduce
the concept of model update towards the development of an automatic system
modification tool that extends model checking functions. We define primitive
update operations on the models of Computation Tree Logic (CTL) and formalize
the principle of minimal change for CTL model update. These primitive update
operations, together with the underlying minimal change principle, serve as the
foundation for CTL model update. Essential semantic and computational
characterizations are provided for our CTL model update approach. We then
describe a formal algorithm that implements this approach. We also illustrate
two case studies of CTL model updates for the well-known microwave oven example
and the Andrew File System 1, from which we further propose a method to
optimize the update results in complex system modifications.
",CTL Model Update for System Modifications,"Yulin Ding, Y. Ding, Yan Zhang, Y. Zhang",2008,Artificial Intelligence,
"  Ontologies and automated reasoning are the building blocks of the Semantic
Web initiative. Derivation rules can be included in an ontology to define
derived concepts, based on base concepts. For example, rules allow to define
the extension of a class or property, based on a complex relation between the
extensions of the same or other classes and properties. On the other hand, the
inclusion of negative information both in the form of negation-as-failure and
explicit negative information is also needed to enable various forms of
reasoning. In this paper, we extend RDF graphs with weak and strong negation,
as well as derivation rules. The ERDF stable model semantics of the extended
framework (Extended RDF) is defined, extending RDF(S) semantics. A distinctive
feature of our theory, which is based on Partial Logic, is that both truth and
falsity extensions of properties and classes are considered, allowing for truth
value gaps. Our framework supports both closed-world and open-world reasoning
through the explicit representation of the particular closed-world assumptions
and the ERDF ontological categories of total properties and total classes.
",Extended RDF as a Semantic Foundation of Rule Markup Languages,"Anastasia Analyti, Grigoris Antoniou, Carlos Viegas Dam\'asio, Gerd
  Wagner",2008,Artificial Intelligence,
"  We present three new complexity results for classes of planning problems with
simple causal graphs. First, we describe a polynomial-time algorithm that uses
macros to generate plans for the class 3S of planning problems with binary
state variables and acyclic causal graphs. This implies that plan generation
may be tractable even when a planning problem has an exponentially long minimal
solution. We also prove that the problem of plan existence for planning
problems with multi-valued variables and chain causal graphs is NP-hard.
Finally, we show that plan existence for planning problems with binary state
variables and polytree causal graphs is NP-complete.
",The Complexity of Planning Problems With Simple Causal Graphs,"Omer Gim\'enez, Anders Jonsson",2008,Artificial Intelligence,
"  We represent planning as a set of loosely coupled network flow problems,
where each network corresponds to one of the state variables in the planning
domain. The network nodes correspond to the state variable values and the
network arcs correspond to the value transitions. The planning problem is to
find a path (a sequence of actions) in each network such that, when merged,
they constitute a feasible plan. In this paper we present a number of integer
programming formulations that model these loosely coupled networks with varying
degrees of flexibility. Since merging may introduce exponentially many ordering
constraints we implement a so-called branch-and-cut algorithm, in which these
constraints are dynamically generated and added to the formulation when needed.
Our results are very promising, they improve upon previous planning as integer
programming approaches and lay the foundation for integer programming
approaches for cost optimal planning.
","Loosely Coupled Formulations for Automated Planning: An Integer
  Programming Perspective","Menkes Hector Louis van den Briel, Thomas Vossen, Subbarao Kambhampati",2008,Artificial Intelligence,
"  In a facility with front room and back room operations, it is useful to
switch workers between the rooms in order to cope with changing customer
demand. Assuming stochastic customer arrival and service times, we seek a
policy for switching workers such that the expected customer waiting time is
minimized while the expected back room staffing is sufficient to perform all
work. Three novel constraint programming models and several shaving procedures
for these models are presented. Experimental results show that a model based on
closed-form expressions together with a combination of shaving procedures is
the most efficient. This model is able to find and prove optimal solutions for
many problem instances within a reasonable run-time. Previously, the only
available approach was a heuristic algorithm. Furthermore, a hybrid method
combining the heuristic and the best constraint programming method is shown to
perform as well as the heuristic in terms of solution quality over time, while
achieving the same performance in terms of proving optimality as the pure
constraint programming model. This is the first work of which we are aware that
solves such queueing-based problems with constraint programming.
",A Constraint Programming Approach for Solving a Queueing Control Problem,"Daria Terekhov, J. Christopher Beck",2008,Artificial Intelligence,
"  Decision-theoretic planning is a popular approach to sequential decision
making problems, because it treats uncertainty in sensing and acting in a
principled way. In single-agent frameworks like MDPs and POMDPs, planning can
be carried out by resorting to Q-value functions: an optimal Q-value function
Q* is computed in a recursive manner by dynamic programming, and then an
optimal policy is extracted from Q*. In this paper we study whether similar
Q-value functions can be defined for decentralized POMDP models (Dec-POMDPs),
and how policies can be extracted from such value functions. We define two
forms of the optimal Q-value function for Dec-POMDPs: one that gives a
normative description as the Q-value function of an optimal pure joint policy
and another one that is sequentially rational and thus gives a recipe for
computation. This computation, however, is infeasible for all but the smallest
problems. Therefore, we analyze various approximate Q-value functions that
allow for efficient computation. We describe how they relate, and we prove that
they all provide an upper bound to the optimal Q-value function Q*. Finally,
unifying some previous approaches for solving Dec-POMDPs, we describe a family
of algorithms for extracting policies from such Q-value functions, and perform
an experimental evaluation on existing test problems, including a new
firefighting benchmark problem.
",Optimal and Approximate Q-value Functions for Decentralized POMDPs,"Frans A. Oliehoek, Matthijs T. J. Spaan, Nikos Vlassis",2008,Artificial Intelligence,
"  Multi-agent planning in stochastic environments can be framed formally as a
decentralized Markov decision problem. Many real-life distributed problems that
arise in manufacturing, multi-robot coordination and information gathering
scenarios can be formalized using this framework. However, finding the optimal
solution in the general case is hard, limiting the applicability of recently
developed algorithms. This paper provides a practical approach for solving
decentralized control problems when communication among the decision makers is
possible, but costly. We develop the notion of communication-based mechanism
that allows us to decompose a decentralized MDP into multiple single-agent
problems. In this framework, referred to as decentralized semi-Markov decision
process with direct communication (Dec-SMDP-Com), agents operate separately
between communications. We show that finding an optimal mechanism is equivalent
to solving optimally a Dec-SMDP-Com. We also provide a heuristic search
algorithm that converges on the optimal decomposition. Restricting the
decomposition to some specific types of local behaviors reduces significantly
the complexity of planning. In particular, we present a polynomial-time
algorithm for the case in which individual agents perform goal-oriented
behaviors between communications. The paper concludes with an additional
tractable algorithm that enables the introduction of human knowledge, thereby
reducing the overall problem to finding the best time to communicate. Empirical
results show that these approaches provide good approximate solutions.
",Communication-Based Decomposition Mechanisms for Decentralized MDPs,"Claudia V. Goldman, Shlomo Zilberstein",2008,Artificial Intelligence,
"  Informally, a set of abstractions of a state space S is additive if the
distance between any two states in S is always greater than or equal to the sum
of the corresponding distances in the abstract spaces. The first known additive
abstractions, called disjoint pattern databases, were experimentally
demonstrated to produce state of the art performance on certain state spaces.
However, previous applications were restricted to state spaces with special
properties, which precludes disjoint pattern databases from being defined for
several commonly used testbeds, such as Rubiks Cube, TopSpin and the Pancake
puzzle. In this paper we give a general definition of additive abstractions
that can be applied to any state space and prove that heuristics based on
additive abstractions are consistent as well as admissible. We use this new
definition to create additive abstractions for these testbeds and show
experimentally that well chosen additive abstractions can reduce search time
substantially for the (18,4)-TopSpin puzzle and by three orders of magnitude
over state of the art methods for the 17-Pancake puzzle. We also derive a way
of testing if the heuristic value returned by additive abstractions is provably
too low and show that the use of this test can reduce search time for the
15-puzzle and TopSpin by roughly a factor of two.
",A General Theory of Additive State Space Abstractions,"Fan Yang, Joseph Culberson, Robert Holte, Uzi Zahavi, Ariel Felner",2008,Artificial Intelligence,
"  Markov decision processes capture sequential decision making under
uncertainty, where an agent must choose actions so as to optimize long term
reward. The paper studies efficient reasoning mechanisms for Relational Markov
Decision Processes (RMDP) where world states have an internal relational
structure that can be naturally described in terms of objects and relations
among them. Two contributions are presented. First, the paper develops First
Order Decision Diagrams (FODD), a new compact representation for functions over
relational structures, together with a set of operators to combine FODDs, and
novel reduction techniques to keep the representation small. Second, the paper
shows how FODDs can be used to develop solutions for RMDPs, where reasoning is
performed at the abstract level and the resulting optimal policy is independent
of domain size (number of objects) or instantiation. In particular, a variant
of the value iteration algorithm is developed by using special operations over
FODDs, and the algorithm is shown to converge to the optimal policy.
",First Order Decision Diagrams for Relational MDPs,"Chenggang Wang, Saket Joshi, Roni Khardon",2008,Artificial Intelligence,
"  Resolution is the rule of inference at the basis of most procedures for
automated reasoning. In these procedures, the input formula is first translated
into an equisatisfiable formula in conjunctive normal form (CNF) and then
represented as a set of clauses. Deduction starts by inferring new clauses by
resolution, and goes on until the empty clause is generated or satisfiability
of the set of clauses is proven, e.g., because no new clauses can be generated.
  In this paper, we restrict our attention to the problem of evaluating
Quantified Boolean Formulas (QBFs). In this setting, the above outlined
deduction process is known to be sound and complete if given a formula in CNF
and if a form of resolution, called Q-resolution, is used. We introduce
Q-resolution on terms, to be used for formulas in disjunctive normal form. We
show that the computation performed by most of the available procedures for
QBFs --based on the Davis-Logemann-Loveland procedure (DLL) for propositional
satisfiability-- corresponds to a tree in which Q-resolution on terms and
clauses alternate. This poses the theoretical bases for the introduction of
learning, corresponding to recording Q-resolution formulas associated with the
nodes of the tree. We discuss the problems related to the introduction of
learning in DLL based procedures, and present solutions extending
state-of-the-art proposals coming from the literature on propositional
satisfiability. Finally, we show that our DLL based solver extended with
learning, performs significantly better on benchmarks used in the 2003 QBF
solvers comparative evaluation.
","Clause/Term Resolution and Learning in the Evaluation of Quantified
  Boolean Formulas","E. Giunchiglia, M. Narizzano, A. Tacchella",2006,Artificial Intelligence,
"  It has been widely observed that there is no single ""dominant"" SAT solver;
instead, different solvers perform best on different instances. Rather than
following the traditional approach of choosing the best solver for a given
class of instances, we advocate making this decision online on a per-instance
basis. Building on previous work, we describe SATzilla, an automated approach
for constructing per-instance algorithm portfolios for SAT that use so-called
empirical hardness models to choose among their constituent solvers. This
approach takes as input a distribution of problem instances and a set of
component solvers, and constructs a portfolio optimizing a given objective
function (such as mean runtime, percent of instances solved, or score in a
competition). The excellent performance of SATzilla was independently verified
in the 2007 SAT Competition, where our SATzilla07 solvers won three gold, one
silver and one bronze medal. In this article, we go well beyond SATzilla07 by
making the portfolio construction scalable and completely automated, and
improving it by integrating local search solvers as candidate solvers, by
predicting performance score instead of runtime, and by using hierarchical
hardness models that take into account different types of SAT instances. We
demonstrate the effectiveness of these new techniques in extensive experimental
results on data sets including instances from the most recent SAT competition.
",SATzilla: Portfolio-based Algorithm Selection for SAT,"Lin Xu, Frank Hutter, Holger H. Hoos, Kevin Leyton-Brown",2008,Artificial Intelligence,
"  This paper shows that maintaining logical consistency of an iris recognition
system is a matter of finding a suitable partitioning of the input space in
enrollable and unenrollable pairs by negotiating the user comfort and the
safety of the biometric system. In other words, consistent enrollment is
mandatory in order to preserve system consistency. A fuzzy 3-valued
disambiguated model of iris recognition is proposed and analyzed in terms of
completeness, consistency, user comfort and biometric safety. It is also shown
here that the fuzzy 3-valued model of iris recognition is hosted by an 8-valued
Boolean algebra of modulo 8 integers that represents the computational
formalization in which a biometric system (a software agent) can achieve the
artificial understanding of iris recognition in a logically consistent manner.
",8-Valent Fuzzy Logic for Iris Recognition and Biometry,"N. Popescu-Bodorin, V.E. Balas, I.M. Motoc",2011,Artificial Intelligence,
"  In voting contexts, some new candidates may show up in the course of the
process. In this case, we may want to determine which of the initial candidates
are possible winners, given that a fixed number $k$ of new candidates will be
added. We give a computational study of this problem, focusing on scoring
rules, and we provide a formal comparison with related problems such as control
via adding candidates or cloning.
","New Candidates Welcome! Possible Winners with respect to the Addition of
  New Candidates","Yann Chevaleyre, J\'er\^ome Lang, Nicolas Maudet, J\'er\^ome Monnot,
  Lirong Xia",2012,Artificial Intelligence,
"  Orseau and Ring, as well as Dewey, have recently described problems,
including self-delusion, with the behavior of agents using various definitions
of utility functions. An agent's utility function is defined in terms of the
agent's history of interactions with its environment. This paper argues, via
two examples, that the behavior problems can be avoided by formulating the
utility function in two steps: 1) inferring a model of the environment from
interactions, and 2) computing utility as a function of the environment model.
Basing a utility function on a model that the agent must learn implies that the
utility function must initially be expressed in terms of specifications to be
matched to structures in the learned model. These specifications constitute
prior assumptions about the environment so this approach will not work with
arbitrary environments. But the approach should work for agents designed by
humans to act in the physical world. The paper also addresses the issue of
self-modifying agents and shows that if provided with the possibility to modify
their utility functions agents will not choose to do so, under some usual
assumptions.
",Model-based Utility Functions,Bill Hibbard,2012,Artificial Intelligence,
"  Vertical Total Electron Content (vTEC) is an ionospheric characteristic used
to derive the signal delay imposed by the ionosphere on near-vertical
trans-ionospheric links. The major aim of this paper is to design a prediction
model based on the main factors that influence the variability of this
parameter on a diurnal, seasonal and long-term time-scale. The model should be
accurate and general (comprehensive) enough for efficiently approximating the
high variations of vTEC. However, good approximation and generalization are
conflicting objectives. For this reason a Genetic Programming (GP) with
Multi-objective Evolutionary Algorithm based on Decomposition characteristics
(GP-MOEA/D) is designed and proposed for modeling vTEC over Cyprus.
Experimental results show that the Multi-Objective GP-model, considering real
vTEC measurements obtained over a period of 11 years, has produced a good
approximation of the modeled parameter and can be implemented as a local model
to account for the ionospheric imposed error in positioning. Particulary, the
GP-MOEA/D approach performs better than a Single Objective Optimization GP, a
GP with Non-dominated Sorting Genetic Algorithm-II (NSGA-II) characteristics
and the previously proposed Neural Network-based approach in most cases.
",A GP-MOEA/D Approach for Modelling Total Electron Content over Cyprus,"Andreas Konstantinidis, Haris Haralambous, Alexandros Agapitos and
  Harris Papadopoulos",2010,Artificial Intelligence,
"  In this paper a method is proposed for performance evaluation of road traffic
control systems. The method is designed to be implemented in an on-line
simulation environment, which enables optimisation of adaptive traffic control
strategies. Performance measures are computed using a fuzzy cellular traffic
model, formulated as a hybrid system combining cellular automata and fuzzy
calculus. Experimental results show that the introduced method allows the
performance to be evaluated using imprecise traffic measurements. Moreover, the
fuzzy definitions of performance measures are convenient for uncertainty
determination in traffic control decisions.
","Performance Evaluation of Road Traffic Control Using a Fuzzy Cellular
  Model",Bart{\l}omiej P{\l}aczek,2011,Artificial Intelligence,
"  We describe an inference task in which a set of timestamped event
observations must be clustered into an unknown number of temporal sequences
with independent and varying rates of observations. Various existing approaches
to multi-object tracking assume a fixed number of sources and/or a fixed
observation rate; we develop an approach to inferring structure in timestamped
data produced by a mixture of an unknown and varying number of similar Markov
renewal processes, plus independent clutter noise. The inference simultaneously
distinguishes signal from noise as well as clustering signal observations into
separate source streams. We illustrate the technique via a synthetic experiment
as well as an experiment to track a mixture of singing birds.
",Segregating event streams and noise with a Markov renewal process model,Dan Stowell and Mark D. Plumbley,2013,Artificial Intelligence,
"  Case Based Reasoning (CBR) is an intelligent way of thinking based on
experience and capitalization of already solved cases (source cases) to find a
solution to a new problem (target case). Retrieval phase consists on
identifying source cases that are similar to the target case. This phase may
lead to erroneous results if the existing knowledge imperfections are not taken
into account. This work presents a novel solution based on Fuzzy logic
techniques and adaptation measures which aggregate weighted similarities to
improve the retrieval results. To confirm the efficiency of our solution, we
have applied it to the industrial diagnosis domain. The obtained results are
more efficient results than those obtained by applying typical measures.
",A Logic and Adaptive Approach for Efficient Diagnosis Systems using CBR,"Ibrahim El Bitar, Fatima-Zahra Belouadha, Ounsa Roudies",2012,Artificial Intelligence,
"  Recently, much of the existing work in manifold learning has been done under
the assumption that the data is sampled from a manifold without boundaries and
singularities or that the functions of interest are evaluated away from such
points. At the same time, it can be argued that singularities and boundaries
are an important aspect of the geometry of realistic data.
  In this paper we consider the behavior of graph Laplacians at points at or
near boundaries and two main types of other singularities: intersections, where
different manifolds come together and sharp ""edges"", where a manifold sharply
changes direction. We show that the behavior of graph Laplacian near these
singularities is quite different from that in the interior of the manifolds. In
fact, a phenomenon somewhat reminiscent of the Gibbs effect in the analysis of
Fourier series, can be observed in the behavior of graph Laplacian near such
points. Unlike in the interior of the domain, where graph Laplacian converges
to the Laplace-Beltrami operator, near singularities graph Laplacian tends to a
first-order differential operator, which exhibits different scaling behavior as
a function of the kernel width. One important implication is that while points
near the singularities occupy only a small part of the total volume, the
difference in scaling results in a disproportionately large contribution to the
total behavior. Another significant finding is that while the scaling behavior
of the operator is the same near different types of singularities, they are
very distinct at a more refined level of analysis.
  We believe that a comprehensive understanding of these structures in addition
to the standard case of a smooth manifold can take us a long way toward better
methods for analysis of complex non-linear data and can lead to significant
progress in algorithm design.
","Graph Laplacians on Singular Manifolds: Toward understanding complex
  spaces: graph Laplacians on manifolds with singularities and boundaries",Mikhail Belkin and Qichao Que and Yusu Wang and Xueyuan Zhou,2012,Artificial Intelligence,
"  The considerable mathematical knowledge encoded by the Flyspeck project is
combined with external automated theorem provers (ATPs) and machine-learning
premise selection methods trained on the proofs, producing an AI system capable
of answering a wide range of mathematical queries automatically. The
performance of this architecture is evaluated in a bootstrapping scenario
emulating the development of Flyspeck from axioms to the last theorem, each
time using only the previous theorems and proofs. It is shown that 39% of the
14185 theorems could be proved in a push-button mode (without any high-level
advice and user interaction) in 30 seconds of real time on a fourteen-CPU
workstation. The necessary work involves: (i) an implementation of sound
translations of the HOL Light logic to ATP formalisms: untyped first-order,
polymorphic typed first-order, and typed higher-order, (ii) export of the
dependency information from HOL Light and ATP proofs for the machine learners,
and (iii) choice of suitable representations and methods for learning from
previous proofs, and their integration as advisors with HOL Light. This work is
described and discussed here, and an initial analysis of the body of proofs
that were found fully automatically is provided.
",Learning-Assisted Automated Reasoning with Flyspeck,Cezary Kaliszyk and Josef Urban,2014,Artificial Intelligence,
"  Computational thinking is a new problem soling method named for its extensive
use of computer science techniques. It synthesizes critical thinking and
existing knowledge and applies them in solving complex technological problems.
The term was coined by J. Wing, but the relationship between computational and
critical thinking, the two modes of thiking in solving problems, has not been
yet learly established. This paper aims at shedding some light into this
relationship. We also present two classroom experiments performed recently at
the Graduate Technological Educational Institute of Patras in Greece. The
results of these experiments give a strong indication that the use of computers
as a tool for problem solving enchances the students' abilities in solving real
world problems involving mathematical modelling. This is also crossed by
earlier findings of other researchers for the problem solving process in
general (not only for mathematical problems).
",Problem Solving and Computational Thinking in a Learning Environment,"Michael Gr. Voskoglou, Sheryl Buckley",2012,Artificial Intelligence,
"  Recent attempts to automate business processes and medical-treatment
processes have uncovered the need for a formal framework that can accommodate
not only temporal constraints, but also observations and actions with
uncontrollable durations. To meet this need, this paper defines a Conditional
Simple Temporal Network with Uncertainty (CSTNU) that combines the simple
temporal constraints from a Simple Temporal Network (STN) with the conditional
nodes from a Conditional Simple Temporal Problem (CSTP) and the contingent
links from a Simple Temporal Network with Uncertainty (STNU). A notion of
dynamic controllability for a CSTNU is defined that generalizes the dynamic
consistency of a CTP and the dynamic controllability of an STNU. The paper also
presents some sound constraint-propagation rules for dynamic controllability
that are expected to form the backbone of a dynamic-controllability-checking
algorithm for CSTNUs.
",The Dynamic Controllability of Conditional STNs with Uncertainty,Luke Hunsberger and Roberto Posenato and Carlo Combi,2012,Artificial Intelligence,
"  We use princiles of fuzzy logic to develop a general model representing
several processes in a system's operation characterized by a degree of
vagueness and/or uncertainy. Further, we introduce three altenative measures of
a fuzzy system's effectiveness connected to the above model. An applcation is
also developed for the Mathematical Modelling process illustrating our results.
",A Study on Fuzzy Systems,Michael Gr. Voskoglou,2012,Artificial Intelligence,
"  The problem of replicating the flexibility of human common-sense reasoning
has captured the imagination of computer scientists since the early days of
Alan Turing's foundational work on computation and the philosophy of artificial
intelligence. In the intervening years, the idea of cognition as computation
has emerged as a fundamental tenet of Artificial Intelligence (AI) and
cognitive science. But what kind of computation is cognition?
  We describe a computational formalism centered around a probabilistic Turing
machine called QUERY, which captures the operation of probabilistic
conditioning via conditional simulation. Through several examples and analyses,
we demonstrate how the QUERY abstraction can be used to cast common-sense
reasoning as probabilistic inference in a statistical model of our observations
and the uncertain structure of the world that generated that experience. This
formulation is a recent synthesis of several research programs in AI and
cognitive science, but it also represents a surprising convergence of several
of Turing's pioneering insights in AI, the foundations of computation, and
statistics.
","Towards common-sense reasoning via conditional simulation: legacies of
  Turing in Artificial Intelligence",Cameron E. Freer and Daniel M. Roy and Joshua B. Tenenbaum,2014,Artificial Intelligence,
"  Data association, the problem of reasoning over correspondence between
targets and measurements, is a fundamental problem in tracking. This paper
presents a graphical model formulation of data association and applies an
approximate inference method, belief propagation (BP), to obtain estimates of
marginal association probabilities. We prove that BP is guaranteed to converge,
and bound the number of iterations necessary. Experiments reveal a favourable
comparison to prior methods in terms of accuracy and computational complexity.
","Approximate evaluation of marginal association probabilities with belief
  propagation",Jason L. Williams and Roslyn A. Lau,2014,Artificial Intelligence,
"  The main contribution of this paper, is to propose a novel semantic approach
based on a Natural Language Processing technique in order to ensure a semantic
unification of unstructured process patterns which are expressed not only in
different formats but also, in different forms. This approach is implemented
using the GATE text engineering framework and then evaluated leading up to
high-quality results motivating us to continue in this direction.
","A Semantic Approach for Automatic Structuring and Analysis of Software
  Process Patterns","Nahla Jlaiel, Khouloud Madhbouh, Mohamed Ben Ahmed",2012,Artificial Intelligence,
"  We address the relative expressiveness of defeasible logics in the framework
DL. Relative expressiveness is formulated as the ability to simulate the
reasoning of one logic within another logic. We show that such simulations must
be modular, in the sense that they also work if applied only to part of a
theory, in order to achieve a useful notion of relative expressiveness. We
present simulations showing that logics in DL with and without the capability
of team defeat are equally expressive. We also show that logics that handle
ambiguity differently -- ambiguity blocking versus ambiguity propagating --
have distinct expressiveness, with neither able to simulate the other under a
different formulation of expressiveness.
",Relative Expressiveness of Defeasible Logics,Michael Maher,2012,Artificial Intelligence,
"  Datalog is one of the best-known rule-based languages, and extensions of it
are used in a wide context of applications. An important Datalog extension is
Disjunctive Datalog, which significantly increases the expressivity of the
basic language. Disjunctive Datalog is useful in a wide range of applications,
ranging from Databases (e.g., Data Integration) to Artificial Intelligence
(e.g., diagnosis and planning under incomplete knowledge). However, in recent
years an important shortcoming of Datalog-based languages became evident, e.g.
in the context of data-integration (consistent query-answering, ontology-based
data access) and Semantic Web applications: The language does not permit any
generation of and reasoning with unnamed individuals in an obvious way. In
general, it is weak in supporting many cases of existential quantification. To
overcome this problem, Datalogex has recently been proposed, which extends
traditional Datalog by existential quantification in rule heads. In this work,
we propose a natural extension of Disjunctive Datalog and Datalogex, called
Datalogexor, which allows both disjunctions and existential quantification in
rule heads and is therefore an attractive language for knowledge representation
and reasoning, especially in domains where ontology-based reasoning is needed.
We formally define syntax and semantics of the language Datalogexor, and
provide a notion of instantiation, which we prove to be adequate for
Datalogexor. A main issue of Datalogex and hence also of Datalogexor is that
decidability is no longer guaranteed for typical reasoning tasks. In order to
address this issue, we identify many decidable fragments of the language, which
extend, in a natural way, analog classes defined in the non-disjunctive case.
Moreover, we carry out an in-depth complexity analysis, deriving interesting
results which range from Logarithmic Space to Exponential Time.
","Disjunctive Datalog with Existential Quantifiers: Semantics,
  Decidability, and Complexity Issues","Mario Alviano, Wolfgang Faber, Nicola Leone and Marco Manna",2012,Artificial Intelligence,
"  The improvement of medical care quality is a significant interest for the
future years. The fight against nosocomial infections (NI) in the intensive
care units (ICU) is a good example. We will focus on a set of observations
which reflect the dynamic aspect of the decision, result of the application of
a Medical Decision Support System (MDSS). This system has to make dynamic
decision on temporal data. We use dynamic Bayesian network (DBN) to model this
dynamic process. It is a temporal reasoning within a real-time environment; we
are interested in the Dynamic Decision Support Systems in healthcare domain
(MDDSS).
","Dynamic Decision Support System Based on Bayesian Networks Application
  to fight against the Nosocomial Infections","Hela Ltifi, Ghada Trabelsi, Mounir Ben Ayed, Adel M. Alimi",2012,Artificial Intelligence,
"  Gas Transmission Networks are large-scale complex systems, and corresponding
design and control problems are challenging. In this paper, we consider the
problem of control and management of these systems in crisis situations. We
present these networks by a hybrid systems framework that provides required
analysis models. Further, we discuss decision-making using computational
discrete and hybrid optimization methods. In particular, several reinforcement
learning methods are employed to explore decision space and achieve the best
policy in a specific crisis situation. Simulations are presented to illustrate
the efficiency of the method.
",Hybrid systems modeling for gas transmission network,"Amir Noori, Mohammad Bagher Menhaj, Masoud Shafiee",2007,Artificial Intelligence,
"  People use conjunctions and disjunctions of concepts in ways that violate the
rules of classical logic, such as the law of compositionality. Specifically,
they overextend conjunctions of concepts, a phenomenon referred to as the Guppy
Effect. We build on previous efforts to develop a quantum model that explains
the Guppy Effect in terms of interference. Using a well-studied data set with
16 exemplars that exhibit the Guppy Effect, we developed a 17-dimensional
complex Hilbert space H that models the data and demonstrates the relationship
between overextension and interference. We view the interference effect as, not
a logical fallacy on the conjunction, but a signal that out of the two
constituent concepts, a new concept has emerged.
",The Guppy Effect as Interference,"Diederik Aerts, Jan Broekaert, Liane Gabora and Tomas Veloz",2012,Artificial Intelligence,
"  Network intrusion detection systems have become a crucial issue for computer
systems security infrastructures. Different methods and algorithms are
developed and proposed in recent years to improve intrusion detection systems.
The most important issue in current systems is that they are poor at detecting
novel anomaly attacks. These kinds of attacks refer to any action that
significantly deviates from the normal behaviour which is considered intrusion.
This paper proposed a model to improve this problem based on data mining
techniques. Apriori algorithm is used to predict novel attacks and generate
real-time rules for firewall. Apriori algorithm extracts interesting
correlation relationships among large set of data items. This paper illustrates
how to use Apriori algorithm in intrusion detection systems to cerate a
automatic firewall rules generator to detect novel anomaly attack. Apriori is
the best-known algorithm to mine association rules. This is an innovative way
to find association rules on large scale.
","Automatic firewall rules generator for anomaly detection systems with
  Apriori algorithm","Ehsan Saboori, Shafigh Parsazad, Yasaman Sanatkhani",2010,Artificial Intelligence,
"  Knowledge representation (KR) and inference mechanism are most desirable
thing to make the system intelligent. System is known to an intelligent if its
intelligence is equivalent to the intelligence of human being for a particular
domain or general. Because of incomplete ambiguous and uncertain information
the task of making intelligent system is very difficult. The objective of this
paper is to present the hybrid KR technique for making the system effective &
Optimistic. The requirement for (effective & optimistic) is because the system
must be able to reply the answer with a confidence of some factor. This paper
also presents the comparison between various hybrid KR techniques with the
proposed one.
","Hybrid technique for effective knowledge representation & a comparative
  study","Poonam Tanwar, T. V. Prasad, Dr. Kamlesh Datta",2012,Artificial Intelligence,
"  For the past few decades, man has been trying to create an intelligent
computer which can talk and respond like he can. The task of creating a system
that can talk like a human being is the primary objective of Automatic Speech
Recognition. Various Speech Recognition techniques have been developed in
theory and have been applied in practice. This paper discusses the problems
that have been encountered in developing Speech Recognition, the techniques
that have been applied to automate the task, and a representation of the core
problems of present day Speech Recognition by using Fuzzy Mathematics.
","Application of Fuzzy Mathematics to Speech-to-Text Conversion by
  Elimination of Paralinguistic Content","Sachin Lakra, T.V. Prasad, Deepak Kumar Sharma, Shree Harsh Atrey,
  Anubhav Kumar Sharma",2009,Artificial Intelligence,
"  Social data mining is an interesting phe-nomenon which colligates different
sources of social data to extract information. This information can be used in
relationship prediction, decision making, pat-tern recognition, social mapping,
responsibility distri-bution and many other applications. This paper presents a
systematical data mining architecture to mine intellectual knowledge from
social data. In this research, we use social networking site facebook as
primary data source. We collect different attributes such as about me,
comments, wall post and age from facebook as raw data and use advanced data
mining approaches to excavate intellectual knowledge. We also analyze our mined
knowledge with comparison for possible usages like as human behavior
prediction, pattern recognition, job responsibility distribution, decision
making and product promoting.
",Mining Social Data to Extract Intellectual Knowledge,Muhammad Mahbubur Rahman,2012,Artificial Intelligence,
"  A number of representation schemes have been presented for use within
Learning Classifier Systems, ranging from binary encodings to neural networks.
This paper presents results from an investigation into using a discrete
dynamical system representation within the XCS Learning Classifier System. In
particular, asynchronous random Boolean networks are used to represent the
traditional condition-action production system rules. It is shown possible to
use self-adaptive, open-ended evolution to design an ensemble of such discrete
dynamical systems within XCS to solve a number of well-known test problems.
",Discrete Dynamical Genetic Programming in XCS,Richard J. Preen and Larry Bull,2009,Artificial Intelligence,
"  A number of representation schemes have been presented for use within
Learning Classifier Systems, ranging from binary encodings to Neural Networks,
and more recently Dynamical Genetic Programming (DGP). This paper presents
results from an investigation into using a fuzzy DGP representation within the
XCSF Learning Classifier System. In particular, asynchronous Fuzzy Logic
Networks are used to represent the traditional condition-action production
system rules. It is shown possible to use self-adaptive, open-ended evolution
to design an ensemble of such fuzzy dynamical systems within XCSF to solve
several well-known continuous-valued test problems.
",Fuzzy Dynamical Genetic Programming in XCSF,Richard J. Preen and Larry Bull,2011,Artificial Intelligence,
"  The Information Artifact Ontology is an ontology in the domain of information
entities. Core to the definition of what it is to be an information entity is
the claim that an information entity must be `about' something, which is
encoded in an axiom expressing that all information entities are about some
entity. This axiom comes into conflict with ontological realism, since many
information entities seem to be about non-existing entities, such as
hypothetical molecules. We discuss this problem in the context of diagrams of
molecules, a kind of information entity pervasively used throughout
computational chemistry. We then propose a solution that recognizes that
information entities such as diagrams are expressions of diagrammatic
languages. In so doing, we not only address the problem of classifying diagrams
that seem to be about non-existing entities but also allow a more sophisticated
categorisation of information entities.
","What's in an `is about' link? Chemical diagrams and the Information
  Artifact Ontology","Janna Hastings and Colin Batchelor and Fabian Neuhaus and Christoph
  Steinbeck",2011,Artificial Intelligence,
"  We identify the presence of typically quantum effects, namely 'superposition'
and 'interference', in what happens when human concepts are combined, and
provide a quantum model in complex Hilbert space that represents faithfully
experimental data measuring the situation of combining concepts. Our model
shows how 'interference of concepts' explains the effects of underextension and
overextension when two concepts combine to the disjunction of these two
concepts. This result supports our earlier hypothesis that human thought has a
superposed two-layered structure, one layer consisting of 'classical logical
thought' and a superposed layer consisting of 'quantum conceptual thought'.
Possible connections with recent findings of a 'grid-structure' for the brain
are analyzed, and influences on the mind/brain relation, and consequences on
applied disciplines, such as artificial intelligence and quantum computation,
are considered.
",Quantum Interference in Cognition: Structural Aspects of the Brain,Diederik Aerts and Sandro Sozzo,2012,Artificial Intelligence,
"  In this paper we develop a fuzzy model for the description of the process of
Analogical Reasoning by representing its main steps as fuzzy subsets of a set
of linguistic labels characterizing the individuals' performance in each step
and we use the Shannon- Wiener diversity index as a measure of the individuals'
abilities in analogical problem solving. This model is compared with a
stochastic model presented in author's earlier papers by introducing a finite
Markov chain on the steps of the process of Analogical Reasoning. A classroom
experiment is also presented to illustrate the use of our results in practice.
",A Fuzzy Model for Analogical Problem Solving,Michael Gr. Voskoglou,2012,Artificial Intelligence,
"  Speech Segmentation is the process change point detection for partitioning an
input audio stream into regions each of which corresponds to only one audio
source or one speaker. One application of this system is in Speaker Diarization
systems. There are several methods for speaker segmentation; however, most of
the Speaker Diarization Systems use BIC-based Segmentation methods. The main
goal of this paper is to propose a new method for speaker segmentation with
higher speed than the current methods - e.g. BIC - and acceptable accuracy. Our
proposed method is based on the pitch frequency of the speech. The accuracy of
this method is similar to the accuracy of common speaker segmentation methods.
However, its computation cost is much less than theirs. We show that our method
is about 2.4 times faster than the BIC-based method, while the average accuracy
of pitch-based method is slightly higher than that of the BIC-based method.
","A Novel Method For Speech Segmentation Based On Speakers'
  Characteristics",Behrouz Abdolali and Hossein Sameti,2012,Artificial Intelligence,
"  Pertinence Feedback is a technique that enables a user to interactively
express his information requirement by modifying his original query formulation
with further information. This information is provided by explicitly confirming
the pertinent of some indicating objects and/or goals extracted by the system.
Obviously the user cannot mark objects and/or goals as pertinent until some are
extracted, so the first search has to be initiated by a query and the initial
query specification has to be good enough to pick out some pertinent objects
and/or goals from the Semantic Network. In this paper we present a short survey
of fuzzy and Semantic approaches to Knowledge Extraction. The goal of such
approaches is to define flexible Knowledge Extraction Systems able to deal with
the inherent vagueness and uncertainty of the Extraction process. It has long
been recognised that interactivity improves the effectiveness of Knowledge
Extraction systems. Novice user's queries are the most natural and interactive
medium of communication and recent progress in recognition is making it
possible to build systems that interact with the user. However, given the
typical novice user's queries submitted to Knowledge Extraction Systems, it is
easy to imagine that the effects of goal recognition errors in novice user's
queries must be severely destructive on the system's effectiveness. The
experimental work reported in this paper shows that the use of possibility
theory in classical Knowledge Extraction techniques for novice user's query
processing is more robust than the use of the probability theory. Moreover,
both possibilistic and probabilistic pertinence feedback can be effectively
employed to improve the effectiveness of novice user's query processing.
","Possibilistic Pertinence Feedback and Semantic Networks for Goal's
  Extraction",Mohamed Nazih Omri,2004,Artificial Intelligence,
"  We analyze different aspects of our quantum modeling approach of human
concepts, and more specifically focus on the quantum effects of contextuality,
interference, entanglement and emergence, illustrating how each of them makes
its appearance in specific situations of the dynamics of human concepts and
their combinations. We point out the relation of our approach, which is based
on an ontology of a concept as an entity in a state changing under influence of
a context, with the main traditional concept theories, i.e. prototype theory,
exemplar theory and theory theory. We ponder about the question why quantum
theory performs so well in its modeling of human concepts, and shed light on
this question by analyzing the role of complex amplitudes, showing how they
allow to describe interference in the statistics of measurement outcomes, while
in the traditional theories statistics of outcomes originates in classical
probability weights, without the possibility of interference. The relevance of
complex numbers, the appearance of entanglement, and the role of Fock space in
explaining contextual emergence, all as unique features of the quantum
modeling, are explicitly revealed in this paper by analyzing human concepts and
their dynamics.
","Concepts and Their Dynamics: A Quantum-Theoretic Modeling of Human
  Thought","Diederik Aerts, Liane Gabora and Sandro Sozzo",2013,Artificial Intelligence,
"  Medical diagnosis process vary in the degree to which they attempt to deal
with different complicating aspects of diagnosis such as relative importance of
symptoms, varied symptom pattern and the relation between diseases them selves.
Based on decision theory, in the past many mathematical models such as crisp
set, probability distribution, fuzzy set, intuitionistic fuzzy set were
developed to deal with complicating aspects of diagnosis. But, many such models
are failed to include important aspects of the expert decisions. Therefore, an
effort has been made to process inconsistencies in data being considered by
Pawlak with the introduction of rough set theory. Though rough set has major
advantages over the other methods, but it generates too many rules that create
many difficulties while taking decisions. Therefore, it is essential to
minimize the decision rules. In this paper, we use two processes such as pre
process and post process to mine suitable rules and to explore the relationship
among the attributes. In pre process we use rough set theory to mine suitable
rules, whereas in post process we use formal concept analysis from these
suitable rules to explore better knowledge and most important factors affecting
the decision making.
","A Framework for Intelligent Medical Diagnosis using Rough Set with
  Formal Concept Analysis","B.K.Tripathy, D.P.Acharjya and V.Cynthya",2011,Artificial Intelligence,
"  Research in the application of quantum structures to cognitive science
confirms that these structures quite systematically appear in the dynamics of
concepts and their combinations and quantum-based models faithfully represent
experimental data of situations where classical approaches are problematical.
In this paper, we analyze the data we collected in an experiment on a specific
conceptual combination, showing that Bell's inequalities are violated in the
experiment. We present a new refined entanglement scheme to model these data
within standard quantum theory rules, where 'entangled measurements and
entangled evolutions' occur, in addition to the expected 'entangled states',
and present a full quantum representation in complex Hilbert space of the data.
This stronger form of entanglement in measurements and evolutions might have
relevant applications in the foundations of quantum theory, as well as in the
interpretation of nonlocality tests. It could indeed explain some
non-negligible 'anomalies' identified in EPR-Bell experiments.
",Quantum Entanglement in Concept Combinations,Diederik Aerts and Sandro Sozzo,2014,Artificial Intelligence,
"  A new generalized multilinear regression model, termed the Higher-Order
Partial Least Squares (HOPLS), is introduced with the aim to predict a tensor
(multiway array) $\tensor{Y}$ from a tensor $\tensor{X}$ through projecting the
data onto the latent space and performing regression on the corresponding
latent variables. HOPLS differs substantially from other regression models in
that it explains the data by a sum of orthogonal Tucker tensors, while the
number of orthogonal loadings serves as a parameter to control model complexity
and prevent overfitting. The low dimensional latent space is optimized
sequentially via a deflation operation, yielding the best joint subspace
approximation for both $\tensor{X}$ and $\tensor{Y}$. Instead of decomposing
$\tensor{X}$ and $\tensor{Y}$ individually, higher order singular value
decomposition on a newly defined generalized cross-covariance tensor is
employed to optimize the orthogonal loadings. A systematic comparison on both
synthetic data and real-world decoding of 3D movement trajectories from
electrocorticogram (ECoG) signals demonstrate the advantages of HOPLS over the
existing methods in terms of better predictive ability, suitability to handle
small sample sizes, and robustness to noise.
","Higher-Order Partial Least Squares (HOPLS): A Generalized Multi-Linear
  Regression Method","Qibin Zhao, Cesar F. Caiafa, Danilo P. Mandic, Zenas C. Chao, Yasuo
  Nagasaka, Naotaka Fujii, Liqing Zhang and Andrzej Cichocki",2013,Artificial Intelligence,
"  Ontologies are key enablers for sharing precise and machine-understandable
semantics among different applications and parties. Yet, for ontologies to meet
these expectations, their quality must be of a good standard. The quality of an
ontology is strongly based on the design method employed. This paper addresses
the design problems related to the modelling of ontologies, with specific
concentration on the issues related to the quality of the conceptualisations
produced. The paper aims to demonstrate the impact of the modelling paradigm
adopted on the quality of ontological models and, consequently, the potential
impact that such a decision can have in relation to the development of software
applications. To this aim, an ontology that is conceptualised based on the
Object-Role Modelling (ORM) approach (a representative of endurantism) is
re-engineered into a one modelled on the basis of the Object Paradigm (OP) (a
representative of perdurantism). Next, the two ontologies are analytically
compared using the specified criteria. The conducted comparison highlights that
using the OP for ontology conceptualisation can provide more expressive,
reusable, objective and temporal ontologies than those conceptualised on the
basis of the ORM approach.
","Conceptual Modelling and The Quality of Ontologies: Endurantism Vs.
  Perdurantism","Mutaz M. Al-Debei, Mohammad Mourhaf Al Asswad, Sergio de Cesare and
  Mark Lycett",2012,Artificial Intelligence,
"  In this paper, we present a formalization of an algorithm to construct
admissible discrete vector fields in the Coq theorem prover taking advantage of
the SSReflect library. Discrete vector fields are a tool which has been
welcomed in the homological analysis of digital images since it provides a
procedure to reduce the amount of information but preserving the homological
properties. In particular, thanks to discrete vector fields, we are able to
compute, inside Coq, homological properties of biomedical images which
otherwise are out of the reach of this system.
","Verifying an algorithm computing Discrete Vector Fields for digital
  imaging","J\'onathan Heras, Mar\'ia Poza, and Julio Rubio",2012,Artificial Intelligence,
"  Understanding the structure and dynamics of biological networks is one of the
important challenges in system biology. In addition, increasing amount of
experimental data in biological networks necessitate the use of efficient
methods to analyze these huge amounts of data. Such methods require to
recognize common patterns to analyze data. As biological networks can be
modeled by graphs, the problem of common patterns recognition is equivalent
with frequent sub graph mining in a set of graphs. In this paper, at first the
challenges of frequent subgrpahs mining in biological networks are introduced
and the existing approaches are classified for each challenge. then the
algorithms are analyzed on the basis of the type of the approach they apply for
each of the challenges.
","Classification of Approaches and Challenges of Frequent Subgraphs Mining
  in Biological Networks",Mohammadreza Keyvanpour and Fereshteh Azizani,2011,Artificial Intelligence,
"  The behavior composition problem involves automatically building a controller
that is able to realize a desired, but unavailable, target system (e.g., a
house surveillance) by suitably coordinating a set of available components
(e.g., video cameras, blinds, lamps, a vacuum cleaner, phones, etc.) Previous
work has almost exclusively aimed at bringing about the desired component in
its totality, which is highly unsatisfactory for unsolvable problems. In this
work, we develop an approach for approximate behavior composition without
departing from the classical setting, thus making the problem applicable to a
much wider range of cases. Based on the notion of simulation, we characterize
what a maximal controller and the ""closest"" implementable target module
(optimal approximation) are, and show how these can be computed using ATL model
checking technology for a special case. We show the uniqueness of optimal
approximations, and prove their soundness and completeness with respect to
their imported controllers.
",Qualitative Approximate Behavior Composition,Nitin Yadav and Sebastian Sardina,2012,Artificial Intelligence,
"  We propose a variant of Alternating-time Temporal Logic (ATL) grounded in the
agents' operational know-how, as defined by their libraries of abstract plans.
Inspired by ATLES, a variant itself of ATL, it is possible in our logic to
explicitly refer to ""rational"" strategies for agents developed under the
Belief-Desire-Intention agent programming paradigm. This allows us to express
and verify properties of BDI systems using ATL-type logical frameworks.
",Reasoning about Agent Programs using ATL-like Logics,Nitin Yadav and Sebastian Sardina,2012,Artificial Intelligence,
"  Nurse rostering is a complex scheduling problem that affects hospital
personnel on a daily basis all over the world. This paper presents a new
component-based approach with evolutionary eliminations, for a nurse scheduling
problem arising at a major UK hospital. The main idea behind this technique is
to decompose a schedule into its components (i.e. the allocated shift pattern
of each nurse), and then to implement two evolutionary elimination strategies
mimicking natural selection and natural mutation process on these components
respectively to iteratively deliver better schedules. The worthiness of all
components in the schedule has to be continuously demonstrated in order for
them to remain there. This demonstration employs an evaluation function which
evaluates how well each component contributes towards the final objective. Two
elimination steps are then applied: the first elimination eliminates a number
of components that are deemed not worthy to stay in the current schedule; the
second elimination may also throw out, with a low level of probability, some
worthy components. The eliminated components are replenished with new ones
using a set of constructive heuristics using local optimality criteria.
Computational results using 52 data instances demonstrate the applicability of
the proposed approach in solving real-world problems.
",A Component Based Heuristic Search Method with Evolutionary Eliminations,"Jingpeng Li, Uwe Aickelin, Edmund Burke",2008,Artificial Intelligence,
"  The quest for robust heuristics that are able to solve more than one problem
is ongoing. In this paper, we present, discuss and analyse a technique called
Evolutionary Squeaky Wheel Optimisation and apply it to two different personnel
scheduling problems. Evolutionary Squeaky Wheel Optimisation improves the
original Squeaky Wheel Optimisation's effectiveness and execution speed by
incorporating two extra steps (Selection and Mutation) for added evolution. In
the Evolutionary Squeaky Wheel Optimisation, a cycle of
Analysis-Selection-Mutation-Prioritization-Construction continues until
stopping conditions are reached. The aim of the Analysis step is to identify
below average solution components by calculating a fitness value for all
components. The Selection step then chooses amongst these underperformers and
discards some probabilistically based on fitness. The Mutation step further
discards a few components at random. Solutions can become incomplete and thus
repairs may be required. The repairs are carried out by using the
Prioritization to first produce priorities that determine an order by which the
following Construction step then schedules the remaining components. Therefore,
improvement in the Evolutionary Squeaky Wheel Optimisation is achieved by
selective solution disruption mixed with interative improvement and
constructive repair. Strong experimental results are reported on two different
domains of personnel scheduling: bus and rail driver scheduling and hospital
nurse scheduling.
","An Evolutionary Squeaky Wheel Optimisation Approach to Personnel
  Scheduling","Uwe Aickelin, Jingpeng Li, Edmund Burke",2009,Artificial Intelligence,
"  A combined Short-Term Learning (STL) and Long-Term Learning (LTL) approach to
solving mobile robot navigation problems is presented and tested in both real
and simulated environments. The LTL consists of rapid simulations that use a
Genetic Algorithm to derive diverse sets of behaviours. These sets are then
transferred to an idiotypic Artificial Immune System (AIS), which forms the STL
phase, and the system is said to be seeded. The combined LTL-STL approach is
compared with using STL only, and with using a handdesigned controller. In
addition, the STL phase is tested when the idiotypic mechanism is turned off.
The results provide substantial evidence that the best option is the seeded
idiotypic system, i.e. the architecture that merges LTL with an idiotypic AIS
for the STL. They also show that structurally different environments can be
used for the two phases without compromising transferability
","An Idiotypic Immune Network as a Short Term Learning Architecture for
  Mobile Robots","Amanda Whitbrook, Uwe Aickelin, Jonathan M Garibaldi",2008,Artificial Intelligence,
"  The immune system provides a rich metaphor for computer security: anomaly
detection that works in nature should work for machines. However, early
artificial immune system approaches for computer security had only limited
success. Arguably, this was due to these artificial systems being based on too
simplistic a view of the immune system. We present here a second generation
artificial immune system for process anomaly detection. It improves on earlier
systems by having different artificial cell types that process information.
Following detailed information about how to build such second generation
systems, we find that communication between cells types is key to performance.
Through realistic testing and validation we show that second generation
artificial immune systems are capable of anomaly detection beyond generic
system policies. The paper concludes with a discussion and outline of the next
steps in this exciting area of computer security.
",An Immune Inspired Approach to Anomaly Detection,"Jamie Twycross, Uwe Aickelin",2008,Artificial Intelligence,
"  Motivated by Zadeh's paradigm of computing with words rather than numbers,
several formal models of computing with words have recently been proposed.
These models are based on automata and thus are not well-suited for concurrent
computing. In this paper, we incorporate the well-known model of concurrent
computing, Petri nets, together with fuzzy set theory and thereby establish a
concurrency model of computing with words--fuzzy Petri nets for computing with
words (FPNCWs). The new feature of such fuzzy Petri nets is that the labels of
transitions are some special words modeled by fuzzy sets. By employing the
methodology of fuzzy reasoning, we give a faithful extension of an FPNCW which
makes it possible for computing with more words. The language expressiveness of
the two formal models of computing with words, fuzzy automata for computing
with words and FPNCWs, is compared as well. A few small examples are provided
to illustrate the theoretical development.
",A Fuzzy Petri Nets Model for Computing With Words,Yongzhi Cao and Guoqing Chen,2010,Artificial Intelligence,
"  The biological immune system is a robust, complex, adaptive system that
defends the body from foreign pathogens. It is able to categorize all cells (or
molecules) within the body as self-cells or non-self cells. It does this with
the help of a distributed task force that has the intelligence to take action
from a local and also a global perspective using its network of chemical
messengers for communication. There are two major branches of the immune
system. The innate immune system is an unchanging mechanism that detects and
destroys certain invading organisms, whilst the adaptive immune system responds
to previously unknown foreign cells and builds a response to them that can
remain in the body over a long period of time. This remarkable information
processing biological system has caught the attention of computer science in
recent years. A novel computational intelligence technique, inspired by
immunology, has emerged, called Artificial Immune Systems. Several concepts
from the immune have been extracted and applied for solution to real world
science and engineering problems. In this tutorial, we briefly describe the
immune system metaphors that are relevant to existing Artificial Immune Systems
methods. We will then show illustrative real-world problems suitable for
Artificial Immune Systems and give a step-by-step algorithm walkthrough for one
such problem. A comparison of the Artificial Immune Systems to other well-known
algorithms, areas for future work, tips & tricks and a list of resources will
round this tutorial off. It should be noted that as Artificial Immune Systems
is still a young and evolving field, there is not yet a fixed algorithm
template and hence actual implementations might differ somewhat from time to
time and from those examples given here.
",Artificial Immune Systems,"Uwe Aickelin, Dipankar Dasgupta",2005,Artificial Intelligence,
"  The Dendritic Cell algorithm (DCA) is inspired by recent work in innate
immunity. In this paper a formal description of the DCA is given. The DCA is
described in detail, and its use as an anomaly detector is illustrated within
the context of computer security. A port scan detection task is performed to
substantiate the influence of signal selection on the behaviour of the
algorithm. Experimental results provide a comparison of differing input signal
mappings.
",Articulation and Clarification of the Dendritic Cell Algorithm,"Julie Greensmith, Uwe Aickelin, Jamie Twycross",2006,Artificial Intelligence,
"  Explaining adaptive behavior is a central problem in artificial intelligence
research. Here we formalize adaptive agents as mixture distributions over
sequences of inputs and outputs (I/O). Each distribution of the mixture
constitutes a `possible world', but the agent does not know which of the
possible worlds it is actually facing. The problem is to adapt the I/O stream
in a way that is compatible with the true world. A natural measure of
adaptation can be obtained by the Kullback-Leibler (KL) divergence between the
I/O distribution of the true world and the I/O distribution expected by the
agent that is uncertain about possible worlds. In the case of pure input
streams, the Bayesian mixture provides a well-known solution for this problem.
We show, however, that in the case of I/O streams this solution breaks down,
because outputs are issued by the agent itself and require a different
probabilistic syntax as provided by intervention calculus. Based on this
calculus, we obtain a Bayesian control rule that allows modeling adaptive
behavior with mixture distributions over I/O streams. This rule might allow for
a novel approach to adaptive control based on a minimum KL-principle.
",A Bayesian Rule for Adaptive Control based on Causal Interventions,"Pedro A. Ortega, Daniel A. Braun",2010,Artificial Intelligence,
"  Rewards typically express desirabilities or preferences over a set of
alternatives. Here we propose that rewards can be defined for any probability
distribution based on three desiderata, namely that rewards should be
real-valued, additive and order-preserving, where the latter implies that more
probable events should also be more desirable. Our main result states that
rewards are then uniquely determined by the negative information content. To
analyze stochastic processes, we define the utility of a realization as its
reward rate. Under this interpretation, we show that the expected utility of a
stochastic process is its negative entropy rate. Furthermore, we apply our
results to analyze agent-environment interactions. We show that the expected
utility that will actually be achieved by the agent is given by the negative
cross-entropy from the input-output (I/O) distribution of the coupled
interaction system and the agent's I/O distribution. Thus, our results allow
for an information-theoretic interpretation of the notion of utility and the
characterization of agent-environment interactions in terms of entropy
dynamics.
",A conversion between utility and information,"Pedro A. Ortega, Daniel A. Braun",2010,Artificial Intelligence,
"  Rough set theory, a mathematical tool to deal with inexact or uncertain
knowledge in information systems, has originally described the indiscernibility
of elements by equivalence relations. Covering rough sets are a natural
extension of classical rough sets by relaxing the partitions arising from
equivalence relations to coverings. Recently, some topological concepts such as
neighborhood have been applied to covering rough sets. In this paper, we
further investigate the covering rough sets based on neighborhoods by
approximation operations. We show that the upper approximation based on
neighborhoods can be defined equivalently without using neighborhoods. To
analyze the coverings themselves, we introduce unary and composition operations
on coverings. A notion of homomorphismis provided to relate two covering
approximation spaces. We also examine the properties of approximations
preserved by the operations and homomorphisms, respectively.
","Covering rough sets based on neighborhoods: An approach without using
  neighborhoods",Ping Zhu,2011,Artificial Intelligence,
"  In Pawlak's rough set theory, a set is approximated by a pair of lower and
upper approximations. To measure numerically the roughness of an approximation,
Pawlak introduced a quantitative measure of roughness by using the ratio of the
cardinalities of the lower and upper approximations. Although the roughness
measure is effective, it has the drawback of not being strictly monotonic with
respect to the standard ordering on partitions. Recently, some improvements
have been made by taking into account the granularity of partitions. In this
paper, we approach the roughness measure in an axiomatic way. After
axiomatically defining roughness measure and partition measure, we provide a
unified construction of roughness measure, called strong Pawlak roughness
measure, and then explore the properties of this measure. We show that the
improved roughness measures in the literature are special instances of our
strong Pawlak roughness measure and introduce three more strong Pawlak
roughness measures as well. The advantage of our axiomatic approach is that
some properties of a roughness measure follow immediately as soon as the
measure satisfies the relevant axiomatic definition.
",An axiomatic approach to the roughness measure of rough sets,Ping Zhu,2011,Artificial Intelligence,
"  Identification of critical or weak buses for a given operating condition is
an important task in the load dispatch centre. It has become more vital in view
of the threat of voltage instability leading to voltage collapse. This paper
presents a fuzzy approach for ranking critical buses in a power system under
normal and network contingencies based on Line Flow index and voltage profiles
at load buses. The Line Flow index determines the maximum load that is possible
to be connected to a bus in order to maintain stability before the system
reaches its bifurcation point. Line Flow index (LF index) along with voltage
profiles at the load buses are represented in Fuzzy Set notation. Further they
are evaluated using fuzzy rules to compute Criticality Index. Based on this
index, critical buses are ranked. The bus with highest rank is the weakest bus
as it can withstand a small amount of load before causing voltage collapse. The
proposed method is tested on Five Bus Test System.
","Fuzzy Approach to Critical Bus Ranking under Normal and Line Outage
  Contingencies","Shobha Shankar, Dr. T. Ananthapadmanabha",2011,Artificial Intelligence,
"  Signature used as a biometric is implemented in various systems as well as
every signature signed by each person is distinct at the same time. So, it is
very important to have a computerized signature verification system. In offline
signature verification system dynamic features are not available obviously, but
one can use a signature as an image and apply image processing techniques to
make an effective offline signature verification system. Author proposes a
intelligent network used directional feature and energy density both as inputs
to the same network and classifies the signature. Neural network is used as a
classifier for this system. The results are compared with both the very basic
energy density method and a simple directional feature method of offline
signature verification system and this proposed new network is found very
effective as compared to the above two methods, specially for less number of
training samples, which can be implemented practically.
","A Directional Feature with Energy based Offline Signature Verification
  Network",Minal Tomar and Pratibha Singh,2011,Artificial Intelligence,
"  Nowadays, the huge amount of information distributed through the Web
motivates studying techniques to be adopted in order to extract relevant data
in an efficient and reliable way. Both academia and enterprises developed
several approaches of Web data extraction, for example using techniques of
artificial intelligence or machine learning. Some commonly adopted procedures,
namely wrappers, ensure a high degree of precision of information extracted
from Web pages, and, at the same time, have to prove robustness in order not to
compromise quality and reliability of data themselves. In this paper we focus
on some experimental aspects related to the robustness of the data extraction
process and the possibility of automatically adapting wrappers. We discuss the
implementation of algorithms for finding similarities between two different
version of a Web page, in order to handle modifications, avoiding the failure
of data extraction tasks and ensuring reliability of information extracted. Our
purpose is to evaluate performances, advantages and draw-backs of our novel
system of automatic wrapper adaptation.
",Design of Automatically Adaptable Web Wrappers,Emilio Ferrara and Robert Baumgartner,2011,Artificial Intelligence,
"  The constraint satisfaction problem (CSP) is a general problem central to
computer science and artificial intelligence. Although the CSP is NP-hard in
general, considerable effort has been spent on identifying tractable
subclasses. The main two approaches consider structural properties
(restrictions on the hypergraph of constraint scopes) and relational properties
(restrictions on the language of constraint relations). Recently, some authors
have considered hybrid properties that restrict the constraint hypergraph and
the relations simultaneously.
  Our key contribution is the novel concept of a CSP pattern and classes of
problems defined by forbidden patterns (which can be viewed as forbidding
generic subproblems). We describe the theoretical framework which can be used
to reason about classes of problems defined by forbidden patterns. We show that
this framework generalises relational properties and allows us to capture known
hybrid tractable classes.
  Although we are not close to obtaining a dichotomy concerning the
tractability of general forbidden patterns, we are able to make some progress
in a special case: classes of problems that arise when we can only forbid
binary negative patterns (generic subproblems in which only inconsistent tuples
are specified). In this case we are able to characterise very large classes of
tractable and NP-hard forbidden patterns. This leaves the complexity of just
one case unresolved and we conjecture that this last case is tractable.
",The tractability of CSP classes defined by forbidden patterns,"David A. Cohen, Martin C. Cooper, P\'aid\'i Creed, Andr\'as Z. Salamon",2012,Artificial Intelligence,
"  Some recent works in conditional planning have proposed reachability
heuristics to improve planner scalability, but many lack a formal description
of the properties of their distance estimates. To place previous work in
context and extend work on heuristics for conditional planning, we provide a
formal basis for distance estimates between belief states. We give a definition
for the distance between belief states that relies on aggregating underlying
state distance measures. We give several techniques to aggregate state
distances and their associated properties. Many existing heuristics exhibit a
subset of the properties, but in order to provide a standardized comparison we
present several generalizations of planning graph heuristics that are used in a
single planner. We compliment our belief state distance estimate framework by
also investigating efficient planning graph data structures that incorporate
BDDs to compute the most effective heuristics.
  We developed two planners to serve as test-beds for our investigation. The
first, CAltAlt, is a conformant regression planner that uses A* search. The
second, POND, is a conditional progression planner that uses AO* search. We
show the relative effectiveness of our heuristic techniques within these
planners. We also compare the performance of these planners with several state
of the art approaches in conditional planning.
",Planning Graph Heuristics for Belief Space Search,"D. Bryce, S. Kambhampati, D. E. Smith",2006,Artificial Intelligence,
"  We show that several important resource allocation problems in wireless
networks fit within the common framework of Constraint Satisfaction Problems
(CSPs). Inspired by the requirements of these applications, where variables are
located at distinct network devices that may not be able to communicate but may
interfere, we define natural criteria that a CSP solver must possess in order
to be practical. We term these algorithms decentralized CSP solvers. The best
known CSP solvers were designed for centralized problems and do not meet these
criteria. We introduce a stochastic decentralized CSP solver and prove that it
will find a solution in almost surely finite time, should one exist, also
showing it has many practically desirable properties. We benchmark the
algorithm's performance on a well-studied class of CSPs, random k-SAT,
illustrating that the time the algorithm takes to find a satisfying assignment
is competitive with stochastic centralized solvers on problems with order a
thousand variables despite its decentralized nature. We demonstrate the
solver's practical utility for the problems that motivated its introduction by
using it to find a non-interfering channel allocation for a network formed from
data from downtown Manhattan.
",Decentralized Constraint Satisfaction,K. R. Duffy and C. Bordenave and D. J. Leith,2013,Artificial Intelligence,
"  this paper presents an enhancement of the medial axis algorithm to be used
for finding the optimal shortest path for developed cognitive map. The
cognitive map has been developed, based on the architectural blueprint maps.
The idea for using the medial-axis is to find main path central pixels; each
center pixel represents the center distance between two side boarder pixels.
The need for these pixels in the algorithm comes from the need of building a
network of nodes for the path, where each node represents a turning in the real
world (left, right, critical left, critical right...). The algorithm also
ignores from finding the center pixels paths that are too small for intelligent
robot navigation. The Idea of this algorithm is to find the possible shortest
path between start and end points. The goal of this research is to extract a
simple, robust representation of the shape of the cognitive map together with
the optimal shortest path between start and end points. The intelligent robot
will use this algorithm in order to decrease the time that is needed for
sweeping the targeted building.
",Finding Shortest Path for Developed Cognitive Map Using Medial Axis,"Hazim A. Farhan, Hussein H. Owaied, Suhaib I. Al-Ghazi",2011,Artificial Intelligence,
"  A definition of intelligence is given in terms of performance that can be
quantitatively measured. In this study, we have presented a conceptual model of
Intelligent Agent System for Automatic Vehicle Checking Agent (VCA). To achieve
this goal, we have introduced several kinds of agents that exhibit intelligent
features. These are the Management agent, internal agent, External Agent,
Watcher agent and Report agent. Metrics and measurements are suggested for
evaluating the performance of Automatic Vehicle Checking Agent (VCA). Calibrate
data and test facilities are suggested to facilitate the development of
intelligent systems.
",Automatic Vehicle Checking Agent (VCA),"Bashir Ahmad, Shakeel Ahmad, Shahid Hussain, Muhammad Zaheer Aslam and
  Zafar Abbas",2011,Artificial Intelligence,
"  This paper presents the design and development of a proposed rule based
Decision Support System that will help students in selecting the best suitable
faculty/major decision while taking admission in Gomal University, Dera Ismail
Khan, Pakistan. The basic idea of our approach is to design a model for testing
and measuring the student capabilities like intelligence, understanding,
comprehension, mathematical concepts plus his/her past academic record plus
his/her intelligence level, and applying the module results to a rule-based
decision support system to determine the compatibility of those capabilities
with the available faculties/majors in Gomal University. The result is shown as
a list of suggested faculties/majors with the student capabilities and
abilities.
","A Proposed Decision Support System/Expert System for Guiding Fresh
  Students in Selecting a Faculty in Gomal University, Pakistan","Muhammad Zaheer Aslam, Nasimullah, Abdur Rashid Khan",2011,Artificial Intelligence,
"  Heuristics are crucial tools in decreasing search effort in varied fields of
AI. In order to be effective, a heuristic must be efficient to compute, as well
as provide useful information to the search algorithm. However, some well-known
heuristics which do well in reducing backtracking are so heavy that the gain of
deploying them in a search algorithm might be outweighed by their overhead.
  We propose a rational metareasoning approach to decide when to deploy
heuristics, using CSP backtracking search as a case study. In particular, a
value of information approach is taken to adaptive deployment of solution-count
estimation heuristics for value ordering. Empirical results show that indeed
the proposed mechanism successfully balances the tradeoff between decreasing
backtracking and heuristic computational overhead, resulting in a significant
overall search time reduction.
",Rational Deployment of CSP Heuristics,"David Tolpin, Solomon Eyal Shimony",2011,Artificial Intelligence,
"  We explore self-organizing strategies for role assignment in a foraging task
carried out by a colony of artificial agents. Our strategies are inspired by
various mechanisms of division of labor (polyethism) observed in eusocial
insects like ants, termites, or bees. Specifically we instantiate models of
caste polyethism and age or temporal polyethism to evaluated the benefits to
foraging in a dynamic environment. Our experiment is directly related to the
exploration/exploitation trade of in machine learning.
",Polyethism in a colony of artificial ants,Chris Marriott and Carlos Gershenson,2011,Artificial Intelligence,
"  Experiments in cognitive science and decision theory show that the ways in
which people combine concepts and make decisions cannot be described by
classical logic and probability theory. This has serious implications for
applied disciplines such as information retrieval, artificial intelligence and
robotics. Inspired by a mathematical formalism that generalizes quantum
mechanics the authors have constructed a contextual framework for both concept
representation and decision making, together with quantum models that are in
strong alignment with experimental data. The results can be interpreted by
assuming the existence in human thought of a double-layered structure, a
'classical logical thought' and a 'quantum conceptual thought', the latter
being responsible of the above paradoxes and nonclassical effects. The presence
of a quantum structure in cognition is relevant, for it shows that quantum
mechanics provides not only a useful modeling tool for experimental data but
also supplies a structural model for human and artificial thought processes.
This approach has strong connections with theories formalizing meaning, such as
semantic analysis, and has also a deep impact on computer science, information
retrieval and artificial intelligence. More specifically, the links with
information retrieval are discussed in this paper.
",Quantum Structure in Cognition: Fundamentals and Applications,"Diederik Aerts, Liane Gabora, Sandro Sozzo and Tomas Veloz",2011,Artificial Intelligence,
"  The mathematical formalism of quantum mechanics has been successfully
employed in the last years to model situations in which the use of classical
structures gives rise to problematical situations, and where typically quantum
effects, such as 'contextuality' and 'entanglement', have been recognized. This
'Quantum Interaction Approach' is briefly reviewed in this paper focusing, in
particular, on the quantum models that have been elaborated to describe how
concepts combine in cognitive science, and on the ensuing identification of a
quantum structure in human thought. We point out that these results provide
interesting insights toward the development of a unified theory for meaning and
knowledge formalization and representation. Then, we analyze the technological
aspects and implications of our approach, and a particular attention is devoted
to the connections with symbolic artificial intelligence, quantum computation
and robotics.
","Quantum Interaction Approach in Cognition, Artificial Intelligence and
  Robotics","Diederik Aerts, Marek Czachor and Sandro Sozzo",2011,Artificial Intelligence,
"  The Semantic Web is an extension of the current web in which information is
given well-defined meaning. The perspective of Semantic Web is to promote the
quality and intelligence of the current web by changing its contents into
machine understandable form. Therefore, semantic level information is one of
the cornerstones of the Semantic Web. The process of adding semantic metadata
to web resources is called Semantic Annotation. There are many obstacles
against the Semantic Annotation, such as multilinguality, scalability, and
issues which are related to diversity and inconsistency in content of different
web pages. Due to the wide range of domains and the dynamic environments that
the Semantic Annotation systems must be performed on, the problem of automating
annotation process is one of the significant challenges in this domain. To
overcome this problem, different machine learning approaches such as supervised
learning, unsupervised learning and more recent ones like, semi-supervised
learning and active learning have been utilized. In this paper we present an
inclusive layered classification of Semantic Annotation challenges and discuss
the most important issues in this field. Also, we review and analyze machine
learning applications for solving semantic annotation problems. For this goal,
the article tries to closely study and categorize related researches for better
understanding and to reach a framework that can map machine learning techniques
into the Semantic Annotation challenges and requirements.
","A Machine Learning Based Analytical Framework for Semantic Annotation
  Requirements",Hamed Hassanzadeh and MohammadReza Keyvanpour,2011,Artificial Intelligence,
"  One of the key challenges in electronic government (e-government) is the
development of systems that can be easily integrated and interoperated to
provide seamless services delivery to citizens. In recent years, Semantic Web
technologies based on ontology have emerged as promising solutions to the above
engineering problems. However, current research practicing semantic development
in e-government does not focus on the application of available methodologies
and platforms for developing government domain ontologies. Furthermore, only a
few of these researches provide detailed guidelines for developing semantic
ontology models from a government service domain. This research presents a case
study combining an ontology building methodology and two state-of-the-art
Semantic Web platforms namely Protege and Java Jena ontology API for semantic
ontology development in e-government. Firstly, a framework adopted from the
Uschold and King ontology building methodology is employed to build a domain
ontology describing the semantic content of a government service domain.
Thereafter, UML is used to semi-formally represent the domain ontology.
Finally, Protege and Jena API are employed to create the Web Ontology Language
(OWL) and Resource Description Framework (RDF) representations of the domain
ontology respectively to enable its computer processing. The study aims at: (1)
providing e-government developers, particularly those from the developing world
with detailed guidelines for practicing semantic content development in their
e-government projects and (2), strengthening the adoption of semantic
technologies in e-government. The study would also be of interest to novice
Semantic Web developers who might used it as a starting point for further
investigations.
","Combining Ontology Development Methodologies and Semantic Web Platforms
  for E-government Domain Ontology Development",Jean Vincent Fonou Dombeu and Magda Huisman,2011,Artificial Intelligence,
"  This paper presents a novel pairwise constraint propagation approach by
decomposing the challenging constraint propagation problem into a set of
independent semi-supervised learning subproblems which can be solved in
quadratic time using label propagation based on k-nearest neighbor graphs.
Considering that this time cost is proportional to the number of all possible
pairwise constraints, our approach actually provides an efficient solution for
exhaustively propagating pairwise constraints throughout the entire dataset.
The resulting exhaustive set of propagated pairwise constraints are further
used to adjust the similarity matrix for constrained spectral clustering. Other
than the traditional constraint propagation on single-source data, our approach
is also extended to more challenging constraint propagation on multi-source
data where each pairwise constraint is defined over a pair of data points from
different sources. This multi-source constraint propagation has an important
application to cross-modal multimedia retrieval. Extensive results have shown
the superior performance of our approach.
","Exhaustive and Efficient Constraint Propagation: A Semi-Supervised
  Learning Perspective and Its Applications","Zhiwu Lu, Horace H.S. Ip, Yuxin Peng",2012,Artificial Intelligence,
"  We provide an overview of the organization and results of the deterministic
part of the 4th International Planning Competition, i.e., of the part concerned
with evaluating systems doing deterministic planning. IPC-4 attracted even more
competing systems than its already large predecessors, and the competition
event was revised in several important respects. After giving an introduction
to the IPC, we briefly explain the main differences between the deterministic
part of IPC-4 and its predecessors. We then introduce formally the language
used, called PDDL2.2 that extends PDDL2.1 by derived predicates and timed
initial literals. We list the competing systems and overview the results of the
competition. The entire set of data is far too large to be presented in full.
We provide a detailed summary; the complete data is available in an online
appendix. We explain how we awarded the competition prizes.
",The Deterministic Part of IPC-4: An Overview,"S. Edelkamp, J. Hoffmann",2005,Artificial Intelligence,
"  PDDL2.1 was designed to push the envelope of what planning algorithms can do,
and it has succeeded. It adds two important features: durative actions,which
take time (and may have continuous effects); and objective functions for
measuring the quality of plans. The concept of durative actions is flawed; and
the treatment of their semantics reveals too strong an attachment to the way
many contemporary planners work. Future PDDL innovators should focus on
producing a clean semantics for additions to the language, and let planner
implementers worry about coupling their algorithms to problems expressed in the
latest version of the language.
",PDDL2.1 - The Art of the Possible? Commentary on Fox and Long,D. McDermott,2003,Artificial Intelligence,
"  The addition of durative actions to PDDL2.1 sparked some controversy. Fox and
Long argued that actions should be considered as instantaneous, but can start
and stop processes. Ultimately, a limited notion of durative actions was
incorporated into the language. I argue that this notion is still impoverished,
and that the underlying philosophical position of regarding durative actions as
being a shorthand for a start action, process, and stop action ignores the
realities of modelling and execution for complex systems.
",The Case for Durative Actions: A Commentary on PDDL2.1,D. E. Smith,2003,Artificial Intelligence,
"  We present a partial-order, conformant, probabilistic planner, Probapop which
competed in the blind track of the Probabilistic Planning Competition in IPC-4.
We explain how we adapt distance based heuristics for use with probabilistic
domains. Probapop also incorporates heuristics based on probability of success.
We explain the successes and difficulties encountered during the design and
implementation of Probapop.
",Engineering a Conformant Probabilistic Planner,"L. Li, N. Onder, G. C. Whelan",2006,Artificial Intelligence,
"  Between 1998 and 2004, the planning community has seen vast progress in terms
of the sizes of benchmark examples that domain-independent planners can tackle
successfully. The key technique behind this progress is the use of heuristic
functions based on relaxing the planning task at hand, where the relaxation is
to assume that all delete lists are empty. The unprecedented success of such
methods, in many commonly used benchmark examples, calls for an understanding
of what classes of domains these methods are well suited for. In the
investigation at hand, we derive a formal background to such an understanding.
We perform a case study covering a range of 30 commonly used STRIPS and ADL
benchmark domains, including all examples used in the first four international
planning competitions. We *prove* connections between domain structure and
local search topology -- heuristic cost surface properties -- under an
idealized version of the heuristic functions used in modern planners. The
idealized heuristic function is called h^+, and differs from the practically
used functions in that it returns the length of an *optimal* relaxed plan,
which is NP-hard to compute. We identify several key characteristics of the
topology under h^+, concerning the existence/non-existence of unrecognized dead
ends, as well as the existence/non-existence of constant upper bounds on the
difficulty of escaping local minima and benches. These distinctions divide the
(set of all) planning domains into a taxonomy of classes of varying h^+
topology. As it turns out, many of the 30 investigated domains lie in classes
with a relatively easy topology. Most particularly, 12 of the domains lie in
classes where FFs search algorithm, provided with h^+, is a polynomial solving
mechanism. We also present results relating h^+ to its approximation as
implemented in FF. The behavior regarding dead ends is provably the same. We
summarize the results of an empirical investigation showing that, in many
domains, the topological qualities of h^+ are largely inherited by the
approximation. The overall investigation gives a rare example of a successful
analysis of the connections between typical-case problem structure, and search
performance. The theoretical investigation also gives hints on how the
topological phenomena might be automatically recognizable by domain analysis
techniques. We outline some preliminary steps we made into that direction.
","Where 'Ignoring Delete Lists' Works: Local Search Topology in Planning
  Benchmarks",J. Hoffmann,2005,Artificial Intelligence,
"  A non-binary Constraint Satisfaction Problem (CSP) can be solved directly
using extended versions of binary techniques. Alternatively, the non-binary
problem can be translated into an equivalent binary one. In this case, it is
generally accepted that the translated problem can be solved by applying
well-established techniques for binary CSPs. In this paper we evaluate the
applicability of the latter approach. We demonstrate that the use of standard
techniques for binary CSPs in the encodings of non-binary problems is
problematic and results in models that are very rarely competitive with the
non-binary representation. To overcome this, we propose specialized arc
consistency and search algorithms for binary encodings, and we evaluate them
theoretically and empirically. We consider three binary representations; the
hidden variable encoding, the dual encoding, and the double encoding.
Theoretical and empirical results show that, for certain classes of non-binary
constraints, binary encodings are a competitive option, and in many cases, a
better one than the non-binary representation.
","Binary Encodings of Non-binary Constraint Satisfaction Problems:
  Algorithms and Experimental Results","N. Samaras, K. Stergiou",2005,Artificial Intelligence,
"  In a peer-to-peer inference system, each peer can reason locally but can also
solicit some of its acquaintances, which are peers sharing part of its
vocabulary. In this paper, we consider peer-to-peer inference systems in which
the local theory of each peer is a set of propositional clauses defined upon a
local vocabulary. An important characteristic of peer-to-peer inference systems
is that the global theory (the union of all peer theories) is not known (as
opposed to partition-based reasoning systems). The main contribution of this
paper is to provide the first consequence finding algorithm in a peer-to-peer
setting: DeCA. It is anytime and computes consequences gradually from the
solicited peer to peers that are more and more distant. We exhibit a sufficient
condition on the acquaintance graph of the peer-to-peer inference system for
guaranteeing the completeness of this algorithm. Another important contribution
is to apply this general distributed reasoning setting to the setting of the
Semantic Web through the Somewhere semantic peer-to-peer data management
system. The last contribution of this paper is to provide an experimental
analysis of the scalability of the peer-to-peer infrastructure that we propose,
on large networks of 1000 peers.
","Distributed Reasoning in a Peer-to-Peer Setting: Application to the
  Semantic Web","P. Adjiman, P. Chatalic, F. Goasdoue, M. C. Rousset, L. Simon",2006,Artificial Intelligence,
"  In this paper, we introduce DLS-MC, a new stochastic local search algorithm
for the maximum clique problem. DLS-MC alternates between phases of iterative
improvement, during which suitable vertices are added to the current clique,
and plateau search, during which vertices of the current clique are swapped
with vertices not contained in the current clique. The selection of vertices is
solely based on vertex penalties that are dynamically adjusted during the
search, and a perturbation mechanism is used to overcome search stagnation. The
behaviour of DLS-MC is controlled by a single parameter, penalty delay, which
controls the frequency at which vertex penalties are reduced. We show
empirically that DLS-MC achieves substantial performance improvements over
state-of-the-art algorithms for the maximum clique problem over a large range
of the commonly used DIMACS benchmark instances.
",Dynamic Local Search for the Maximum Clique Problem,"H. H. Hoos, W. Pullan",2006,Artificial Intelligence,
"  Open distributed multi-agent systems are gaining interest in the academic
community and in industry. In such open settings, agents are often coordinated
using standardized agent conversation protocols. The representation of such
protocols (for analysis, validation, monitoring, etc) is an important aspect of
multi-agent applications. Recently, Petri nets have been shown to be an
interesting approach to such representation, and radically different approaches
using Petri nets have been proposed. However, their relative strengths and
weaknesses have not been examined. Moreover, their scalability and suitability
for different tasks have not been addressed. This paper addresses both these
challenges. First, we analyze existing Petri net representations in terms of
their scalability and appropriateness for overhearing, an important task in
monitoring open multi-agent systems. Then, building on the insights gained, we
introduce a novel representation using Colored Petri nets that explicitly
represent legal joint conversation states and messages. This representation
approach offers significant improvements in scalability and is particularly
suitable for overhearing. Furthermore, we show that this new representation
offers a comprehensive coverage of all conversation features of FIPA
conversation standards. We also present a procedure for transforming AUML
conversation protocol diagrams (a standard human-readable representation), to
our Colored Petri net representation.
",Representing Conversations for Scalable Overhearing,"G. Gutnik, G. A. Kaminka",2006,Artificial Intelligence,
"  The hm admissible heuristics for (sequential and temporal) regression
planning are defined by a parameterized relaxation of the optimal cost function
in the regression search space, where the parameter m offers a trade-off
between the accuracy and computational cost of theheuristic. Existing methods
for computing the hm heuristic require time exponential in m, limiting them to
small values (m andlt= 2). The hm heuristic can also be viewed as the optimal
cost function in a relaxation of the search space: this paper presents relaxed
search, a method for computing this function partially by searching in the
relaxed space. The relaxed search method, because it computes hm only
partially, is computationally cheaper and therefore usable for higher values of
m. The (complete) hm heuristic is combined with partial hm heuristics, for m =
3,..., computed by relaxed search, resulting in a more accurate heuristic.
  This use of the relaxed search method to improve on the hm heuristic is
evaluated by comparing two optimal temporal planners: TP4, which does not use
it, and HSP*a, which uses it but is otherwise identical to TP4. The comparison
is made on the domains used in the 2004 International Planning Competition, in
which both planners participated. Relaxed search is found to be cost effective
in some of these domains, but not all. Analysis reveals a characterization of
the domains in which relaxed search can be expected to be cost effective, in
terms of two measures on the original and relaxed search spaces. In the domains
where relaxed search is cost effective, expanding small states is
computationally cheaper than expanding large states and small states tend to
have small successor states.
","Improving Heuristics Through Relaxed Search - An Analysis of TP4 and
  HSP*a in the 2004 Planning Competition",P. Haslum,2006,Artificial Intelligence,
"  Multiple sequence alignment (MSA) is a ubiquitous problem in computational
biology. Although it is NP-hard to find an optimal solution for an arbitrary
number of sequences, due to the importance of this problem researchers are
trying to push the limits of exact algorithms further. Since MSA can be cast as
a classical path finding problem, it is attracting a growing number of AI
researchers interested in heuristic search algorithms as a challenge with
actual practical relevance. In this paper, we first review two previous,
complementary lines of research. Based on Hirschbergs algorithm, Dynamic
Programming needs O(kN^(k-1)) space to store both the search frontier and the
nodes needed to reconstruct the solution path, for k sequences of length N.
Best first search, on the other hand, has the advantage of bounding the search
space that has to be explored using a heuristic. However, it is necessary to
maintain all explored nodes up to the final solution in order to prevent the
search from re-expanding them at higher cost. Earlier approaches to reduce the
Closed list are either incompatible with pruning methods for the Open list, or
must retain at least the boundary of the Closed list. In this article, we
present an algorithm that attempts at combining the respective advantages; like
A* it uses a heuristic for pruning the search space, but reduces both the
maximum Open and Closed size to O(kN^(k-1)), as in Dynamic Programming. The
underlying idea is to conduct a series of searches with successively increasing
upper bounds, but using the DP ordering as the key for the Open priority queue.
With a suitable choice of thresholds, in practice, a running time below four
times that of A* can be expected. In our experiments we show that our algorithm
outperforms one of the currently most successful algorithms for optimal
multiple sequence alignments, Partial Expansion A*, both in time and memory.
Moreover, we apply a refined heuristic based on optimal alignments not only of
pairs of sequences, but of larger subsets. This idea is not new; however, to
make it practically relevant we show that it is equally important to bound the
heuristic computation appropriately, or the overhead can obliterate any
possible gain. Furthermore, we discuss a number of improvements in time and
space efficiency with regard to practical implementations. Our algorithm, used
in conjunction with higher-dimensional heuristics, is able to calculate for the
first time the optimal alignment for almost all of the problems in Reference 1
of the benchmark database BAliBASE.
",An Improved Search Algorithm for Optimal Multiple-Sequence Alignment,S. Schroedl,2005,Artificial Intelligence,
"  This article develops Probabilistic Hybrid Action Models (PHAMs), a realistic
causal model for predicting the behavior generated by modern percept-driven
robot plans. PHAMs represent aspects of robot behavior that cannot be
represented by most action models used in AI planning: the temporal structure
of continuous control processes, their non-deterministic effects, several modes
of their interferences, and the achievement of triggering conditions in
closed-loop robot plans.
  The main contributions of this article are: (1) PHAMs, a model of concurrent
percept-driven behavior, its formalization, and proofs that the model generates
probably, qualitatively accurate predictions; and (2) a resource-efficient
inference method for PHAMs based on sampling projections from probabilistic
action models and state descriptions. We show how PHAMs can be applied to
planning the course of action of an autonomous robot office courier based on
analytical and experimental results.
","Probabilistic Hybrid Action Models for Predicting Concurrent
  Percept-driven Robot Behavior","M. Beetz, H. Grosskreutz",2005,Artificial Intelligence,
"  We present a novel framework for integrating prior knowledge into
discriminative classifiers. Our framework allows discriminative classifiers
such as Support Vector Machines (SVMs) to utilize prior knowledge specified in
the generative setting. The dual objective of fitting the data and respecting
prior knowledge is formulated as a bilevel program, which is solved
(approximately) via iterative application of second-order cone programming. To
test our approach, we consider the problem of using WordNet (a semantic
database of English language) to improve low-sample classification accuracy of
newsgroup categorization. WordNet is viewed as an approximate, but readily
available source of background knowledge, and our framework is capable of
utilizing it in a flexible way.
",Generative Prior Knowledge for Discriminative Classification,"G. DeJong, A. Epshteyn",2006,Artificial Intelligence,
"  Fast Downward is a classical planning system based on heuristic search. It
can deal with general deterministic planning problems encoded in the
propositional fragment of PDDL2.2, including advanced features like ADL
conditions and effects and derived predicates (axioms). Like other well-known
planners such as HSP and FF, Fast Downward is a progression planner, searching
the space of world states of a planning task in the forward direction. However,
unlike other PDDL planning systems, Fast Downward does not use the
propositional PDDL representation of a planning task directly. Instead, the
input is first translated into an alternative representation called
multi-valued planning tasks, which makes many of the implicit constraints of a
propositional planning task explicit. Exploiting this alternative
representation, Fast Downward uses hierarchical decompositions of planning
tasks for computing its heuristic function, called the causal graph heuristic,
which is very different from traditional HSP-like heuristics based on ignoring
negative interactions of operators.
  In this article, we give a full account of Fast Downwards approach to solving
multi-valued planning tasks. We extend our earlier discussion of the causal
graph heuristic to tasks involving axioms and conditional effects and present
some novel techniques for search control that are used within Fast Downwards
best-first search algorithm: preferred operators transfer the idea of helpful
actions from local search to global best-first search, deferred evaluation of
heuristic functions mitigates the negative effect of large branching factors on
search performance, and multi-heuristic best-first search combines several
heuristic evaluation functions within a single search algorithm in an
orthogonal way. We also describe efficient data structures for fast state
expansion (successor generators and axiom evaluators) and present a new
non-heuristic search algorithm called focused iterative-broadening search,
which utilizes the information encoded in causal graphs in a novel way.
  Fast Downward has proven remarkably successful: It won the ""classical (i.e.,
propositional, non-optimising) track of the 4th International Planning
Competition at ICAPS 2004, following in the footsteps of planners such as FF
and LPG. Our experiments show that it also performs very well on the benchmarks
of the earlier planning competitions and provide some insights about the
usefulness of the new search enhancements.
",The Fast Downward Planning System,M. Helmert,2006,Artificial Intelligence,
"  Distributed Constraint Satisfaction (DCSP) has long been considered an
important problem in multi-agent systems research. This is because many
real-world problems can be represented as constraint satisfaction and these
problems often present themselves in a distributed form. In this article, we
present a new complete, distributed algorithm called Asynchronous Partial
Overlay (APO) for solving DCSPs that is based on a cooperative mediation
process. The primary ideas behind this algorithm are that agents, when acting
as a mediator, centralize small, relevant portions of the DCSP, that these
centralized subproblems overlap, and that agents increase the size of their
subproblems along critical paths within the DCSP as the problem solving
unfolds. We present empirical evidence that shows that APO outperforms other
known, complete DCSP techniques.
","Asynchronous Partial Overlay: A New Algorithm for Solving Distributed
  Constraint Satisfaction Problems","V. R. Lesser, R. Mailler",2006,Artificial Intelligence,
"  As partial justification of their framework for iterated belief revision
Darwiche and Pearl convincingly argued against Boutiliers natural revision and
provided a prototypical revision operator that fits into their scheme. We show
that the Darwiche-Pearl arguments lead naturally to the acceptance of a smaller
class of operators which we refer to as admissible. Admissible revision ensures
that the penultimate input is not ignored completely, thereby eliminating
natural revision, but includes the Darwiche-Pearl operator, Nayaks
lexicographic revision operator, and a newly introduced operator called
restrained revision. We demonstrate that restrained revision is the most
conservative of admissible revision operators, effecting as few changes as
possible, while lexicographic revision is the least conservative, and point out
that restrained revision can also be viewed as a composite operator, consisting
of natural revision preceded by an application of a ""backwards revision""
operator previously studied by Papini. Finally, we propose the establishment of
a principled approach for choosing an appropriate revision operator in
different contexts and discuss future work.
",Admissible and Restrained Revision,"R. Booth, T. Meyer",2006,Artificial Intelligence,
"  In recent years, CP-nets have emerged as a useful tool for supporting
preference elicitation, reasoning, and representation. CP-nets capture and
support reasoning with qualitative conditional preference statements,
statements that are relatively natural for users to express. In this paper, we
extend the CP-nets formalism to handle another class of very natural
qualitative statements one often uses in expressing preferences in daily life -
statements of relative importance of attributes. The resulting formalism,
TCP-nets, maintains the spirit of CP-nets, in that it remains focused on using
only simple and natural preference statements, uses the ceteris paribus
semantics, and utilizes a graphical representation of this information to
reason about its consistency and to perform, possibly constrained, optimization
using it. The extra expressiveness it provides allows us to better model
tradeoffs users would like to make, more faithfully representing their
preferences.
",On Graphical Modeling of Preference and Importance,"R. I. Brafman, C. Domshlak, S. E. Shimony",2006,Artificial Intelligence,
"  Linear Temporal Logic (LTL) is widely used for defining conditions on the
execution paths of dynamic systems. In the case of dynamic systems that allow
for nondeterministic evolutions, one has to specify, along with an LTL formula
f, which are the paths that are required to satisfy the formula. Two extreme
cases are the universal interpretation A.f, which requires that the formula be
satisfied for all execution paths, and the existential interpretation E.f,
which requires that the formula be satisfied for some execution path.
  When LTL is applied to the definition of goals in planning problems on
nondeterministic domains, these two extreme cases are too restrictive. It is
often impossible to develop plans that achieve the goal in all the
nondeterministic evolutions of a system, and it is too weak to require that the
goal is satisfied by some execution.
  In this paper we explore alternative interpretations of an LTL formula that
are between these extreme cases. We define a new language that permits an
arbitrary combination of the A and E quantifiers, thus allowing, for instance,
to require that each finite execution can be extended to an execution
satisfying an LTL formula (AE.f), or that there is some finite execution whose
extensions all satisfy an LTL formula (EA.f). We show that only eight of these
combinations of path quantifiers are relevant, corresponding to an alternation
of the quantifiers of length one (A and E), two (AE and EA), three (AEA and
EAE), and infinity ((AE)* and (EA)*). We also present a planning algorithm for
the new language that is based on an automata-theoretic approach, and study its
complexity.
","The Planning Spectrum - One, Two, Three, Infinity","M. Pistore, M. Y. Vardi",2007,Artificial Intelligence,
"  A delta-model is a satisfying assignment of a Boolean formula for which any
small alteration, such as a single bit flip, can be repaired by flips to some
small number of other bits, yielding a new satisfying assignment. These
satisfying assignments represent robust solutions to optimization problems
(e.g., scheduling) where it is possible to recover from unforeseen events
(e.g., a resource becoming unavailable). The concept of delta-models was
introduced by Ginsberg, Parkes and Roy (AAAI 1998), where it was proved that
finding delta-models for general Boolean formulas is NP-complete. In this
paper, we extend that result by studying the complexity of finding delta-models
for classes of Boolean formulas which are known to have polynomial time
satisfiability solvers. In particular, we examine 2-SAT, Horn-SAT, Affine-SAT,
dual-Horn-SAT, 0-valid and 1-valid SAT. We see a wide variation in the
complexity of finding delta-models, e.g., while 2-SAT and Affine-SAT have
polynomial time tests for delta-models, testing whether a Horn-SAT formula has
one is NP-complete.
",Fault Tolerant Boolean Satisfiability,A. Roy,2006,Artificial Intelligence,
"  Multimodal conversational interfaces provide a natural means for users to
communicate with computer systems through multiple modalities such as speech
and gesture. To build effective multimodal interfaces, automated interpretation
of user multimodal inputs is important. Inspired by the previous investigation
on cognitive status in multimodal human machine interaction, we have developed
a greedy algorithm for interpreting user referring expressions (i.e.,
multimodal reference resolution). This algorithm incorporates the cognitive
principles of Conversational Implicature and Givenness Hierarchy and applies
constraints from various sources (e.g., temporal, semantic, and contextual) to
resolve references. Our empirical results have shown the advantage of this
algorithm in efficiently resolving a variety of user references. Because of its
simplicity and generality, this approach has the potential to improve the
robustness of multimodal input interpretation.
",Cognitive Principles in Robust Multimodal Interpretation,"J. Y. Chai, Z. Prasov, S. Qu",2006,Artificial Intelligence,
"  This paper presents a new framework for anytime heuristic search where the
task is to achieve as many goals as possible within the allocated resources. We
show the inadequacy of traditional distance-estimation heuristics for tasks of
this type and present alternative heuristics that are more appropriate for
multiple-goal search. In particular, we introduce the marginal-utility
heuristic, which estimates the cost and the benefit of exploring a subtree
below a search node. We developed two methods for online learning of the
marginal-utility heuristic. One is based on local similarity of the partial
marginal utility of sibling nodes, and the other generalizes marginal-utility
over the state feature space. We apply our adaptive and non-adaptive
multiple-goal search algorithms to several problems, including focused
crawling, and show their superiority over existing methods.
",Multiple-Goal Heuristic Search,"D. Davidov, S. Markovitch",2006,Artificial Intelligence,
"  We present a heuristic search algorithm for solving first-order Markov
Decision Processes (FOMDPs). Our approach combines first-order state
abstraction that avoids evaluating states individually, and heuristic search
that avoids evaluating all states. Firstly, in contrast to existing systems,
which start with propositionalizing the FOMDP and then perform state
abstraction on its propositionalized version we apply state abstraction
directly on the FOMDP avoiding propositionalization. This kind of abstraction
is referred to as first-order state abstraction. Secondly, guided by an
admissible heuristic, the search is restricted to those states that are
reachable from the initial state. We demonstrate the usefulness of the above
techniques for solving FOMDPs with a system, referred to as FluCaP (formerly,
FCPlanner), that entered the probabilistic track of the 2004 International
Planning Competition (IPC2004) and demonstrated an advantage over other
planners on the problems represented in first-order terms.
",FluCaP: A Heuristic Search Planner for First-Order MDPs,"S. Hoelldobler, E. Karabaev, O. Skvortsova",2006,Artificial Intelligence,
"  It was recently proved that a sound and complete qualitative simulator does
not exist, that is, as long as the input-output vocabulary of the
state-of-the-art QSIM algorithm is used, there will always be input models
which cause any simulator with a coverage guarantee to make spurious
predictions in its output. In this paper, we examine whether a meaningfully
expressive restriction of this vocabulary is possible so that one can build a
simulator with both the soundness and completeness properties. We prove several
negative results: All sound qualitative simulators, employing subsets of the
QSIM representation which retain the operating region transition feature, and
support at least the addition and constancy constraints, are shown to be
inherently incomplete. Even when the simulations are restricted to run in a
single operating region, a constraint vocabulary containing just the addition,
constancy, derivative, and multiplication relations makes the construction of
sound and complete qualitative simulators impossible.
",Causes of Ineradicable Spurious Predictions in Qualitative Simulation,"\""O. Y{\i}lmaz, A. C. C. Say",2006,Artificial Intelligence,
"  We study properties of programs with monotone and convex constraints. We
extend to these formalisms concepts and results from normal logic programming.
They include the notions of strong and uniform equivalence with their
characterizations, tight programs and Fages Lemma, program completion and loop
formulas. Our results provide an abstract account of properties of some recent
extensions of logic programming with aggregates, especially the formalism of
lparse programs. They imply a method to compute stable models of lparse
programs by means of off-the-shelf solvers of pseudo-boolean constraints, which
is often much faster than the smodels system.
","Properties and Applications of Programs with Monotone and Convex
  Constraints","L. Liu, M. Truszczynski",2006,Artificial Intelligence,
"  We characterize the search landscape of random instances of the job shop
scheduling problem (JSP). Specifically, we investigate how the expected values
of (1) backbone size, (2) distance between near-optimal schedules, and (3)
makespan of random schedules vary as a function of the job to machine ratio
(N/M). For the limiting cases N/M approaches 0 and N/M approaches infinity we
provide analytical results, while for intermediate values of N/M we perform
experiments. We prove that as N/M approaches 0, backbone size approaches 100%,
while as N/M approaches infinity the backbone vanishes. In the process we show
that as N/M approaches 0 (resp. N/M approaches infinity), simple priority rules
almost surely generate an optimal schedule, providing theoretical evidence of
an ""easy-hard-easy"" pattern of typical-case instance difficulty in job shop
scheduling. We also draw connections between our theoretical results and the
""big valley"" picture of JSP landscapes.
","How the Landscape of Random Job Shop Scheduling Instances Depends on the
  Ratio of Jobs to Machines","S. F. Smith, M. J. Streeter",2006,Artificial Intelligence,
"  We consider interactive tools that help users search for their most preferred
item in a large collection of options. In particular, we examine
example-critiquing, a technique for enabling users to incrementally construct
preference models by critiquing example options that are presented to them. We
present novel techniques for improving the example-critiquing technology by
adding suggestions to its displayed options. Such suggestions are calculated
based on an analysis of users current preference model and their potential
hidden preferences. We evaluate the performance of our model-based suggestion
techniques with both synthetic and real users. Results show that such
suggestions are highly attractive to users and can stimulate them to express
more preferences to improve the chance of identifying their most preferred item
by up to 78%.
",Preference-based Search using Example-Critiquing with Suggestions,"B. Faltings, P. Pu, P. Viappiani",2006,Artificial Intelligence,
"  The Partially Observable Markov Decision Process has long been recognized as
a rich framework for real-world planning and control problems, especially in
robotics. However exact solutions in this framework are typically
computationally intractable for all but the smallest problems. A well-known
technique for speeding up POMDP solving involves performing value backups at
specific belief points, rather than over the entire belief simplex. The
efficiency of this approach, however, depends greatly on the selection of
points. This paper presents a set of novel techniques for selecting informative
belief points which work well in practice. The point selection procedure is
combined with point-based value backups to form an effective anytime POMDP
algorithm called Point-Based Value Iteration (PBVI). The first aim of this
paper is to introduce this algorithm and present a theoretical analysis
justifying the choice of belief selection technique. The second aim of this
paper is to provide a thorough empirical comparison between PBVI and other
state-of-the-art POMDP methods, in particular the Perseus algorithm, in an
effort to highlight their similarities and differences. Evaluation is performed
using both standard POMDP domains and realistic robotic tasks.
",Anytime Point-Based Approximations for Large POMDPs,"J. Pineau, G. Gordon, S. Thrun",2006,Artificial Intelligence,
"  Efficient representations and solutions for large decision problems with
continuous and discrete variables are among the most important challenges faced
by the designers of automated decision support systems. In this paper, we
describe a novel hybrid factored Markov decision process (MDP) model that
allows for a compact representation of these problems, and a new hybrid
approximate linear programming (HALP) framework that permits their efficient
solutions. The central idea of HALP is to approximate the optimal value
function by a linear combination of basis functions and optimize its weights by
linear programming. We analyze both theoretical and computational aspects of
this approach, and demonstrate its scale-up potential on several hybrid
optimization problems.
",Solving Factored MDPs with Hybrid State and Action Variables,"C. Guestrin, M. Hauskrecht, B. Kveton",2006,Artificial Intelligence,
"  This paper introduces and analyzes a battery of inference models for the
problem of semantic role labeling: one based on constraint satisfaction, and
several strategies that model the inference as a meta-learning problem using
discriminative classifiers. These classifiers are developed with a rich set of
novel features that encode proposition and sentence-level information. To our
knowledge, this is the first work that: (a) performs a thorough analysis of
learning-based inference models for semantic role labeling, and (b) compares
several inference strategies in this context. We evaluate the proposed
inference strategies in the framework of the CoNLL-2005 shared task using only
automatically-generated syntactic information. The extensive experimental
evaluation and analysis indicates that all the proposed inference strategies
are successful -they all outperform the current best results reported in the
CoNLL-2005 evaluation exercise- but each of the proposed approaches has its
advantages and disadvantages. Several important traits of a state-of-the-art
SRL combination strategy emerge from this analysis: (i) individual models
should be combined at the granularity of candidate arguments rather than at the
granularity of complete solutions; (ii) the best combination strategy uses an
inference model based in learning; and (iii) the learning-based inference
benefits from max-margin classifiers and global feedback.
",Combination Strategies for Semantic Role Labeling,"M. Surdeanu, L. Marquez, X. Carreras, P. R. Comas",2007,Artificial Intelligence,
"  In a field of research about general reasoning mechanisms, it is essential to
have appropriate benchmarks. Ideally, the benchmarks should reflect possible
applications of the developed technology. In AI Planning, researchers more and
more tend to draw their testing examples from the benchmark collections used in
the International Planning Competition (IPC). In the organization of (the
deterministic part of) the fourth IPC, IPC-4, the authors therefore invested
significant effort to create a useful set of benchmarks. They come from five
different (potential) real-world applications of planning: airport ground
traffic control, oil derivative transportation in pipeline networks,
model-checking safety properties, power supply restoration, and UMTS call
setup. Adapting and preparing such an application for use as a benchmark in the
IPC involves, at the time, inevitable (often drastic) simplifications, as well
as careful choice between, and engineering of, domain encodings. For the first
time in the IPC, we used compilations to formulate complex domain features in
simple languages such as STRIPS, rather than just dropping the more interesting
problem constraints in the simpler language subsets. The article explains and
discusses the five application domains and their adaptation to form the PDDL
test suites used in IPC-4. We summarize known theoretical results on structural
properties of the domains, regarding their computational complexity and
provable properties of their topology under the h+ function (an idealized
version of the relaxed plan heuristic). We present new (empirical) results
illuminating properties such as the quality of the most wide-spread heuristic
functions (planning graph, serial planning graph, and relaxed plan), the growth
of propositional representations over instance size, and the number of actions
available to achieve each fact; we discuss these data in conjunction with the
best results achieved by the different kinds of planners participating in
IPC-4.
","Engineering Benchmarks for Planning: the Domains Used in the
  Deterministic Part of IPC-4","S. Edelkamp, R. Englert, J. Hoffmann, F. Liporace, S. Thiebaux, S.
  Trueg",2006,Artificial Intelligence,
"  In this paper we present pddl+, a planning domain description language for
modelling mixed discrete-continuous planning domains. We describe the syntax
and modelling style of pddl+, showing that the language makes convenient the
modelling of complex time-dependent effects. We provide a formal semantics for
pddl+ by mapping planning instances into constructs of hybrid automata. Using
the syntax of HAs as our semantic model we construct a semantic mapping to
labelled transition systems to complete the formal interpretation of pddl+
planning instances. An advantage of building a mapping from pddl+ to HA theory
is that it forms a bridge between the Planning and Real Time Systems research
communities. One consequence is that we can expect to make use of some of the
theoretical properties of HAs. For example, for a restricted class of HAs the
Reachability problem (which is equivalent to Plan Existence) is decidable.
pddl+ provides an alternative to the continuous durative action model of
pddl2.1, adding a more flexible and robust model of time-dependent behaviour.
",Modelling Mixed Discrete-Continuous Domains for Planning,"M. Fox, D. Long",2006,Artificial Intelligence,
"  In this paper, we show that there is a close relation between consistency in
a constraint network and set intersection. A proof schema is provided as a
generic way to obtain consistency properties from properties on set
intersection. This approach not only simplifies the understanding of and
unifies many existing consistency results, but also directs the study of
consistency to that of set intersection properties in many situations, as
demonstrated by the results on the convexity and tightness of constraints in
this paper. Specifically, we identify a new class of tree convex constraints
where local consistency ensures global consistency. This generalizes row convex
constraints. Various consistency results are also obtained on constraint
networks where only some, in contrast to all in the existing work,constraints
are tight.
",Set Intersection and Consistency in Constraint Networks,"R. H. C. Yap, Y. Zhang",2006,Artificial Intelligence,
"  In this paper, we study the possibility of designing non-trivial random CSP
models by exploiting the intrinsic connection between structures and
typical-case hardness. We show that constraint consistency, a notion that has
been developed to improve the efficiency of CSP algorithms, is in fact the key
to the design of random CSP models that have interesting phase transition
behavior and guaranteed exponential resolution complexity without putting much
restriction on the parameter of constraint tightness or the domain size of the
problem. We propose a very flexible framework for constructing problem
instances withinteresting behavior and develop a variety of concrete methods to
construct specific random CSP models that enforce different levels of
constraint consistency. A series of experimental studies with interesting
observations are carried out to illustrate the effectiveness of introducing
structural elements in random instances, to verify the robustness of our
proposal, and to investigate features of some specific models based on our
framework that are highly related to the behavior of backtracking search
algorithms.
",Consistency and Random Constraint Satisfaction Models,"J. Culberson, Y. Gao",2007,Artificial Intelligence,
"  In this paper, we present two alternative approaches to defining answer sets
for logic programs with arbitrary types of abstract constraint atoms (c-atoms).
These approaches generalize the fixpoint-based and the level mapping based
answer set semantics of normal logic programs to the case of logic programs
with arbitrary types of c-atoms. The results are four different answer set
definitions which are equivalent when applied to normal logic programs. The
standard fixpoint-based semantics of logic programs is generalized in two
directions, called answer set by reduct and answer set by complement. These
definitions, which differ from each other in the treatment of
negation-as-failure (naf) atoms, make use of an immediate consequence operator
to perform answer set checking, whose definition relies on the notion of
conditional satisfaction of c-atoms w.r.t. a pair of interpretations. The other
two definitions, called strongly and weakly well-supported models, are
generalizations of the notion of well-supported models of normal logic programs
to the case of programs with c-atoms. As for the case of fixpoint-based
semantics, the difference between these two definitions is rooted in the
treatment of naf atoms. We prove that answer sets by reduct (resp. by
complement) are equivalent to weakly (resp. strongly) well-supported models of
a program, thus generalizing the theorem on the correspondence between stable
models and well-supported models of a normal logic program to the class of
programs with c-atoms. We show that the newly defined semantics coincide with
previously introduced semantics for logic programs with monotone c-atoms, and
they extend the original answer set semantics of normal logic programs. We also
study some properties of answer sets of programs with c-atoms, and relate our
definitions to several semantics for logic programs with aggregates presented
in the literature.
",Answer Sets for Logic Programs with Arbitrary Abstract Constraint Atoms,"E. Pontelli, T. C. Son, P. H. Tu",2007,Artificial Intelligence,
"  Many combinatorial optimization problems such as the bin packing and multiple
knapsack problems involve assigning a set of discrete objects to multiple
containers. These problems can be used to model task and resource allocation
problems in multi-agent systems and distributed systms, and can also be found
as subproblems of scheduling problems. We propose bin completion, a
branch-and-bound strategy for one-dimensional, multicontainer packing problems.
Bin completion combines a bin-oriented search space with a powerful dominance
criterion that enables us to prune much of the space. The performance of the
basic bin completion framework can be enhanced by using a number of extensions,
including nogood-based pruning techniques that allow further exploitation of
the dominance criterion. Bin completion is applied to four problems: multiple
knapsack, bin covering, min-cost covering, and bin packing. We show that our
bin completion algorithms yield new, state-of-the-art results for the multiple
knapsack, bin covering, and min-cost covering problems, outperforming previous
algorithms by several orders of magnitude with respect to runtime on some
classes of hard, random problem instances. For the bin packing problem, we
demonstrate significant improvements compared to most previous results, but
show that bin completion is not competitive with current state-of-the-art
cutting-stock based approaches.
","Bin Completion Algorithms for Multicontainer Packing, Knapsack, and
  Covering Problems","A. S. Fukunaga, R. E. Korf",2007,Artificial Intelligence,
"  In real-life temporal scenarios, uncertainty and preferences are often
essential and coexisting aspects. We present a formalism where quantitative
temporal constraints with both preferences and uncertainty can be defined. We
show how three classical notions of controllability (that is, strong, weak, and
dynamic), which have been developed for uncertain temporal problems, can be
generalized to handle preferences as well. After defining this general
framework, we focus on problems where preferences follow the fuzzy approach,
and with properties that assure tractability. For such problems, we propose
algorithms to check the presence of the controllability properties. In
particular, we show that in such a setting dealing simultaneously with
preferences and uncertainty does not increase the complexity of controllability
testing. We also develop a dynamic execution algorithm, of polynomial
complexity, that produces temporal plans under uncertainty that are optimal
with respect to fuzzy preferences.
","Uncertainty in Soft Temporal Constraint Problems:A General Framework and
  Controllability Algorithms for the Fuzzy Case","F. Rossi, K. B. Venable, N. Yorke-Smith",2006,Artificial Intelligence,
"  In the recent years several research efforts have focused on the concept of
time granularity and its applications. A first stream of research investigated
the mathematical models behind the notion of granularity and the algorithms to
manage temporal data based on those models. A second stream of research
investigated symbolic formalisms providing a set of algebraic operators to
define granularities in a compact and compositional way. However, only very
limited manipulation algorithms have been proposed to operate directly on the
algebraic representation making it unsuitable to use the symbolic formalisms in
applications that need manipulation of granularities.
  This paper aims at filling the gap between the results from these two streams
of research, by providing an efficient conversion from the algebraic
representation to the equivalent low-level representation based on the
mathematical models. In addition, the conversion returns a minimal
representation in terms of period length. Our results have a major practical
impact: users can more easily define arbitrary granularities in terms of
algebraic operators, and then access granularity reasoning and other services
operating efficiently on the equivalent, minimal low-level representation. As
an example, we illustrate the application to temporal constraint reasoning with
multiple granularities.
  From a technical point of view, we propose an hybrid algorithm that
interleaves the conversion of calendar subexpressions into periodical sets with
the minimization of the period length. The algorithm returns set-based
granularity representations having minimal period length, which is the most
relevant parameter for the performance of the considered reasoning services.
Extensive experimental work supports the techniques used in the algorithm, and
shows the efficiency and effectiveness of the algorithm.
","Supporting Temporal Reasoning by Mapping Calendar Expressions to Minimal
  Periodic Sets","C. Bettini, S. Mascetti, X. S. Wang",2007,Artificial Intelligence,
"  We consider the problem of computing a lightest derivation of a global
structure using a set of weighted rules. A large variety of inference problems
in AI can be formulated in this framework. We generalize A* search and
heuristics derived from abstractions to a broad class of lightest derivation
problems. We also describe a new algorithm that searches for lightest
derivations using a hierarchy of abstractions. Our generalization of A* gives a
new algorithm for searching AND/OR graphs in a bottom-up fashion. We discuss
how the algorithms described here provide a general architecture for addressing
the pipeline problem --- the problem of passing information back and forth
between various stages of processing in a perceptual system. We consider
examples in computer vision and natural language processing. We apply the
hierarchical search algorithm to the problem of estimating the boundaries of
convex objects in grayscale images and compare it to other search methods. A
second set of experiments demonstrate the use of a new compositional model for
finding salient curves in images.
",The Generalized A* Architecture,"P. F. Felzenszwalb, D. McAllester",2007,Artificial Intelligence,
"  In this paper, we construct and investigate a hierarchy of spatio-temporal
formalisms that result from various combinations of propositional spatial and
temporal logics such as the propositional temporal logic PTL, the spatial
logics RCC-8, BRCC-8, S4u and their fragments. The obtained results give a
clear picture of the trade-off between expressiveness and computational
realisability within the hierarchy. We demonstrate how different combining
principles as well as spatial and temporal primitives can produce NP-, PSPACE-,
EXPSPACE-, 2EXPSPACE-complete, and even undecidable spatio-temporal logics out
of components that are at most NP- or PSPACE-complete.
",Combining Spatial and Temporal Logics: Expressiveness vs. Complexity,"D. Gabelaia, R. Kontchakov, A. Kurucz, F. Wolter, M. Zakharyaschev",2005,Artificial Intelligence,
"  The treatment of exogenous events in planning is practically important in
many real-world domains where the preconditions of certain plan actions are
affected by such events. In this paper we focus on planning in temporal domains
with exogenous events that happen at known times, imposing the constraint that
certain actions in the plan must be executed during some predefined time
windows. When actions have durations, handling such temporal constraints adds
an extra difficulty to planning. We propose an approach to planning in these
domains which integrates constraint-based temporal reasoning into a graph-based
planning framework using local search. Our techniques are implemented in a
planner that took part in the 4th International Planning Competition (IPC-4). A
statistical analysis of the results of IPC-4 demonstrates the effectiveness of
our approach in terms of both CPU-time and plan quality. Additional experiments
show the good performance of the temporal reasoning techniques integrated into
our planner.
","An Approach to Temporal Planning and Scheduling in Domains with
  Predictable Exogenous Events","A. Gerevini, A. Saetti, I. Serina",2006,Artificial Intelligence,
"  In this commentary I argue that although PDDL is a very useful standard for
the planning competition, its design does not properly consider the issue of
domain modeling. Hence, I would not advocate its use in specifying planning
domains outside of the context of the planning competition. Rather, the field
needs to explore different approaches and grapple more directly with the
problem of effectively modeling and utilizing all of the diverse pieces of
knowledge we typically have about planning domains.
",The Power of Modeling - a Response to PDDL2.1,F. Bacchus,2003,Artificial Intelligence,
"  PDDL was originally conceived and constructed as a lingua franca for the
International Planning Competition. PDDL2.1 embodies a set of extensions
intended to support the expression of something closer to real planning
problems. This objective has only been partially achieved, due in large part to
a deliberate focus on not moving too far from classical planning models and
solution methods.
",Imperfect Match: PDDL 2.1 and Real Applications,M. S. Boddy,2003,Artificial Intelligence,
"  I comment on the PDDL 2.1 language and its use in the planning competition,
focusing on the choices made for accommodating time and concurrency. I also
discuss some methodological issues that have to do with the move toward more
expressive planning languages and the balance needed in planning research
between semantics and computation.
",PDDL 2.1: Representation vs. Computation,H. A. Geffner,2003,Artificial Intelligence,
"  Most classical scheduling formulations assume a fixed and known duration for
each activity. In this paper, we weaken this assumption, requiring instead that
each duration can be represented by an independent random variable with a known
mean and variance. The best solutions are ones which have a high probability of
achieving a good makespan. We first create a theoretical framework, formally
showing how Monte Carlo simulation can be combined with deterministic
scheduling algorithms to solve this problem. We propose an associated
deterministic scheduling problem whose solution is proved, under certain
conditions, to be a lower bound for the probabilistic problem. We then propose
and investigate a number of techniques for solving such problems based on
combinations of Monte Carlo simulation, solutions to the associated
deterministic problem, and either constraint programming or tabu search. Our
empirical results demonstrate that a combination of the use of the associated
deterministic problem and Monte Carlo simulation results in algorithms that
scale best both in terms of problem size and uncertainty. Further experiments
point to the correlation between the quality of the deterministic solution and
the quality of the probabilistic solution as a major factor responsible for
this success.
","Proactive Algorithms for Job Shop Scheduling with Probabilistic
  Durations","J. C. Beck, N. Wilson",2007,Artificial Intelligence,
"  This paper is concerned with a class of algorithms that perform exhaustive
search on propositional knowledge bases. We show that each of these algorithms
defines and generates a propositional language. Specifically, we show that the
trace of a search can be interpreted as a combinational circuit, and a search
algorithm then defines a propositional language consisting of circuits that are
generated across all possible executions of the algorithm. In particular, we
show that several versions of exhaustive DPLL search correspond to such
well-known languages as FBDD, OBDD, and a precisely-defined subset of d-DNNF.
By thus mapping search algorithms to propositional languages, we provide a
uniform and practical framework in which successful search techniques can be
harnessed for compilation of knowledge into various languages of interest, and
a new methodology whereby the power and limitations of search algorithms can be
understood by looking up the tractability and succinctness of the corresponding
propositional languages.
",The Language of Search,"A. Darwiche, J. Huang",2007,Artificial Intelligence,
"  The best performing algorithms for a particular oversubscribed scheduling
application, Air Force Satellite Control Network (AFSCN) scheduling, appear to
have little in common. Yet, through careful experimentation and modeling of
performance in real problem instances, we can relate characteristics of the
best algorithms to characteristics of the application. In particular, we find
that plateaus dominate the search spaces (thus favoring algorithms that make
larger changes to solutions) and that some randomization in exploration is
critical to good performance (due to the lack of gradient information on the
plateaus). Based on our explanations of algorithm performance, we develop a new
algorithm that combines characteristics of the best performers; the new
algorithms performance is better than the previous best. We show how hypothesis
driven experimentation and search modeling can both explain algorithm
performance and motivate the design of a new algorithm.
","Understanding Algorithm Performance on an Oversubscribed Scheduling
  Application","L. Barbulescu, A. E. Howe, M. Roberts, L. D. Whitley",2006,Artificial Intelligence,
"  This paper describes Marvin, a planner that competed in the Fourth
International Planning Competition (IPC 4). Marvin uses
action-sequence-memoisation techniques to generate macro-actions, which are
then used during search for a solution plan. We provide an overview of its
architecture and search behaviour, detailing the algorithms used. We also
empirically demonstrate the effectiveness of its features in various planning
domains; in particular, the effects on performance due to the use of
macro-actions, the novel features of its search behaviour, and the native
support of ADL and Derived Predicates.
",Marvin: A Heuristic Search Planner with Online Macro-Action Learning,"A. I. Coles, A. J. Smith",2007,Artificial Intelligence,
"  We describe how to convert the heuristic search algorithm A* into an anytime
algorithm that finds a sequence of improved solutions and eventually converges
to an optimal solution. The approach we adopt uses weighted heuristic search to
find an approximate solution quickly, and then continues the weighted search to
find improved solutions as well as to improve a bound on the suboptimality of
the current solution. When the time available to solve a search problem is
limited or uncertain, this creates an anytime heuristic search algorithm that
allows a flexible tradeoff between search time and solution quality. We analyze
the properties of the resulting Anytime A* algorithm, and consider its
performance in three domains; sliding-tile puzzles, STRIPS planning, and
multiple sequence alignment. To illustrate the generality of this approach, we
also describe how to transform the memory-efficient search algorithm Recursive
Best-First Search (RBFS) into an anytime algorithm.
",Anytime Heuristic Search,"E. A. Hansen, R. Zhou",2007,Artificial Intelligence,
"  In this paper we apply computer-aided theorem discovery technique to discover
theorems about strongly equivalent logic programs under the answer set
semantics. Our discovered theorems capture new classes of strongly equivalent
logic programs that can lead to new program simplification rules that preserve
strong equivalence. Specifically, with the help of computers, we discovered
exact conditions that capture the strong equivalence between a rule and the
empty set, between two rules, between two rules and one of the two rules,
between two rules and another rule, and between three rules and two of the
three rules.
",Discovering Classes of Strongly Equivalent Logic Programs,"Y. Chen, F. Lin",2007,Artificial Intelligence,
"  The QXORSAT problem is the quantified version of the satisfiability problem
XORSAT in which the connective exclusive-or is used instead of the usual or. We
study the phase transition associated with random QXORSAT instances. We give a
description of this phase transition in the case of one alternation of
quantifiers, thus performing an advanced practical and theoretical study on the
phase transition of a quantified roblem.
",Phase Transition for Random Quantified XOR-Formulas,"N. Creignou, H. Daude, U. Egly",2007,Artificial Intelligence,
"  The paper presents a new sampling methodology for Bayesian networks that
samples only a subset of variables and applies exact inference to the rest.
Cutset sampling is a network structure-exploiting application of the
Rao-Blackwellisation principle to sampling in Bayesian networks. It improves
convergence by exploiting memory-based inference algorithms. It can also be
viewed as an anytime approximation of the exact cutset-conditioning algorithm
developed by Pearl. Cutset sampling can be implemented efficiently when the
sampled variables constitute a loop-cutset of the Bayesian network and, more
generally, when the induced width of the networks graph conditioned on the
observed sampled variables is bounded by a constant w. We demonstrate
empirically the benefit of this scheme on a range of benchmarks.
",Cutset Sampling for Bayesian Networks,"B. Bidyuk, R. Dechter",2007,Artificial Intelligence,
"  Numerous formalisms and dedicated algorithms have been designed in the last
decades to model and solve decision making problems. Some formalisms, such as
constraint networks, can express ""simple"" decision problems, while others are
designed to take into account uncertainties, unfeasible decisions, and
utilities. Even in a single formalism, several variants are often proposed to
model different types of uncertainty (probability, possibility...) or utility
(additive or not). In this article, we introduce an algebraic graphical model
that encompasses a large number of such formalisms: (1) we first adapt previous
structures from Friedman, Chu and Halpern for representing uncertainty,
utility, and expected utility in order to deal with generic forms of sequential
decision making; (2) on these structures, we then introduce composite graphical
models that express information via variables linked by ""local"" functions,
thanks to conditional independence; (3) on these graphical models, we finally
define a simple class of queries which can represent various scenarios in terms
of observabilities and controllabilities. A natural decision-tree semantics for
such queries is completed by an equivalent operational semantics, which induces
generic algorithms. The proposed framework, called the
Plausibility-Feasibility-Utility (PFU) framework, not only provides a better
understanding of the links between existing formalisms, but it also covers yet
unpublished frameworks (such as possibilistic influence diagrams) and unifies
formalisms such as quantified boolean formulas and influence diagrams. Our
backtrack and variable elimination generic algorithms are a first step towards
unified algorithms.
","An Algebraic Graphical Model for Decision with Uncertainties,
  Feasibilities, and Utilities","C. Pralet, T. Schiex, G. Verfaillie",2007,Artificial Intelligence,
"  Matchmaking arises when supply and demand meet in an electronic marketplace,
or when agents search for a web service to perform some task, or even when
recruiting agencies match curricula and job profiles. In such open
environments, the objective of a matchmaking process is to discover best
available offers to a given request. We address the problem of matchmaking from
a knowledge representation perspective, with a formalization based on
Description Logics. We devise Concept Abduction and Concept Contraction as
non-monotonic inferences in Description Logics suitable for modeling
matchmaking in a logical framework, and prove some related complexity results.
We also present reasonable algorithms for semantic matchmaking based on the
devised inferences, and prove that they obey to some commonsense properties.
Finally, we report on the implementation of the proposed matchmaking framework,
which has been used both as a mediator in e-marketplaces and for semantic web
services discovery.
","Semantic Matchmaking as Non-Monotonic Reasoning: A Description Logic
  Approach","T. Di Noia, E. Di Sciascio, F. M. Donini",2007,Artificial Intelligence,
"  Solution-Guided Multi-Point Constructive Search (SGMPCS) is a novel
constructive search technique that performs a series of resource-limited tree
searches where each search begins either from an empty solution (as in
randomized restart) or from a solution that has been encountered during the
search. A small number of these ""elite solutions is maintained during the
search. We introduce the technique and perform three sets of experiments on the
job shop scheduling problem. First, a systematic, fully crossed study of SGMPCS
is carried out to evaluate the performance impact of various parameter
settings. Second, we inquire into the diversity of the elite solution set,
showing, contrary to expectations, that a less diverse set leads to stronger
performance. Finally, we compare the best parameter setting of SGMPCS from the
first two experiments to chronological backtracking, limited discrepancy
search, randomized restart, and a sophisticated tabu search algorithm on a set
of well-known benchmark problems. Results demonstrate that SGMPCS is
significantly better than the other constructive techniques tested, though lags
behind the tabu search.
",Solution-Guided Multi-Point Constructive Search for Job Shop Scheduling,J. C. Beck,2007,Artificial Intelligence,
"  There has been a noticeable shift in the relative composition of the industry
in the developed countries in recent years; manufacturing is decreasing while
the service sector is becoming more important. However, currently most
simulation models for investigating service systems are still built in the same
way as manufacturing simulation models, using a process-oriented world view,
i.e. they model the flow of passive entities through a system. These kinds of
models allow studying aspects of operational management but are not well suited
for studying the dynamics that appear in service systems due to human
behaviour. For these kinds of studies we require tools that allow modelling the
system and entities using an object-oriented world view, where intelligent
objects serve as abstract ""actors"" that are goal directed and can behave
proactively. In our work we combine process-oriented discrete event simulation
modelling and object-oriented agent based simulation modelling to investigate
the impact of people management practices on retail productivity. In this
paper, we reveal in a series of experiments what impact considering proactivity
can have on the output accuracy of simulation models of human centric systems.
The model and data we use for this investigation are based on a case study in a
UK department store. We show that considering proactivity positively influences
the validity of these kinds of models and therefore allows analysts to make
better recommendations regarding strategies to apply people management
practises.
","A First Approach on Modelling Staff Proactiveness in Retail Simulation
  Models","Peer-Olaf Siebers, Uwe Aickelin",2011,Artificial Intelligence,
"  For some computational problems (e.g., product configuration, planning,
diagnosis, query answering, phylogeny reconstruction) computing a set of
similar/diverse solutions may be desirable for better decision-making. With
this motivation, we studied several decision/optimization versions of this
problem in the context of Answer Set Programming (ASP), analyzed their
computational complexity, and introduced offline/online methods to compute
similar/diverse solutions of such computational problems with respect to a
given distance function. All these methods rely on the idea of computing
solutions to a problem by means of finding the answer sets for an ASP program
that describes the problem. The offline methods compute all solutions in
advance using the ASP formulation of the problem with an ASP solver, like
Clasp, and then identify similar/diverse solutions using clustering methods.
The online methods compute similar/diverse solutions following one of the three
approaches: by reformulating the ASP representation of the problem to compute
similar/diverse solutions at once using an ASP solver; by computing
similar/diverse solutions iteratively (one after other) using an ASP solver; by
modifying the search algorithm of an ASP solver to compute similar/diverse
solutions incrementally. We modified Clasp to implement the last online method
and called it Clasp-NK. In the first two online methods, the given distance
function is represented in ASP; in the last one it is implemented in C++. We
showed the applicability and the effectiveness of these methods on
reconstruction of similar/diverse phylogenies for Indo-European languages, and
on several planning problems in Blocks World. We observed that in terms of
computational efficiency the last online method outperforms the others; also it
allows us to compute similar/diverse solutions when the distance function
cannot be represented in ASP.
",Finding Similar/Diverse Solutions in Answer Set Programming,Thomas Eiter and Esra Erdem and Halit Erdogan and Michael Fink,2013,Artificial Intelligence,
"  We propose a structured approach to the problem of retrieval of images by
content and present a description logic that has been devised for the semantic
indexing and retrieval of images containing complex objects. As other
approaches do, we start from low-level features extracted with image analysis
to detect and characterize regions in an image. However, in contrast with
feature-based approaches, we provide a syntax to describe segmented regions as
basic objects and complex objects as compositions of basic ones. Then we
introduce a companion extensional semantics for defining reasoning services,
such as retrieval, classification, and subsumption. These services can be used
for both exact and approximate matching, using similarity measures. Using our
logical approach as a formal specification, we implemented a complete
client-server image retrieval system, which allows a user to pose both queries
by sketch and queries by example. A set of experiments has been carried out on
a testbed of images to assess the retrieval capabilities of the system in
comparison with expert users ranking. Results are presented adopting a
well-established measure of quality borrowed from textual information
retrieval.
",Structured Knowledge Representation for Image Retrieval,"E. Di Sciascio, F. M. Donini, M. Mongiello",2002,Artificial Intelligence,
"  Software agents can be used to automate many of the tedious, time-consuming
information processing tasks that humans currently have to complete manually.
However, to do so, agent plans must be capable of representing the myriad of
actions and control flows required to perform those tasks. In addition, since
these tasks can require integrating multiple sources of remote information ?
typically, a slow, I/O-bound process ? it is desirable to make execution as
efficient as possible. To address both of these needs, we present a flexible
software agent plan language and a highly parallel execution system that enable
the efficient execution of expressive agent plans. The plan language allows
complex tasks to be more easily expressed by providing a variety of operators
for flexibly processing the data as well as supporting subplans (for
modularity) and recursion (for indeterminate looping). The executor is based on
a streaming dataflow model of execution to maximize the amount of operator and
data parallelism possible at runtime. We have implemented both the language and
executor in a system called THESEUS. Our results from testing THESEUS show that
streaming dataflow execution can yield significant speedups over both
traditional serial (von Neumann) as well as non-streaming dataflow-style
execution that existing software and robot agent execution systems currently
support. In addition, we show how plans written in the language we present can
represent certain types of subtasks that cannot be accomplished using the
languages supported by network query engines. Finally, we demonstrate that the
increased expressivity of our plan language does not hamper performance;
specifically, we show how data can be integrated from multiple remote sources
just as efficiently using our architecture as is possible with a
state-of-the-art streaming-dataflow network query engine.
","An Expressive Language and Efficient Execution System for Software
  Agents","G. Barish, C. A. Knoblock",2005,Artificial Intelligence,
"  This paper studies the problem of learning diagnostic policies from training
examples. A diagnostic policy is a complete description of the decision-making
actions of a diagnostician (i.e., tests followed by a diagnostic decision) for
all possible combinations of test results. An optimal diagnostic policy is one
that minimizes the expected total cost, which is the sum of measurement costs
and misdiagnosis costs. In most diagnostic settings, there is a tradeoff
between these two kinds of costs. This paper formalizes diagnostic decision
making as a Markov Decision Process (MDP). The paper introduces a new family of
systematic search algorithms based on the AO* algorithm to solve this MDP. To
make AO* efficient, the paper describes an admissible heuristic that enables
AO* to prune large parts of the search space. The paper also introduces several
greedy algorithms including some improvements over previously-published
methods. The paper then addresses the question of learning diagnostic policies
from examples. When the probabilities of diseases and test results are computed
from training data, there is a great danger of overfitting. To reduce
overfitting, regularizers are integrated into the search algorithms. Finally,
the paper compares the proposed methods on five benchmark diagnostic data sets.
The studies show that in most cases the systematic search methods produce
better diagnostic policies than the greedy methods. In addition, the studies
show that for training sets of realistic size, the systematic search algorithms
are practical on todays desktop computers.
","Integrating Learning from Examples into the Search for Diagnostic
  Policies","V. Bayer-Zubek, T. G. Dietterich",2005,Artificial Intelligence,
"  Variable elimination is a general technique for constraint processing. It is
often discarded because of its high space complexity. However, it can be
extremely useful when combined with other techniques. In this paper we study
the applicability of variable elimination to the challenging problem of finding
still-lifes. We illustrate several alternatives: variable elimination as a
stand-alone algorithm, interleaved with search, and as a source of good quality
lower bounds. We show that these techniques are the best known option both
theoretically and empirically. In our experiments we have been able to solve
the n=20 instance, which is far beyond reach with alternative approaches.
","On the Practical use of Variable Elimination in Constraint Optimization
  Problems: 'Still-life' as a Case Study","J. Larrosa, E. Morancho, D. Niso",2005,Artificial Intelligence,
"  This is the second of three planned papers describing ZAP, a satisfiability
engine that substantially generalizes existing tools while retaining the
performance characteristics of modern high performance solvers. The fundamental
idea underlying ZAP is that many problems passed to such engines contain rich
internal structure that is obscured by the Boolean representation used; our
goal is to define a representation in which this structure is apparent and can
easily be exploited to improve computational performance. This paper presents
the theoretical basis for the ideas underlying ZAP, arguing that existing ideas
in this area exploit a single, recurring structure in that multiple database
axioms can be obtained by operating on a single axiom using a subgroup of the
group of permutations on the literals in the problem. We argue that the group
structure precisely captures the general structure at which earlier approaches
hinted, and give numerous examples of its use. We go on to extend the
Davis-Putnam-Logemann-Loveland inference procedure to this broader setting, and
show that earlier computational improvements are either subsumed or left intact
by the new method. The third paper in this series discusses ZAPs implementation
and presents experimental performance results.
",Generalizing Boolean Satisfiability II: Theory,"H. E. Dixon, M. L. Ginsberg, E. M. Luks, A. J. Parkes",2004,Artificial Intelligence,
"  This paper extends the framework of partially observable Markov decision
processes (POMDPs) to multi-agent settings by incorporating the notion of agent
models into the state space. Agents maintain beliefs over physical states of
the environment and over models of other agents, and they use Bayesian updates
to maintain their beliefs over time. The solutions map belief states to
actions. Models of other agents may include their belief states and are related
to agent types considered in games of incomplete information. We express the
agents autonomy by postulating that their models are not directly manipulable
or observable by other agents. We show that important properties of POMDPs,
such as convergence of value iteration, the rate of convergence, and piece-wise
linearity and convexity of the value functions carry over to our framework. Our
approach complements a more traditional approach to interactive settings which
uses Nash equilibria as a solution paradigm. We seek to avoid some of the
drawbacks of equilibria which may be non-unique and do not capture
off-equilibrium behaviors. We do so at the cost of having to represent, process
and continuously revise models of other agents. Since the agents beliefs may be
arbitrarily nested, the optimal solutions to decision making problems are only
asymptotically computable. However, approximate belief updates and
approximately optimal plans are computable. We illustrate our framework using a
simple application domain, and we show examples of belief updates and value
functions.
",A Framework for Sequential Planning in Multi-Agent Settings,"P. Doshi, P. J. Gmytrasiewicz",2005,Artificial Intelligence,
"  Stochastic processes that involve the creation of objects and relations over
time are widespread, but relatively poorly studied. For example, accurate fault
diagnosis in factory assembly processes requires inferring the probabilities of
erroneous assembly operations, but doing this efficiently and accurately is
difficult. Modeled as dynamic Bayesian networks, these processes have discrete
variables with very large domains and extremely high dimensionality. In this
paper, we introduce relational dynamic Bayesian networks (RDBNs), which are an
extension of dynamic Bayesian networks (DBNs) to first-order logic. RDBNs are a
generalization of dynamic probabilistic relational models (DPRMs), which we had
proposed in our previous work to model dynamic uncertain domains. We first
extend the Rao-Blackwellised particle filtering described in our earlier work
to RDBNs. Next, we lift the assumptions associated with Rao-Blackwellization in
RDBNs and propose two new forms of particle filtering. The first one uses
abstraction hierarchies over the predicates to smooth the particle filters
estimates. The second employs kernel density estimation with a kernel function
specifically designed for relational domains. Experiments show these two
methods greatly outperform standard particle filtering on the task of assembly
plan execution monitoring.
",Relational Dynamic Bayesian Networks,"P. Domingos, S. Sanghai, D. Weld",2005,Artificial Intelligence,
"  We present a uniform non-monotonic solution to the problems of reasoning
about action on the basis of an argumentation-theoretic approach. Our theory is
provably correct relative to a sensible minimisation policy introduced on top
of a temporal propositional logic. Sophisticated problem domains can be
formalised in our framework. As much attention of researchers in the field has
been paid to the traditional and basic problems in reasoning about actions such
as the frame, the qualification and the ramification problems, approaches to
these problems within our formalisation lie at heart of the expositions
presented in this paper.
",Reasoning about Action: An Argumentation - Theoretic Approach,"N. Y. Foo, Q. B. Vo",2005,Artificial Intelligence,
"  In this paper we present a new approach to modeling finite set domain
constraint problems using Reduced Ordered Binary Decision Diagrams (ROBDDs). We
show that it is possible to construct an efficient set domain propagator which
compactly represents many set domains and set constraints using ROBDDs. We
demonstrate that the ROBDD-based approach provides unprecedented flexibility in
modeling constraint satisfaction problems, leading to performance improvements.
We also show that the ROBDD-based modeling approach can be extended to the
modeling of integer and multiset constraint problems in a straightforward
manner. Since domain propagation is not always practical, we also show how to
incorporate less strict consistency notions into the ROBDD framework, such as
set bounds, cardinality bounds and lexicographic bounds consistency. Finally,
we present experimental results that demonstrate the ROBDD-based solver
performs better than various more conventional constraint solvers on several
standard set constraint problems.
",Solving Set Constraint Satisfaction Problems using ROBDDs,"P. J. Hawkins, V. Lagoon, P. J. Stuckey",2005,Artificial Intelligence,
"  We present a novel approach to the automatic acquisition of taxonomies or
concept hierarchies from a text corpus. The approach is based on Formal Concept
Analysis (FCA), a method mainly used for the analysis of data, i.e. for
investigating and processing explicitly given information. We follow Harris
distributional hypothesis and model the context of a certain term as a vector
representing syntactic dependencies which are automatically acquired from the
text corpus with a linguistic parser. On the basis of this context information,
FCA produces a lattice that we convert into a special kind of partial order
constituting a concept hierarchy. The approach is evaluated by comparing the
resulting concept hierarchies with hand-crafted taxonomies for two domains:
tourism and finance. We also directly compare our approach with hierarchical
agglomerative clustering as well as with Bi-Section-KMeans as an instance of a
divisive clustering algorithm. Furthermore, we investigate the impact of using
different measures weighting the contribution of each attribute as well as of
applying a particular smoothing technique to cope with data sparseness.
","Learning Concept Hierarchies from Text Corpora using Formal Concept
  Analysis","P. Cimiano, A. Hotho, S. Staab",2005,Artificial Intelligence,
"  This is the third of three papers describing ZAP, a satisfiability engine
that substantially generalizes existing tools while retaining the performance
characteristics of modern high-performance solvers. The fundamental idea
underlying ZAP is that many problems passed to such engines contain rich
internal structure that is obscured by the Boolean representation used; our
goal has been to define a representation in which this structure is apparent
and can be exploited to improve computational performance. The first paper
surveyed existing work that (knowingly or not) exploited problem structure to
improve the performance of satisfiability engines, and the second paper showed
that this structure could be understood in terms of groups of permutations
acting on individual clauses in any particular Boolean theory. We conclude the
series by discussing the techniques needed to implement our ideas, and by
reporting on their performance on a variety of problem instances.
",Generalizing Boolean Satisfiability III: Implementation,"H. E. Dixon, M. L. Ginsberg, D. Hofer, E. M. Luks, A. J. Parkes",2005,Artificial Intelligence,
"  When dealing with incomplete data in statistical learning, or incomplete
observations in probabilistic inference, one needs to distinguish the fact that
a certain event is observed from the fact that the observed event has happened.
Since the modeling and computational complexities entailed by maintaining this
proper distinction are often prohibitive, one asks for conditions under which
it can be safely ignored. Such conditions are given by the missing at random
(mar) and coarsened at random (car) assumptions. In this paper we provide an
in-depth analysis of several questions relating to mar/car assumptions. Main
purpose of our study is to provide criteria by which one may evaluate whether a
car assumption is reasonable for a particular data collecting or observational
process. This question is complicated by the fact that several distinct
versions of mar/car assumptions exist. We therefore first provide an overview
over these different versions, in which we highlight the distinction between
distributional and coarsening variable induced versions. We show that
distributional versions are less restrictive and sufficient for most
applications. We then address from two different perspectives the question of
when the mar/car assumption is warranted. First we provide a static analysis
that characterizes the admissibility of the car assumption in terms of the
support structure of the joint probability distribution of complete data and
incomplete observations. Here we obtain an equivalence characterization that
improves and extends a recent result by Grunwald and Halpern. We then turn to a
procedural analysis that characterizes the admissibility of the car assumption
in terms of procedural models for the actual data (or observation) generating
process. The main result of this analysis is that the stronger coarsened
completely at random (ccar) condition is arguably the most reasonable
assumption, as it alone corresponds to data coarsening procedures that satisfy
a natural robustness property.
",Ignorability in Statistical and Probabilistic Inference,M. Jaeger,2005,Artificial Intelligence,
"  Partially observable Markov decision processes (POMDPs) form an attractive
and principled framework for agent planning under uncertainty. Point-based
approximate techniques for POMDPs compute a policy based on a finite set of
points collected in advance from the agents belief space. We present a
randomized point-based value iteration algorithm called Perseus. The algorithm
performs approximate value backup stages, ensuring that in each backup stage
the value of each point in the belief set is improved; the key observation is
that a single backup may improve the value of many belief points. Contrary to
other point-based methods, Perseus backs up only a (randomly selected) subset
of points in the belief set, sufficient for improving the value of each belief
point in the set. We show how the same idea can be extended to dealing with
continuous action spaces. Experimental results show the potential of Perseus in
large scale POMDP problems.
",Perseus: Randomized Point-based Value Iteration for POMDPs,"M. T.J. Spaan, N. Vlassis",2005,Artificial Intelligence,
"  Logical hidden Markov models (LOHMMs) upgrade traditional hidden Markov
models to deal with sequences of structured symbols in the form of logical
atoms, rather than flat characters.
  This note formally introduces LOHMMs and presents solutions to the three
central inference problems for LOHMMs: evaluation, most likely hidden state
sequence and parameter estimation. The resulting representation and algorithms
are experimentally evaluated on problems from the domain of bioinformatics.
",Logical Hidden Markov Models,"L. De Raedt, K. Kersting, T. Raiko",2006,Artificial Intelligence,
"  We describe the version of the GPT planner used in the probabilistic track of
the 4th International Planning Competition (IPC-4). This version, called mGPT,
solves Markov Decision Processes specified in the PPDDL language by extracting
and using different classes of lower bounds along with various heuristic-search
algorithms. The lower bounds are extracted from deterministic relaxations where
the alternative probabilistic effects of an action are mapped into different,
independent, deterministic actions. The heuristic-search algorithms use these
lower bounds for focusing the updates and delivering a consistent value
function over all states reachable from the initial state and the greedy
policy.
",mGPT: A Probabilistic Planner Based on Heuristic Search,"B. Bonet, H. Geffner",2005,Artificial Intelligence,
"  Despite recent progress in AI planning, many benchmarks remain challenging
for current planners. In many domains, the performance of a planner can greatly
be improved by discovering and exploiting information about the domain
structure that is not explicitly encoded in the initial PDDL formulation. In
this paper we present and compare two automated methods that learn relevant
information from previous experience in a domain and use it to solve new
problem instances. Our methods share a common four-step strategy. First, a
domain is analyzed and structural information is extracted, then
macro-operators are generated based on the previously discovered structure. A
filtering and ranking procedure selects the most useful macro-operators.
Finally, the selected macros are used to speed up future searches. We have
successfully used such an approach in the fourth international planning
competition IPC-4. Our system, Macro-FF, extends Hoffmanns state-of-the-art
planner FF 2.3 with support for two kinds of macro-operators, and with
engineering enhancements. We demonstrate the effectiveness of our ideas on
benchmarks from international planning competitions. Our results indicate a
large reduction in search effort in those complex domains where structural
information can be inferred.
","Macro-FF: Improving AI Planning with Automatically Learned
  Macro-Operators","A. Botea, M. Enzenberger, M. Mueller, J. Schaeffer",2005,Artificial Intelligence,
"  The Optiplan planning system is the first integer programming-based planner
that successfully participated in the international planning competition. This
engineering note describes the architecture of Optiplan and provides the
integer programming formulation that enabled it to perform reasonably well in
the competition. We also touch upon some recent developments that make integer
programming encodings significantly more competitive.
",Optiplan: Unifying IP-based and Graph-based Planning,"S. Kambhampati, M.H.L. van den Briel",2005,Artificial Intelligence,
"  We study an approach to policy selection for large relational Markov Decision
Processes (MDPs). We consider a variant of approximate policy iteration (API)
that replaces the usual value-function learning step with a learning step in
policy space. This is advantageous in domains where good policies are easier to
represent and learn than the corresponding value functions, which is often the
case for the relational MDPs we are interested in. In order to apply API to
such problems, we introduce a relational policy language and corresponding
learner. In addition, we introduce a new bootstrapping routine for goal-based
planning domains, based on random walks. Such bootstrapping is necessary for
many large relational MDPs, where reward is extremely sparse, as API is
ineffective in such domains when initialized with an uninformed policy. Our
experiments show that the resulting system is able to find good policies for a
number of classical planning domains and their stochastic variants by solving
them as extremely large relational MDPs. The experiments also point to some
limitations of our approach, suggesting future work.
","Approximate Policy Iteration with a Policy Language Bias: Solving
  Relational Markov Decision Processes","A. Fern, R. Givan, S. Yoon",2006,Artificial Intelligence,
"  Tabu search is one of the most effective heuristics for locating high-quality
solutions to a diverse array of NP-hard combinatorial optimization problems.
Despite the widespread success of tabu search, researchers have a poor
understanding of many key theoretical aspects of this algorithm, including
models of the high-level run-time dynamics and identification of those search
space features that influence problem difficulty. We consider these questions
in the context of the job-shop scheduling problem (JSP), a domain where tabu
search algorithms have been shown to be remarkably effective. Previously, we
demonstrated that the mean distance between random local optima and the nearest
optimal solution is highly correlated with problem difficulty for a well-known
tabu search algorithm for the JSP introduced by Taillard. In this paper, we
discuss various shortcomings of this measure and develop a new model of problem
difficulty that corrects these deficiencies. We show that Taillards algorithm
can be modeled with high fidelity as a simple variant of a straightforward
random walk. The random walk model accounts for nearly all of the variability
in the cost required to locate both optimal and sub-optimal solutions to random
JSPs, and provides an explanation for differences in the difficulty of random
versus structured JSPs. Finally, we discuss and empirically substantiate two
novel predictions regarding tabu search algorithm behavior. First, the method
for constructing the initial solution is highly unlikely to impact the
performance of tabu search. Second, tabu tenure should be selected to be as
small as possible while simultaneously avoiding search stagnation; values
larger than necessary lead to significant degradations in performance.
","Linking Search Space Structure, Run-Time Dynamics, and Problem
  Difficulty: A Step Toward Demystifying Tabu Search","A. E. Howe, J. P. Watson, L. D. Whitley",2005,Artificial Intelligence,
"  Code optimization and high level synthesis can be posed as constraint
satisfaction and optimization problems, such as graph coloring used in register
allocation. Graph coloring is also used to model more traditional CSPs relevant
to AI, such as planning, time-tabling and scheduling. Provably optimal
solutions may be desirable for commercial and defense applications.
Additionally, for applications such as register allocation and code
optimization, naturally-occurring instances of graph coloring are often small
and can be solved optimally. A recent wave of improvements in algorithms for
Boolean satisfiability (SAT) and 0-1 Integer Linear Programming (ILP) suggests
generic problem-reduction methods, rather than problem-specific heuristics,
because (1) heuristics may be upset by new constraints, (2) heuristics tend to
ignore structure, and (3) many relevant problems are provably inapproximable.
  Problem reductions often lead to highly symmetric SAT instances, and
symmetries are known to slow down SAT solvers. In this work, we compare several
avenues for symmetry breaking, in particular when certain kinds of symmetry are
present in all generated instances. Our focus on reducing CSPs to SAT allows us
to leverage recent dramatic improvement in SAT solvers and automatically
benefit from future progress. We can use a variety of black-box SAT solvers
without modifying their source code because our symmetry-breaking techniques
are static, i.e., we detect symmetries and add symmetry breaking predicates
(SBPs) during pre-processing.
  An important result of our work is that among the types of
instance-independent SBPs we studied and their combinations, the simplest and
least complete constructions are the most effective. Our experiments also
clearly indicate that instance-independent symmetries should mostly be
processed together with instance-specific symmetries rather than at the
specification level, contrary to what has been suggested in the literature.
",Breaking Instance-Independent Symmetries In Exact Graph Coloring,"F. A. Aloul, I. L. Markov, A. Ramani, K. A. Sakallah",2006,Artificial Intelligence,
"  A decision process in which rewards depend on history rather than merely on
the current state is called a decision process with non-Markovian rewards
(NMRDP). In decision-theoretic planning, where many desirable behaviours are
more naturally expressed as properties of execution sequences rather than as
properties of states, NMRDPs form a more natural model than the commonly
adopted fully Markovian decision process (MDP) model. While the more tractable
solution methods developed for MDPs do not directly apply in the presence of
non-Markovian rewards, a number of solution methods for NMRDPs have been
proposed in the literature. These all exploit a compact specification of the
non-Markovian reward function in temporal logic, to automatically translate the
NMRDP into an equivalent MDP which is solved using efficient MDP solution
methods. This paper presents NMRDPP (Non-Markovian Reward Decision Process
Planner), a software platform for the development and experimentation of
methods for decision-theoretic planning with non-Markovian rewards. The current
version of NMRDPP implements, under a single interface, a family of methods
based on existing as well as new approaches which we describe in detail. These
include dynamic programming, heuristic search, and structured methods. Using
NMRDPP, we compare the methods and identify certain problem features that
affect their performance. NMRDPPs treatment of non-Markovian rewards is
inspired by the treatment of domain-specific search control knowledge in the
TLPlan planner, which it incorporates as a special case. In the First
International Probabilistic Planning Competition, NMRDPP was able to compete
and perform well in both the domain-independent and hand-coded tracks, using
search control knowledge in the latter.
",Decision-Theoretic Planning with non-Markovian Rewards,"C. Gretton, F. Kabanza, D. Price, J. Slaney, S. Thiebaux",2006,Artificial Intelligence,
"  This paper presents an optimal Bangla Keyboard Layout, which distributes the
load equally on both hands so that maximizing the ease and minimizing the
effort. Bangla alphabet has a large number of letters, for this it is difficult
to type faster using Bangla keyboard. Our proposed keyboard will maximize the
speed of operator as they can type with both hands parallel. Here we use the
association rule of data mining to distribute the Bangla characters in the
keyboard. First, we analyze the frequencies of data consisting of monograph,
digraph and trigraph, which are derived from data wire-house, and then used
association rule of data mining to distribute the Bangla characters in the
layout. Experimental results on several data show the effectiveness of the
proposed approach with better performance.
",Optimal Bangla Keyboard Layout using Data Mining Technique,"S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, and
  Md. Mahadi Hassan",2005,Artificial Intelligence,
"  Bangla alphabet has a large number of letters, for this it is complicated to
type faster using Bangla keyboard. The proposed keyboard will maximize the
speed of operator as they can type with both hands parallel. Association rule
of data mining to distribute the Bangla characters in the keyboard is used
here. The frequencies of data consisting of monograph, digraph and trigraph are
analyzed, which are derived from data wire-house, and then used association
rule of data mining to distribute the Bangla characters in the layout.
Experimental results on several data show the effectiveness of the proposed
approach with better performance. This paper presents an optimal Bangla
Keyboard Layout, which distributes the load equally on both hands so that
maximizing the ease and minimizing the effort.
",The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique,"Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, and S. M.
  Kamruzzaman",2007,Artificial Intelligence,
"  Human can be distinguished by different limb movements and unique ground
reaction force. Cumulative foot pressure image is a 2-D cumulative ground
reaction force during one gait cycle. Although it contains pressure spatial
distribution information and pressure temporal distribution information, it
suffers from several problems including different shoes and noise, when putting
it into practice as a new biometric for pedestrian identification. In this
paper, we propose a hierarchical translation-invariant representation for
cumulative foot pressure images, inspired by the success of Convolutional deep
belief network for digital classification. Key contribution in our approach is
discriminative hierarchical sparse coding scheme which helps to learn useful
discriminative high-level visual features. Based on the feature representation
of cumulative foot pressure images, we develop a pedestrian recognition system
which is invariant to three different shoes and slight local shape change.
Experiments are conducted on a proposed open dataset that contains more than
2800 cumulative foot pressure images from 118 subjects. Evaluations suggest the
effectiveness of the proposed method and the potential of cumulative foot
pressure images as a biometric.
",Translation-Invariant Representation for Cumulative Foot Pressure Images,Shuai Zheng and Kaiqi Huang and Tieniu Tan,2010,Artificial Intelligence,
"  The task of verifying the compatibility between interacting web services has
traditionally been limited to checking the compatibility of the interaction
protocol in terms of message sequences and the type of data being exchanged.
Since web services are developed largely in an uncoordinated way, different
services often use independently developed ontologies for the same domain
instead of adhering to a single ontology as standard. In this work we
investigate the approaches that can be taken by the server to verify the
possibility to reach a state with semantically inconsistent results during the
execution of a protocol with a client, if the client ontology is published.
Often database is used to store the actual data along with the ontologies
instead of storing the actual data as a part of the ontology description. It is
important to observe that at the current state of the database the semantic
conflict state may not be reached even if the verification done by the server
indicates the possibility of reaching a conflict state. A relational algebra
based decision procedure is also developed to incorporate the current state of
the client and the server databases in the overall verification procedure.
","Detecting Ontological Conflicts in Protocols between Semantic Web
  Services",Priyankar Ghosh and Pallab Dasgupta,2010,Artificial Intelligence,
"  Our work has focused on support for film or television scriptwriting. Since
this involves potentially varied story-lines, we note the implicit or latent
support for interactivity. Furthermore the film, television, games, publishing
and other sectors are converging, so that cross-over and re-use of one form of
product in another of these sectors is ever more common. Technically our work
has been largely based on mathematical algorithms for data clustering and
display. Operationally, we also discuss how our algorithms can support
collective, distributed problem-solving.
","New Methods of Analysis of Narrative and Semantics in Support of
  Interactivity","Fionn Murtagh, Adam Ganz and Joe Reddington",2011,Artificial Intelligence,
"  The paper investigates a novel approach, based on Constraint Logic
Programming (CLP), to predict the 3D conformation of a protein via fragments
assembly. The fragments are extracted by a preprocessor-also developed for this
work- from a database of known protein structures that clusters and classifies
the fragments according to similarity and frequency. The problem of assembling
fragments into a complete conformation is mapped to a constraint solving
problem and solved using CLP. The constraint-based model uses a medium
discretization degree Ca-side chain centroid protein model that offers
efficiency and a good approximation for space filling. The approach adapts
existing energy models to the protein representation used and applies a large
neighboring search strategy. The results shows the feasibility and efficiency
of the method. The declarative nature of the solution allows to include future
extensions, e.g., different size fragments for better accuracy.
",CLP-based protein fragment assembly,"Alessandro Dal Palu', Agostino Dovier, Federico Fogolari, Enrico
  Pontelli",2010,Artificial Intelligence,
"  The two standard branching schemes for CSPs are d-way and 2-way branching.
Although it has been shown that in theory the latter can be exponentially more
effective than the former, there is a lack of empirical evidence showing such
differences. To investigate this, we initially make an experimental comparison
of the two branching schemes over a wide range of benchmarks. Experimental
results verify the theoretical gap between d-way and 2-way branching as we move
from a simple variable ordering heuristic like dom to more sophisticated ones
like dom/ddeg. However, perhaps surprisingly, experiments also show that when
state-of-the-art variable ordering heuristics like dom/wdeg are used then d-way
can be clearly more efficient than 2-way branching in many cases. Motivated by
this observation, we develop two generic heuristics that can be applied at
certain points during search to decide whether 2-way branching or a restricted
version of 2-way branching, which is close to d-way branching, will be
followed. The application of these heuristics results in an adaptive branching
scheme. Experiments with instantiations of the two generic heuristics confirm
that search with adaptive branching outperforms search with a fixed branching
scheme on a wide range of problems.
",Adaptive Branching for Constraint Satisfaction Problems,Thanasis Balafoutis and Kostas Stergiou,2010,Artificial Intelligence,
"  In order to study the communication between information systems, Gong and
Xiao [Z. Gong and Z. Xiao, Communicating between information systems based on
including degrees, International Journal of General Systems 39 (2010) 189--206]
proposed the concept of general relation mappings based on including degrees.
Some properties and the extension for fuzzy information systems of the general
relation mappings have been investigated there. In this paper, we point out by
counterexamples that several assertions (Lemma 3.1, Lemma 3.2, Theorem 4.1, and
Theorem 4.3) in the aforementioned work are not true in general.
","A note on communicating between information systems based on including
  degrees",Ping Zhu and Qiaoyan Wen,2011,Artificial Intelligence,
"  In this paper we introduce a novel method for automatically tuning the search
parameters of a chess program using genetic algorithms. Our results show that a
large set of parameter values can be learned automatically, such that the
resulting performance is comparable with that of manually tuned parameters of
top tournament-playing chess programs.
",Optimizing Selective Search in Chess,"Omid David-Tabibi, Moshe Koppel, Nathan S. Netanyahu",2010,Artificial Intelligence,
"  In this paper we present an optimal Bangla Keyboard Layout, which distributes
the load equally on both hands so that maximizing the ease and minimizing the
effort. Bangla alphabet has a large number of letters, for this it is difficult
to type faster using Bangla keyboard. Our proposed keyboard will maximize the
speed of operator as they can type with both hands parallel. Here we use the
association rule of data mining to distribute the Bangla characters in the
keyboard. First, we analyze the frequencies of data consisting of monograph,
digraph and trigraph, which are derived from data wire-house, and then used
association rule of data mining to distribute the Bangla characters in the
layout. Finally, we propose a Bangla Keyboard Layout. Experimental results on
several keyboard layout shows the effectiveness of the proposed approach with
better performance.
",Optimal Bangla Keyboard Layout using Association Rule of Data Mining,"Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan,
  and S. M. Kamruzzaman",2004,Artificial Intelligence,
"  This paper discusses a system that accelerates reinforcement learning by
using transfer from related tasks. Without such transfer, even if two tasks are
very similar at some abstract level, an extensive re-learning effort is
required. The system achieves much of its power by transferring parts of
previously learned solutions rather than a single complete solution. The system
exploits strong features in the multi-dimensional function produced by
reinforcement learning in solving a particular task. These features are stable
and easy to recognize early in the learning process. They generate a
partitioning of the state space and thus the function. The partition is
represented as a graph. This is used to index and compose functions stored in a
case base to form a close approximation to the solution of the new task.
Experiments demonstrate that function composition often produces more than an
order of magnitude increase in learning rate compared to a basic reinforcement
learning algorithm.
","Accelerating Reinforcement Learning by Composing Solutions of
  Automatically Identified Subtasks",C. Drummond,2002,Artificial Intelligence,
"  We propose a logical/mathematical framework for statistical parameter
learning of parameterized logic programs, i.e. definite clause programs
containing probabilistic facts with a parameterized distribution. It extends
the traditional least Herbrand model semantics in logic programming to
distribution semantics, possible world semantics with a probability
distribution which is unconditionally applicable to arbitrary logic programs
including ones for HMMs, PCFGs and Bayesian networks. We also propose a new EM
algorithm, the graphical EM algorithm, that runs for a class of parameterized
logic programs representing sequential decision processes where each decision
is exclusive and independent. It runs on a new data structure called support
graphs describing the logical relationship between observations and their
explanations, and learns parameters by computing inside and outside probability
generalized for logic programs. The complexity analysis shows that when
combined with OLDT search for all explanations for observations, the graphical
EM algorithm, despite its generality, has the same time complexity as existing
EM algorithms, i.e. the Baum-Welch algorithm for HMMs, the Inside-Outside
algorithm for PCFGs, and the one for singly connected Bayesian networks that
have been developed independently in each research field. Learning experiments
with PCFGs using two corpora of moderate size indicate that the graphical EM
algorithm can significantly outperform the Inside-Outside algorithm.
",Parameter Learning of Logic Programs for Symbolic-Statistical Modeling,"T. Sato, Y. Kameya",2001,Artificial Intelligence,
"  I consider the problem of learning an optimal path graphical model from data
and show the problem to be NP-hard for the maximum likelihood and minimum
description length approaches and a Bayesian approach. This hardness result
holds despite the fact that the problem is a restriction of the polynomially
solvable problem of finding the optimal tree graphical model.
",Finding a Path is Harder than Finding a Tree,C. Meek,2001,Artificial Intelligence,
"  Simple conceptual graphs are considered as the kernel of most knowledge
representation formalisms built upon Sowa's model. Reasoning in this model can
be expressed by a graph homomorphism called projection, whose semantics is
usually given in terms of positive, conjunctive, existential FOL. We present
here a family of extensions of this model, based on rules and constraints,
keeping graph homomorphism as the basic operation. We focus on the formal
definitions of the different models obtained, including their operational
semantics and relationships with FOL, and we analyze the decidability and
complexity of the associated problems (consistency and deduction). As soon as
rules are involved in reasonings, these problems are not decidable, but we
exhibit a condition under which they fall in the polynomial hierarchy. These
results extend and complete the ones already published by the authors. Moreover
we systematically study the complexity of some particular cases obtained by
restricting the form of constraints and/or rules.
","Extensions of Simple Conceptual Graphs: the Complexity of Rules and
  Constraints","J. F. Baget, M. L. Mugnier",2002,Artificial Intelligence,
"  Fusions are a simple way of combining logics. For normal modal logics,
fusions have been investigated in detail. In particular, it is known that,
under certain conditions, decidability transfers from the component logics to
their fusion. Though description logics are closely related to modal logics,
they are not necessarily normal. In addition, ABox reasoning in description
logics is not covered by the results from modal logics. In this paper, we
extend the decidability transfer results from normal modal logics to a large
class of description logics. To cover different description logics in a uniform
way, we introduce abstract description systems, which can be seen as a common
generalization of description and modal logics, and show the transfer results
in this general setting.
",Fusions of Description Logics and Abstract Description Systems,"F. Baader, C. Lutz, H. Sturm, F. Wolter",2002,Artificial Intelligence,
"  Inductive logic programming, or relational learning, is a powerful paradigm
for machine learning or data mining. However, in order for ILP to become
practically useful, the efficiency of ILP systems must improve substantially.
To this end, the notion of a query pack is introduced: it structures sets of
similar queries. Furthermore, a mechanism is described for executing such query
packs. A complexity analysis shows that considerable efficiency improvements
can be achieved through the use of this query pack execution mechanism. This
claim is supported by empirical results obtained by incorporating support for
query pack execution in two existing learning systems.
","Improving the Efficiency of Inductive Logic Programming Through the Use
  of Query Packs","H. Blockeel, L. Dehaspe, B. Demoen, G. Janssens, J. Ramon, H.
  Vandecasteele",2002,Artificial Intelligence,
"  Recent trends in planning research have led to empirical comparison becoming
commonplace. The field has started to settle into a methodology for such
comparisons, which for obvious practical reasons requires running a subset of
planners on a subset of problems. In this paper, we characterize the
methodology and examine eight implicit assumptions about the problems, planners
and metrics used in many of these comparisons. The problem assumptions are:
PR1) the performance of a general purpose planner should not be
penalized/biased if executed on a sampling of problems and domains, PR2) minor
syntactic differences in representation do not affect performance, and PR3)
problems should be solvable by STRIPS capable planners unless they require ADL.
The planner assumptions are: PL1) the latest version of a planner is the best
one to use, PL2) default parameter settings approximate good performance, and
PL3) time cut-offs do not unduly bias outcome. The metrics assumptions are: M1)
performance degrades similarly for each planner when run on degraded runtime
environments (e.g., machine platform) and M2) the number of plan steps
distinguishes performance. We find that most of these assumptions are not
supported empirically; in particular, that planners are affected differently by
these assumptions. We conclude with a call to the community to devote research
resources to improving the state of the practice and especially to enhancing
the available benchmark problems.
",A Critical Assessment of Benchmark Comparison in Planning,"E. Dahlman, A. E. Howe",2002,Artificial Intelligence,
"  An approach to the construction of classifiers from imbalanced datasets is
described. A dataset is imbalanced if the classification categories are not
approximately equally represented. Often real-world data sets are predominately
composed of ""normal"" examples with only a small percentage of ""abnormal"" or
""interesting"" examples. It is also the case that the cost of misclassifying an
abnormal (interesting) example as a normal example is often much higher than
the cost of the reverse error. Under-sampling of the majority (normal) class
has been proposed as a good means of increasing the sensitivity of a classifier
to the minority class. This paper shows that a combination of our method of
over-sampling the minority (abnormal) class and under-sampling the majority
(normal) class can achieve better classifier performance (in ROC space) than
only under-sampling the majority class. This paper also shows that a
combination of our method of over-sampling the minority class and
under-sampling the majority class can achieve better classifier performance (in
ROC space) than varying the loss ratios in Ripper or class priors in Naive
Bayes. Our method of over-sampling the minority class involves creating
synthetic minority class examples. Experiments are performed using C4.5, Ripper
and a Naive Bayes classifier. The method is evaluated using the area under the
Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.
",SMOTE: Synthetic Minority Over-sampling Technique,"N. V. Chawla, K. W. Bowyer, L. O. Hall, W. P. Kegelmeyer",2002,Artificial Intelligence,
"  Common wisdom has it that small distinctions in the probabilities
(parameters) quantifying a belief network do not matter much for the results of
probabilistic queries. Yet, one can develop realistic scenarios under which
small variations in network parameters can lead to significant changes in
computed queries. A pending theoretical question is then to analytically
characterize parameter changes that do or do not matter. In this paper, we
study the sensitivity of probabilistic queries to changes in network parameters
and prove some tight bounds on the impact that such parameters can have on
queries. Our analytic results pinpoint some interesting situations under which
parameter changes do or do not matter. These results are important for
knowledge engineers as they help them identify influential network parameters.
They also help explain some of the previous experimental results and
observations with regards to network robustness against parameter changes.
",When do Numbers Really Matter?,"H. Chan, A. Darwiche",2002,Artificial Intelligence,
"  Recent years are seeing an increasing need for on-line monitoring of teams of
cooperating agents, e.g., for visualization, or performance tracking. However,
in monitoring deployed teams, we often cannot rely on the agents to always
communicate their state to the monitoring system. This paper presents a
non-intrusive approach to monitoring by 'overhearing', where the monitored
team's state is inferred (via plan-recognition) from team-members' routine
communications, exchanged as part of their coordinated task execution, and
observed (overheard) by the monitoring system. Key challenges in this approach
include the demanding run-time requirements of monitoring, the scarceness of
observations (increasing monitoring uncertainty), and the need to scale-up
monitoring to address potentially large teams. To address these, we present a
set of complementary novel techniques, exploiting knowledge of the social
structures and procedures in the monitored team: (i) an efficient probabilistic
plan-recognition algorithm, well-suited for processing communications as
observations; (ii) an approach to exploiting knowledge of the team's social
behavior to predict future observations during execution (reducing monitoring
uncertainty); and (iii) monitoring algorithms that trade expressivity for
scalability, representing only certain useful monitoring hypotheses, but
allowing for any number of agents and their different activities to be
represented in a single coherent entity. We present an empirical evaluation of
these techniques, in combination and apart, in monitoring a deployed team of
agents, running on machines physically distributed across the country, and
engaged in complex, dynamic task execution. We also compare the performance of
these techniques to human expert and novice monitors, and show that the
techniques presented are capable of monitoring at human-expert levels, despite
the difficulty of the task.
",Monitoring Teams by Overhearing: A Multi-Agent Plan-Recognition Approach,"G. A. Kaminka, D. V. Pynadath, M. Tambe",2002,Artificial Intelligence,
"  Spoken dialogue systems promise efficient and natural access to a large
variety of information sources and services from any phone. However, current
spoken dialogue systems are deficient in their strategies for preventing,
identifying and repairing problems that arise in the conversation. This paper
reports results on automatically training a Problematic Dialogue Predictor to
predict problematic human-computer dialogues using a corpus of 4692 dialogues
collected with the 'How May I Help You' (SM) spoken dialogue system. The
Problematic Dialogue Predictor can be immediately applied to the system's
decision of whether to transfer the call to a human customer care agent, or be
used as a cue to the system's dialogue manager to modify its behavior to repair
problems, and even perhaps, to prevent them. We show that a Problematic
Dialogue Predictor using automatically-obtainable features from the first two
exchanges in the dialogue can predict problematic dialogues 13.2% more
accurately than the baseline.
","Automatically Training a Problematic Dialogue Predictor for a Spoken
  Dialogue System","A. Gorin, I. Langkilde-Geary, M. A. Walker, J. Wright, H. Wright
  Hastie",2002,Artificial Intelligence,
"  Recent advances in the study of voting classification algorithms have brought
empirical and theoretical results clearly showing the discrimination power of
ensemble classifiers. It has been previously argued that the search of this
classification power in the design of the algorithms has marginalized the need
to obtain interpretable classifiers. Therefore, the question of whether one
might have to dispense with interpretability in order to keep classification
strength is being raised in a growing number of machine learning or data mining
papers. The purpose of this paper is to study both theoretically and
empirically the problem. First, we provide numerous results giving insight into
the hardness of the simplicity-accuracy tradeoff for voting classifiers. Then
we provide an efficient ""top-down and prune"" induction heuristic, WIDC, mainly
derived from recent results on the weak learning and boosting frameworks. It is
to our knowledge the first attempt to build a voting classifier as a base
formula using the weak learning framework (the one which was previously highly
successful for decision tree induction), and not the strong learning framework
(as usual for such classifiers with boosting-like approaches). While it uses a
well-known induction scheme previously successful in other classes of concept
representations, thus making it easy to implement and compare, WIDC also relies
on recent or new results we give about particular cases of boosting known as
partition boosting and ranking loss boosting. Experimental results on
thirty-one domains, most of which readily available, tend to display the
ability of WIDC to produce small, accurate, and interpretable decision
committees.
","Inducing Interpretable Voting Classifiers without Trading Accuracy for
  Simplicity: Theoretical Results, Approximation Algorithms",R. Nock,2002,Artificial Intelligence,
"  We propose a perspective on knowledge compilation which calls for analyzing
different compilation approaches according to two key dimensions: the
succinctness of the target compilation language, and the class of queries and
transformations that the language supports in polytime. We then provide a
knowledge compilation map, which analyzes a large number of existing target
compilation languages according to their succinctness and their polytime
transformations and queries. We argue that such analysis is necessary for
placing new compilation approaches within the context of existing ones. We also
go beyond classical, flat target compilation languages based on CNF and DNF,
and consider a richer, nested class based on directed acyclic graphs (such as
OBDDs), which we show to include a relatively large number of target
compilation languages.
",A Knowledge Compilation Map,"A. Darwiche, P. Marquis",2002,Artificial Intelligence,
"  The problem of organizing information for multidocument summarization so that
the generated summary is coherent has received relatively little attention.
While sentence ordering for single document summarization can be determined
from the ordering of sentences in the input article, this is not the case for
multidocument summarization where summary sentences may be drawn from different
input articles. In this paper, we propose a methodology for studying the
properties of ordering information in the news genre and describe experiments
done on a corpus of multiple acceptable orderings we developed for the task.
Based on these experiments, we implemented a strategy for ordering information
that combines constraints from chronological order of events and topical
relatedness. Evaluation of our augmented algorithm shows a significant
improvement of the ordering over two baseline strategies.
","Inferring Strategies for Sentence Ordering in Multidocument News
  Summarization","R. Barzilay, N. Elhadad",2002,Artificial Intelligence,
"  We consider the problem of designing the the utility functions of the
utility-maximizing agents in a multi-agent system so that they work
synergistically to maximize a global utility. The particular problem domain we
explore is the control of network routing by placing agents on all the routers
in the network. Conventional approaches to this task have the agents all use
the Ideal Shortest Path routing Algorithm (ISPA). We demonstrate that in many
cases, due to the side-effects of one agent's actions on another agent's
performance, having agents use ISPA's is suboptimal as far as global aggregate
cost is concerned, even when they are only used to route infinitesimally small
amounts of traffic. The utility functions of the individual agents are not
""aligned"" with the global utility, intuitively speaking. As a particular
example of this we present an instance of Braess' paradox in which adding new
links to a network whose agents all use the ISPA results in a decrease in
overall throughput. We also demonstrate that load-balancing, in which the
agents' decisions are collectively made to optimize the global cost incurred by
all traffic currently being routed, is suboptimal as far as global cost
averaged across time is concerned. This is also due to 'side-effects', in this
case of current routing decision on future traffic. The mathematics of
Collective Intelligence (COIN) is concerned precisely with the issue of
avoiding such deleterious side-effects in multi-agent systems, both over time
and space. We present key concepts from that mathematics and use them to derive
an algorithm whose ideal version should have better performance than that of
having all agents use the ISPA, even in the infinitesimal limit. We present
experiments verifying this, and also showing that a machine-learning-based
version of this COIN algorithm in which costs are only imprecisely estimated
via empirical means (a version potentially applicable in the real world) also
outperforms the ISPA, despite having access to less information than does the
ISPA. In particular, this COIN algorithm almost always avoids Braess' paradox.
","Collective Intelligence, Data Routing and Braess' Paradox","K. Tumer, D. H. Wolpert",2002,Artificial Intelligence,
"  This paper addresses the problem of planning under uncertainty in large
Markov Decision Processes (MDPs). Factored MDPs represent a complex state space
using state variables and the transition model using a dynamic Bayesian
network. This representation often allows an exponential reduction in the
representation size of structured MDPs, but the complexity of exact solution
algorithms for such MDPs can grow exponentially in the representation size. In
this paper, we present two approximate solution algorithms that exploit
structure in factored MDPs. Both use an approximate value function represented
as a linear combination of basis functions, where each basis function involves
only a small subset of the domain variables. A key contribution of this paper
is that it shows how the basic operations of both algorithms can be performed
efficiently in closed form, by exploiting both additive and context-specific
structure in a factored MDP. A central element of our algorithms is a novel
linear program decomposition technique, analogous to variable elimination in
Bayesian networks, which reduces an exponentially large LP to a provably
equivalent, polynomial-sized one. One algorithm uses approximate linear
programming, and the second approximate dynamic programming. Our dynamic
programming algorithm is novel in that it uses an approximation based on
max-norm, a technique that more directly minimizes the terms that appear in
error bounds for approximate MDP algorithms. We provide experimental results on
problems with over 10^40 states, demonstrating a promising indication of the
scalability of our approach, and compare our algorithm to an existing
state-of-the-art approach, showing, in some problems, exponential gains in
computation time.
",Efficient Solution Algorithms for Factored MDPs,"C. Guestrin, D. Koller, R. Parr, S. Venkataraman",2003,Artificial Intelligence,
"  The amount of information available on the Web grows at an incredible high
rate. Systems and procedures devised to extract these data from Web sources
already exist, and different approaches and techniques have been investigated
during the last years. On the one hand, reliable solutions should provide
robust algorithms of Web data mining which could automatically face possible
malfunctioning or failures. On the other, in literature there is a lack of
solutions about the maintenance of these systems. Procedures that extract Web
data may be strictly interconnected with the structure of the data source
itself; thus, malfunctioning or acquisition of corrupted data could be caused,
for example, by structural modifications of data sources brought by their
owners. Nowadays, verification of data integrity and maintenance are mostly
manually managed, in order to ensure that these systems work correctly and
reliably. In this paper we propose a novel approach to create procedures able
to extract data from Web sources -- the so called Web wrappers -- which can
face possible malfunctioning caused by modifications of the structure of the
data source, and can automatically repair themselves.
",Intelligent Self-Repairable Web Wrappers,Emilio Ferrara and Robert Baumgartner,2011,Artificial Intelligence,
"  The study of opinions, their formation and change, is one of the defining
topics addressed by social psychology, but in recent years other disciplines,
like computer science and complexity, have tried to deal with this issue.
Despite the flourishing of different models and theories in both fields,
several key questions still remain unanswered. The understanding of how
opinions change and the way they are affected by social influence are
challenging issues requiring a thorough analysis of opinion per se but also of
the way in which they travel between agents' minds and are modulated by these
exchanges. To account for the two-faceted nature of opinions, which are mental
entities undergoing complex social processes, we outline a preliminary model in
which a cognitive theory of opinions is put forward and it is paired with a
formal description of them and of their spreading among minds. Furthermore,
investigating social influence also implies the necessity to account for the
way in which people change their minds, as a consequence of interacting with
other people, and the need to explain the higher or lower persistence of such
changes.
","Rooting opinions in the minds: a cognitive model and a formal account of
  opinions and their dynamics","Francesca Giardini, Walter Quattrociocchi, Rosaria Conte",2011,Artificial Intelligence,
"  The study of opinions, their formation and change, is one of the defining
topics addressed by social psychology, but in recent years other disciplines,
as computer science and complexity, have addressed this challenge. Despite the
flourishing of different models and theories in both fields, several key
questions still remain unanswered. The aim of this paper is to challenge the
current theories on opinion by putting forward a cognitively grounded model
where opinions are described as specific mental representations whose main
properties are put forward. A comparison with reputation will be also
presented.
",Understanding opinions. A cognitive and formal account,"Francesca Giardini, Walter Quattrociocchi, Rosaria Conte",2011,Artificial Intelligence,
"  Prediction markets show considerable promise for developing flexible
mechanisms for machine learning. Here, machine learning markets for
multivariate systems are defined, and a utility-based framework is established
for their analysis. This differs from the usual approach of defining static
betting functions. It is shown that such markets can implement model
combination methods used in machine learning, such as product of expert and
mixture of expert approaches as equilibrium pricing models, by varying agent
utility functions. They can also implement models composed of local potentials,
and message passing methods. Prediction markets also allow for more flexible
combinations, by combining multiple different utility functions. Conversely,
the market mechanisms implement inference in the relevant probabilistic models.
This means that market mechanism can be utilized for implementing parallelized
model building and inference for probabilistic modelling.
",Machine Learning Markets,Amos Storkey,2011,Artificial Intelligence,
"  For large, real-world inductive learning problems, the number of training
examples often must be limited due to the costs associated with procuring,
preparing, and storing the training examples and/or the computational costs
associated with learning from them. In such circumstances, one question of
practical importance is: if only n training examples can be selected, in what
proportion should the classes be represented? In this article we help to answer
this question by analyzing, for a fixed training-set size, the relationship
between the class distribution of the training data and the performance of
classification trees induced from these data. We study twenty-six data sets
and, for each, determine the best class distribution for learning. The
naturally occurring class distribution is shown to generally perform well when
classifier performance is evaluated using undifferentiated error rate (0/1
loss). However, when the area under the ROC curve is used to evaluate
classifier performance, a balanced distribution is shown to perform well. Since
neither of these choices for class distribution always generates the
best-performing classifier, we introduce a budget-sensitive progressive
sampling algorithm for selecting training examples based on the class
associated with each example. An empirical analysis of this algorithm shows
that the class distribution of the resulting training set yields classifiers
with good (nearly-optimal) classification performance.
","Learning When Training Data are Costly: The Effect of Class Distribution
  on Tree Induction","F. Provost, G. M. Weiss",2003,Artificial Intelligence,
"  In recent years research in the planning community has moved increasingly
toward s application of planners to realistic problems involving both time and
many typ es of resources. For example, interest in planning demonstrated by the
space res earch community has inspired work in observation scheduling,
planetary rover ex ploration and spacecraft control domains. Other temporal and
resource-intensive domains including logistics planning, plant control and
manufacturing have also helped to focus the community on the modelling and
reasoning issues that must be confronted to make planning technology meet the
challenges of application. The International Planning Competitions have acted
as an important motivating fo rce behind the progress that has been made in
planning since 1998. The third com petition (held in 2002) set the planning
community the challenge of handling tim e and numeric resources. This
necessitated the development of a modelling langua ge capable of expressing
temporal and numeric properties of planning domains. In this paper we describe
the language, PDDL2.1, that was used in the competition. We describe the syntax
of the language, its formal semantics and the validation of concurrent plans.
We observe that PDDL2.1 has considerable modelling power --- exceeding the
capabilities of current planning technology --- and presents a number of
important challenges to the research community.
",PDDL2.1: An Extension to PDDL for Expressing Temporal Planning Domains,"M. Fox, D. Long",2003,Artificial Intelligence,
"  Despite the significant progress in multiagent teamwork, existing research
does not address the optimality of its prescriptions nor the complexity of the
teamwork problem. Without a characterization of the optimality-complexity
tradeoffs, it is impossible to determine whether the assumptions and
approximations made by a particular theory gain enough efficiency to justify
the losses in overall performance. To provide a tool for use by multiagent
researchers in evaluating this tradeoff, we present a unified framework, the
COMmunicative Multiagent Team Decision Problem (COM-MTDP). The COM-MTDP model
combines and extends existing multiagent theories, such as decentralized
partially observable Markov decision processes and economic team theory. In
addition to their generality of representation, COM-MTDPs also support the
analysis of both the optimality of team performance and the computational
complexity of the agents' decision problem. In analyzing complexity, we present
a breakdown of the computational complexity of constructing optimal teams under
various classes of problem domains, along the dimensions of observability and
communication cost. In analyzing optimality, we exploit the COM-MTDP's ability
to encode existing teamwork theories and models to encode two instantiations of
joint intentions theory taken from the literature. Furthermore, the COM-MTDP
model provides a basis for the development of novel team coordination
algorithms. We derive a domain-independent criterion for optimal communication
and provide a comparative analysis of the two joint intentions instantiations
with respect to this optimal policy. We have implemented a reusable,
domain-independent software package based on COM-MTDPs to analyze teamwork
coordination strategies, and we demonstrate its use by encoding and evaluating
the two joint intentions strategies within an example domain.
","The Communicative Multiagent Team Decision Problem: Analyzing Teamwork
  Theories and Models","D. V. Pynadath, M. Tambe",2002,Artificial Intelligence,
"  We develop, analyze, and evaluate a novel, supervised, specific-to-general
learner for a simple temporal logic and use the resulting algorithm to learn
visual event definitions from video sequences. First, we introduce a simple,
propositional, temporal, event-description language called AMA that is
sufficiently expressive to represent many events yet sufficiently restrictive
to support learning. We then give algorithms, along with lower and upper
complexity bounds, for the subsumption and generalization problems for AMA
formulas. We present a positive-examples--only specific-to-general learning
method based on these algorithms. We also present a polynomial-time--computable
``syntactic'' subsumption test that implies semantic subsumption without being
equivalent to it. A generalization algorithm based on syntactic subsumption can
be used in place of semantic generalization to improve the asymptotic
complexity of the resulting learning algorithm. Finally, we apply this
algorithm to the task of learning relational event definitions from video and
show that it yields definitions that are competitive with hand-coded ones.
","Specific-to-General Learning for Temporal Events with Application to
  Learning Event Definitions from Video","A. Fern, R. Givan, J. M. Siskind",2002,Artificial Intelligence,
"  Adjustable autonomy refers to entities dynamically varying their own
autonomy, transferring decision-making control to other entities (typically
agents transferring control to human users) in key situations. Determining
whether and when such transfers-of-control should occur is arguably the
fundamental research problem in adjustable autonomy. Previous work has
investigated various approaches to addressing this problem but has often
focused on individual agent-human interactions. Unfortunately, domains
requiring collaboration between teams of agents and humans reveal two key
shortcomings of these previous approaches. First, these approaches use rigid
one-shot transfers of control that can result in unacceptable coordination
failures in multiagent settings. Second, they ignore costs (e.g., in terms of
time delays or effects on actions) to an agent's team due to such
transfers-of-control. To remedy these problems, this article presents a novel
approach to adjustable autonomy, based on the notion of a transfer-of-control
strategy. A transfer-of-control strategy consists of a conditional sequence of
two types of actions: (i) actions to transfer decision-making control (e.g.,
from an agent to a user or vice versa) and (ii) actions to change an agent's
pre-specified coordination constraints with team members, aimed at minimizing
miscoordination costs. The goal is for high-quality individual decisions to be
made with minimal disruption to the coordination of the team. We present a
mathematical model of transfer-of-control strategies. The model guides and
informs the operationalization of the strategies using Markov Decision
Processes, which select an optimal strategy, given an uncertain environment and
costs to the individuals and teams. The approach has been carefully evaluated,
including via its use in a real-world, deployed multi-agent system that assists
a research group in its daily activities.
",Towards Adjustable Autonomy for the Real World,"D. V. Pynadath, P. Scerri, M. Tambe",2002,Artificial Intelligence,
"  In this paper, we analyze the decision version of the NK landscape model from
the perspective of threshold phenomena and phase transitions under two random
distributions, the uniform probability model and the fixed ratio model. For the
uniform probability model, we prove that the phase transition is easy in the
sense that there is a polynomial algorithm that can solve a random instance of
the problem with the probability asymptotic to 1 as the problem size tends to
infinity. For the fixed ratio model, we establish several upper bounds for the
solubility threshold, and prove that random instances with parameters above
these upper bounds can be solved polynomially. This, together with our
empirical study for random instances generated below and in the phase
transition region, suggests that the phase transition of the fixed ratio model
is also easy.
",An Analysis of Phase Transition in NK Landscapes,"J. Culberson, Y. Gao",2002,Artificial Intelligence,
"  This paper presents an approach to expert-guided subgroup discovery. The main
step of the subgroup discovery process, the induction of subgroup descriptions,
is performed by a heuristic beam search algorithm, using a novel parametrized
definition of rule quality which is analyzed in detail. The other important
steps of the proposed subgroup discovery process are the detection of
statistically significant properties of selected subgroups and subgroup
visualization: statistically significant properties are used to enrich the
descriptions of induced subgroups, while the visualization shows subgroup
properties in the form of distributions of the numbers of examples in the
subgroups. The approach is illustrated by the results obtained for a medical
problem of early detection of patient risk groups.
",Expert-Guided Subgroup Discovery: Methodology and Application,"D. Gamberger, N. Lavrac",2002,Artificial Intelligence,
"  Independence -- the study of what is relevant to a given problem of reasoning
-- has received an increasing attention from the AI community. In this paper,
we consider two basic forms of independence, namely, a syntactic one and a
semantic one. We show features and drawbacks of them. In particular, while the
syntactic form of independence is computationally easy to check, there are
cases in which things that intuitively are not relevant are not recognized as
such. We also consider the problem of forgetting, i.e., distilling from a
knowledge base only the part that is relevant to the set of queries constructed
from a subset of the alphabet. While such process is computationally hard, it
allows for a simplification of subsequent reasoning, and can thus be viewed as
a form of compilation: once the relevant part of a knowledge base has been
extracted, all reasoning tasks to be performed can be simplified.
","Propositional Independence - Formula-Variable Independence and
  Forgetting","J. Lang, P. Liberatore, P. Marquis",2003,Artificial Intelligence,
"  We present a probabilistic generative model for timing deviations in
expressive music performance. The structure of the proposed model is equivalent
to a switching state space model. The switch variables correspond to discrete
note locations as in a musical score. The continuous hidden variables denote
the tempo. We formulate two well known music recognition problems, namely tempo
tracking and automatic transcription (rhythm quantization) as filtering and
maximum a posteriori (MAP) state estimation tasks. Exact computation of
posterior features such as the MAP state is intractable in this model class, so
we introduce Monte Carlo methods for integration and optimization. We compare
Markov Chain Monte Carlo (MCMC) methods (such as Gibbs sampling, simulated
annealing and iterative improvement) and sequential Monte Carlo methods
(particle filters). Our simulation results suggest better results with
sequential methods. The methods can be applied in both online and batch
scenarios such as tempo tracking and transcription and are thus potentially
useful in a number of music applications such as adaptive automatic
accompaniment, score typesetting and music information retrieval.
",Monte Carlo Methods for Tempo Tracking and Rhythm Quantization,"A. T. Cemgil, B. Kappen",2003,Artificial Intelligence,
"  Bayesian belief networks have grown to prominence because they provide
compact representations for many problems for which probabilistic inference is
appropriate, and there are algorithms to exploit this compactness. The next
step is to allow compact representations of the conditional probabilities of a
variable given its parents. In this paper we present such a representation that
exploits contextual independence in terms of parent contexts; which variables
act as parents may depend on the value of other variables. The internal
representation is in terms of contextual factors (confactors) that is simply a
pair of a context and a table. The algorithm, contextual variable elimination,
is based on the standard variable elimination algorithm that eliminates the
non-query variables in turn, but when eliminating a variable, the tables that
need to be multiplied can depend on the context. This algorithm reduces to
standard variable elimination when there is no contextual independence
structure to exploit. We show how this can be much more efficient than variable
elimination when there is structure to exploit. We explain why this new method
can exploit more structure than previous methods for structured belief network
inference and an analogous algorithm that uses trees.
",Exploiting Contextual Independence In Probabilistic Inference,"D. Poole, N. L. Zhang",2003,Artificial Intelligence,
"  In this article we present an algorithm to compute bounds on the marginals of
a graphical model. For several small clusters of nodes upper and lower bounds
on the marginal values are computed independently of the rest of the network.
The range of allowed probability distributions over the surrounding nodes is
restricted using earlier computed bounds. As we will show, this can be
considered as a set of constraints in a linear programming problem of which the
objective function is the marginal probability of the center nodes. In this way
knowledge about the maginals of neighbouring clusters is passed to other
clusters thereby tightening the bounds on their marginals. We show that sharp
bounds can be obtained for undirected and directed graphs that are used for
practical applications, but for which exact computations are infeasible.
",Bound Propagation,"B. Kappen, M. Leisink",2003,Artificial Intelligence,
"  Policies of Markov Decision Processes (MDPs) determine the next action to
execute from the current state and, possibly, the history (the past states).
When the number of states is large, succinct representations are often used to
compactly represent both the MDPs and the policies in a reduced amount of
space. In this paper, some problems related to the size of succinctly
represented policies are analyzed. Namely, it is shown that some MDPs have
policies that can only be represented in space super-polynomial in the size of
the MDP, unless the polynomial hierarchy collapses. This fact motivates the
study of the problem of deciding whether a given MDP has a policy of a given
size and reward. Since some algorithms for MDPs work by finding a succinct
representation of the value function, the problem of deciding the existence of
a succinct representation of a value function of a given size and reward is
also considered.
",On Polynomial Sized MDP Succinct Policies,P. Liberatore,2004,Artificial Intelligence,
"  We describe a system for specifying the effects of actions. Unlike those
commonly used in AI planning, our system uses an action description language
that allows one to specify the effects of actions using domain rules, which are
state constraints that can entail new action effects from old ones.
Declaratively, an action domain in our language corresponds to a nonmonotonic
causal theory in the situation calculus. Procedurally, such an action domain is
compiled into a set of logical theories, one for each action in the domain,
from which fully instantiated successor state-like axioms and STRIPS-like
systems are then generated. We expect the system to be a useful tool for
knowledge engineers writing action specifications for classical AI planning
systems, GOLOG systems, and other systems where formal specifications of
actions are needed.
","Compiling Causal Theories to Successor State Axioms and STRIPS-Like
  Systems",F. Lin,2003,Artificial Intelligence,
"  VHPOP is a partial order causal link (POCL) planner loosely based on UCPOP.
It draws from the experience gained in the early to mid 1990's on flaw
selection strategies for POCL planning, and combines this with more recent
developments in the field of domain independent planning such as distance based
heuristics and reachability analysis. We present an adaptation of the additive
heuristic for plan space planning, and modify it to account for possible reuse
of existing actions in a plan. We also propose a large set of novel flaw
selection strategies, and show how these can help us solve more problems than
previously possible by POCL planners. VHPOP also supports planning with
durative actions by incorporating standard techniques for temporal constraint
reasoning. We demonstrate that the same heuristic techniques used to boost the
performance of classical POCL planning can be effective in domains with
durative actions as well. The result is a versatile heuristic POCL planner
competitive with established CSP-based and heuristic state space planners.
",VHPOP: Versatile Heuristic Partial Order Planner,"R. G. Simmons, H. L.S. Younes",2003,Artificial Intelligence,
"  The SHOP2 planning system received one of the awards for distinguished
performance in the 2002 International Planning Competition. This paper
describes the features of SHOP2 which enabled it to excel in the competition,
especially those aspects of SHOP2 that deal with temporal and metric planning
domains.
",SHOP2: An HTN Planning System,"T. C. Au, O. Ilghami, U. Kuter, J. W. Murdock, D. S. Nau, D. Wu, F.
  Yaman",2003,Artificial Intelligence,
"  Hierarchical task decomposition is a method used in many agent systems to
organize agent knowledge. This work shows how the combination of a hierarchy
and persistent assertions of knowledge can lead to difficulty in maintaining
logical consistency in asserted knowledge. We explore the problematic
consequences of persistent assumptions in the reasoning process and introduce
novel potential solutions. Having implemented one of the possible solutions,
Dynamic Hierarchical Justification, its effectiveness is demonstrated with an
empirical analysis.
","An Architectural Approach to Ensuring Consistency in Hierarchical
  Execution","J. E. Laird, R. E. Wray",2003,Artificial Intelligence,
"  The proliferation of online information sources has led to an increased use
of wrappers for extracting data from Web sources. While most of the previous
research has focused on quick and efficient generation of wrappers, the
development of tools for wrapper maintenance has received less attention. This
is an important research problem because Web sources often change in ways that
prevent the wrappers from extracting data correctly. We present an efficient
algorithm that learns structural information about data from positive examples
alone. We describe how this information can be used for two wrapper maintenance
applications: wrapper verification and reinduction. The wrapper verification
system detects when a wrapper is not extracting correct data, usually because
the Web source has changed its format. The reinduction algorithm automatically
recovers from changes in the Web source by identifying data on Web pages so
that a new wrapper may be generated for this source. To validate our approach,
we monitored 27 wrappers over a period of a year. The verification algorithm
correctly discovered 35 of the 37 wrapper changes, and made 16 mistakes,
resulting in precision of 0.73 and recall of 0.95. We validated the reinduction
algorithm on ten Web sources. We were able to successfully reinduce the
wrappers, obtaining precision and recall values of 0.90 and 0.80 on the data
extraction task.
",Wrapper Maintenance: A Machine Learning Approach,"C. A. Knoblock, K. Lerman, S. N. Minton",2003,Artificial Intelligence,
"  Unary operator domains -- i.e., domains in which operators have a single
effect -- arise naturally in many control problems. In its most general form,
the problem of STRIPS planning in unary operator domains is known to be as hard
as the general STRIPS planning problem -- both are PSPACE-complete. However,
unary operator domains induce a natural structure, called the domain's causal
graph. This graph relates between the preconditions and effect of each domain
operator. Causal graphs were exploited by Williams and Nayak in order to
analyze plan generation for one of the controllers in NASA's Deep-Space One
spacecraft. There, they utilized the fact that when this graph is acyclic, a
serialization ordering over any subgoal can be obtained quickly. In this paper
we conduct a comprehensive study of the relationship between the structure of a
domain's causal graph and the complexity of planning in this domain. On the
positive side, we show that a non-trivial polynomial time plan generation
algorithm exists for domains whose causal graph induces a polytree with a
constant bound on its node indegree. On the negative side, we show that even
plan existence is hard when the graph is a directed-path singly connected DAG.
More generally, we show that the number of paths in the causal graph is closely
related to the complexity of planning in the associated domain. Finally we
relate our results to the question of complexity of planning with serializable
subgoals.
",Structure and Complexity in Planning with Unary Operators,"R. I. Brafman, C. Domshlak",2003,Artificial Intelligence,
"  Recently, planning based on answer set programming has been proposed as an
approach towards realizing declarative planning systems. In this paper, we
present the language Kc, which extends the declarative planning language K by
action costs. Kc provides the notion of admissible and optimal plans, which are
plans whose overall action costs are within a given limit resp. minimum over
all plans (i.e., cheapest plans). As we demonstrate, this novel language allows
for expressing some nontrivial planning tasks in a declarative way.
Furthermore, it can be utilized for representing planning problems under other
optimality criteria, such as computing ``shortest'' plans (with the least
number of steps), and refinement combinations of cheapest and fastest plans. We
study complexity aspects of the language Kc and provide a transformation to
logic programs, such that planning problems are solved via answer set
programming. Furthermore, we report experimental results on selected problems.
Our experience is encouraging that answer set planning may be a valuable
approach to expressive planning systems in which intricate planning problems
can be naturally specified and solved.
",Answer Set Planning Under Action Costs,"T. Eiter, W. Faber, N. Leone, G. Pfeifer, A. Polleres",2003,Artificial Intelligence,
"  In common-interest stochastic games all players receive an identical payoff.
Players participating in such games must learn to coordinate with each other in
order to receive the highest-possible value. A number of reinforcement learning
algorithms have been proposed for this problem, and some have been shown to
converge to good solutions in the limit. In this paper we show that using very
simple model-based algorithms, much better (i.e., polynomial) convergence rates
can be attained. Moreover, our model-based algorithms are guaranteed to
converge to the optimal value, unlike many of the existing algorithms.
",Learning to Coordinate Efficiently: A Model-based Approach,"R. I. Brafman, M. Tennenholtz",2003,Artificial Intelligence,
"  SAPA is a domain-independent heuristic forward chaining planner that can
handle durative actions, metric resource constraints, and deadline goals. It is
designed to be capable of handling the multi-objective nature of metric
temporal planning. Our technical contributions include (i) planning-graph based
methods for deriving heuristics that are sensitive to both cost and makespan
(ii) techniques for adjusting the heuristic estimates to take action
interactions and metric resource limitations into account and (iii) a linear
time greedy post-processing technique to improve execution flexibility of the
solution plans. An implementation of SAPA using many of the techniques
presented in this paper was one of the best domain independent planners for
domains with metric and temporal constraints in the third International
Planning Competition, held at AIPS-02. We describe the technical details of
extracting the heuristics and present an empirical evaluation of the current
implementation of SAPA.
",SAPA: A Multi-objective Metric Temporal Planner,"M. Do, S. Kambhampati",2003,Artificial Intelligence,
"  The recent emergence of heavily-optimized modal decision procedures has
highlighted the key role of empirical testing in this domain. Unfortunately,
the introduction of extensive empirical tests for modal logics is recent, and
so far none of the proposed test generators is very satisfactory. To cope with
this fact, we present a new random generation method that provides benefits
over previous methods for generating empirical tests. It fixes and much
generalizes one of the best-known methods, the random CNF_[]m test, allowing
for generating a much wider variety of problems, covering in principle the
whole input space. Our new method produces much more suitable test sets for the
current generation of modal decision procedures. We analyze the features of the
new method by means of an extensive collection of empirical tests.
","A New General Method to Generate Random Modal Formulae for Testing
  Decision Procedures","P. F. Patel-Schneider, R. Sebastiani",2003,Artificial Intelligence,
"  Despite their near dominance, heuristic state search planners still lag
behind disjunctive planners in the generation of parallel plans in classical
planning. The reason is that directly searching for parallel solutions in state
space planners would require the planners to branch on all possible subsets of
parallel actions, thus increasing the branching factor exponentially. We
present a variant of our heuristic state search planner AltAlt, called AltAltp
which generates parallel plans by using greedy online parallelization of
partial plans. The greedy approach is significantly informed by the use of
novel distance heuristics that AltAltp derives from a graphplan-style planning
graph for the problem. While this approach is not guaranteed to provide optimal
parallel plans, empirical results show that AltAltp is capable of generating
good quality parallel plans at a fraction of the cost incurred by the
disjunctive planners.
",AltAltp: Online Parallelization of Plans with Heuristic State Search,"S. Kambhampati, R. Sanchez",2003,Artificial Intelligence,
"  We address the problem of propositional logic-based abduction, i.e., the
problem of searching for a best explanation for a given propositional
observation according to a given propositional knowledge base. We give a
general algorithm, based on the notion of projection; then we study
restrictions over the representations of the knowledge base and of the query,
and find new polynomial classes of abduction problems.
",New Polynomial Classes for Logic-Based Abduction,B. Zanuttini,2003,Artificial Intelligence,
"  We present some techniques for planning in domains specified with the recent
standard language PDDL2.1, supporting 'durative actions' and numerical
quantities. These techniques are implemented in LPG, a domain-independent
planner that took part in the 3rd International Planning Competition (IPC). LPG
is an incremental, any time system producing multi-criteria quality plans. The
core of the system is based on a stochastic local search method and on a
graph-based representation called 'Temporal Action Graphs' (TA-graphs). This
paper focuses on temporal planning, introducing TA-graphs and proposing some
techniques to guide the search in LPG using this representation. The
experimental results of the 3rd IPC, as well as further results presented in
this paper, show that our techniques can be very effective. Often LPG
outperforms all other fully-automated planners of the 3rd IPC in terms of speed
to derive a solution, or quality of the solutions that can be produced.
","Planning Through Stochastic Local Search and Temporal Action Graphs in
  LPG","A. Gerevini, A. Saetti, I. Serina",2003,Artificial Intelligence,
"  TALplanner is a forward-chaining planner that relies on domain knowledge in
the shape of temporal logic formulas in order to prune irrelevant parts of the
search space. TALplanner recently participated in the third International
Planning Competition, which had a clear emphasis on increasing the complexity
of the problem domains being used as benchmark tests and the expressivity
required to represent these domains in a planning system. Like many other
planners, TALplanner had support for some but not all aspects of this increase
in expressivity, and a number of changes to the planner were required. After a
short introduction to TALplanner, this article describes some of the changes
that were made before and during the competition. We also describe the process
of introducing suitable domain knowledge for several of the competition
domains.
",TALplanner in IPC-2002: Extensions and Control Rules,"J. Kvarnstr\""om, M. Magnusson",2003,Artificial Intelligence,
"  The automatic generation of decision trees based on off-line reasoning on
models of a domain is a reasonable compromise between the advantages of using a
model-based approach in technical domains and the constraints imposed by
embedded applications. In this paper we extend the approach to deal with
temporal information. We introduce a notion of temporal decision tree, which is
designed to make use of relevant information as long as it is acquired, and we
present an algorithm for compiling such trees from a model-based reasoning
system.
","Temporal Decision Trees: Model-based Diagnosis of Dynamic Systems
  On-Board","L. Console, C. Picardi, D. Theseider Dupr\`e",2003,Artificial Intelligence,
"  The performance of anytime algorithms can be improved by simultaneously
solving several instances of algorithm-problem pairs. These pairs may include
different instances of a problem (such as starting from a different initial
state), different algorithms (if several alternatives exist), or several runs
of the same algorithm (for non-deterministic algorithms). In this paper we
present a methodology for designing an optimal scheduling policy based on the
statistical characteristics of the algorithms involved. We formally analyze the
case where the processes share resources (a single-processor model), and
provide an algorithm for optimal scheduling. We analyze, theoretically and
empirically, the behavior of our scheduling algorithm for various distribution
types. Finally, we present empirical results of applying our scheduling
algorithm to the Latin Square problem.
","Optimal Schedules for Parallelizing Anytime Algorithms: The Case of
  Shared Resources","L. Finkelstein, S. Markovitch, E. Rivlin",2003,Artificial Intelligence,
"  Auctions are becoming an increasingly popular method for transacting
business, especially over the Internet. This article presents a general
approach to building autonomous bidding agents to bid in multiple simultaneous
auctions for interacting goods. A core component of our approach learns a model
of the empirical price dynamics based on past data and uses the model to
analytically calculate, to the greatest extent possible, optimal bids. We
introduce a new and general boosting-based algorithm for conditional density
estimation problems of this kind, i.e., supervised learning problems in which
the goal is to estimate the entire conditional distribution of the real-valued
label. This approach is fully implemented as ATTac-2001, a top-scoring agent in
the second Trading Agent Competition (TAC-01). We present experiments
demonstrating the effectiveness of our boosting-based price predictor relative
to several reasonable alternatives.
","Decision-Theoretic Bidding Based on Learned Density Models in
  Simultaneous, Interacting Auctions","J. A. Csirik, M. L. Littman, D. McAllester, R. E. Schapire, P. Stone",2003,Artificial Intelligence,
"  Planning with numeric state variables has been a challenge for many years,
and was a part of the 3rd International Planning Competition (IPC-3). Currently
one of the most popular and successful algorithmic techniques in STRIPS
planning is to guide search by a heuristic function, where the heuristic is
based on relaxing the planning task by ignoring the delete lists of the
available actions. We present a natural extension of ``ignoring delete lists''
to numeric state variables, preserving the relevant theoretical properties of
the STRIPS relaxation under the condition that the numeric task at hand is
``monotonic''. We then identify a subset of the numeric IPC-3 competition
language, ``linear tasks'', where monotonicity can be achieved by
pre-processing. Based on that, we extend the algorithms used in the heuristic
planning system FF to linear tasks. The resulting system Metric-FF is,
according to the IPC-3 results which we discuss, one of the two currently most
efficient numeric planners.
","The Metric-FF Planning System: Translating ""Ignoring Delete Lists"" to
  Numeric State Variables",J. Hoffmann,2003,Artificial Intelligence,
"  The relation between self awareness and intelligence is an open problem these
days. Despite the fact that self awarness is usually related to Emotional
Intelligence, this is not the case here. The problem described in this paper is
how to model an agent which knows (Cognitive) Binary Logic and which is also
able to pass (without any mistake) a certain family of Turing Tests designed to
verify its knowledge and its discourse about the modal states of truth
corresponding to well-formed formulae within the language of Propositional
Binary Logic.
",From Cognitive Binary Logic to Cognitive Intelligent Agents,"Nicolaie Popescu-Bodorin, Valentina E. Balas",2010,Artificial Intelligence,
"  This paper reports the outcome of the third in the series of biennial
international planning competitions, held in association with the International
Conference on AI Planning and Scheduling (AIPS) in 2002. In addition to
describing the domains, the planners and the objectives of the competition, the
paper includes analysis of the results. The results are analysed from several
perspectives, in order to address the questions of comparative performance
between planners, comparative difficulty of domains, the degree of agreement
between planners about the relative difficulty of individual problem instances
and the question of how well planners scale relative to one another over
increasingly difficult problems. The paper addresses these questions through
statistical analysis of the raw results of the competition, in order to
determine which results can be considered to be adequately supported by the
data. The paper concludes with a discussion of some challenges for the future
of the competition series.
",The 3rd International Planning Competition: Results and Analysis,"M. Fox, D. Long",2003,Artificial Intelligence,
"  As computational agents are developed for increasingly complicated e-commerce
applications, the complexity of the decisions they face demands advances in
artificial intelligence techniques. For example, an agent representing a seller
in an auction should try to maximize the seller's profit by reasoning about a
variety of possibly uncertain pieces of information, such as the maximum prices
various buyers might be willing to pay, the possible prices being offered by
competing sellers, the rules by which the auction operates, the dynamic arrival
and matching of offers to buy and sell, and so on. A naive application of
multiagent reasoning techniques would require the seller's agent to explicitly
model all of the other agents through an extended time horizon, rendering the
problem intractable for many realistically-sized problems. We have instead
devised a new strategy that an agent can use to determine its bid price based
on a more tractable Markov chain model of the auction process. We have
experimentally identified the conditions under which our new strategy works
well, as well as how well it works in comparison to the optimal performance the
agent could have achieved had it known the future. Our results show that our
new strategy in general performs well, outperforming other tractable heuristic
strategies in a majority of experiments, and is particularly effective in a
'seller?s market', where many buy offers are available.
","Use of Markov Chains to Design an Agent Bidding Strategy for Continuous
  Double Auctions","W. P. Birmingham, E. H. Durfee, S. Park",2004,Artificial Intelligence,
"  This paper presents a new classifier combination technique based on the
Dempster-Shafer theory of evidence. The Dempster-Shafer theory of evidence is a
powerful method for combining measures of evidence from different classifiers.
However, since each of the available methods that estimates the evidence of
classifiers has its own limitations, we propose here a new implementation which
adapts to training data so that the overall mean square error is minimized. The
proposed technique is shown to outperform most available classifier combination
methods when tested on three different classification problems.
","A New Technique for Combining Multiple Classifiers using The
  Dempster-Shafer Theory of Evidence","A. Al-Ani, M. Deriche",2002,Artificial Intelligence,
"  Although many algorithms have been designed to construct Bayesian network
structures using different approaches and principles, they all employ only two
methods: those based on independence criteria, and those based on a scoring
function and a search procedure (although some methods combine the two). Within
the score+search paradigm, the dominant approach uses local search methods in
the space of directed acyclic graphs (DAGs), where the usual choices for
defining the elementary modifications (local changes) that can be applied are
arc addition, arc deletion, and arc reversal. In this paper, we propose a new
local search method that uses a different search space, and which takes account
of the concept of equivalence between network structures: restricted acyclic
partially directed graphs (RPDAGs). In this way, the number of different
configurations of the search space is reduced, thus improving efficiency.
Moreover, although the final result must necessarily be a local optimum given
the nature of the search method, the topology of the new search space, which
avoids making early decisions about the directions of the arcs, may help to
find better local optima than those obtained by searching in the DAG space.
Detailed results of the evaluation of the proposed search method on several
test problems, including the well-known Alarm Monitoring System, are also
presented.
","Searching for Bayesian Network Structures in the Space of Restricted
  Acyclic Partially Directed Graphs","S. Acid, L. M. de Campos",2003,Artificial Intelligence,
"  The size and complexity of software and hardware systems have significantly
increased in the past years. As a result, it is harder to guarantee their
correct behavior. One of the most successful methods for automated verification
of finite-state systems is model checking. Most of the current model-checking
systems use binary decision diagrams (BDDs) for the representation of the
tested model and in the verification process of its properties. Generally, BDDs
allow a canonical compact representation of a boolean function (given an order
of its variables). The more compact the BDD is, the better performance one gets
from the verifier. However, finding an optimal order for a BDD is an
NP-complete problem. Therefore, several heuristic methods based on expert
knowledge have been developed for variable ordering. We propose an alternative
approach in which the variable ordering algorithm gains 'ordering experience'
from training models and uses the learned knowledge for finding good orders.
Our methodology is based on offline learning of pair precedence classifiers
from training models, that is, learning which variable pair permutation is more
likely to lead to a good order. For each training model, a number of training
sequences are evaluated. Every training model variable pair permutation is then
tagged based on its performance on the evaluated orders. The tagged
permutations are then passed through a feature extractor and are given as
examples to a classifier creation algorithm. Given a model for which an order
is requested, the ordering algorithm consults each precedence classifier and
constructs a pair precedence table which is used to create the order. Our
algorithm was integrated with SMV, which is one of the most widely used
verification systems. Preliminary empirical evaluation of our methodology,
using real benchmark models, shows performance that is better than random
ordering and is competitive with existing algorithms that use expert knowledge.
We believe that in sub-domains of models (alu, caches, etc.) our system will
prove even more valuable. This is because it features the ability to learn
sub-domain knowledge, something that no other ordering algorithm does.
",Learning to Order BDD Variables in Verification,"O. Grumberg, S. Livne, S. Markovitch",2003,Artificial Intelligence,
"  Supply chain formation is the process of determining the structure and terms
of exchange relationships to enable a multilevel, multiagent production
activity. We present a simple model of supply chains, highlighting two
characteristic features: hierarchical subtask decomposition, and resource
contention. To decentralize the formation process, we introduce a market price
system over the resources produced along the chain. In a competitive
equilibrium for this system, agents choose locally optimal allocations with
respect to prices, and outcomes are optimal overall. To determine prices, we
define a market protocol based on distributed, progressive auctions, and
myopic, non-strategic agent bidding policies. In the presence of resource
contention, this protocol produces better solutions than the greedy protocols
common in the artificial intelligence and multiagent systems literature. The
protocol often converges to high-value supply chains, and when competitive
equilibria exist, typically to approximate competitive equilibria. However,
complementarities in agent production technologies can cause the protocol to
wastefully allocate inputs to agents that do not produce their outputs. A
subsequent decommitment phase recovers a significant fraction of the lost
surplus.
","Decentralized Supply Chain Formation: A Market Protocol and Competitive
  Equilibrium Analysis","W. E. Walsh, M. P. Wellman",2003,Artificial Intelligence,
"  Information about user preferences plays a key role in automated decision
making. In many domains it is desirable to assess such preferences in a
qualitative rather than quantitative way. In this paper, we propose a
qualitative graphical representation of preferences that reflects conditional
dependence and independence of preference statements under a ceteris paribus
(all else being equal) interpretation. Such a representation is often compact
and arguably quite natural in many circumstances. We provide a formal semantics
for this model, and describe how the structure of the network can be exploited
in several inference tasks, such as determining whether one outcome dominates
(is preferred to) another, ordering a set outcomes according to the preference
relation, and constructing the best outcome subject to available evidence.
","CP-nets: A Tool for Representing and Reasoning withConditional Ceteris
  Paribus Preference Statements","C. Boutilier, R. I. Brafman, C. Domshlak, H. H. Hoos, D. Poole",2004,Artificial Intelligence,
"  MAP is the problem of finding a most probable instantiation of a set of
variables given evidence. MAP has always been perceived to be significantly
harder than the related problems of computing the probability of a variable
instantiation Pr, or the problem of computing the most probable explanation
(MPE). This paper investigates the complexity of MAP in Bayesian networks.
Specifically, we show that MAP is complete for NP^PP and provide further
negative complexity results for algorithms based on variable elimination. We
also show that MAP remains hard even when MPE and Pr become easy. For example,
we show that MAP is NP-complete when the networks are restricted to polytrees,
and even then can not be effectively approximated. Given the difficulty of
computing MAP exactly, and the difficulty of approximating MAP while providing
useful guarantees on the resulting approximation, we investigate best effort
approximations. We introduce a generic MAP approximation framework. We provide
two instantiations of the framework; one for networks which are amenable to
exact inference Pr, and one for networks for which even exact inference is too
hard. This allows MAP approximation on networks that are too complex to even
exactly solve the easier problems, Pr and MPE. Experimental results indicate
that using these approximation algorithms provides much better solutions than
standard techniques, and provide accurate MAP estimates in many cases.
",Complexity Results and Approximation Strategies for MAP Explanations,"A. Darwiche, J. D. Park",2006,Artificial Intelligence,
"  The Model Checking Integrated Planning System (MIPS) is a temporal least
commitment heuristic search planner based on a flexible object-oriented
workbench architecture. Its design clearly separates explicit and symbolic
directed exploration algorithms from the set of on-line and off-line computed
estimates and associated data structures. MIPS has shown distinguished
performance in the last two international planning competitions. In the last
event the description language was extended from pure propositional planning to
include numerical state variables, action durations, and plan quality objective
functions. Plans were no longer sequences of actions but time-stamped
schedules. As a participant of the fully automated track of the competition,
MIPS has proven to be a general system; in each track and every benchmark
domain it efficiently computed plans of remarkable quality. This article
introduces and analyzes the most important algorithmic novelties that were
necessary to tackle the new layers of expressiveness in the benchmark problems
and to achieve a high level of performance. The extensions include critical
path analysis of sequentially generated plans to generate corresponding optimal
parallel plans. The linear time algorithm to compute the parallel plan bypasses
known NP hardness results for partial ordering by scheduling plans with respect
to the set of actions and the imposed precedence relations. The efficiency of
this algorithm also allows us to improve the exploration guidance: for each
encountered planning state the corresponding approximate sequential plan is
scheduled. One major strength of MIPS is its static analysis phase that grounds
and simplifies parameterized predicates, functions and operators, that infers
knowledge to minimize the state description length, and that detects domain
object symmetries. The latter aspect is analyzed in detail. MIPS has been
developed to serve as a complete and optimal state space planner, with
admissible estimates, exploration engines and branching cuts. In the
competition version, however, certain performance compromises had to be made,
including floating point arithmetic, weighted heuristic search exploration
according to an inadmissible estimate and parameterized optimization.
","Taming Numbers and Durations in the Model Checking Integrated Planning
  System",S. Edelkamp,2003,Artificial Intelligence,
"  There has been a long history of using fuzzy language equivalence to compare
the behavior of fuzzy systems, but the comparison at this level is too coarse.
Recently, a finer behavioral measure, bisimulation, has been introduced to
fuzzy finite automata. However, the results obtained are applicable only to
finite-state systems. In this paper, we consider bisimulation for general fuzzy
systems which may be infinite-state or infinite-event, by modeling them as
fuzzy transition systems. To help understand and check bisimulation, we
characterize it in three ways by enumerating whole transitions, comparing
individual transitions, and using a monotonic function. In addition, we address
composition operations, subsystems, quotients, and homomorphisms of fuzzy
transition systems and discuss their properties connected with bisimulation.
The results presented here are useful for comparing the behavior of general
fuzzy systems. In particular, this makes it possible to relate an infinite
fuzzy system to a finite one, which is easier to analyze, with the same
behavior.
",Bisimulations for fuzzy transition systems,"Yongzhi Cao, Guoqing Chen, and Etienne Kerre",2011,Artificial Intelligence,
"  This article deals with Part family formation problem which is believed to be
moderately complicated to be solved in polynomial time in the vicinity of Group
Technology (GT). In the past literature researchers investigated that the part
family formation techniques are principally based on production flow analysis
(PFA) which usually considers operational requirements, sequences and time.
Part Coding Analysis (PCA) is merely considered in GT which is believed to be
the proficient method to identify the part families. PCA classifies parts by
allotting them to different families based on their resemblances in: (1) design
characteristics such as shape and size, and/or (2) manufacturing
characteristics (machining requirements). A novel approach based on simulated
annealing namely SAPFOCS is adopted in this study to develop effective part
families exploiting the PCA technique. Thereafter Taguchi's orthogonal design
method is employed to solve the critical issues on the subject of parameters
selection for the proposed metaheuristic algorithm. The adopted technique is
therefore tested on 5 different datasets of size 5 {\times} 9 to 27 {\times} 9
and the obtained results are compared with C-Linkage clustering technique. The
experimental results reported that the proposed metaheuristic algorithm is
extremely effective in terms of the quality of the solution obtained and has
outperformed C-Linkage algorithm in most instances.
","SAPFOCS: a metaheuristic based approach to part family formation
  problems in group technology","Tamal Ghosh, Mousumi Modak and Pranab K Dan",2011,Artificial Intelligence,
"  The World Wide Web (WWW) allows the people to share the information (data)
from the large database repositories globally. The amount of information grows
billions of databases. We need to search the information will specialize tools
known generically search engine. There are many of search engines available
today, retrieving meaningful information is difficult. However to overcome this
problem in search engines to retrieve meaningful information intelligently,
semantic web technologies are playing a major role. In this paper we present
survey on the search engine generations and the role of search engines in
intelligent web and semantic search technologies.
",Intelligent Semantic Web Search Engines: A Brief Survey,"G.Madhu (1), Dr.A.Govardhan (2), Dr.T.V.Rajinikanth (3)",2011,Artificial Intelligence,
"  The ability to predict the intentions of people based solely on their visual
actions is a skill only performed by humans and animals. The intelligence of
current computer algorithms has not reached this level of complexity, but there
are several research efforts that are working towards it. With the number of
classification algorithms available, it is hard to determine which algorithm
works best for a particular situation. In classification of visual human intent
data, Hidden Markov Models (HMM), and their variants, are leading candidates.
  The inability of HMMs to provide a probability in the observation to
observation linkages is a big downfall in this classification technique. If a
person is visually identifying an action of another person, they monitor
patterns in the observations. By estimating the next observation, people have
the ability to summarize the actions, and thus determine, with pretty good
accuracy, the intention of the person performing the action. These visual cues
and linkages are important in creating intelligent algorithms for determining
human actions based on visual observations.
  The Evidence Feed Forward Hidden Markov Model is a newly developed algorithm
which provides observation to observation linkages. The following research
addresses the theory behind Evidence Feed Forward HMMs, provides mathematical
proofs of their learning of these parameters to optimize the likelihood of
observations with a Evidence Feed Forwards HMM, which is important in all
computational intelligence algorithm, and gives comparative examples with
standard HMMs in classification of both visual action data and measurement
data; thus providing a strong base for Evidence Feed Forward HMMs in
classification of many types of problems.
","Evidence Feed Forward Hidden Markov Model: A New Type of Hidden Markov
  Model","Michael DelRose, Christian Wagner, Philip Frederick",2011,Artificial Intelligence,
"  This paper presents a new multi-objective hybrid model that makes cooperation
between the strength of research of neighborhood methods presented by the tabu
search (TS) and the important exploration capacity of evolutionary algorithm.
This model was implemented and tested in benchmark functions (ZDT1, ZDT2, and
ZDT3), using a network of computers.
","Hybrid Model for Solving Multi-Objective Problems Using Evolutionary
  Algorithm and Tabu Search","Rjab Hajlaoui, Mariem Gzara, Abdelaziz Dammak",2011,Artificial Intelligence,
"  MAX-SAT heuristics normally operate from random initial truth assignments to
the variables. We consider the use of what we call preambles, which are
sequences of variables with corresponding single-variable assignment actions
intended to be used to determine a more suitable initial truth assignment for a
given problem instance and a given heuristic. For a number of well established
MAX-SAT heuristics and benchmark instances, we demonstrate that preambles can
be evolved by a genetic algorithm such that the heuristics are outperformed in
a significant fraction of the cases.
",Evolved preambles for MAX-SAT heuristics,"Luis O. Rigo Jr, Valmir C. Barbosa",2011,Artificial Intelligence,
"  Over the years, nonmonotonic rules have proven to be a very expressive and
useful knowledge representation paradigm. They have recently been used to
complement the expressive power of Description Logics (DLs), leading to the
study of integrative formal frameworks, generally referred to as hybrid
knowledge bases, where both DL axioms and rules can be used to represent
knowledge. The need to use these hybrid knowledge bases in dynamic domains has
called for the development of update operators, which, given the substantially
different way Description Logics and rules are usually updated, has turned out
to be an extremely difficult task.
  In [SL10], a first step towards addressing this problem was taken, and an
update operator for hybrid knowledge bases was proposed. Despite its
significance -- not only for being the first update operator for hybrid
knowledge bases in the literature, but also because it has some applications -
this operator was defined for a restricted class of problems where only the
ABox was allowed to change, which considerably diminished its applicability.
Many applications that use hybrid knowledge bases in dynamic scenarios require
both DL axioms and rules to be updated.
  In this paper, motivated by real world applications, we introduce an update
operator for a large class of hybrid knowledge bases where both the DL
component as well as the rule component are allowed to dynamically change. We
introduce splitting sequences and splitting theorem for hybrid knowledge bases,
use them to define a modular update semantics, investigate its basic
properties, and illustrate its use on a realistic example about cargo imports.
",Splitting and Updating Hybrid Knowledge Bases (Extended Version),Martin Slota and Jo\~ao Leite and Terrance Swift,2011,Artificial Intelligence,
"  A fundamental task for propositional logic is to compute models of
propositional formulas. Programs developed for this task are called
satisfiability solvers. We show that transition systems introduced by
Nieuwenhuis, Oliveras, and Tinelli to model and analyze satisfiability solvers
can be adapted for solvers developed for two other propositional formalisms:
logic programming under the answer-set semantics, and the logic PC(ID). We show
that in each case the task of computing models can be seen as ""satisfiability
modulo answer-set programming,"" where the goal is to find a model of a theory
that also is an answer set of a certain program. The unifying perspective we
develop shows, in particular, that solvers CLASP and MINISATID are closely
related despite being developed for different formalisms, one for answer-set
programming and the latter for the logic PC(ID).
",Transition Systems for Model Generators - A Unifying Approach,Yuliya Lierler and Miroslaw Truszczynski,2011,Artificial Intelligence,
"  Developing smart house systems has been a great challenge for researchers and
engineers in this area because of the high cost of implementation and
evaluation process of these systems, while being very time consuming. Testing a
designed smart house before actually building it is considered as an obstacle
towards an efficient smart house project. This is because of the variety of
sensors, home appliances and devices available for a real smart environment. In
this paper, we present the design and implementation of a multi-purpose smart
house simulation system for designing and simulating all aspects of a smart
house environment. This simulator provides the ability to design the house plan
and different virtual sensors and appliances in a two dimensional model of the
virtual house environment. This simulator can connect to any external smart
house remote controlling system, providing evaluation capabilities to their
system much easier than before. It also supports detailed adding of new
emerging sensors and devices to help maintain its compatibility with future
simulation needs. Scenarios can also be defined for testing various possible
combinations of device states; so different criteria and variables can be
simply evaluated without the need of experimenting on a real environment.
",A Multi-Purpose Scenario-based Simulator for Smart House Environments,"Zahra Forootan Jahromi, Amir Rajabzadeh and Ali Reza Manashty",2011,Artificial Intelligence,
"  Functional relationships between objects, called `attributes', are of
considerable importance in knowledge representation languages, including
Description Logics (DLs). A study of the literature indicates that papers have
made, often implicitly, different assumptions about the nature of attributes:
whether they are always required to have a value, or whether they can be
partial functions. The work presented here is the first explicit study of this
difference for subclasses of the CLASSIC DL, involving the same-as concept
constructor. It is shown that although determining subsumption between concept
descriptions has the same complexity (though requiring different algorithms),
the story is different in the case of determining the least common subsumer
(lcs). For attributes interpreted as partial functions, the lcs exists and can
be computed relatively easily; even in this case our results correct and extend
three previous papers about the lcs of DLs. In the case where attributes must
have a value, the lcs may not exist, and even if it exists it may be of
exponential size. Interestingly, it is possible to decide in polynomial time if
the lcs exists.
",What's in an Attribute? Consequences for the Least Common Subsumer,"A. Borgida, R. Kusters",2001,Artificial Intelligence,
"  We show that for several variations of partially observable Markov decision
processes, polynomial-time algorithms for finding control policies are unlikely
to or simply don't have guarantees of finding policies within a constant factor
or a constant summand of optimal. Here ""unlikely"" means ""unless some complexity
classes collapse,"" where the collapses considered are P=NP, P=PSPACE, or P=EXP.
Until or unless these collapses are shown to hold, any control-policy designer
must choose between such performance guarantees and efficient computation.
","Nonapproximability Results for Partially Observable Markov Decision
  Processes","J. Goldsmith, C. Lusena, M. Mundhenk",2001,Artificial Intelligence,
"  The chief aim of this paper is to propose mean-field approximations for a
broad class of Belief networks, of which sigmoid and noisy-or networks can be
seen as special cases. The approximations are based on a powerful mean-field
theory suggested by Plefka. We show that Saul, Jaakkola and Jordan' s approach
is the first order approximation in Plefka's approach, via a variational
derivation. The application of Plefka's theory to belief networks is not
computationally tractable. To tackle this problem we propose new approximations
based on Taylor series. Small scale experiments show that the proposed schemes
are attractive.
",Mean Field Methods for a Special Class of Belief Networks,"C. Bhattacharyya, S. S. Keerthi",2001,Artificial Intelligence,
"  In order to generate plans for agents with multiple actuators, agent teams,
or distributed controllers, we must be able to represent and plan using
concurrent actions with interacting effects. This has historically been
considered a challenging task requiring a temporal planner with the ability to
reason explicitly about time. We show that with simple modifications, the
STRIPS action representation language can be used to represent interacting
actions. Moreover, algorithms for partial-order planning require only small
modifications in order to be applied in such multiagent domains. We demonstrate
this fact by developing a sound and complete partial-order planner for planning
with concurrent interacting actions, POMP, that extends existing partial-order
planners in a straightforward way. These results open the way to the use of
partial-order planners for the centralized control of cooperative multiagent
systems.
",Partial-Order Planning with Concurrent Interacting Actions,"C. Boutilier, R. I. Brafman",2001,Artificial Intelligence,
"  Domain-independent planning is a hard combinatorial problem. Taking into
account plan quality makes the task even more difficult. This article
introduces Planning by Rewriting (PbR), a new paradigm for efficient
high-quality domain-independent planning. PbR exploits declarative
plan-rewriting rules and efficient local search techniques to transform an
easy-to-generate, but possibly suboptimal, initial plan into a high-quality
plan. In addition to addressing the issues of planning efficiency and plan
quality, this framework offers a new anytime planning algorithm. We have
implemented this planner and applied it to several existing domains. The
experimental results show that the PbR approach provides significant savings in
planning effort while generating high-quality plans.
",Planning by Rewriting,"J. L. Ambite, C. A. Knoblock",2001,Artificial Intelligence,
"  Partially observable Markov decision processes (POMDPs) have recently become
popular among many AI researchers because they serve as a natural model for
planning under uncertainty. Value iteration is a well-known algorithm for
finding optimal policies for POMDPs. It typically takes a large number of
iterations to converge. This paper proposes a method for accelerating the
convergence of value iteration. The method has been evaluated on an array of
benchmark problems and was found to be very effective: It enabled value
iteration to converge after only a few iterations on all the test problems.
","Speeding Up the Convergence of Value Iteration in Partially Observable
  Markov Decision Processes","N. L. Zhang, W. Zhang",2001,Artificial Intelligence,
"  In recent years, many improvements to backtracking algorithms for solving
constraint satisfaction problems have been proposed. The techniques for
improving backtracking algorithms can be conveniently classified as look-ahead
schemes and look-back schemes. Unfortunately, look-ahead and look-back schemes
are not entirely orthogonal as it has been observed empirically that the
enhancement of look-ahead techniques is sometimes counterproductive to the
effects of look-back techniques. In this paper, we focus on the relationship
between the two most important look-ahead techniques---using a variable
ordering heuristic and maintaining a level of local consistency during the
backtracking search---and the look-back technique of conflict-directed
backjumping (CBJ). We show that there exists a ""perfect"" dynamic variable
ordering such that CBJ becomes redundant. We also show theoretically that as
the level of local consistency that is maintained in the backtracking search is
increased, the less that backjumping will be an improvement. Our theoretical
results partially explain why a backtracking algorithm doing more in the
look-ahead phase cannot benefit more from the backjumping look-back scheme.
Finally, we show empirically that adding CBJ to a backtracking algorithm that
maintains generalized arc consistency (GAC), an algorithm that we refer to as
GAC-CBJ, can still provide orders of magnitude speedups. Our empirical results
contrast with Bessiere and Regin's conclusion (1996) that CBJ is useless to an
algorithm that maintains arc consistency.
",Conflict-Directed Backjumping Revisited,"X. Chen, P. van Beek",2001,Artificial Intelligence,
"  This paper presents an implemented system for recognizing the occurrence of
events described by simple spatial-motion verbs in short image sequences. The
semantics of these verbs is specified with event-logic expressions that
describe changes in the state of force-dynamic relations between the
participants of the event. An efficient finite representation is introduced for
the infinite sets of intervals that occur when describing liquid and
semi-liquid events. Additionally, an efficient procedure using this
representation is presented for inferring occurrences of compound events,
described with event-logic expressions, from occurrences of primitive events.
Using force dynamics and event logic to specify the lexical semantics of events
allows the system to be more robust than prior systems based on motion profile.
","Grounding the Lexical Semantics of Verbs in Visual Perception using
  Force Dynamics and Event Logic",J. M. Siskind,2001,Artificial Intelligence,
"  This paper presents an evolutionary algorithm with a new goal-sequence
domination scheme for better decision support in multi-objective optimization.
The approach allows the inclusion of advanced hard/soft priority and constraint
information on each objective component, and is capable of incorporating
multiple specifications with overlapping or non-overlapping objective functions
via logical 'OR' and 'AND' connectives to drive the search towards multiple
regions of trade-off. In addition, we propose a dynamic sharing scheme that is
simple and adaptively estimated according to the on-line population
distribution without needing any a priori parameter setting. Each feature in
the proposed algorithm is examined to show its respective contribution, and the
performance of the algorithm is compared with other evolutionary optimization
methods. It is shown that the proposed algorithm has performed well in the
diversity of evolutionary search and uniform distribution of non-dominated
individuals along the final trade-offs, without significant computational
effort. The algorithm is also applied to the design optimization of a practical
servo control system for hard disk drives with a single voice-coil-motor
actuator. Results of the evolutionary designed servo control system show a
superior closed-loop performance compared to classical PID or RPT approaches.
","An Evolutionary Algorithm with Advanced Goal and Priority Specification
  for Multi-objective Optimization","E. F. Khor, T. H. Lee, R. Sathikannan, K. C. Tan",2003,Artificial Intelligence,
"  This paper presents GRT, a domain-independent heuristic planning system for
STRIPS worlds. GRT solves problems in two phases. In the pre-processing phase,
it estimates the distance between each fact and the goals of the problem, in a
backward direction. Then, in the search phase, these estimates are used in
order to further estimate the distance between each intermediate state and the
goals, guiding so the search process in a forward direction and on a best-first
basis. The paper presents the benefits from the adoption of opposite directions
between the preprocessing and the search phases, discusses some difficulties
that arise in the pre-processing phase and introduces techniques to cope with
them. Moreover, it presents several methods of improving the efficiency of the
heuristic, by enriching the representation and by reducing the size of the
problem. Finally, a method of overcoming local optimal states, based on domain
axioms, is proposed. According to it, difficult problems are decomposed into
easier sub-problems that have to be solved sequentially. The performance
results from various domains, including those of the recent planning
competitions, show that GRT is among the fastest planners.
","The GRT Planning System: Backward Heuristic Construction in Forward
  State-Space Planning","I. Refanidis, I. Vlahavas",2001,Artificial Intelligence,
"  Gradient-based approaches to direct policy search in reinforcement learning
have received much recent attention as a means to solve problems of partial
observability and to avoid some of the problems associated with policy
degradation in value-function methods. In this paper we introduce GPOMDP, a
simulation-based algorithm for generating a {\em biased} estimate of the
gradient of the {\em average reward} in Partially Observable Markov Decision
Processes (POMDPs) controlled by parameterized stochastic policies. A similar
algorithm was proposed by Kimura, Yamamura, and Kobayashi (1995). The
algorithm's chief advantages are that it requires storage of only twice the
number of policy parameters, uses one free parameter $\beta\in [0,1)$ (which
has a natural interpretation in terms of bias-variance trade-off), and requires
no knowledge of the underlying state. We prove convergence of GPOMDP, and show
how the correct choice of the parameter $\beta$ is related to the {\em mixing
time} of the controlled POMDP. We briefly describe extensions of GPOMDP to
controlled Markov chains, continuous state, observation and control spaces,
multiple-agents, higher-order derivatives, and a version for training
stochastic policies with internal states. In a companion paper (Baxter,
Bartlett, & Weaver, 2001) we show how the gradient estimates generated by
GPOMDP can be used in both a traditional stochastic gradient algorithm and a
conjugate-gradient procedure to find local optima of the average reward
",Infinite-Horizon Policy-Gradient Estimation,Jonathan Baxter and Peter L. Bartlett,2001,Artificial Intelligence,
"  In this paper, we present algorithms that perform gradient ascent of the
average reward in a partially observable Markov decision process (POMDP). These
algorithms are based on GPOMDP, an algorithm introduced in a companion paper
(Baxter and Bartlett, this volume), which computes biased estimates of the
performance gradient in POMDPs. The algorithm's chief advantages are that it
uses only one free parameter beta, which has a natural interpretation in terms
of bias-variance trade-off, it requires no knowledge of the underlying state,
and it can be applied to infinite state, control and observation spaces. We
show how the gradient estimates produced by GPOMDP can be used to perform
gradient ascent, both with a traditional stochastic-gradient algorithm, and
with an algorithm based on conjugate-gradients that utilizes gradient
information to bracket maxima in line searches. Experimental results are
presented illustrating both the theoretical results of (Baxter and Bartlett,
this volume) on a toy problem, and practical aspects of the algorithms on a
number of more realistic problems.
","Experiments with Infinite-Horizon, Policy-Gradient Estimation","J. Baxter, P. L. Bartlett, L. Weaver",2001,Artificial Intelligence,
"  Description Logics (DLs) are suitable, well-known, logics for managing
structured knowledge. They allow reasoning about individuals and well defined
concepts, i.e., set of individuals with common properties. The experience in
using DLs in applications has shown that in many cases we would like to extend
their capabilities. In particular, their use in the context of Multimedia
Information Retrieval (MIR) leads to the convincement that such DLs should
allow the treatment of the inherent imprecision in multimedia object content
representation and retrieval. In this paper we will present a fuzzy extension
of ALC, combining Zadeh's fuzzy logic with a classical DL. In particular,
concepts becomes fuzzy and, thus, reasoning about imprecise concepts is
supported. We will define its syntax, its semantics, describe its properties
and present a constraint propagation calculus for reasoning in it.
",Reasoning within Fuzzy Description Logics,U. Straccia,2001,Artificial Intelligence,
"  Top-down induction of decision trees has been observed to suffer from the
inadequate functioning of the pruning phase. In particular, it is known that
the size of the resulting tree grows linearly with the sample size, even though
the accuracy of the tree does not improve. Reduced Error Pruning is an
algorithm that has been used as a representative technique in attempts to
explain the problems of decision tree learning. In this paper we present
analyses of Reduced Error Pruning in three different settings. First we study
the basic algorithmic properties of the method, properties that hold
independent of the input decision tree and pruning examples. Then we examine a
situation that intuitively should lead to the subtree under consideration to be
replaced by a leaf node, one in which the class label and attribute values of
the pruning examples are independent of each other. This analysis is conducted
under two different assumptions. The general analysis shows that the pruning
probability of a node fitting pure noise is bounded by a function that
decreases exponentially as the size of the tree grows. In a specific analysis
we assume that the examples are distributed uniformly to the tree. This
assumption lets us approximate the number of subtrees that are pruned because
they do not receive any pruning examples. This paper clarifies the different
variants of the Reduced Error Pruning algorithm, brings new insight to its
algorithmic properties, analyses the algorithm with less imposed assumptions
than before, and includes the previously overlooked empty subtrees to the
analysis.
",An Analysis of Reduced Error Pruning,"T. Elomaa, M. Kaariainen",2001,Artificial Intelligence,
"  This paper investigates the problems arising in the construction of a program
to play the game of contract bridge. These problems include both the difficulty
of solving the game's perfect information variant, and techniques needed to
address the fact that bridge is not, in fact, a perfect information game. GIB,
the program being described, involves five separate technical advances:
partition search, the practical application of Monte Carlo techniques to
realistic problems, a focus on achievable sets to solve problems inherent in
the Monte Carlo approach, an extension of alpha-beta pruning from total orders
to arbitrary distributive lattices, and the use of squeaky wheel optimization
to find approximately optimal solutions to cardplay problems. GIB is currently
believed to be of approximately expert caliber, and is currently the strongest
computer bridge program in the world.
",GIB: Imperfect Information in a Computationally Challenging Game,M. L. Ginsberg,2001,Artificial Intelligence,
"  Enforcing local consistencies is one of the main features of constraint
reasoning. Which level of local consistency should be used when searching for
solutions in a constraint network is a basic question. Arc consistency and
partial forms of arc consistency have been widely studied, and have been known
for sometime through the forward checking or the MAC search algorithms. Until
recently, stronger forms of local consistency remained limited to those that
change the structure of the constraint graph, and thus, could not be used in
practice, especially on large networks. This paper focuses on the local
consistencies that are stronger than arc consistency, without changing the
structure of the network, i.e., only removing inconsistent values from the
domains. In the last five years, several such local consistencies have been
proposed by us or by others. We make an overview of all of them, and highlight
some relations between them. We compare them both theoretically and
experimentally, considering their pruning efficiency and the time required to
enforce them.
",Domain Filtering Consistencies,"C. Bessiere, R. Debruyne",2001,Artificial Intelligence,
"  In this paper, we present a method for recognising an agent's behaviour in
dynamic, noisy, uncertain domains, and across multiple levels of abstraction.
We term this problem on-line plan recognition under uncertainty and view it
generally as probabilistic inference on the stochastic process representing the
execution of the agent's plan. Our contributions in this paper are twofold. In
terms of probabilistic inference, we introduce the Abstract Hidden Markov Model
(AHMM), a novel type of stochastic processes, provide its dynamic Bayesian
network (DBN) structure and analyse the properties of this network. We then
describe an application of the Rao-Blackwellised Particle Filter to the AHMM
which allows us to construct an efficient, hybrid inference method for this
model. In terms of plan recognition, we propose a novel plan recognition
framework based on the AHMM as the plan execution model. The Rao-Blackwellised
hybrid inference for AHMM can take advantage of the independence properties
inherent in a model of plan execution, leading to an algorithm for online
probabilistic plan recognition that scales well with the number of levels in
the plan hierarchy. This illustrates that while stochastic models for plan
execution can be complex, they exhibit special structures which, if exploited,
can lead to efficient plan recognition algorithms. We demonstrate the
usefulness of the AHMM framework via a behaviour recognition system in a
complex spatial environment using distributed video surveillance data.
",Policy Recognition in the Abstract Hidden Markov Model,"H. H. Bui, S. Venkatesh, G. West",2002,Artificial Intelligence,
"  We describe and evaluate the algorithmic techniques that are used in the FF
planning system. Like the HSP system, FF relies on forward state space search,
using a heuristic that estimates goal distances by ignoring delete lists.
Unlike HSP's heuristic, our method does not assume facts to be independent. We
introduce a novel search strategy that combines hill-climbing with systematic
search, and we show how other powerful heuristic information can be extracted
and used to prune the search space. FF was the most successful automatic
planner at the recent AIPS-2000 planning competition. We review the results of
the competition, give data for other benchmark domains, and investigate the
reasons for the runtime performance of FF compared to HSP.
",The FF Planning System: Fast Plan Generation Through Heuristic Search,"J. Hoffmann, B. Nebel",2001,Artificial Intelligence,
"  The First Trading Agent Competition (TAC) was held from June 22nd to July
8th, 2000. TAC was designed to create a benchmark problem in the complex domain
of e-marketplaces and to motivate researchers to apply unique approaches to a
common task. This article describes ATTac-2000, the first-place finisher in
TAC. ATTac-2000 uses a principled bidding strategy that includes several
elements of adaptivity. In addition to the success at the competition, isolated
empirical results are presented indicating the robustness and effectiveness of
ATTac-2000's adaptive strategy.
",ATTac-2000: An Adaptive Autonomous Bidding Agent,"M. Kearns, M. L. Littman, S. Singh, P. Stone",2001,Artificial Intelligence,
"  The theoretical properties of qualitative spatial reasoning in the RCC8
framework have been analyzed extensively. However, no empirical investigation
has been made yet. Our experiments show that the adaption of the algorithms
used for qualitative temporal reasoning can solve large RCC8 instances, even if
they are in the phase transition region -- provided that one uses the maximal
tractable subsets of RCC8 that have been identified by us. In particular, we
demonstrate that the orthogonal combination of heuristic methods is successful
in solving almost all apparently hard instances in the phase transition region
up to a certain size in reasonable time.
",Efficient Methods for Qualitative Spatial Reasoning,"B. Nebel, J. Renz",2001,Artificial Intelligence,
"  Hidden Markov models (HMMs) and partially observable Markov decision
processes (POMDPs) provide useful tools for modeling dynamical systems. They
are particularly useful for representing the topology of environments such as
road networks and office buildings, which are typical for robot navigation and
planning. The work presented here describes a formal framework for
incorporating readily available odometric information and geometrical
constraints into both the models and the algorithm that learns them. By taking
advantage of such information, learning HMMs/POMDPs can be made to generate
better solutions and require fewer iterations, while being robust in the face
of data reduction. Experimental results, obtained from both simulated and real
robot data, demonstrate the effectiveness of the approach.
","Learning Geometrically-Constrained Hidden Markov Models for Robot
  Navigation: Bridging the Topological-Geometrical Gap","L. P. Kaelbling, H. Shatkay",2002,Artificial Intelligence,
"  We propose a formalism for representation of finite languages, referred to as
the class of IDL-expressions, which combines concepts that were only considered
in isolation in existing formalisms. The suggested applications are in natural
language processing, more specifically in surface natural language generation
and in machine translation, where a sentence is obtained by first generating a
large set of candidate sentences, represented in a compact way, and then by
filtering such a set through a parser. We study several formal properties of
IDL-expressions and compare this new formalism with more standard ones. We also
present a novel parsing algorithm for IDL-expressions and prove a non-trivial
upper bound on its time complexity.
","IDL-Expressions: A Formalism for Representing and Parsing Finite
  Languages in Natural Language Processing","M. J. Nederhof, G. Satta",2004,Artificial Intelligence,
"  Hierarchical latent class (HLC) models are tree-structured Bayesian networks
where leaf nodes are observed while internal nodes are latent. There are no
theoretically well justified model selection criteria for HLC models in
particular and Bayesian networks with latent nodes in general. Nonetheless,
empirical studies suggest that the BIC score is a reasonable criterion to use
in practice for learning HLC models. Empirical studies also suggest that
sometimes model selection can be improved if standard model dimension is
replaced with effective model dimension in the penalty term of the BIC score.
Effective dimensions are difficult to compute. In this paper, we prove a
theorem that relates the effective dimension of an HLC model to the effective
dimensions of a number of latent class models. The theorem makes it
computationally feasible to compute the effective dimensions of large HLC
models. The theorem can also be used to compute the effective dimensions of
general tree models.
",Effective Dimensions of Hierarchical Latent Class Models,"T. Kocka, N. L. Zhang",2004,Artificial Intelligence,
"  We introduce an abductive method for a coherent integration of independent
data-sources. The idea is to compute a list of data-facts that should be
inserted to the amalgamated database or retracted from it in order to restore
its consistency. This method is implemented by an abductive solver, called
Asystem, that applies SLDNFA-resolution on a meta-theory that relates
different, possibly contradicting, input databases. We also give a pure
model-theoretic analysis of the possible ways to `recover' consistent data from
an inconsistent database in terms of those models of the database that exhibit
as minimal inconsistent information as reasonably possible. This allows us to
characterize the `recovered databases' in terms of the `preferred' (i.e., most
consistent) models of the theory. The outcome is an abductive-based application
that is sound and complete with respect to a corresponding model-based,
preferential semantics, and -- to the best of our knowledge -- is more
expressive (thus more general) than any other implementation of coherent
integration of databases.
",Coherent Integration of Databases by Abductive Logic Programming,"O. Arieli, M. Bruynooghe, M. Denecker, B. Van Nuffelen",2004,Artificial Intelligence,
"  We present a visually-grounded language understanding model based on a study
of how people verbally describe objects in scenes. The emphasis of the model is
on the combination of individual word meanings to produce meanings for complex
referring expressions. The model has been implemented, and it is able to
understand a broad range of spatial referring expressions. We describe our
implementation of word level visually-grounded semantics and their embedding in
a compositional parsing framework. The implemented system selects the correct
referents in response to natural language expressions for a large percentage of
test cases. In an analysis of the system's successes and failures we reveal how
visual context influences the semantics of utterances and propose future
extensions to the model that take such context into account.
",Grounded Semantic Composition for Visual Scenes,"P. Gorniak, D. Roy",2004,Artificial Intelligence,
"  The 2002 Trading Agent Competition (TAC) presented a challenging market game
in the domain of travel shopping. One of the pivotal issues in this domain is
uncertainty about hotel prices, which have a significant influence on the
relative cost of alternative trip schedules. Thus, virtually all participants
employ some method for predicting hotel prices. We survey approaches employed
in the tournament, finding that agents apply an interesting diversity of
techniques, taking into account differing sources of evidence bearing on
prices. Based on data provided by entrants on their agents' actual predictions
in the TAC-02 finals and semifinals, we analyze the relative efficacy of these
approaches. The results show that taking into account game-specific information
about flight prices is a major distinguishing factor. Machine learning methods
effectively induce the relationship between flight and hotel prices from game
data, and a purely analytical approach based on competitive equilibrium
analysis achieves equal accuracy with no historical data. Employing a new
measure of prediction quality, we relate absolute accuracy to bottom-line
performance in the game.
",Price Prediction in a Trading Agent Competition,"K. M. Lochner, D. M. Reeves, Y. Vorobeychik, M. P. Wellman",2004,Artificial Intelligence,
"  The predominant knowledge-based approach to automated model construction,
compositional modelling, employs a set of models of particular functional
components. Its inference mechanism takes a scenario describing the constituent
interacting components of a system and translates it into a useful mathematical
model. This paper presents a novel compositional modelling approach aimed at
building model repositories. It furthers the field in two respects. Firstly, it
expands the application domain of compositional modelling to systems that can
not be easily described in terms of interacting functional components, such as
ecological systems. Secondly, it enables the incorporation of user preferences
into the model selection process. These features are achieved by casting the
compositional modelling problem as an activity-based dynamic preference
constraint satisfaction problem, where the dynamic constraints describe the
restrictions imposed over the composition of partial models and the preferences
correspond to those of the user of the automated modeller. In addition, the
preference levels are represented through the use of symbolic values that
differ in orders of magnitude.
","Compositional Model Repositories via Dynamic Constraint Satisfaction
  with Order-of-Magnitude Preferences","J. Keppens, Q. Shen",2004,Artificial Intelligence,
"  A novel algorithm for actively trading stocks is presented. While traditional
expert advice and ""universal"" algorithms (as well as standard technical trading
heuristics) attempt to predict winners or trends, our approach relies on
predictable statistical relations between all pairs of stocks in the market.
Our empirical results on historical markets provide strong evidence that this
type of technical trading can ""beat the market"" and moreover, can beat the best
stock in the market. In doing so we utilize a new idea for smoothing critical
parameters in the context of expert learning.
",Can We Learn to Beat the Best Stock,"A. Borodin, R. El-Yaniv, V. Gogan",2004,Artificial Intelligence,
"  Two major goals in machine learning are the discovery and improvement of
solutions to complex problems. In this paper, we argue that complexification,
i.e. the incremental elaboration of solutions through adding new structure,
achieves both these goals. We demonstrate the power of complexification through
the NeuroEvolution of Augmenting Topologies (NEAT) method, which evolves
increasingly complex neural network architectures. NEAT is applied to an
open-ended coevolutionary robot duel domain where robot controllers compete
head to head. Because the robot duel domain supports a wide range of
strategies, and because coevolution benefits from an escalating arms race, it
serves as a suitable testbed for studying complexification. When compared to
the evolution of networks with fixed structure, complexifying evolution
discovers significantly more sophisticated strategies. The results suggest that
in order to discover and improve complex solutions, evolution, and search in
general, should be allowed to complexify as well as optimize.
",Competitive Coevolution through Evolutionary Complexification,"R. Miikkulainen, K. O. Stanley",2004,Artificial Intelligence,
"  When writing a constraint program, we have to choose which variables should
be the decision variables, and how to represent the constraints on these
variables. In many cases, there is considerable choice for the decision
variables. Consider, for example, permutation problems in which we have as many
values as variables, and each variable takes an unique value. In such problems,
we can choose between a primal and a dual viewpoint. In the dual viewpoint,
each dual variable represents one of the primal values, whilst each dual value
represents one of the primal variables. Alternatively, by means of channelling
constraints to link the primal and dual variables, we can have a combined model
with both sets of variables. In this paper, we perform an extensive theoretical
and empirical study of such primal, dual and combined models for two classes of
problems: permutation problems and injection problems. Our results show that it
often be advantageous to use multiple viewpoints, and to have constraints which
channel between them to maintain consistency. They also illustrate a general
methodology for comparing different constraint models.
",Dual Modelling of Permutation and Injection Problems,"B. Hnich, B. M. Smith, T. Walsh",2004,Artificial Intelligence,
"  This is the first of three planned papers describing ZAP, a satisfiability
engine that substantially generalizes existing tools while retaining the
performance characteristics of modern high-performance solvers. The fundamental
idea underlying ZAP is that many problems passed to such engines contain rich
internal structure that is obscured by the Boolean representation used; our
goal is to define a representation in which this structure is apparent and can
easily be exploited to improve computational performance. This paper is a
survey of the work underlying ZAP, and discusses previous attempts to improve
the performance of the Davis-Putnam-Logemann-Loveland algorithm by exploiting
the structure of the problem being solved. We examine existing ideas including
extensions of the Boolean language to allow cardinality constraints,
pseudo-Boolean representations, symmetry, and a limited form of quantification.
While this paper is intended as a survey, our research results are contained in
the two subsequent articles, with the theoretical structure of ZAP described in
the second paper in this series, and ZAP's implementation described in the
third.
","Generalizing Boolean Satisfiability I: Background and Survey of Existing
  Work","H. E. Dixon, M. L. Ginsberg, A. J. Parkes",2004,Artificial Intelligence,
"  We address the problem of finding the shortest path between two points in an
unknown real physical environment, where a traveling agent must move around in
the environment to explore unknown territory. We introduce the Physical-A*
algorithm (PHA*) for solving this problem. PHA* expands all the mandatory nodes
that A* would expand and returns the shortest path between the two points.
However, due to the physical nature of the problem, the complexity of the
algorithm is measured by the traveling effort of the moving agent and not by
the number of generated nodes, as in standard A*. PHA* is presented as a
two-level algorithm, such that its high level, A*, chooses the next node to be
expanded and its low level directs the agent to that node in order to explore
it. We present a number of variations for both the high-level and low-level
procedures and evaluate their performance theoretically and experimentally. We
show that the travel cost of our best variation is fairly close to the optimal
travel cost, assuming that the mandatory nodes of A* are known in advance. We
then generalize our algorithm to the multi-agent case, where a number of
cooperative agents are designed to solve the problem. Specifically, we provide
an experimental implementation for such a system. It should be noted that the
problem addressed here is not a navigation problem, but rather a problem of
finding the shortest path between two points for future usage.
","PHA*: Finding the Shortest Path with A* in An Unknown Physical
  Environment","A. Ben-Yair, A. Felner, S. Kraus, N. Netanyahu, R. Stern",2004,Artificial Intelligence,
"  Value iteration is a popular algorithm for finding near optimal policies for
POMDPs. It is inefficient due to the need to account for the entire belief
space, which necessitates the solution of large numbers of linear programs. In
this paper, we study value iteration restricted to belief subsets. We show
that, together with properly chosen belief subsets, restricted value iteration
yields near-optimal policies and we give a condition for determining whether a
given belief subset would bring about savings in space and time. We also apply
restricted value iteration to two interesting classes of POMDPs, namely
informative POMDPs and near-discernible POMDPs.
",Restricted Value Iteration: Theory and Algorithms,"N. L. Zhang, W. Zhang",2005,Artificial Intelligence,
"  Many researchers in artificial intelligence are beginning to explore the use
of soft constraints to express a set of (possibly conflicting) problem
requirements. A soft constraint is a function defined on a collection of
variables which associates some measure of desirability with each possible
combination of values for those variables. However, the crucial question of the
computational complexity of finding the optimal solution to a collection of
soft constraints has so far received very little attention. In this paper we
identify a class of soft binary constraints for which the problem of finding
the optimal solution is tractable. In other words, we show that for any given
set of such constraints, there exists a polynomial time algorithm to determine
the assignment having the best overall combined measure of desirability. This
tractable class includes many commonly-occurring soft constraints, such as 'as
near as possible' or 'as soon as possible after', as well as crisp constraints
such as 'greater than'. Finally, we show that this tractable class is maximal,
in the sense that adding any other form of soft binary constraint which is not
in the class gives rise to a class of problems which is NP-hard.
",A Maximal Tractable Class of Soft Constraints,"D. Cohen, M. Cooper, P. Jeavons, A. Krokhin",2004,Artificial Intelligence,
"  Efficient implementations of DPLL with the addition of clause learning are
the fastest complete Boolean satisfiability solvers and can handle many
significant real-world problems, such as verification, planning and design.
Despite its importance, little is known of the ultimate strengths and
limitations of the technique. This paper presents the first precise
characterization of clause learning as a proof system (CL), and begins the task
of understanding its power by relating it to the well-studied resolution proof
system. In particular, we show that with a new learning scheme, CL can provide
exponentially shorter proofs than many proper refinements of general resolution
(RES) satisfying a natural property. These include regular and Davis-Putnam
resolution, which are already known to be much stronger than ordinary DPLL. We
also show that a slight variant of CL with unlimited restarts is as powerful as
RES itself. Translating these analytical results to practice, however, presents
a challenge because of the nondeterministic nature of clause learning
algorithms. We propose a novel way of exploiting the underlying problem
structure, in the form of a high level problem description such as a graph or
PDDL specification, to guide clause learning algorithms toward faster
solutions. We show that this leads to exponential speed-ups on grid and
randomized pebbling problems, as well as substantial improvements on certain
ordering formulas.
",Towards Understanding and Harnessing the Potential of Clause Learning,"P. Beame, H. Kautz, A. Sabharwal",2004,Artificial Intelligence,
"  Argumentation is based on the exchange and valuation of interacting
arguments, followed by the selection of the most acceptable of them (for
example, in order to take a decision, to make a choice). Starting from the
framework proposed by Dung in 1995, our purpose is to introduce 'graduality' in
the selection of the best arguments, i.e., to be able to partition the set of
the arguments in more than the two usual subsets of 'selected' and
'non-selected' arguments in order to represent different levels of selection.
Our basic idea is that an argument is all the more acceptable if it can be
preferred to its attackers. First, we discuss general principles underlying a
'gradual' valuation of arguments based on their interactions. Following these
principles, we define several valuation models for an abstract argumentation
system. Then, we introduce 'graduality' in the concept of acceptability of
arguments. We propose new acceptability classes and a refinement of existing
classes taking advantage of an available 'gradual' valuation.
",Graduality in Argumentation,"C. Cayrol, M. C. Lagasquie-Schiex",2005,Artificial Intelligence,
"  Inductive learning is based on inferring a general rule from a finite data
set and using it to label new data. In transduction one attempts to solve the
problem of using a labeled training set to label a set of unlabeled points,
which are given to the learner prior to learning. Although transduction seems
at the outset to be an easier task than induction, there have not been many
provably useful algorithms for transduction. Moreover, the precise relation
between induction and transduction has not yet been determined. The main
theoretical developments related to transduction were presented by Vapnik more
than twenty years ago. One of Vapnik's basic results is a rather tight error
bound for transductive classification based on an exact computation of the
hypergeometric tail. While tight, this bound is given implicitly via a
computational routine. Our first contribution is a somewhat looser but explicit
characterization of a slightly extended PAC-Bayesian version of Vapnik's
transductive bound. This characterization is obtained using concentration
inequalities for the tail of sums of random variables obtained by sampling
without replacement. We then derive error bounds for compression schemes such
as (transductive) support vector machines and for transduction algorithms based
on clustering. The main observation used for deriving these new error bounds
and algorithms is that the unlabeled test points, which in the transductive
setting are known in advance, can be used in order to construct useful data
dependent prior distributions over the hypothesis space.
","Explicit Learning Curves for Transduction and Application to Clustering
  and Compression Algorithms","P. Derbeko, R. El-Yaniv, R. Meir",2004,Artificial Intelligence,
"  Decentralized control of cooperative systems captures the operation of a
group of decision makers that share a single global objective. The difficulty
in solving optimally such problems arises when the agents lack full
observability of the global state of the system when they operate. The general
problem has been shown to be NEXP-complete. In this paper, we identify classes
of decentralized control problems whose complexity ranges between NEXP and P.
In particular, we study problems characterized by independent transitions,
independent observations, and goal-oriented objective functions. Two algorithms
are shown to solve optimally useful classes of goal-oriented decentralized
processes in polynomial time. This paper also studies information sharing among
the decision-makers, which can improve their performance. We distinguish
between three ways in which agents can exchange information: indirect
communication, direct communication and sharing state features that are not
controlled by the agents. Our analysis shows that for every class of problems
we consider, introducing direct or indirect communication does not change the
worst-case complexity. The results provide a better understanding of the
complexity of decentralized control problems that arise in practice and
facilitate the development of planning algorithms for these problems.
","Decentralized Control of Cooperative Systems: Categorization and
  Complexity Analysis","C. V. Goldman, S. Zilberstein",2004,Artificial Intelligence,
"  In this paper, we confront the problem of applying reinforcement learning to
agents that perceive the environment through many sensors and that can perform
parallel actions using many actuators as is the case in complex autonomous
robots. We argue that reinforcement learning can only be successfully applied
to this case if strong assumptions are made on the characteristics of the
environment in which the learning is performed, so that the relevant sensor
readings and motor commands can be readily identified. The introduction of such
assumptions leads to strongly-biased learning systems that can eventually lose
the generality of traditional reinforcement-learning algorithms. In this line,
we observe that, in realistic situations, the reward received by the robot
depends only on a reduced subset of all the executed actions and that only a
reduced subset of the sensor inputs (possibly different in each situation and
for each action) are relevant to predict the reward. We formalize this property
in the so called 'categorizability assumption' and we present an algorithm that
takes advantage of the categorizability of the environment, allowing a decrease
in the learning time with respect to existing reinforcement-learning
algorithms. Results of the application of the algorithm to a couple of
simulated realistic-robotic problems (landmark-based navigation and the
six-legged robot gait generation) are reported to validate our approach and to
compare it to existing flat and generalization-based reinforcement-learning
approaches.
","Reinforcement Learning for Agents with Many Sensors and Actuators Acting
  in Categorizable Environments","E. Celaya, J. M. Porta",2005,Artificial Intelligence,
"  We explore a method for computing admissible heuristic evaluation functions
for search problems. It utilizes pattern databases, which are precomputed
tables of the exact cost of solving various subproblems of an existing problem.
Unlike standard pattern database heuristics, however, we partition our problems
into disjoint subproblems, so that the costs of solving the different
subproblems can be added together without overestimating the cost of solving
the original problem. Previously, we showed how to statically partition the
sliding-tile puzzles into disjoint groups of tiles to compute an admissible
heuristic, using the same partition for each state and problem instance. Here
we extend the method and show that it applies to other domains as well. We also
present another method for additive heuristics which we call dynamically
partitioned pattern databases. Here we partition the problem into disjoint
subproblems for each state of the search dynamically. We discuss the pros and
cons of each of these methods and apply both methods to three different problem
domains: the sliding-tile puzzles, the 4-peg Towers of Hanoi problem, and
finding an optimal vertex cover of a graph. We find that in some problem
domains, static partitioning is most effective, while in others dynamic
partitioning is a better choice. In each of these problem domains, either
statically partitioned or dynamically partitioned pattern database heuristics
are the best known heuristics for the problem.
",Additive Pattern Database Heuristics,"A. Felner, S. Hanan, R. E. Korf",2004,Artificial Intelligence,
"  This paper is concerned with algorithms for prediction of discrete sequences
over a finite alphabet, using variable order Markov models. The class of such
algorithms is large and in principle includes any lossless compression
algorithm. We focus on six prominent prediction algorithms, including Context
Tree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic
Suffix Trees (PSTs). We discuss the properties of these algorithms and compare
their performance using real life sequences from three domains: proteins,
English text and music pieces. The comparison is made with respect to
prediction quality as measured by the average log-loss. We also compare
classification algorithms based on these predictors with respect to a number of
large protein classification tasks. Our results indicate that a ""decomposed""
CTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in
sequence prediction tasks. Somewhat surprisingly, a different algorithm, which
is a modification of the Lempel-Ziv compression algorithm, significantly
outperforms all algorithms on the protein classification problems.
",On Prediction Using Variable Order Markov Models,"R. Begleiter, R. El-Yaniv, G. Yona",2004,Artificial Intelligence,
"  Many known planning tasks have inherent constraints concerning the best order
in which to achieve the goals. A number of research efforts have been made to
detect such constraints and to use them for guiding search, in the hope of
speeding up the planning process. We go beyond the previous approaches by
considering ordering constraints not only over the (top-level) goals, but also
over the sub-goals that will necessarily arise during planning. Landmarks are
facts that must be true at some point in every valid solution plan. We extend
Koehler and Hoffmann's definition of reasonable orders between top level goals
to the more general case of landmarks. We show how landmarks can be found, how
their reasonable orders can be approximated, and how this information can be
used to decompose a given planning task into several smaller sub-tasks. Our
methodology is completely domain- and planner-independent. The implementation
demonstrates that the approach can yield significant runtime performance
improvements when used as a control loop around state-of-the-art sub-optimal
planning systems, as exemplified by FF and LPG.
",Ordered Landmarks in Planning,"J. Hoffmann, J. Porteous, L. Sebastia",2004,Artificial Intelligence,
"  Standard value function approaches to finding policies for Partially
Observable Markov Decision Processes (POMDPs) are generally considered to be
intractable for large models. The intractability of these algorithms is to a
large extent a consequence of computing an exact, optimal policy over the
entire belief space. However, in real-world POMDP problems, computing the
optimal policy for the full belief space is often unnecessary for good control
even for problems with complicated policy classes. The beliefs experienced by
the controller often lie near a structured, low-dimensional subspace embedded
in the high-dimensional belief space. Finding a good approximation to the
optimal value function for only this subspace can be much easier than computing
the full value function. We introduce a new method for solving large-scale
POMDPs by reducing the dimensionality of the belief space. We use Exponential
family Principal Components Analysis (Collins, Dasgupta and Schapire, 2002) to
represent sparse, high-dimensional belief spaces using small sets of learned
features of the belief state. We then plan only in terms of the low-dimensional
belief features. By planning in this low-dimensional space, we can find
policies for POMDP models that are orders of magnitude larger than models that
can be handled by conventional techniques. We demonstrate the use of this
algorithm on a synthetic problem and on mobile robot navigation tasks.
",Finding Approximate POMDP solutions Through Belief Compression,"N. Roy, G. Gordon, S. Thrun",2005,Artificial Intelligence,
"  We propose a model for errors in sung queries, a variant of the hidden Markov
model (HMM). This is a solution to the problem of identifying the degree of
similarity between a (typically error-laden) sung query and a potential target
in a database of musical works, an important problem in the field of music
information retrieval. Similarity metrics are a critical component of
query-by-humming (QBH) applications which search audio and multimedia databases
for strong matches to oral queries. Our model comprehensively expresses the
types of error or variation between target and query: cumulative and
non-cumulative local errors, transposition, tempo and tempo changes,
insertions, deletions and modulation. The model is not only expressive, but
automatically trainable, or able to learn and generalize from query examples.
We present results of simulations, designed to assess the discriminatory
potential of the model, and tests with real sung queries, to demonstrate
relevance to real-world applications.
",A Comprehensive Trainable Error Model for Sung Music Queries,"W. P. Birmingham, C. J. Meek",2004,Artificial Intelligence,
"  In recent years, there has been much interest in phase transitions of
combinatorial problems. Phase transitions have been successfully used to
analyze combinatorial optimization problems, characterize their typical-case
features and locate the hardest problem instances. In this paper, we study
phase transitions of the asymmetric Traveling Salesman Problem (ATSP), an
NP-hard combinatorial optimization problem that has many real-world
applications. Using random instances of up to 1,500 cities in which intercity
distances are uniformly distributed, we empirically show that many properties
of the problem, including the optimal tour cost and backbone size, experience
sharp transitions as the precision of intercity distances increases across a
critical value. Our experimental results on the costs of the ATSP tours and
assignment problem agree with the theoretical result that the asymptotic cost
of assignment problem is pi ^2 /6 the number of cities goes to infinity. In
addition, we show that the average computational cost of the well-known
branch-and-bound subtour elimination algorithm for the problem also exhibits a
thrashing behavior, transitioning from easy to difficult as the distance
precision increases. These results answer positively an open question regarding
the existence of phase transitions in the ATSP, and provide guidance on how
difficult ATSP problem instances should be generated.
","Phase Transitions and Backbones of the Asymmetric Traveling Salesman
  Problem",W. Zhang,2004,Artificial Intelligence,
"  We present in this paper our law that there is always a connection present
between two entities, with a selfconnection being present at least in each
node. An entity is an object, physical or imaginary, that is connected by a
path (or connection) and which is important for achieving the desired result of
the scenario. In machine learning, we state that for any scenario, a subject
entity is always, directly or indirectly, connected and affected by single or
multiple independent / dependent entities, and their impact on the subject
entity is dependent on various factors falling into the categories such as the
existenc
",Law of Connectivity in Machine Learning,Jitesh Dundas,2010,Artificial Intelligence,
"  Many real world domains require the representation of a measure of
uncertainty. The most common such representation is probability, and the
combination of probability with logic programs has given rise to the field of
Probabilistic Logic Programming (PLP), leading to languages such as the
Independent Choice Logic, Logic Programs with Annotated Disjunctions (LPADs),
Problog, PRISM and others. These languages share a similar distribution
semantics, and methods have been devised to translate programs between these
languages. The complexity of computing the probability of queries to these
general PLP programs is very high due to the need to combine the probabilities
of explanations that may not be exclusive. As one alternative, the PRISM system
reduces the complexity of query answering by restricting the form of programs
it can evaluate. As an entirely different alternative, Possibilistic Logic
Programs adopt a simpler metric of uncertainty than probability. Each of these
approaches -- general PLP, restricted PLP, and Possibilistic Logic Programming
-- can be useful in different domains depending on the form of uncertainty to
be represented, on the form of programs needed to model problems, and on the
scale of the problems to be solved. In this paper, we show how the PITA system,
which originally supported the general PLP language of LPADs, can also
efficiently support restricted PLP and Possibilistic Logic Programs. PITA
relies on tabling with answer subsumption and consists of a transformation
along with an API for library functions that interface with answer subsumption.
","The PITA System: Tabling and Answer Subsumption for Reasoning under
  Uncertainty",Fabrizio Riguzzi and Terrance Swift,2011,Artificial Intelligence,
"  Given a causal model of some domain and a particular story that has taken
place in this domain, the problem of actual causation is deciding which of the
possible causes for some effect actually caused it. One of the most influential
approaches to this problem has been developed by Halpern and Pearl in the
context of structural models. In this paper, I argue that this is actually not
the best setting for studying this problem. As an alternative, I offer the
probabilistic logic programming language of CP-logic. Unlike structural models,
CP-logic incorporates the deviant/default distinction that is generally
considered an important aspect of actual causation, and it has an explicitly
dynamic semantics, which helps to formalize the stories that serve as input to
an actual causation problem.
",Actual Causation in CP-logic,Joost Vennekens,2011,Artificial Intelligence,
"  Perfectly rational decision-makers maximize expected utility, but crucially
ignore the resource costs incurred when determining optimal actions. Here we
employ an axiomatic framework for bounded rational decision-making based on a
thermodynamic interpretation of resource costs as information costs. This leads
to a variational ""free utility"" principle akin to thermodynamical free energy
that trades off utility and information costs. We show that bounded optimal
control solutions can be derived from this variational principle, which leads
in general to stochastic policies. Furthermore, we show that risk-sensitive and
robust (minimax) control schemes fall out naturally from this framework if the
environment is considered as a bounded rational and perfectly rational
opponent, respectively. When resource costs are ignored, the maximum expected
utility principle is recovered.
","Information, Utility & Bounded Rationality",Pedro A. Ortega and Daniel A. Braun,2011,Artificial Intelligence,
"  Improvement of time series forecasting accuracy through combining multiple
models is an important as well as a dynamic area of research. As a result,
various forecasts combination methods have been developed in literature.
However, most of them are based on simple linear ensemble strategies and hence
ignore the possible relationships between two or more participating models. In
this paper, we propose a robust weighted nonlinear ensemble technique which
considers the individual forecasts from different models as well as the
correlations among them while combining. The proposed ensemble is constructed
using three well-known forecasting models and is tested for three real-world
time series. A comparison is made among the proposed scheme and three other
widely used linear combination methods, in terms of the obtained forecast
errors. This comparison shows that our ensemble scheme provides significantly
lower forecast errors than each individual model as well as each of the four
linear combination methods.
","Combining Multiple Time Series Models Through A Robust Weighted
  Mechanism","Ratnadip Adhikari, R. K. Agrawal",2012,Artificial Intelligence,
"  Every cellular network deployment requires planning and optimization in order
to provide adequate coverage, capacity, and quality of service (QoS).
Optimization mobile radio network planning is a very complex task, as many
aspects must be taken into account. With the rapid development in mobile
network we need effective network planning tool to satisfy the need of
customers. However, deciding upon the optimum placement for the base stations
(BS s) to achieve best services while reducing the cost is a complex task
requiring vast computational resource. This paper introduces the spatial
clustering to solve the Mobile Networking Planning problem. It addresses
antenna placement problem or the cell planning problem, involves locating and
configuring infrastructure for mobile networks by modified the original
Partitioning Around Medoids PAM algorithm. M-PAM (Modified Partitioning Around
Medoids) has been proposed to satisfy the requirements and constraints. PAM
needs to specify number of clusters (k) before starting to search for the best
locations of base stations. The M-PAM algorithm uses the radio network planning
to determine k. We calculate for each cluster its coverage and capacity and
determine if they satisfy the mobile requirements, if not we will increase (k)
and reapply algorithms depending on two methods for clustering. Implementation
of this algorithm to a real case study is presented. Experimental results and
analysis indicate that the M-PAM algorithm when applying method two is
effective in case of heavy load distribution, and leads to minimum number of
base stations, which directly affected onto the cost of planning the network.
","Using Modified Partitioning Around Medoids Clustering Technique in
  Mobile Network Planning","Lamiaa Fattouh Ibrahim, Manal Hamed Al Harbi",2012,Artificial Intelligence,
"  We analyze the meaning of the violation of the marginal probability law for
situations of correlation measurements where entanglement is identified. We
show that for quantum theory applied to the cognitive realm such a violation
does not lead to the type of problems commonly believed to occur in situations
of quantum theory applied to the physical realm. We briefly situate our quantum
approach for modeling concepts and their combinations with respect to the
notions of 'extension' and 'intension' in theories of meaning, and in existing
concept theories.
","Quantum and Concept Combination, Entangled Measurements and Prototype
  Theory",Diederik Aerts,2014,Artificial Intelligence,
"  Associative classification is a recent and rewarding technique which
integrates association rule mining and classification to a model for prediction
and achieves maximum accuracy. Associative classifiers are especially fit to
applications where maximum accuracy is desired to a model for prediction. There
are many domains such as medical where the maximum accuracy of the model is
desired. Heart disease is a single largest cause of death in developed
countries and one of the main contributors to disease burden in developing
countries. Mortality data from the registrar general of India shows that heart
disease are a major cause of death in India, and in Andhra Pradesh coronary
heart disease cause about 30%of deaths in rural areas. Hence there is a need to
develop a decision support system for predicting heart disease of a patient. In
this paper we propose efficient associative classification algorithm using
genetic approach for heart disease prediction. The main motivation for using
genetic algorithm in the discovery of high level prediction rules is that the
discovered rules are highly comprehensible, having high predictive accuracy and
of high interestingness values. Experimental Results show that most of the
classifier rules help in the best prediction of heart disease which even helps
doctors in their diagnosis decisions.
","Heart Disease Prediction System using Associative Classification and
  Genetic Algorithm","M.Akhil Jabbar, B L Deekshatulu, Priti Chandra",2012,Artificial Intelligence,
"  We put forward a general classification for a structural description of the
entanglement present in compound entities experimentally violating Bell's
inequalities, making use of a new entanglement scheme that we developed
recently. Our scheme, although different from the traditional one, is
completely compatible with standard quantum theory, and enables quantum
modeling in complex Hilbert space for different types of situations. Namely,
situations where entangled states and product measurements appear ('customary
quantum modeling'), and situations where states and measurements and evolutions
between measurements are entangled ('nonlocal box modeling', 'nonlocal
non-marginal box modeling'). The role played by Tsirelson's bound and marginal
distribution law is emphasized. Specific quantum models are worked out in
detail in complex Hilbert space within this new entanglement scheme.
",Entanglement Zoo I: Foundational and Structural Aspects,Diederik Aerts and Sandro Sozzo,2014,Artificial Intelligence,
"  We have recently presented a general scheme enabling quantum modeling of
different types of situations that violate Bell's inequalities. In this paper,
we specify this scheme for a combination of two concepts. We work out a quantum
Hilbert space model where 'entangled measurements' occur in addition to the
expected 'entanglement between the component concepts', or 'state
entanglement'. We extend this result to a macroscopic physical entity, the
'connected vessels of water', which maximally violates Bell's inequalities. We
enlighten the structural and conceptual analogies between the cognitive and
physical situations which are both examples of a nonlocal non-marginal box
modeling in our classification.
",Entanglement Zoo II: Examples in Physics and Cognition,Diederik Aerts and Sandro Sozzo,2014,Artificial Intelligence,
"  Graph knowledge models and ontologies are very powerful modeling and re
asoning tools. We propose an effective approach to model network attacks and
attack prediction which plays important roles in security management. The goals
of this study are: First we model network attacks, their prerequisites and
consequences using knowledge representation methods in order to provide
description logic reasoning and inference over attack domain concepts. And
secondly, we propose an ontology-based system which predicts potential attacks
using inference and observing information which provided by sensory inputs. We
generate our ontology and evaluate corresponding methods using CAPEC, CWE, and
CVE hierarchical datasets. Results from experiments show significant capability
improvements comparing to traditional hierarchical and relational models.
Proposed method also reduces false alarms and improves intrusion detection
effectiveness.
",Predicting Network Attacks Using Ontology-Driven Inference,"Ahmad Salahi, Morteza Ansarinia",2012,Artificial Intelligence,
"  This article addresses the problem of expressing preferences in flexible
queries while basing on a combination of the fuzzy logic theory and Conditional
Preference Networks or CP-Nets.
",Mod\`ele flou d'expression des pr\'ef\'erences bas\'e sur les CP-Nets,Hanene Rezgui and Minyar Sassi-Hidri,2013,Artificial Intelligence,
"  In the interaction between agents we can have an explicative discourse, when
communicating preferences or intentions, and a normative discourse, when
considering normative knowledge. For justifying their actions our agents are
endowed with a Justification and Explanation Logic (JEL), capable to cover both
the justification for their commitments and explanations why they had to act in
that way, due to the current situation in the environment. Social commitments
are used to formalise justificatory and explanatory patterns. The combination
of ex- planation, justification, and commitments
",Justificatory and Explanatory Argumentation for Committing Agents,Ioan Alfred Letia and Adrian Groza,2012,Artificial Intelligence,
"  In the domain of Computing with words (CW), fuzzy linguistic approaches are
known to be relevant in many decision-making problems. Indeed, they allow us to
model the human reasoning in replacing words, assessments, preferences,
choices, wishes... by ad hoc variables, such as fuzzy sets or more
sophisticated variables.
  This paper focuses on a particular model: Herrera & Martinez' 2-tuple
linguistic model and their approach to deal with unbalanced linguistic term
sets. It is interesting since the computations are accomplished without loss of
information while the results of the decision-making processes always refer to
the initial linguistic term set. They propose a fuzzy partition which
distributes data on the axis by using linguistic hierarchies to manage the
non-uniformity. However, the required input (especially the density around the
terms) taken by their fuzzy partition algorithm may be considered as too much
demanding in a real-world application, since density is not always easy to
determine. Moreover, in some limit cases (especially when two terms are very
closed semantically to each other), the partition doesn't comply with the data
themselves, it isn't close to the reality. Therefore we propose to modify the
required input, in order to offer a simpler and more faithful partition. We
have added an extension to the package jFuzzyLogic and to the corresponding
script language FCL. This extension supports both 2-tuple models: Herrera &
Martinez' and ours. In addition to the partition algorithm, we present two
aggregation algorithms: the arithmetic means and the addition. We also discuss
these kinds of 2-tuple models.
","Towards an Extension of the 2-tuple Linguistic Model to Deal With
  Unbalanced Linguistic Term sets",Mohammed-Amine Abchir and Isis Truck,2013,Artificial Intelligence,
"  In this paper we present a new concept called generalized neutrosophic soft
set. This concept incorporates the beneficial properties of both generalized
neutrosophic set introduced by A.A. Salama [7]and soft set techniques proposed
by Molodtsov [4]. We also study some properties of this concept. Some
definitions and operations have been introduced on generalized neutrosophic
soft set. Finally we present an application of generalized neuutrosophic soft
set in decision making problem.
",Generalized Neutrosophic Soft Set,Said Broumi,2013,Artificial Intelligence,
"  This Ontologies are widely used as a means for solving the information
heterogeneity problems on the web because of their capability to provide
explicit meaning to the information. They become an efficient tool for
knowledge representation in a structured manner. There is always more than one
ontology for the same domain. Furthermore, there is no standard method for
building ontologies, and there are many ontology building tools using different
ontology languages. Because of these reasons, interoperability between the
ontologies is very low. Current ontology tools mostly use functions to build,
edit and inference the ontology. Methods for merging heterogeneous domain
ontologies are not included in most tools. This paper presents ontology merging
methodology for building a single global ontology from heterogeneous eXtensible
Markup Language (XML) data sources to capture and maintain all the knowledge
which XML data sources can contain
",Towards an Ontology based integrated Framework for Semantic Web,"Nora Y. Ibrahim, Sahar A. Mokhtar and Hany M. Harb",2012,Artificial Intelligence,
"  This paper presents an adaptation of the harmony search algorithm to solve
the storage allocation problem for inbound and outbound containers. This
problem is studied considering multiple container type (regular, open side,
open top, tank, empty and refrigerated) which lets the situation more
complicated, as various storage constraints appeared. The objective is to find
an optimal container arrangement which respects their departure dates, and
minimize the re-handle operations of containers. The performance of the
proposed approach is verified comparing to the results generated by genetic
algorithm and LIFO algorithm.
","Harmony search to solve the container storage problem with different
  container types","I. Ayachi, R. Kammarti, M.Ksouri, P.Borne LACS, ENIT, Tunis-Belvedere
  Tunisie LAGIS, ECL, Villeneuve d Ascq, France",2012,Artificial Intelligence,
"  This article is an overview of the ""SP theory of intelligence"". The theory
aims to simplify and integrate concepts across artificial intelligence,
mainstream computing and human perception and cognition, with information
compression as a unifying theme. It is conceived as a brain-like system that
receives 'New' information and stores some or all of it in compressed form as
'Old' information. It is realised in the form of a computer model -- a first
version of the SP machine. The concept of ""multiple alignment"" is a powerful
central idea. Using heuristic techniques, the system builds multiple alignments
that are 'good' in terms of information compression. For each multiple
alignment, probabilities may be calculated. These provide the basis for
calculating the probabilities of inferences. The system learns new structures
from partial matches between patterns. Using heuristic techniques, the system
searches for sets of structures that are 'good' in terms of information
compression. These are normally ones that people judge to be 'natural', in
accordance with the 'DONSVIC' principle -- the discovery of natural structures
via information compression. The SP theory may be applied in several areas
including 'computing', aspects of mathematics and logic, representation of
knowledge, natural language processing, pattern recognition, several kinds of
reasoning, information storage and retrieval, planning and problem solving,
information compression, neuroscience, and human perception and cognition.
Examples include the parsing and production of language including discontinuous
dependencies in syntax, pattern recognition at multiple levels of abstraction
and its integration with part-whole relations, nonmonotonic reasoning and
reasoning with default values, reasoning in Bayesian networks including
'explaining away', causal diagnosis, and the solving of a geometric analogy
problem.
",The SP theory of intelligence: an overview,J. Gerard Wolff,2013,Artificial Intelligence,
"  This article is about how the ""SP theory of intelligence"" and its realisation
in the ""SP machine"" may, with advantage, be applied to the management and
analysis of big data. The SP system -- introduced in the article and fully
described elsewhere -- may help to overcome the problem of variety in big data:
it has potential as ""a universal framework for the representation and
processing of diverse kinds of knowledge"" (UFK), helping to reduce the
diversity of formalisms and formats for knowledge and the different ways in
which they are processed. It has strengths in the unsupervised learning or
discovery of structure in data, in pattern recognition, in the parsing and
production of natural language, in several kinds of reasoning, and more. It
lends itself to the analysis of streaming data, helping to overcome the problem
of velocity in big data. Central in the workings of the system is lossless
compression of information: making big data smaller and reducing problems of
storage and management. There is potential for substantial economies in the
transmission of data, for big cuts in the use of energy in computing, for
faster processing, and for smaller and lighter computers. The system provides a
handle on the problem of veracity in big data, with potential to assist in the
management of errors and uncertainties in data. It lends itself to the
visualisation of knowledge structures and inferential processes. A
high-parallel, open-source version of the SP machine would provide a means for
researchers everywhere to explore what can be done with the system and to
create new versions of it.
",Big data and the SP theory of intelligence,J. Gerard Wolff,2014,Artificial Intelligence,
"  Determination of dietary food consumed a day for patients with diseases in
general, greatly affect the health of the body and the healing process, is no
exception for people with kidney disease and urinary tract. This paper presents
the determination of diet composition in the form of food subtance for people
with kidney and urinary tract diseases with a genetic fuzzy approach. This
approach combines fuzzy logic and genetic algorithms, which utilizing fuzzy
logic fuzzy tools and techniques to model the components of the genetic
algorithm and adapting genetic algorithm control parameters, with the aim of
improving system performance. The Mamdani fuzzy inference model and fuzzy rules
based on population parameters and generation are used to determine the
probability of crossover and mutation, and was using In this study, 400 food
survey data along with their substances was used as test material. From the
data, a varying amount of population is established. Each chromosome has 10
genes in which the value of each gene indicates the index number of foodstuffs
in the database. The fuzzy genetic approach produces 10 best food substance and
their compositions. The composition of these foods has nutritional value in
accordance with the number of calories needed by people with kidney and urinary
tract diseases by type of food.
","Computation of Diet Composition for Patients Suffering from Kidney and
  Urinary Tract Diseases with the Fuzzy Genetic System","Sri Hartati, Shofwatul 'Uyun",2011,Artificial Intelligence,
"  Smart home technology is a better choice for the people to care about
security, comfort and power saving as well. It is required to develop
technologies that recognize the Activities of Daily Living (ADLs) of the
residents at home and detect the abnormal behavior in the individual's
patterns. Data mining techniques such as Frequent pattern mining (FPM), High
Utility Pattern (HUP) Mining were used to find those activity patterns from the
collected sensor data. But applying the above technique for Activity
Recognition from the temporal sensor data stream is highly complex and
challenging task. So, a new approach is proposed for activity recognition from
sensor data stream which is achieved by constructing Frequent Pattern Stream
tree (FPS - tree). FPS is a sliding window based approach to discover the
recent activity patterns over time from data streams. The proposed work aims at
identifying the frequent pattern of the user from the sensor data streams which
are later modeled for activity recognition. The proposed FPM algorithm uses a
data structure called Linked Sensor Data Stream (LSDS) for storing the sensor
data stream information which increases the efficiency of frequent pattern
mining algorithm through both space and time. The experimental results show the
efficiency of the proposed algorithm and this FPM is further extended for
applying for power efficiency using HUP to detect the high usage of power
consumption of residents at smart home.
","Activity Modeling in Smart Home using High Utility Pattern Mining over
  Data Streams",Menaka Gandhi.J and K.S.Gayathri,2012,Artificial Intelligence,
"  As the education fees are becoming more expensive, more students apply for
scholarships. Consequently, hundreds and even thousands of applications need to
be handled by the sponsor. To solve the problems, some alternatives based on
several attributes (criteria) need to be selected. In order to make a decision
on such fuzzy problems, Fuzzy Multiple Attribute Decision Making (FMDAM) can be
applied. In this study, Unified Modeling Language (UML) in FMADM with TOPSIS
and Weighted Product (WP) methods is applied to select the candidates for
academic and non-academic scholarships at Universitas Islam Negeri Sunan
Kalijaga. Data used were a crisp and fuzzy data. The results show that TOPSIS
and Weighted Product FMADM methods can be used to select the most suitable
candidates to receive the scholarships since the preference values applied in
this method can show applicants with the highest eligibility
","A Fuzzy Topsis Multiple-Attribute Decision Making for Scholarship
  Selection","Shofwatul 'Uyun, Imam Riadi",2011,Artificial Intelligence,
"  This article describes existing and expected benefits of the ""SP theory of
intelligence"", and some potential applications. The theory aims to simplify and
integrate ideas across artificial intelligence, mainstream computing, and human
perception and cognition, with information compression as a unifying theme. It
combines conceptual simplicity with descriptive and explanatory power across
several areas of computing and cognition. In the ""SP machine"" -- an expression
of the SP theory which is currently realized in the form of a computer model --
there is potential for an overall simplification of computing systems,
including software. The SP theory promises deeper insights and better solutions
in several areas of application including, most notably, unsupervised learning,
natural language processing, autonomous robots, computer vision, intelligent
databases, software engineering, information compression, medical diagnosis and
big data. There is also potential in areas such as the semantic web,
bioinformatics, structuring of documents, the detection of computer viruses,
data fusion, new kinds of computer, and the development of scientific theories.
The theory promises seamless integration of structures and functions within and
between different areas of application. The potential value, worldwide, of
these benefits and applications is at least $190 billion each year. Further
development would be facilitated by the creation of a high-parallel,
open-source version of the SP machine, available to researchers everywhere.
",The SP theory of intelligence: benefits and applications,J Gerard Wolff,2014,Artificial Intelligence,
"  Ontologies provide a formal description of concepts and their relationships
in a knowledge domain. The goal of ontology alignment is to identify
semantically matching concepts and relationships across independently developed
ontologies that purport to describe the same knowledge. In order to handle the
widest possible class of ontologies, many alignment algorithms rely on
terminological and structural meth- ods, but the often fuzzy nature of concepts
complicates the matching process. However, one area that should provide clear
matching solutions due to its mathematical nature, is units of measurement.
Several on- tologies for units of measurement are available, but there has been
no attempt to align them, notwithstanding the obvious importance for tech-
nical interoperability. We propose a general strategy to map these (and
similar) ontologies by introducing MathML to accurately capture the semantic
description of concepts specified therein. We provide mapping results for three
ontologies, and show that our approach improves on lexical comparisons.
","Using MathML to Represent Units of Measurement for Improved Ontology
  Alignment",Chau Do and Eric J. Pauwels,2013,Artificial Intelligence,
"  In this work we are interested in the problem of energy management in Mobile
Ad-hoc Network (MANET). The solving and optimization of MANET allow assisting
the users to efficiently use their devices in order to minimize the batteries
power consumption. In this framework, we propose a modelling of the MANET in
form of a Constraint Optimization Problem called COMANET. Then, in the
objective to minimize the consumption of batteries power, we present an
approach based on an adaptation of the A star algorithm to the MANET problem
called MANED. Finally, we expose some experimental results showing utility of
this approach.
",How to minimize the energy consumption in mobile ad-hoc networks,Abdellah Idrissi,2012,Artificial Intelligence,
"  This work aims to investigate the reliability of software products as an
important attribute of computer programs; it helps to decide the degree of
trustworthiness a program has in accomplishing its specific functions. This is
done using the Software Reliability Growth Models (SRGMs) through the
estimation of their parameters. The parameters are estimated in this work based
on the available failure data and with the search techniques of Swarm
Intelligence, namely, the Cuckoo Search (CS) due to its efficiency,
effectiveness and robustness. A number of SRGMs is studied, and the results are
compared to Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO)
and extended ACO. Results show that CS outperformed both PSO and ACO in finding
better parameters tested using identical datasets. It was sometimes
outperformed by the extended ACO. Also in this work, the percentages of
training data to testing data are investigated to show their impact on the
results.
","The Use of Cuckoo Search in Estimating the Parameters of Software
  Reliability Growth Models",Dr. Najla Akram AL-Saati and Marwa Abd-AlKareem,2013,Artificial Intelligence,
"  One of the basic tasks which is responded for head of each university
department, is employing lecturers based on some default factors such as
experience, evidences, qualifies and etc. In this respect, to help the heads,
some automatic systems have been proposed until now using machine learning
methods, decision support systems (DSS) and etc. According to advantages and
disadvantages of the previous methods, a full automatic system is designed in
this paper using expert systems. The proposed system is included two main
steps. In the first one, the human expert's knowledge is designed as decision
trees. The second step is included an expert system which is evaluated using
extracted rules of these decision trees. Also, to improve the quality of the
proposed system, a majority voting algorithm is proposed as post processing
step to choose the best lecturer which satisfied more expert's decision trees
for each course. The results are shown that the designed system average
accuracy is 78.88. Low computational complexity, simplicity to program and are
some of other advantages of the proposed system.
","Design and Development of an Expert System to Help Head of University
  Departments","Shervan Fekri-Ershad, Hadi Tajalizadeh, Shahram Jafari",2013,Artificial Intelligence,
"  This paper advances a framework for modeling the component interactions
between cognitive and social aspects of scientific creativity and technological
innovation. Specifically, it aims to characterize Innovation Networks; those
networks that involve the interplay of people, ideas and organizations to
create new, technologically feasible, commercially-realizable products,
processes and organizational structures. The tri-partite framework captures
networks of ideas (Concept Level), people (Individual Level) and social
structures (Social-Organizational Level) and the interactions between these
levels. At the concept level, new ideas are the nodes that are created and
linked, kept open for further investigation or closed if solved by actors at
the individual or organizational levels. At the individual level, the nodes are
actors linked by shared worldviews (based on shared professional, educational,
experiential backgrounds) who are the builders of the concept level. At the
social-organizational level, the nodes are organizations linked by common
efforts on a given project (e.g., a company-university collaboration) that by
virtue of their intellectual property or rules of governance constrain the
actions of individuals (at the Individual Level) or ideas (at the Concept
Level). After describing this framework and its implications we paint a number
of scenarios to flesh out how it can be applied.
",Innovation networks,Petra Ahrweiler and Mark T. Keane,2013,Artificial Intelligence,
"  Recent work has suggested reducing electricity generation cost by cutting the
peak to average ratio (PAR) without reducing the total amount of the loads.
However, most of these proposals rely on consumer's willingness to act. In this
paper, we propose an approach to cut PAR explicitly from the supply side. The
resulting cut loads are then distributed among consumers by the means of a
multiunit auction which is done by an intelligent agent on behalf of the
consumer. This approach is also in line with the future vision of the smart
grid to have the demand side matched with the supply side. Experiments suggest
that our approach reduces overall system cost and gives benefit to both
consumers and the energy provider.
","Matching Demand with Supply in the Smart Grid using Agent-Based
  Multiunit Auction","Tri Kurniawan Wijaya, Kate Larson and Karl Aberer",2013,Artificial Intelligence,
"  Goalkeeper (GK) is an expert in soccer and goalkeeping is a complete
professional job. In fact, achieving success seems impossible without a
reliable GK. His effect in successes and failures is more dominant than other
players. The most visible mistakes in a game are those of goalkeeper's. In this
paper the expert fuzzy system is used as a suitable tool to study the quality
of a goalkeeper and compare it with others. Previously done researches are used
to find the goalkeepers' indexes in soccer. Soccer experts have found that a
successful GK should have some qualifications. A new pattern is offered here
which is called ""Soccer goalkeeper quality recognition using fuzzy expert
systems"". This pattern has some important capabilities. Firstly, among some
goalkeepers the one with the best quality for the main team arrange can be
chosen. Secondly, the need to expert coaches for choosing a GK using their
senses and experiences decreases a lot. Thirdly, in the survey of a GK,
quantitative criteria can be included, and finally this pattern is simple and
easy to understand.
",A Fuzzy expert system for goalkeeper quality recognition,"Mohammad Bazmara, Shahram Jafari, Fatemeh Pasand",2012,Artificial Intelligence,
"  In this paper, a new method for dynamic balancing of double four-bar crank
slider mechanism by meta- heuristic-based optimization algorithms is proposed.
For this purpose, a proper objective function which is necessary for balancing
of this mechanism and corresponding constraints has been obtained by dynamic
modeling of the mechanism. Then PSO, ABC, BGA and HGAPSO algorithms have been
applied for minimizing the defined cost function in optimization step. The
optimization results have been studied completely by extracting the cost
function, fitness, convergence speed and runtime values of applied algorithms.
It has been shown that PSO and ABC are more efficient than BGA and HGAPSO in
terms of convergence speed and result quality. Also, a laboratory scale
experimental doublefour-bar crank-slider mechanism was provided for validating
the proposed balancing method practically.
","Double four-bar crank-slider mechanism dynamic balancing by
  meta-heuristic algorithms","Habib Emdadi, Mahsa Yazdanian, Mir Mohammad Ettefagh and Mohammad-Reza
  Feizi-Derakhshi",2013,Artificial Intelligence,
"  A well known N P-hard problem called the Generalized Traveling Salesman
Problem (GTSP) is considered. In GTSP the nodes of a complete undirected graph
are partitioned into clusters. The objective is to find a minimum cost tour
passing through exactly one node from each cluster. An exact exponential time
algorithm and an effective meta-heuristic algorithm for the problem are
presented. The meta-heuristic proposed is a modified Ant Colony System (ACS)
algorithm called Reinforcing Ant Colony System (RACS) which introduces new
correction rules in the ACS algorithm. Computational results are reported for
many standard test problems. The proposed algorithm is competitive with the
other already proposed heuristics for the GTSP in both solution quality and
computational time.
",The Generalized Traveling Salesman Problem solved with Ant Algorithms,"Camelia-M. Pintea, Petrica C. Pop, Camelia Chira",2017,Artificial Intelligence,
"  Artificial Immune System (AIS-MACA) a novel computational intelligence
technique is can be used for strengthening the automated protein prediction
system with more adaptability and incorporating more parallelism to the system.
Most of the existing approaches are sequential which will classify the input
into four major classes and these are designed for similar sequences. AIS-MACA
is designed to identify ten classes from the sequences that share twilight zone
similarity and identity with the training sequences with mixed and hybrid
variations. This method also predicts three states (helix, strand, and coil)
for the secondary structure. Our comprehensive design considers 10 feature
selection methods and 4 classifiers to develop MACA (Multiple Attractor
Cellular Automata) based classifiers that are build for each of the ten
classes. We have tested the proposed classifier with twilight-zone and
1-high-similarity benchmark datasets with over three dozens of modern competing
predictors shows that AIS-MACA provides the best overall accuracy that ranges
between 80% and 89.8% depending on the dataset.
","An Extensive Report on Cellular Automata Based Artificial Immune System
  for Strengthening Automated Protein Prediction","Pokkuluri Kiran Sree, Inampudi Ramesh Babuhor, SSSN Usha Devi N3",2013,Artificial Intelligence,
"  Dynamics of a chaotic spiking neuron model are being studied mathematically
and experimentally. The Nonlinear Dynamic State neuron (NDS) is analysed to
further understand the model and improve it. Chaos has many interesting
properties such as sensitivity to initial conditions, space filling, control
and synchronization. As suggested by biologists, these properties may be
exploited and play vital role in carrying out computational tasks in human
brain. The NDS model has some limitations; in thus paper the model is
investigated to overcome some of these limitations in order to enhance the
model. Therefore, the models parameters are tuned and the resulted dynamics are
studied. Also, the discretization method of the model is considered. Moreover,
a mathematical analysis is carried out to reveal the underlying dynamics of the
model after tuning of its parameters. The results of the aforementioned methods
revealed some facts regarding the NDS attractor and suggest the stabilization
of a large number of unstable periodic orbits (UPOs) which might correspond to
memories in phase space.
",Studying a Chaotic Spiking Neural Model,"Mohammad Alhawarat, Waleed Nazih and Mohammad Eldesouki",2013,Artificial Intelligence,
"  The aim of this work is to present a meta-heuristically approach of the
spatial assignment problem of human resources in multi-sites enterprise.
Usually, this problem consists to move employees from one site to another based
on one or more criteria. Our goal in this new approach is to improve the
quality of service and performance of all sites with maximizing an objective
function under some managers imposed constraints. The formulation presented
here of this problem coincides perfectly with a Combinatorial Optimization
Problem (COP) which is in the most cases NP-hard to solve optimally. To avoid
this difficulty, we have opted to use a meta-heuristic popular method, which is
the genetic algorithm, to solve this problem in concrete cases. The results
obtained have shown the effectiveness of our approach, which remains until now
very costly in time. But the reduction of the time can be obtained by different
ways that we plan to do in the next work.
","A Meta-heuristically Approach of the Spatial Assignment Problem of Human
  Resources in Multi-sites Enterprise","Tkatek Said, Abdoun Otman, Abouchabaka Jaafar and Rafalia Najat",2013,Artificial Intelligence,
"  Suuplier selection is one of the most important functions of a purchasing
department. Since by deciding the best supplier, companies can save material
costs and increase competitive advantage.However this decision becomes
compilcated in case of multiple suppliers, multiple conflicting criteria, and
imprecise parameters. In addition the uncertainty and vagueness of the experts'
opinion is the prominent characteristic of the problem. therefore an
extensively used multi criteria decision making tool Fuzzy AHP can be utilized
as an approach for supplier selection problem. This paper reveals the
application of Fuzzy AHP in a gear motor company determining the best supplier
with respect to selected criteria. the contribution of this study is not only
the application of the Fuzzy AHP methodology for supplier selection problem,
but also releasing a comprehensive literature review of multi criteria decision
making problems. In addition by stating the steps of Fuzzy AHP clearly and
numerically, this study can be a guide of the methodology to be implemented to
other multiple criteria decision making problems.
","A Fuzzy AHP Approach for Supplier Selection Problem: A Case Study in a
  Gear Motor Company",Mustafa Batuhan Ayhan,2013,Artificial Intelligence,
"  Reasoning, the most important human brain operation, is charactrized by a
degree fuzziness. In the present paper we construct a fuzzy model for the
reasoning process giving through the calculation of the possibilities of all
possible individuals' profiles a quantitative/qualitative view of their
behaviour during the above process and we use the centroid defuzzification
technique for measuring the reasoning skills. We also present a number of
classroom experiments illustrating our results in practice.
",Dealing with the Fuzziness of Human Reasoning,"Michael Gr. Voskoglou, Igor Ya. Subbotin",2013,Artificial Intelligence,
"  In this paper, we introduce for the first time the notions of neutrosophic
measure and neutrosophic integral, and we develop the 1995 notion of
neutrosophic probability. We present many practical examples. It is possible to
define the neutrosophic measure and consequently the neutrosophic integral and
neutrosophic probability in many ways, because there are various types of
indeterminacies, depending on the problem we need to solve. Neutrosophics study
the indeterminacy. Indeterminacy is different from randomness. It can be caused
by physical space materials and type of construction, by items involved in the
space, etc.
","Introduction to Neutrosophic Measure, Neutrosophic Integral, and
  Neutrosophic Probability",Florentin Smarandache,2013,Artificial Intelligence,
"  Expert System is developed as consulting service for users spread or public
requires affordable access. The Internet has become a medium for such services,
but presence of mobile devices make the access becomes more widespread by
utilizing mobile web and WAP (Wireless Application Protocol). Applying expert
systems applications over the web and WAP requires a knowledge base
representation that can be accessed simultaneously. This paper proposes single
database to accommodate the knowledge representation with decision tree mapping
approach. Because of the database exist, consulting application through both
web and WAP can access it to provide expert system services options for more
affordable for public.
","Representing Knowledge Base into Database for WAP and Web-based Expert
  System","Istiadi, Emma Budi Sulistiarini",2013,Artificial Intelligence,
"  BP3TKI Palembang is the government agencies that coordinate, execute and
selection of prospective migrants registration and placement. To simplify the
existing procedures and improve decision-making is necessary to build a
decision support system (DSS) to determine eligibility for employment abroad by
applying Fuzzy Multiple Attribute Decision Making (FMADM), using the linear
sequential systems development methods. The system is built using Microsoft
Visual Basic. Net 2010 and SQL Server 2008 database. The design of the system
using use case diagrams and class diagrams to identify the needs of users and
systems as well as systems implementation guidelines. This Decision Support
System able to rank and produce the prospective migrants, making it easier for
parties to take decision BP3TKI the workers who will be working out of the
country.
","Sistem pendukung keputusan kelayakan TKI ke luar negeri menggunakan
  FMADM","Ardina Ariani, Leon Andretti Abdillah, Firamon Syakti",2013,Artificial Intelligence,
"  A Constraint Satisfaction Problem (CSP) is a framework used for modeling and
solving constrained problems. Tree-search algorithms like backtracking try to
construct a solution to a CSP by selecting the variables of the problem one
after another. The order in which these algorithm select the variables
potentially have significant impact on the search performance. Various
heuristics have been proposed for choosing good variable ordering. Many
powerful variable ordering heuristics weigh the constraints first and then
utilize the weights for selecting good order of the variables. Constraint
weighting are basically employed to identify global bottlenecks in a CSP.
  In this paper, we propose a new approach for learning weights for the
constraints using competitive coevolutionary Genetic Algorithm (GA). Weights
learned by the coevolutionary GA later help to make better choices for the
first few variables in a search. In the competitive coevolutionary GA,
constraints and candidate solutions for a CSP evolve together through an
inverse fitness interaction process. We have conducted experiments on several
random, quasi-random and patterned instances to measure the efficiency of the
proposed approach. The results and analysis show that the proposed approach is
good at learning weights to distinguish the hard constraints for quasi-random
instances and forced satisfiable random instances generated with the Model RB.
For other type of instances, RNDI still seems to be the best approach as our
experiments show.
","A New Approach to Constraint Weight Learning for Variable Ordering in
  CSPs",Muhammad Rezaul Karim,2014,Artificial Intelligence,
"  Case-Bsed Reasoning (CBR) is a recent theory for problem-solving and learning
in computers and people.Broadly construed it is the process of solving new
problems based on the solution of similar past problems. In the present paper
we introduce an absorbing Markov chain on the main steps of the CBR process.In
this way we succeed in obtaining the probabilities for the above process to be
in a certain step at a certain phase of the solution of the corresponding
problem, and a measure for the efficiency of a CBR system. Examples are given
to illustrate our results.
",A stochastic model for Case-Based Reasoning,Michael Gr. Voskoglou,2010,Artificial Intelligence,
"  We consider the problem of optimal planning in stochastic domains with
resource constraints, where the resources are continuous and the choice of
action at each step depends on resource availability. We introduce the HAO*
algorithm, a generalization of the AO* algorithm that performs search in a
hybrid state space that is modeled using both discrete and continuous state
variables, where the continuous variables represent monotonic resources. Like
other heuristic search algorithms, HAO* leverages knowledge of the start state
and an admissible heuristic to focus computational effort on those parts of the
state space that could be reached from the start state by following an optimal
policy. We show that this approach is especially effective when resource
constraints limit how much of the state space is reachable. Experimental
results demonstrate its effectiveness in the domain that motivates our
research: automated planning for planetary exploration rovers.
","A Heuristic Search Approach to Planning with Continuous Resources in
  Stochastic Domains","Nicolas Meuleau, Emmanuel Benazera, Ronen I. Brafman, Eric A. Hansen,
  Mausam",2009,Artificial Intelligence,
"  Literature on Constraint Satisfaction exhibits the definition of several
structural properties that can be possessed by CSPs, like (in)consistency,
substitutability or interchangeability. Current tools for constraint solving
typically detect such properties efficiently by means of incomplete yet
effective algorithms, and use them to reduce the search space and boost search.
  In this paper, we provide a unifying framework encompassing most of the
properties known so far, both in CSP and other fields literature, and shed
light on the semantical relationships among them. This gives a unified and
comprehensive view of the topic, allows new, unknown, properties to emerge, and
clarifies the computational complexity of the various detection problems.
  In particular, among the others, two new concepts, fixability and
removability emerge, that come out to be the ideal characterisations of values
that may be safely assigned or removed from a variables domain, while
preserving problem satisfiability. These two notions subsume a large number of
known properties, including inconsistency, substitutability and others.
  Because of the computational intractability of all the property-detection
problems, by following the CSP approach we then determine a number of
relaxations which provide sufficient conditions for their tractability. In
particular, we exploit forms of language restrictions and local reasoning.
","A Unifying Framework for Structural Properties of CSPs: Definitions,
  Complexity, Tractability","Lucas Bordeaux, Marco Cadoli, Toni Mancini",2008,Artificial Intelligence,
"  In this paper we explore a class of belief update operators, in which the
definition of the operator is compositional with respect to the sentence to be
added. The goal is to provide an update operator that is intuitive, in that its
definition is based on a recursive decomposition of the update sentences
structure, and that may be reasonably implemented. In addressing update, we
first provide a definition phrased in terms of the models of a knowledge base.
While this operator satisfies a core group of the benchmark Katsuno-Mendelzon
update postulates, not all of the postulates are satisfied. Other
Katsuno-Mendelzon postulates can be obtained by suitably restricting the
syntactic form of the sentence for update, as we show. In restricting the
syntactic form of the sentence for update, we also obtain a hierarchy of update
operators with Winsletts standard semantics as the most basic interesting
approach captured. We subsequently give an algorithm which captures this
approach; in the general case the algorithm is exponential, but with some
not-unreasonable assumptions we obtain an algorithm that is linear in the size
of the knowledge base. Hence the resulting approach has much better complexity
characteristics than other operators in some situations. We also explore other
compositional belief change operators: erasure is developed as a dual operator
to update; we show that a forget operator is definable in terms of update; and
we give a definition of the compositional revision operator. We obtain that
compositional revision, under the most natural definition, yields the Satoh
revision operator.
",Compositional Belief Update,"James Delgrande, Yi Jin, Francis Jeffry Pelletier",2008,Artificial Intelligence,
"  This paper proposes and experimentally validates a Bayesian network model of
a range finder adapted to dynamic environments. All modeling assumptions are
rigorously explained, and all model parameters have a physical interpretation.
This approach results in a transparent and intuitive model. With respect to the
state of the art beam model this paper: (i) proposes a different functional
form for the probability of range measurements caused by unmodeled objects,
(ii) intuitively explains the discontinuity encountered in te state of the art
beam model, and (iii) reduces the number of model parameters, while maintaining
the same representational power for experimental data. The proposed beam model
is called RBBM, short for Rigorously Bayesian Beam Model. A maximum likelihood
and a variational Bayesian estimator (both based on expectation-maximization)
are proposed to learn the model parameters.
  Furthermore, the RBBM is extended to a full scan model in two steps: first,
to a full scan model for static environments and next, to a full scan model for
general, dynamic environments. The full scan model accounts for the dependency
between beams and adapts to the local sample density when using a particle
filter. In contrast to Gaussian-based state of the art models, the proposed
full scan model uses a sample-based approximation. This sample-based
approximation enables handling dynamic environments and capturing
multi-modality, which occurs even in simple static environments.
","A Rigorously Bayesian Beam Model and an Adaptive Full Scan Model for
  Range Finders in Dynamic Environments","Tinne De Laet, Joris De Schutter, Herman Bruyninckx",2008,Artificial Intelligence,
"  Partially Observable Markov Decision Processes (POMDPs) provide a rich
framework for sequential decision-making under uncertainty in stochastic
domains. However, solving a POMDP is often intractable except for small
problems due to their complexity. Here, we focus on online approaches that
alleviate the computational complexity by computing good local policies at each
decision step during the execution. Online algorithms generally consist of a
lookahead search to find the best action to execute at each time step in an
environment. Our objectives here are to survey the various existing online
POMDP methods, analyze their properties and discuss their advantages and
disadvantages; and to thoroughly evaluate these online approaches in different
environments under various metrics (return, error bound reduction, lower bound
improvement). Our experimental results indicate that state-of-the-art online
heuristic search methods can handle large POMDP domains efficiently.
",Online Planning Algorithms for POMDPs,"St\'ephane Ross, Joelle Pineau, S\'ebastien Paquet, Brahim Chaib-draa",2008,Artificial Intelligence,
"  We present exact algorithms for identifying deterministic-actions effects and
preconditions in dynamic partially observable domains. They apply when one does
not know the action model(the way actions affect the world) of a domain and
must learn it from partial observations over time. Such scenarios are common in
real world applications. They are challenging for AI tasks because traditional
domain structures that underly tractability (e.g., conditional independence)
fail there (e.g., world features become correlated). Our work departs from
traditional assumptions about partial observations and action models. In
particular, it focuses on problems in which actions are deterministic of simple
logical structure and observation models have all features observed with some
frequency. We yield tractable algorithms for the modified problem for such
domains.
  Our algorithms take sequences of partial observations over time as input, and
output deterministic action models that could have lead to those observations.
The algorithms output all or one of those models (depending on our choice), and
are exact in that no model is misclassified given the observations. Our
algorithms take polynomial time in the number of time steps and state features
for some traditional action classes examined in the AI-planning literature,
e.g., STRIPS actions. In contrast, traditional approaches for HMMs and
Reinforcement Learning are inexact and exponentially intractable for such
domains. Our experiments verify the theoretical tractability guarantees, and
show that we identify action models exactly. Several applications in planning,
autonomous exploration, and adventure-game playing already use these results.
They are also promising for probabilistic settings, partially observable
reinforcement learning, and diagnosis.
",Learning Partially Observable Deterministic Action Models,"Eyal Amir, Allen Chang",2008,Artificial Intelligence,
"  A phylogenetic tree shows the evolutionary relationships among species.
Internal nodes of the tree represent speciation events and leaf nodes
correspond to species. A goal of phylogenetics is to combine such trees into
larger trees, called supertrees, whilst respecting the relationships in the
original trees. A rooted tree exhibits an ultrametric property; that is, for
any three leaves of the tree it must be that one pair has a deeper most recent
common ancestor than the other pairs, or that all three have the same most
recent common ancestor. This inspires a constraint programming encoding for
rooted trees. We present an efficient constraint that enforces the ultrametric
property over a symmetric array of constrained integer variables, with the
inevitable property that the lower bounds of any three variables are mutually
supportive. We show that this allows an efficient constraint-based solution to
the supertree construction problem. We demonstrate that the versatility of
constraint programming can be exploited to allow solutions to variants of the
supertree construction problem.
",The Ultrametric Constraint and its Application to Phylogenetics,"Neil C.A. Moore, Patrick Prosser",2008,Artificial Intelligence,
"  We present Confidence-Based Autonomy (CBA), an interactive algorithm for
policy learning from demonstration. The CBA algorithm consists of two
components which take advantage of the complimentary abilities of humans and
computer agents. The first component, Confident Execution, enables the agent to
identify states in which demonstration is required, to request a demonstration
from the human teacher and to learn a policy based on the acquired data. The
algorithm selects demonstrations based on a measure of action selection
confidence, and our results show that using Confident Execution the agent
requires fewer demonstrations to learn the policy than when demonstrations are
selected by a human teacher. The second algorithmic component, Corrective
Demonstration, enables the teacher to correct any mistakes made by the agent
through additional demonstrations in order to improve the policy and future
task performance. CBA and its individual components are compared and evaluated
in a complex simulated driving domain. The complete CBA algorithm results in
the best overall learning performance, successfully reproducing the behavior of
the teacher while balancing the tradeoff between number of demonstrations and
number of incorrect actions during learning.
",Interactive Policy Learning through Confidence-Based Autonomy,"Sonia Chernova, Manuela Veloso",2009,Artificial Intelligence,
"  A new search algorithm for solving distributed constraint optimization
problems (DisCOPs) is presented. Agents assign variables sequentially and
compute bounds on partial assignments asynchronously. The asynchronous bounds
computation is based on the propagation of partial assignments. The
asynchronous forward-bounding algorithm (AFB) is a distributed optimization
search algorithm that keeps one consistent partial assignment at all times. The
algorithm is described in detail and its correctness proven. Experimental
evaluation shows that AFB outperforms synchronous branch and bound by many
orders of magnitude, and produces a phase transition as the tightness of the
problem increases. This is an analogous effect to the phase transition that has
been observed when local consistency maintenance is applied to MaxCSPs. The AFB
algorithm is further enhanced by the addition of a backjumping mechanism,
resulting in the AFB-BJ algorithm. Distributed backjumping is based on
accumulated information on bounds of all values and on processing concurrently
a queue of candidate goals for the next move back. The AFB-BJ algorithm is
compared experimentally to other DisCOP algorithms (ADOPT, DPOP, OptAPO) and is
shown to be a very efficient algorithm for DisCOPs.
",Asynchronous Forward Bounding for Distributed COPs,"Amir Gershman, Amnon Meisels, Roie Zivan",2009,Artificial Intelligence,
"  This paper presents the computational logic foundations of a model of agency
called the KGP (Knowledge, Goals and Plan model. This model allows the
specification of heterogeneous agents that can interact with each other, and
can exhibit both proactive and reactive behaviour allowing them to function in
dynamic environments by adjusting their goals and plans when changes happen in
such environments. KGP provides a highly modular agent architecture that
integrates a collection of reasoning and physical capabilities, synthesised
within transitions that update the agents state in response to reasoning,
sensing and acting. Transitions are orchestrated by cycle theories that specify
the order in which transitions are executed while taking into account the
dynamic context and agent preferences, as well as selection operators for
providing inputs to transitions.
",Computational Logic Foundations of KGP Agents,"Antonis Kakas, Paolo Mancarella, Fariba Sadri, Kostas Stathis,
  Francesca Toni",2008,Artificial Intelligence,
"  Making a decision is often a matter of listing and comparing positive and
negative arguments. In such cases, the evaluation scale for decisions should be
considered bipolar, that is, negative and positive values should be explicitly
distinguished. That is what is done, for example, in Cumulative Prospect
Theory. However, contraryto the latter framework that presupposes genuine
numerical assessments, human agents often decide on the basis of an ordinal
ranking of the pros and the cons, and by focusing on the most salient
arguments. In other terms, the decision process is qualitative as well as
bipolar. In this article, based on a bipolar extension of possibility theory,
we define and axiomatically characterize several decision rules tailored for
the joint handling of positive and negative arguments in an ordinal setting.
The simplest rules can be viewed as extensions of the maximin and maximax
criteria to the bipolar case, and consequently suffer from poor decisive power.
More decisive rules that refine the former are also proposed. These refinements
agree both with principles of efficiency and with the spirit of
order-of-magnitude reasoning, that prevails in qualitative decision theory. The
most refined decision rule uses leximin rankings of the pros and the cons, and
the ideas of counting arguments of equal strength and cancelling pros by cons.
It is shown to come down to a special case of Cumulative Prospect Theory, and
to subsume the Take the Best heuristic studied by cognitive psychologists.
","On the Qualitative Comparison of Decisions Having Positive and Negative
  Features","Didier Dubois, H\'el\`ene Fargier, Jean-Fran\c{c}ois Bonnefon",2008,Artificial Intelligence,
"  Inspired by the recently introduced framework of AND/OR search spaces for
graphical models, we propose to augment Multi-Valued Decision Diagrams (MDD)
with AND nodes, in order to capture function decomposition structure and to
extend these compiled data structures to general weighted graphical models
(e.g., probabilistic models). We present the AND/OR Multi-Valued Decision
Diagram (AOMDD) which compiles a graphical model into a canonical form that
supports polynomial (e.g., solution counting, belief updating) or constant time
(e.g. equivalence of graphical models) queries. We provide two algorithms for
compiling the AOMDD of a graphical model. The first is search-based, and works
by applying reduction rules to the trace of the memory intensive AND/OR search
algorithm. The second is inference-based and uses a Bucket Elimination schedule
to combine the AOMDDs of the input functions via the the APPLY operator. For
both algorithms, the compilation time and the size of the AOMDD are, in the
worst case, exponential in the treewidth of the graphical model, rather than
pathwidth as is known for ordered binary decision diagrams (OBDDs). We
introduce the concept of semantic treewidth, which helps explain why the size
of a decision diagram is often much smaller than the worst case bound. We
provide an experimental evaluation that demonstrates the potential of AOMDDs.
",AND/OR Multi-Valued Decision Diagrams (AOMDDs) for Graphical Models,"Robert Mateescu, Rina Dechter, Radu Marinescu",2008,Artificial Intelligence,
"  Asynchronous Partial Overlay (APO) is a search algorithm that uses
cooperative mediation to solve Distributed Constraint Satisfaction Problems
(DisCSPs). The algorithm partitions the search into different subproblems of
the DisCSP. The original proof of completeness of the APO algorithm is based on
the growth of the size of the subproblems. The present paper demonstrates that
this expected growth of subproblems does not occur in some situations, leading
to a termination problem of the algorithm. The problematic parts in the APO
algorithm that interfere with its completeness are identified and necessary
modifications to the algorithm that fix these problematic parts are given. The
resulting version of the algorithm, Complete Asynchronous Partial Overlay
(CompAPO), ensures its completeness. Formal proofs for the soundness and
completeness of CompAPO are given. A detailed performance evaluation of CompAPO
comparing it to other DisCSP algorithms is presented, along with an extensive
experimental evaluation of the algorithm's unique behavior. Additionally, an
optimization version of the algorithm, CompOptAPO, is presented, discussed, and
evaluated.
",Completeness and Performance Of The APO Algorithm,"Tal Grinshpoun, Amnon Meisels",2008,Artificial Intelligence,
"  We investigate the computational complexity of testing dominance and
consistency in CP-nets. Previously, the complexity of dominance has been
determined for restricted classes in which the dependency graph of the CP-net
is acyclic. However, there are preferences of interest that define cyclic
dependency graphs; these are modeled with general CP-nets. In our main results,
we show here that both dominance and consistency for general CP-nets are
PSPACE-complete. We then consider the concept of strong dominance, dominance
equivalence and dominance incomparability, and several notions of optimality,
and identify the complexity of the corresponding decision problems. The
reductions used in the proofs are from STRIPS planning, and thus reinforce the
earlier established connections between both areas.
",The Computational Complexity of Dominance and Consistency in CP-Nets,"Judy Goldsmith, Jerome Lang, Miroslaw Truszczyski, Nic Wilson",2008,Artificial Intelligence,
"  Partially observable Markov decision processes (POMDPs) provide a principled
framework for sequential planning in uncertain single agent settings. An
extension of POMDPs to multiagent settings, called interactive POMDPs
(I-POMDPs), replaces POMDP belief spaces with interactive hierarchical belief
systems which represent an agent's belief about the physical world, about
beliefs of other agents, and about their beliefs about others' beliefs. This
modification makes the difficulties of obtaining solutions due to complexity of
the belief and policy spaces even more acute. We describe a general method for
obtaining approximate solutions of I-POMDPs based on particle filtering (PF).
We introduce the interactive PF, which descends the levels of the interactive
belief hierarchies and samples and propagates beliefs at each level. The
interactive PF is able to mitigate the belief space complexity, but it does not
address the policy space complexity. To mitigate the policy space complexity --
sometimes also called the curse of history -- we utilize a complementary method
based on sampling likely observations while building the look ahead
reachability tree. While this approach does not completely address the curse of
history, it beats back the curse's impact substantially. We provide
experimental results and chart future work.
",Monte Carlo Sampling Methods for Approximating Interactive POMDPs,"Prashant Doshi, Piotr J. Gmytrasiewicz",2009,Artificial Intelligence,
"  Inference in Bayes Nets (BAYES) is an important problem with numerous
applications in probabilistic reasoning. Counting the number of satisfying
assignments of a propositional formula (#SAT) is a closely related problem of
fundamental theoretical importance. Both these problems, and others, are
members of the class of sum-of-products (SUMPROD) problems. In this paper we
show that standard backtracking search when augmented with a simple memoization
scheme (caching) can solve any sum-of-products problem with time complexity
that is at least as good any other state-of-the-art exact algorithm, and that
it can also achieve the best known time-space tradeoff. Furthermore,
backtracking's ability to utilize more flexible variable orderings allows us to
prove that it can achieve an exponential speedup over other standard algorithms
for SUMPROD on some instances.
  The ideas presented here have been utilized in a number of solvers that have
been applied to various types of sum-of-product problems. These system's have
exploited the fact that backtracking can naturally exploit more of the
problem's structure to achieve improved performance on a range of
probleminstances. Empirical evidence of this performance gain has appeared in
published works describing these solvers, and we provide references to these
works.
",Solving #SAT and Bayesian Inference with Backtracking Search,"Fahiem Bacchus, Shannon Dalmao, Toniann Pitassi",2009,Artificial Intelligence,
"  Various tasks in decision making and decision support systems require
selecting a preferred subset of a given set of items. Here we focus on problems
where the individual items are described using a set of characterizing
attributes, and a generic preference specification is required, that is, a
specification that can work with an arbitrary set of items. For example,
preferences over the content of an online newspaper should have this form: At
each viewing, the newspaper contains a subset of the set of articles currently
available. Our preference specification over this subset should be provided
offline, but we should be able to use it to select a subset of any currently
available set of articles, e.g., based on their tags. We present a general
approach for lifting formalisms for specifying preferences over objects with
multiple attributes into ones that specify preferences over subsets of such
objects. We also show how we can compute an optimal subset given such a
specification in a relatively efficient manner. We provide an empirical
evaluation of the approach as well as some worst-case complexity results.
",Generic Preferences over Subsets of Structured Objects,"Maxim Binshtok, Ronen I. Brafman, Carmel Domshlak, Solomon Eyal
  Shimony",2009,Artificial Intelligence,
"  Coordination of distributed agents is required for problems arising in many
areas, including multi-robot systems, networking and e-commerce. As a formal
framework for such problems, we use the decentralized partially observable
Markov decision process (DEC-POMDP). Though much work has been done on optimal
dynamic programming algorithms for the single-agent version of the problem,
optimal algorithms for the multiagent case have been elusive. The main
contribution of this paper is an optimal policy iteration algorithm for solving
DEC-POMDPs. The algorithm uses stochastic finite-state controllers to represent
policies. The solution can include a correlation device, which allows agents to
correlate their actions without communicating. This approach alternates between
expanding the controller and performing value-preserving transformations, which
modify the controller without sacrificing value. We present two efficient
value-preserving transformations: one can reduce the size of the controller and
the other can improve its value while keeping the size fixed. Empirical results
demonstrate the usefulness of value-preserving transformations in increasing
value while keeping controller size to a minimum. To broaden the applicability
of the approach, we also present a heuristic version of the policy iteration
algorithm, which sacrifices convergence to optimality. This algorithm further
reduces the size of the controllers at each step by assuming that probability
distributions over the other agents actions are known. While this assumption
may not hold in general, it helps produce higher quality solutions in our test
problems.
",Policy Iteration for Decentralized Control of Markov Decision Processes,"Daniel S. Bernstein, Christopher Amato, Eric A. Hansen, Shlomo
  Zilberstein",2009,Artificial Intelligence,
"  Multiagent planning and coordination problems are common and known to be
computationally hard. We show that a wide range of two-agent problems can be
formulated as bilinear programs. We present a successive approximation
algorithm that significantly outperforms the coverage set algorithm, which is
the state-of-the-art method for this class of multiagent problems. Because the
algorithm is formulated for bilinear programs, it is more general and simpler
to implement. The new algorithm can be terminated at any time and-unlike the
coverage set algorithm-it facilitates the derivation of a useful online
performance bound. It is also much more efficient, on average reducing the
computation time of the optimal solution by about four orders of magnitude.
Finally, we introduce an automatic dimensionality reduction method that
improves the effectiveness of the algorithm, extending its applicability to new
domains and providing a new way to analyze a subclass of bilinear programs.
",A Bilinear Programming Approach for Multiagent Planning,"Marek Petrik, Shlomo Zilberstein",2009,Artificial Intelligence,
"  Recently, considerable focus has been given to the problem of determining the
boundary between tractable and intractable planning problems. In this paper, we
study the complexity of planning in the class C_n of planning problems,
characterized by unary operators and directed path causal graphs. Although this
is one of the simplest forms of causal graphs a planning problem can have, we
show that planning is intractable for C_n (unless P = NP), even if the domains
of state variables have bounded size. In particular, we show that plan
existence for C_n^k is NP-hard for k>=5 by reduction from CNFSAT. Here, k
denotes the upper bound on the size of the state variable domains. Our result
reduces the complexity gap for the class C_n^k to cases k=3 and k=4 only, since
C_n^2 is known to be tractable.
","Planning over Chain Causal Graphs for Variables with Domains of Size 5
  Is NP-Hard","Omer Gim\'enez, Anders Jonsson",2009,Artificial Intelligence,
"  Conformant planning is the problem of finding a sequence of actions for
achieving a goal in the presence of uncertainty in the initial state or action
effects. The problem has been approached as a path-finding problem in belief
space where good belief representations and heuristics are critical for scaling
up. In this work, a different formulation is introduced for conformant problems
with deterministic actions where they are automatically converted into
classical ones and solved by an off-the-shelf classical planner. The
translation maps literals L and sets of assumptions t about the initial
situation, into new literals KL/t that represent that L must be true if t is
initially true. We lay out a general translation scheme that is sound and
establish the conditions under which the translation is also complete. We show
that the complexity of the complete translation is exponential in a parameter
of the problem called the conformant width, which for most benchmarks is
bounded. The planner based on this translation exhibits good performance in
comparison with existing planners, and is the basis for T0, the best performing
planner in the Conformant Track of the 2006 International Planning Competition.
","Compiling Uncertainty Away in Conformant Planning Problems with Bounded
  Width","Hector Palacios, Hector Geffner",2009,Artificial Intelligence,
"  Symmetries in discrete constraint satisfaction problems have been explored
and exploited in the last years, but symmetries in continuous constraint
problems have not received the same attention. Here we focus on permutations of
the variables consisting of one single cycle. We propose a procedure that takes
advantage of these symmetries by interacting with a continuous constraint
solver without interfering with it. A key concept in this procedure are the
classes of symmetric boxes formed by bisecting a n-dimensional cube at the same
point in all dimensions at the same time. We analyze these classes and quantify
them as a function of the cube dimensionality. Moreover, we propose a simple
algorithm to generate the representatives of all these classes for any number
of variables at very high rates. A problem example from the chemical
and#64257;eld and the cyclic n-roots problem are used to show the performance
of the approach in practice.
",Exploiting Single-Cycle Symmetries in Continuous Constraint Problems,"Vicente Ruiz de Angulo, Carme Torras",2009,Artificial Intelligence,
"  Thanks to recent advances, AI Planning has become the underlying technique
for several applications. Figuring prominently among these is automated Web
Service Composition (WSC) at the ""capability"" level, where services are
described in terms of preconditions and effects over ontological concepts. A
key issue in addressing WSC as planning is that ontologies are not only formal
vocabularies; they also axiomatize the possible relationships between concepts.
Such axioms correspond to what has been termed ""integrity constraints"" in the
actions and change literature, and applying a web service is essentially a
belief update operation. The reasoning required for belief update is known to
be harder than reasoning in the ontology itself. The support for belief update
is severely limited in current planning tools.
  Our first contribution consists in identifying an interesting special case of
WSC which is both significant and more tractable. The special case, which we
term ""forward effects"", is characterized by the fact that every ramification of
a web service application involves at least one new constant generated as
output by the web service. We show that, in this setting, the reasoning
required for belief update simplifies to standard reasoning in the ontology
itself. This relates to, and extends, current notions of ""message-based"" WSC,
where the need for belief update is removed by a strong (often implicit or
informal) assumption of ""locality"" of the individual messages. We clarify the
computational properties of the forward effects case, and point out a strong
relation to standard notions of planning under uncertainty, suggesting that
effective tools for the latter can be successfully adapted to address the
former.
  Furthermore, we identify a significant sub-case, named ""strictly forward
effects"", where an actual compilation into planning under uncertainty exists.
This enables us to exploit off-the-shelf planning tools to solve message-based
WSC in a general form that involves powerful ontologies, and requires reasoning
about partial matches between concepts. We provide empirical evidence that this
approach may be quite effective, using Conformant-FF as the underlying planner.
","Message-Based Web Service Composition, Integrity Constraints, and
  Planning under Uncertainty: A New Connection","J\""org Hoffmann, Piergiorgio Bertoli, Malte Helmert, Marco Pistore",2009,Artificial Intelligence,
"  In this paper we formulate the problem of inference under incomplete
information in very general terms. This includes modelling the process
responsible for the incompleteness, which we call the incompleteness process.
We allow the process behaviour to be partly unknown. Then we use Walleys theory
of coherent lower previsions, a generalisation of the Bayesian theory to
imprecision, to derive the rule to update beliefs under incompleteness that
logically follows from our assumptions, and that we call conservative inference
rule. This rule has some remarkable properties: it is an abstract rule to
update beliefs that can be applied in any situation or domain; it gives us the
opportunity to be neither too optimistic nor too pessimistic about the
incompleteness process, which is a necessary condition to draw reliable while
strong enough conclusions; and it is a coherent rule, in the sense that it
cannot lead to inconsistencies. We give examples to show how the new rule can
be applied in expert systems, in parametric statistical inference, and in
pattern classification, and discuss more generally the view of incompleteness
processes defended here as well as some of its consequences.
",Conservative Inference Rule for Uncertain Reasoning under Incompleteness,"Marco Zaffalon, Enrique Miranda",2009,Artificial Intelligence,
"  Many real-world decision making tasks require us to choose among several
expensive observations. In a sensor network, for example, it is important to
select the subset of sensors that is expected to provide the strongest
reduction in uncertainty. In medical decision making tasks, one needs to select
which tests to administer before deciding on the most effective treatment. It
has been general practice to use heuristic-guided procedures for selecting
observations. In this paper, we present the first efficient optimal algorithms
for selecting observations for a class of probabilistic graphical models. For
example, our algorithms allow to optimally label hidden variables in Hidden
Markov Models (HMMs). We provide results for both selecting the optimal subset
of observations, and for obtaining an optimal conditional observation plan.
  Furthermore we prove a surprising result: In most graphical models tasks, if
one designs an efficient algorithm for chain graphs, such as HMMs, this
procedure can be generalized to polytree graphical models. We prove that the
optimizing value of information is $NP^{PP}$-hard even for polytrees. It also
follows from our results that just computing decision theoretic value of
information objective functions, which are commonly used in practice, is a
#P-complete problem even on Naive Bayes models (a simple special case of
polytrees).
  In addition, we consider several extensions, such as using our algorithms for
scheduling observation selection for multiple sensors. We demonstrate the
effectiveness of our approach on several real-world datasets, including a
prototype sensor network deployment for energy conservation in buildings.
",Optimal Value of Information in Graphical Models,"Andreas Krause, Carlos Guestrin",2009,Artificial Intelligence,
"  A weighted constraint satisfaction problem (WCSP) is a constraint
satisfaction problem in which preferences among solutions can be expressed.
Bucket elimination is a complete technique commonly used to solve this kind of
constraint satisfaction problem. When the memory required to apply bucket
elimination is too high, a heuristic method based on it (denominated
mini-buckets) can be used to calculate bounds for the optimal solution.
Nevertheless, the curse of dimensionality makes these techniques impractical on
large scale problems. In response to this situation, we present a memetic
algorithm for WCSPs in which bucket elimination is used as a mechanism for
recombining solutions, providing the best possible child from the parental set.
Subsequently, a multi-level model in which this exact/metaheuristic hybrid is
further hybridized with branch-and-bound techniques and mini-buckets is
studied. As a case study, we have applied these algorithms to the resolution of
the maximum density still life problem, a hard constraint optimization problem
based on Conways game of life. The resulting algorithm consistently finds
optimal patterns for up to date solved instances in less time than current
approaches. Moreover, it is shown that this proposal provides new best known
solutions for very large instances.
","Solving Weighted Constraint Satisfaction Problems with Memetic/Exact
  Hybrid Algorithms","Jos\'e Enrique Gallardo, Carlos Cotta, Antonio Jos\'e Fern\'andez",2009,Artificial Intelligence,
"  The Weighted Constraint Satisfaction Problem (WCSP) framework allows
representing and solving problems involving both hard constraints and cost
functions. It has been applied to various problems, including resource
allocation, bioinformatics, scheduling, etc. To solve such problems, solvers
usually rely on branch-and-bound algorithms equipped with local consistency
filtering, mostly soft arc consistency. However, these techniques are not well
suited to solve problems with very large domains. Motivated by the resolution
of an RNA gene localization problem inside large genomic sequences, and in the
spirit of bounds consistency for large domains in crisp CSPs, we introduce soft
bounds arc consistency, a new weighted local consistency specifically designed
for WCSP with very large domains. Compared to soft arc consistency, BAC
provides significantly improved time and space asymptotic complexity. In this
paper, we show how the semantics of cost functions can be exploited to further
improve the time complexity of BAC. We also compare both in theory and in
practice the efficiency of BAC on a WCSP with bounds consistency enforced on a
crisp CSP using cost variables. On two different real problems modeled as WCSP,
including our RNA gene localization problem, we observe that maintaining bounds
arc consistency outperforms arc consistency and also improves over bounds
consistency enforced on a constraint model with cost variables.
",Bounds Arc Consistency for Weighted CSPs,"Matthias Zytnicki, Christine Gaspin, Simon de Givry, Thomas Schiex",2009,Artificial Intelligence,
"  The survey propagation (SP) algorithm has been shown to work well on large
instances of the random 3-SAT problem near its phase transition. It was shown
that SP estimates marginals over covers that represent clusters of solutions.
The SP-y algorithm generalizes SP to work on the maximum satisfiability
(Max-SAT) problem, but the cover interpretation of SP does not generalize to
SP-y. In this paper, we formulate the relaxed survey propagation (RSP)
algorithm, which extends the SP algorithm to apply to the weighted Max-SAT
problem. We show that RSP has an interpretation of estimating marginals over
covers violating a set of clauses with minimal weight. This naturally
generalizes the cover interpretation of SP. Empirically, we show that RSP
outperforms SP-y and other state-of-the-art Max-SAT solvers on random Max-SAT
instances. RSP also outperforms state-of-the-art weighted Max-SAT solvers on
random weighted Max-SAT instances.
","Relaxed Survey Propagation for The Weighted Maximum Satisfiability
  Problem","Hai Leong Chieu, Wee Sun Sun Lee",2009,Artificial Intelligence,
"  This paper presents several new tractability results for planning based on
macros. We describe an algorithm that optimally solves planning problems in a
class that we call inverted tree reducible, and is provably tractable for
several subclasses of this class. By using macros to store partial plans that
recur frequently in the solution, the algorithm is polynomial in time and space
even for exponentially long plans. We generalize the inverted tree reducible
class in several ways and describe modifications of the algorithm to deal with
these new classes. Theoretical results are validated in experiments.
",The Role of Macros in Tractable Planning,Anders Jonsson,2009,Artificial Intelligence,
"  The paper investigates parameterized approximate message-passing schemes that
are based on bounded inference and are inspired by Pearl's belief propagation
algorithm (BP). We start with the bounded inference mini-clustering algorithm
and then move to the iterative scheme called Iterative Join-Graph Propagation
(IJGP), that combines both iteration and bounded inference. Algorithm IJGP
belongs to the class of Generalized Belief Propagation algorithms, a framework
that allowed connections with approximate algorithms from statistical physics
and is shown empirically to surpass the performance of mini-clustering and
belief propagation, as well as a number of other state-of-the-art algorithms on
several classes of networks. We also provide insight into the accuracy of
iterative BP and IJGP by relating these algorithms to well known classes of
constraint propagation schemes.
",Join-Graph Propagation Algorithms,"Robert Mateescu, Kalev Kask, Vibhav Gogate, Rina Dechter",2010,Artificial Intelligence,
"  Distributed constraint optimization (DCOP) problems are a popular way of
formulating and solving agent-coordination problems. A DCOP problem is a
problem where several agents coordinate their values such that the sum of the
resulting constraint costs is minimal. It is often desirable to solve DCOP
problems with memory-bounded and asynchronous algorithms. We introduce
Branch-and-Bound ADOPT (BnB-ADOPT), a memory-bounded asynchronous DCOP search
algorithm that uses the message-passing and communication framework of ADOPT
(Modi, Shen, Tambe, and Yokoo, 2005), a well known memory-bounded asynchronous
DCOP search algorithm, but changes the search strategy of ADOPT from best-first
search to depth-first branch-and-bound search. Our experimental results show
that BnB-ADOPT finds cost-minimal solutions up to one order of magnitude faster
than ADOPT for a variety of large DCOP problems and is as fast as NCBB, a
memory-bounded synchronous DCOP search algorithm, for most of these DCOP
problems. Additionally, it is often desirable to find bounded-error solutions
for DCOP problems within a reasonable amount of time since finding cost-minimal
solutions is NP-hard. The existing bounded-error approximation mechanism allows
users only to specify an absolute error bound on the solution cost but a
relative error bound is often more intuitive. Thus, we present two new
bounded-error approximation mechanisms that allow for relative error bounds and
implement them on top of BnB-ADOPT.
",BnB-ADOPT: An Asynchronous Branch-and-Bound DCOP Algorithm,"William Yeoh, Ariel Felner, Sven Koenig",2010,Artificial Intelligence,
"  Soft goals extend the classical model of planning with a simple model of
preferences. The best plans are then not the ones with least cost but the ones
with maximum utility, where the utility of a plan is the sum of the utilities
of the soft goals achieved minus the plan cost. Finding plans with high utility
appears to involve two linked problems: choosing a subset of soft goals to
achieve and finding a low-cost plan to achieve them. New search algorithms and
heuristics have been developed for planning with soft goals, and a new track
has been introduced in the International Planning Competition (IPC) to test
their performance. In this note, we show however that these extensions are not
needed: soft goals do not increase the expressive power of the basic model of
planning with action costs, as they can easily be compiled away. We apply this
compilation to the problems of the net-benefit track of the most recent IPC,
and show that optimal and satisficing cost-based planners do better on the
compiled problems than optimal and satisficing net-benefit planners on the
original problems with explicit soft goals. Furthermore, we show that
penalties, or negative preferences expressing conditions to avoid, can also be
compiled away using a similar idea.
",Soft Goals Can Be Compiled Away,"Emil Keyder, Hector Geffner",2009,Artificial Intelligence,
"  The identification of performance-optimizing parameter settings is an
important part of the development and application of algorithms. We describe an
automatic framework for this algorithm configuration problem. More formally, we
provide methods for optimizing a target algorithm's performance on a given
class of problem instances by varying a set of ordinal and/or categorical
parameters. We review a family of local-search-based algorithm configuration
procedures and present novel techniques for accelerating them by adaptively
limiting the time spent for evaluating individual configurations. We describe
the results of a comprehensive experimental evaluation of our methods, based on
the configuration of prominent complete and incomplete algorithms for SAT. We
also present what is, to our knowledge, the first published work on
automatically configuring the CPLEX mixed integer programming solver. All the
algorithms we considered had default parameter settings that were manually
identified with considerable effort. Nevertheless, using our automated
algorithm configuration procedures, we achieved substantial and consistent
performance improvements.
",ParamILS: An Automatic Algorithm Configuration Framework,"Frank Hutter, Thomas Stuetzle, Kevin Leyton-Brown, Holger H. Hoos",2009,Artificial Intelligence,
"  Korf, Reid, and Edelkamp introduced a formula to predict the number of nodes
IDA* will expand on a single iteration for a given consistent heuristic, and
experimentally demonstrated that it could make very accurate predictions. In
this paper we show that, in addition to requiring the heuristic to be
consistent, their formulas predictions are accurate only at levels of the
brute-force search tree where the heuristic values obey the unconditional
distribution that they defined and then used in their formula. We then propose
a new formula that works well without these requirements, i.e., it can make
accurate predictions of IDA*s performance for inconsistent heuristics and if
the heuristic values in any level do not obey the unconditional distribution.
In order to achieve this we introduce the conditional distribution of heuristic
values which is a generalization of their unconditional heuristic distribution.
We also provide extensions of our formula that handle individual start states
and the augmentation of IDA* with bidirectional pathmax (BPMX), a technique for
propagating heuristic values when inconsistent heuristics are used.
Experimental results demonstrate the accuracy of our new method and all its
variations.
",Predicting the Performance of IDA* using Conditional Distributions,"Uzi Zahavi, Ariel Felner, Neil Burch, Robert C. Holte",2010,Artificial Intelligence,
"  We present modeling for conceptual combinations which uses the mathematical
formalism of quantum theory. Our model faithfully describes a large amount of
experimental data collected by different scholars on concept conjunctions and
disjunctions. Furthermore, our approach sheds a new light on long standing
drawbacks connected with vagueness, or fuzziness, of concepts, and puts forward
a completely novel possible solution to the 'combination problem' in concept
theory. Additionally, we introduce an explanation for the occurrence of quantum
structures in the mechanisms and dynamics of concepts and, more generally, in
cognitive and decision processes, according to which human thought is a well
structured superposition of a 'logical thought' and a 'conceptual thought', and
the latter usually prevails over the former, at variance with some widespread
beliefs
",Modeling Concept Combinations in a Quantum-theoretic Framework,Diederik Aerts and Sandro Sozzo,2014,Artificial Intelligence,
"  We present DCL-PC: a logic for reasoning about how the abilities of agents
and coalitions of agents are altered by transferring control from one agent to
another. The logical foundation of DCL-PC is CL-PC, a logic for reasoning about
cooperation in which the abilities of agents and coalitions of agents stem from
a distribution of atomic Boolean variables to individual agents -- the choices
available to a coalition correspond to assignments to the variables the
coalition controls. The basic modal constructs of DCL-PC are of the form
coalition C can cooperate to bring about phi. DCL-PC extends CL-PC with dynamic
logic modalities in which atomic programs are of the form agent i gives control
of variable p to agent j; as usual in dynamic logic, these atomic programs may
be combined using sequence, iteration, choice, and test operators to form
complex programs. By combining such dynamic transfer programs with cooperation
modalities, it becomes possible to reason about how the power of agents and
coalitions is affected by the transfer of control. We give two alternative
semantics for the logic: a direct semantics, in which we capture the
distributions of Boolean variables to agents; and a more conventional Kripke
semantics. We prove that these semantics are equivalent, and then present an
axiomatization for the logic. We investigate the computational complexity of
model checking and satisfiability for DCL-PC, and show that both problems are
PSPACE-complete (and hence no worse than the underlying logic CL-PC). Finally,
we investigate the characterisation of control in DCL-PC. We distinguish
between first-order control -- the ability of an agent or coalition to control
some state of affairs through the assignment of values to the variables under
the control of the agent or coalition -- and second-order control -- the
ability of an agent to exert control over the control that other agents have by
transferring variables to other agents. We give a logical characterisation of
second-order control.
",Reasoning About the Transfer of Control,"Wiebe van der Hoek, Dirk Walther, Michael Wooldridge",2010,Artificial Intelligence,
"  Deciding how to act in partially observable environments remains an active
area of research. Identifying good sequences of decisions is particularly
challenging when good control performance requires planning multiple steps into
the future in domains with many states. Towards addressing this challenge, we
present an online, forward-search algorithm called the Posterior Belief
Distribution (PBD). PBD leverages a novel method for calculating the posterior
distribution over beliefs that result after a sequence of actions is taken,
given the set of observation sequences that could be received during this
process. This method allows us to efficiently evaluate the expected reward of a
sequence of primitive actions, which we refer to as macro-actions. We present a
formal analysis of our approach, and examine its performance on two very large
simulation experiments: scientific exploration and a target monitoring domain.
We also demonstrate our algorithm being used to control a real robotic
helicopter in a target monitoring experiment, which suggests that our approach
has practical potential for planning in real-world, large partially observable
domains where a multi-step lookahead is required to achieve good performance.
",Efficient Planning under Uncertainty with Macro-actions,"Ruijie He, Emma Brunskill, Nicholas Roy",2011,Artificial Intelligence,
"  In many AI domains such as product configuration, a user should interactively
specify a solution that must satisfy a set of constraints. In such scenarios,
offline compilation of feasible solutions into a tractable representation is an
important approach to delivering efficient backtrack-free user interaction
online. In particular,binary decision diagrams (BDDs) have been successfully
used as a compilation target for product and service configuration. In this
paper we discuss how to extend BDD-based configuration to scenarios involving
cost functions which express user preferences.
  We first show that an efficient, robust and easy to implement extension is
possible if the cost function is additive, and feasible solutions are
represented using multi-valued decision diagrams (MDDs). We also discuss the
effect on MDD size if the cost function is non-additive or if it is encoded
explicitly into MDD. We then discuss interactive configuration in the presence
of multiple cost functions. We prove that even in its simplest form,
multiple-cost configuration is NP-hard in the input MDD. However, for solving
two-cost configuration we develop a pseudo-polynomial scheme and a fully
polynomial approximation scheme. The applicability of our approach is
demonstrated through experiments over real-world configuration models and
product-catalogue datasets. Response times are generally within a fraction of a
second even for very large instances.
",Interactive Cost Configuration Over Decision Diagrams,"Henrik Reif Andersen, Tarik Hadzic, David Pisinger",2010,Artificial Intelligence,
"  Decentralized planning in uncertain environments is a complex task generally
dealt with by using a decision-theoretic approach, mainly through the framework
of Decentralized Partially Observable Markov Decision Processes (DEC-POMDPs).
Although DEC-POMDPS are a general and powerful modeling tool, solving them is a
task with an overwhelming complexity that can be doubly exponential. In this
paper, we study an alternate formulation of DEC-POMDPs relying on a
sequence-form representation of policies. From this formulation, we show how to
derive Mixed Integer Linear Programming (MILP) problems that, once solved, give
exact optimal solutions to the DEC-POMDPs. We show that these MILPs can be
derived either by using some combinatorial characteristics of the optimal
solutions of the DEC-POMDPs or by using concepts borrowed from game theory.
Through an experimental validation on classical test problems from the
DEC-POMDP literature, we compare our approach to existing algorithms. Results
show that mathematical programming outperforms dynamic programming but is less
efficient than forward search, except for some particular problems. The main
contributions of this work are the use of mathematical programming for
DEC-POMDPs and a better understanding of DEC-POMDPs and of their solutions.
Besides, we argue that our alternate representation of DEC-POMDPs could be
helpful for designing novel algorithms looking for approximate solutions to
DEC-POMDPs.
","An Investigation into Mathematical Programming for Finite Horizon
  Decentralized POMDPs","Raghav Aras, Alain Dutech",2010,Artificial Intelligence,
"  The paper presents a scheme for computing lower and upper bounds on the
posterior marginals in Bayesian networks with discrete variables. Its power
lies in its ability to use any available scheme that bounds the probability of
evidence or posterior marginals and enhance its performance in an anytime
manner. The scheme uses the cutset conditioning principle to tighten existing
bounding schemes and to facilitate anytime behavior, utilizing a fixed number
of cutset tuples. The accuracy of the bounds improves as the number of used
cutset tuples increases and so does the computation time. We demonstrate
empirically the value of our scheme for bounding posterior marginals and
probability of evidence using a variant of the bound propagation algorithm as a
plug-in scheme.
",Active Tuples-based Scheme for Bounding Posterior Beliefs,"Bozhena Bidyuk, Rina Dechter, Emma Rollon",2010,Artificial Intelligence,
"  As historically acknowledged in the Reasoning about Actions and Change
community, intuitiveness of a logical domain description cannot be fully
automated. Moreover, like any other logical theory, action theories may also
evolve, and thus knowledge engineers need revision methods to help in
accommodating new incoming information about the behavior of actions in an
adequate manner. The present work is about changing action domain descriptions
in multimodal logic. Its contribution is threefold: first we revisit the
semantics of action theory contraction proposed in previous work, giving more
robust operators that express minimal change based on a notion of distance
between Kripke-models. Second we give algorithms for syntactical action theory
contraction and establish their correctness with respect to our semantics for
those action theories that satisfy a principle of modularity investigated in
previous work. Since modularity can be ensured for every action theory and, as
we show here, needs to be computed at most once during the evolution of a
domain description, it does not represent a limitation at all to the method
here studied. Finally we state AGM-like postulates for action theory
contraction and assess the behavior of our operators with respect to them.
Moreover, we also address the revision counterpart of action theory change,
showing that it benefits from our semantics for contraction.
",On Action Theory Change,Ivan Jos\'e Varzinczak,2010,Artificial Intelligence,
"  In this paper, we address the problem of change in an abstract argumentation
system. We focus on a particular change: the addition of a new argument which
interacts with previous arguments. We study the impact of such an addition on
the outcome of the argumentation system, more particularly on the set of its
extensions. Several properties for this change operation are defined by
comparing the new set of extensions to the initial one, these properties are
called structural when the comparisons are based on set-cardinality or
set-inclusion relations. Several other properties are proposed where
comparisons are based on the status of some particular arguments: the accepted
arguments; these properties refer to the evolution of this status during the
change, e.g., Monotony and Priority to Recency. All these properties may be
more or less desirable according to specific applications. They are studied
under two particular semantics: the grounded and preferred semantics.
",Change in Abstract Argumentation Frameworks: Adding an Argument,"Claudette Cayrol, Florence Dupin de Saint-Cyr, Marie-Christine
  Lagasquie-Schiex",2010,Artificial Intelligence,
"  LAMA is a classical planning system based on heuristic forward search. Its
core feature is the use of a pseudo-heuristic derived from landmarks,
propositional formulas that must be true in every solution of a planning task.
LAMA builds on the Fast Downward planning system, using finite-domain rather
than binary state variables and multi-heuristic search. The latter is employed
to combine the landmark heuristic with a variant of the well-known FF
heuristic. Both heuristics are cost-sensitive, focusing on high-quality
solutions in the case where actions have non-uniform cost. A weighted A* search
is used with iteratively decreasing weights, so that the planner continues to
search for plans of better quality until the search is terminated. LAMA showed
best performance among all planners in the sequential satisficing track of the
International Planning Competition 2008. In this paper we present the system in
detail and investigate which features of LAMA are crucial for its performance.
We present individual results for some of the domains used at the competition,
demonstrating good and bad cases for the techniques implemented in LAMA.
Overall, we find that using landmarks improves performance, whereas the
incorporation of action costs into the heuristic estimators proves not to be
beneficial. We show that in some domains a search that ignores cost solves far
more problems, raising the question of how to deal with action costs more
effectively in the future. The iterated weighted A* search greatly improves
results, and shows synergy effects with the use of landmarks.
",The LAMA Planner: Guiding Cost-Based Anytime Planning with Landmarks,"Silvia Richter, Matthias Westphal",2010,Artificial Intelligence,
"  Narrative, and in particular storytelling, is an important part of the human
experience. Consequently, computational systems that can reason about narrative
can be more effective communicators, entertainers, educators, and trainers. One
of the central challenges in computational narrative reasoning is narrative
generation, the automated creation of meaningful event sequences. There are
many factors -- logical and aesthetic -- that contribute to the success of a
narrative artifact. Central to this success is its understandability. We argue
that the following two attributes of narratives are universal: (a) the logical
causal progression of plot, and (b) character believability. Character
believability is the perception by the audience that the actions performed by
characters do not negatively impact the audiences suspension of disbelief.
Specifically, characters must be perceived by the audience to be intentional
agents. In this article, we explore the use of refinement search as a technique
for solving the narrative generation problem -- to find a sound and believable
sequence of character actions that transforms an initial world state into a
world state in which goal propositions hold. We describe a novel refinement
search planning algorithm -- the Intent-based Partial Order Causal Link (IPOCL)
planner -- that, in addition to creating causally sound plot progression,
reasons about character intentionality by identifying possible character goals
that explain their actions and creating plan structures that explain why those
characters commit to their goals. We present the results of an empirical
evaluation that demonstrates that narrative plans generated by the IPOCL
algorithm support audience comprehension of character intentions better than
plans generated by conventional partial-order planners.
",Narrative Planning: Balancing Plot and Character,"Mark Owen Riedl, Robert Michael Young",2010,Artificial Intelligence,
"  Call control features (e.g., call-divert, voice-mail) are primitive options
to which users can subscribe off-line to personalise their service. The
configuration of a feature subscription involves choosing and sequencing
features from a catalogue and is subject to constraints that prevent
undesirable feature interactions at run-time. When the subscription requested
by a user is inconsistent, one problem is to find an optimal relaxation, which
is a generalisation of the feedback vertex set problem on directed graphs, and
thus it is an NP-hard task. We present several constraint programming
formulations of the problem. We also present formulations using partial
weighted maximum Boolean satisfiability and mixed integer linear programming.
We study all these formulations by experimentally comparing them on a variety
of randomly generated instances of the feature subscription problem.
","Developing Approaches for Solving a Telecommunications Feature
  Subscription Problem","David Lesaint, Deepak Mehta, Barry O'Sullivan, Luis Quesada, Nic
  Wilson",2010,Artificial Intelligence,
"  Binary Decision Diagram (BDD) based set bounds propagation is a powerful
approach to solving set-constraint satisfaction problems. However, prior BDD
based techniques in- cur the significant overhead of constructing and
manipulating graphs during search. We present a set-constraint solver which
combines BDD-based set-bounds propagators with the learning abilities of a
modern SAT solver. Together with a number of improvements beyond the basic
algorithm, this solver is highly competitive with existing propagation based
set constraint solvers.
",Fast Set Bounds Propagation Using a BDD-SAT Hybrid,"Graeme Gange, Peter James Stuckey, Vitaly Lagoon",2010,Artificial Intelligence,
"  Domain-specific features are important in representing problem structure
throughout machine learning and decision-theoretic planning. In planning, once
state features are provided, domain-independent algorithms such as approximate
value iteration can learn weighted combinations of those features that often
perform well as heuristic estimates of state value (e.g., distance to the
goal). Successful applications in real-world domains often require features
crafted by human experts. Here, we propose automatic processes for learning
useful domain-specific feature sets with little or no human intervention. Our
methods select and add features that describe state-space regions of high
inconsistency in the Bellman equation (statewise Bellman error) during
approximate value iteration. Our method can be applied using any
real-valued-feature hypothesis space and corresponding learning method for
selecting features from training sets of state-value pairs. We evaluate the
method with hypothesis spaces defined by both relational and propositional
feature languages, using nine probabilistic planning domains. We show that
approximate value iteration using a relational feature space performs at the
state-of-the-art in domain-independent stochastic relational planning. Our
method provides the first domain-independent approach that plays Tetris
successfully (without human-engineered features).
",Automatic Induction of Bellman-Error Features for Probabilistic Planning,"Jia-Hong Wu, Robert Givan",2010,Artificial Intelligence,
"  We propose a StochAstic Fault diagnosis AlgoRIthm, called SAFARI, which
trades off guarantees of computing minimal diagnoses for computational
efficiency. We empirically demonstrate, using the 74XXX and ISCAS-85 suites of
benchmark combinatorial circuits, that SAFARI achieves several
orders-of-magnitude speedup over two well-known deterministic algorithms, CDA*
and HA*, for multiple-fault diagnoses; further, SAFARI can compute a range of
multiple-fault diagnoses that CDA* and HA* cannot. We also prove that SAFARI is
optimal for a range of propositional fault models, such as the widely-used
weak-fault models (models with ignorance of abnormal behavior). We discuss the
optimality of SAFARI in a class of strong-fault circuit models with stuck-at
failure modes. By modeling the algorithm itself as a Markov chain, we provide
exact bounds on the minimality of the diagnosis computed. SAFARI also displays
strong anytime behavior, and will return a diagnosis after any non-trivial
inference time.
",Approximate Model-Based Diagnosis Using Greedy Stochastic Search,"Alexander Feldman, Gregory Provan, Arjan van Gemund",2010,Artificial Intelligence,
"  Model-based diagnostic reasoning often leads to a large number of diagnostic
hypotheses. The set of diagnoses can be reduced by taking into account extra
observations (passive monitoring), measuring additional variables (probing) or
executing additional tests (sequential diagnosis/test sequencing). In this
paper we combine the above approaches with techniques from Automated Test
Pattern Generation (ATPG) and Model-Based Diagnosis (MBD) into a framework
called FRACTAL (FRamework for ACtive Testing ALgorithms). Apart from the inputs
and outputs that connect a system to its environment, in active testing we
consider additional input variables to which a sequence of test vectors can be
supplied. We address the computationally hard problem of computing optimal
control assignments (as defined in FRACTAL) in terms of a greedy approximation
algorithm called FRACTAL-G. We compare the decrease in the number of remaining
minimal cardinality diagnoses of FRACTAL-G to that of two more FRACTAL
algorithms: FRACTAL-ATPG and FRACTAL-P. FRACTAL-ATPG is based on ATPG and
sequential diagnosis while FRACTAL-P is based on probing and, although not an
active testing algorithm, provides a baseline for comparing the lower bound on
the number of reachable diagnoses for the FRACTAL algorithms. We empirically
evaluate the trade-offs of the three FRACTAL algorithms by performing extensive
experimentation on the ISCAS85/74XXX benchmark of combinational circuits.
",A Model-Based Active Testing Approach to Sequential Diagnosis,"Alexander Feldman, Gregory Provan, Arjan van Gemund",2010,Artificial Intelligence,
"  Intrusion detection systems (IDSs) fall into two high-level categories:
network-based systems (NIDS) that monitor network behaviors, and host-based
systems (HIDS) that monitor system calls. In this work, we present a general
technique for both systems. We use anomaly detection, which identifies patterns
not conforming to a historic norm. In both types of systems, the rates of
change vary dramatically over time (due to burstiness) and over components (due
to service difference). To efficiently model such systems, we use continuous
time Bayesian networks (CTBNs) and avoid specifying a fixed update interval
common to discrete-time models. We build generative models from the normal
training data, and abnormal behaviors are flagged based on their likelihood
under this norm. For NIDS, we construct a hierarchical CTBN model for the
network packet traces and use Rao-Blackwellized particle filtering to learn the
parameters. We illustrate the power of our method through experiments on
detecting real worms and identifying hosts on two publicly available network
traces, the MAWI dataset and the LBNL dataset. For HIDS, we develop a novel
learning method to deal with the finite resolution of system log file time
stamps, without losing the benefits of our continuous time model. We
demonstrate the method by detecting intrusions in the DARPA 1998 BSM dataset.
",Intrusion Detection using Continuous Time Bayesian Networks,"Jing Xu, Christian R. Shelton",2010,Artificial Intelligence,
"  State-space search with explicit abstraction heuristics is at the state of
the art of cost-optimal planning. These heuristics are inherently limited,
nonetheless, because the size of the abstract space must be bounded by some,
even if a very large, constant. Targeting this shortcoming, we introduce the
notion of (additive) implicit abstractions, in which the planning task is
abstracted by instances of tractable fragments of optimal planning. We then
introduce a concrete setting of this framework, called fork-decomposition, that
is based on two novel fragments of tractable cost-optimal planning. The induced
admissible heuristics are then studied formally and empirically. This study
testifies for the accuracy of the fork decomposition heuristics, yet our
empirical evaluation also stresses the tradeoff between their accuracy and the
runtime complexity of computing them. Indeed, some of the power of the explicit
abstraction heuristics comes from precomputing the heuristic function offline
and then determining h(s) for each evaluated state s by a very fast lookup in a
database. By contrast, while fork-decomposition heuristics can be calculated in
polynomial time, computing them is far from being fast. To address this
problem, we show that the time-per-node complexity bottleneck of the
fork-decomposition heuristics can be successfully overcome. We demonstrate that
an equivalent of the explicit abstraction notion of a database exists for the
fork-decomposition abstractions as well, despite their exponential-size
abstract spaces. We then verify empirically that heuristic search with the
databased"" fork-decomposition heuristics favorably competes with the state of
the art of cost-optimal planning.
",Implicit Abstraction Heuristics,"Michael Katz, Carmel Domshlak",2010,Artificial Intelligence,
"  Diagrammatic reasoning (DR) is pervasive in human problem solving as a
powerful adjunct to symbolic reasoning based on language-like representations.
The research reported in this paper is a contribution to building a general
purpose DR system as an extension to a SOAR-like problem solving architecture.
The work is in a framework in which DR is modeled as a process where subtasks
are solved, as appropriate, either by inference from symbolic representations
or by interaction with a diagram, i.e., perceiving specified information from a
diagram or modifying/creating objects in a diagram in specified ways according
to problem solving needs. The perceptions and actions in most DR systems built
so far are hand-coded for the specific application, even when the rest of the
system is built using the general architecture. The absence of a general
framework for executing perceptions/actions poses as a major hindrance to using
them opportunistically -- the essence of open-ended search in problem solving.
Our goal is to develop a framework for executing a wide variety of specified
perceptions and actions across tasks/domains without human intervention. We
observe that the domain/task-specific visual perceptions/actions can be
transformed into domain/task-independent spatial problems. We specify a spatial
problem as a quantified constraint satisfaction problem in the real domain
using an open-ended vocabulary of properties, relations and actions involving
three kinds of diagrammatic objects -- points, curves, regions. Solving a
spatial problem from this specification requires computing the equivalent
simplified quantifier-free expression, the complexity of which is inherently
doubly exponential. We represent objects as configuration of simple elements to
facilitate decomposition of complex problems into simpler and similar
subproblems. We show that, if the symbolic solution to a subproblem can be
expressed concisely, quantifiers can be eliminated from spatial problems in
low-order polynomial time using similar previously solved subproblems. This
requires determining the similarity of two problems, the existence of a mapping
between them computable in polynomial time, and designing a memory for storing
previously solved problems so as to facilitate search. The efficacy of the idea
is shown by time complexity analysis. We demonstrate the proposed approach by
executing perceptions and actions involved in DR tasks in two army
applications.
","A Constraint Satisfaction Framework for Executing Perceptions and
  Actions in Diagrammatic Reasoning","Bonny Banerjee, B. Chandrasekaran",2010,Artificial Intelligence,
"  Real-time heuristic search algorithms satisfy a constant bound on the amount
of planning per action, independent of problem size. As a result, they scale up
well as problems become larger. This property would make them well suited for
video games where Artificial Intelligence controlled agents must react quickly
to user commands and to other agents actions. On the downside, real-time search
algorithms employ learning methods that frequently lead to poor solution
quality and cause the agent to appear irrational by re-visiting the same
problem states repeatedly. The situation changed recently with a new algorithm,
D LRTA*, which attempted to eliminate learning by automatically selecting
subgoals. D LRTA* is well poised for video games, except it has a complex and
memory-demanding pre-computation phase during which it builds a database of
subgoals. In this paper, we propose a simpler and more memory-efficient way of
pre-computing subgoals thereby eliminating the main obstacle to applying
state-of-the-art real-time search methods in video games. The new algorithm
solves a number of randomly chosen problems off-line, compresses the solutions
into a series of subgoals and stores them in a database. When presented with a
novel problem on-line, it queries the database for the most similar previously
solved case and uses its subgoals to solve the problem. In the domain of
pathfinding on four large video game maps, the new algorithm delivers solutions
eight times better while using 57 times less memory and requiring 14% less
pre-computation time.
","Case-Based Subgoaling in Real-Time Heuristic Search for Video Game
  Pathfinding","Vadim Bulitko, Yngvi Bj\""ornsson, Ramon Lawrence",2010,Artificial Intelligence,
"  Online offerings such as web search, news portals, and e-commerce
applications face the challenge of providing high-quality service to a large,
heterogeneous user base. Recent efforts have highlighted the potential to
improve performance by introducing methods to personalize services based on
special knowledge about users and their context. For example, a users
demographics, location, and past search and browsing may be useful in enhancing
the results offered in response to web search queries. However, reasonable
concerns about privacy by both users, providers, and government agencies acting
on behalf of citizens, may limit access by services to such information. We
introduce and explore an economics of privacy in personalization, where people
can opt to share personal information, in a standing or on-demand manner, in
return for expected enhancements in the quality of an online service. We focus
on the example of web search and formulate realistic objective functions for
search efficacy and privacy. We demonstrate how we can find a provably
near-optimal optimization of the utility-privacy tradeoff in an efficient
manner. We evaluate our methodology on data drawn from a log of the search
activity of volunteer participants. We separately assess users' preferences
about privacy and utility via a large-scale survey, aimed at eliciting
preferences about peoples' willingness to trade the sharing of personal data in
returns for gains in search efficiency. We show that a significant level of
personalization can be achieved using a relatively small amount of information
about users.
",A Utility-Theoretic Approach to Privacy in Online Services,"Andreas Krause, Eric Horvitz",2010,Artificial Intelligence,
"  Noisy probabilistic relational rules are a promising world model
representation for several reasons. They are compact and generalize over world
instantiations. They are usually interpretable and they can be learned
effectively from the action experiences in complex worlds. We investigate
reasoning with such rules in grounded relational domains. Our algorithms
exploit the compactness of rules for efficient and flexible decision-theoretic
planning. As a first approach, we combine these rules with the Upper Confidence
Bounds applied to Trees (UCT) algorithm based on look-ahead trees. Our second
approach converts these rules into a structured dynamic Bayesian network
representation and predicts the effects of action sequences using approximate
inference and beliefs over world states. We evaluate the effectiveness of our
approaches for planning in a simulated complex 3D robot manipulation scenario
with an articulated manipulator and realistic physics and in domains of the
probabilistic planning competition. Empirical results show that our methods can
solve problems where existing methods fail.
",Planning with Noisy Probabilistic Relational Rules,"Tobias Lang, Marc Toussaint",2010,Artificial Intelligence,
"  To harness modern multicore processors, it is imperative to develop parallel
versions of fundamental algorithms. In this paper, we compare different
approaches to parallel best-first search in a shared-memory setting. We present
a new method, PBNF, that uses abstraction to partition the state space and to
detect duplicate states without requiring frequent locking. PBNF allows
speculative expansions when necessary to keep threads busy. We identify and fix
potential livelock conditions in our approach, proving its correctness using
temporal logic. Our approach is general, allowing it to extend easily to
suboptimal and anytime heuristic search. In an empirical comparison on STRIPS
planning, grid pathfinding, and sliding tile puzzle problems using 8-core
machines, we show that A*, weighted A* and Anytime weighted A* implemented
using PBNF yield faster search than improved versions of previous parallel
search proposals.
",Best-First Heuristic Search for Multicore Machines,"Ethan Burns, Sofia Lemons, Wheeler Ruml, Rong Zhou",2010,Artificial Intelligence,
"  The Hamiltonian cycle problem (HCP) is an important combinatorial problem
with applications in many areas. It is among the first problems used for
studying intrinsic properties, including phase transitions, of combinatorial
problems. While thorough theoretical and experimental analyses have been made
on the HCP in undirected graphs, a limited amount of work has been done for the
HCP in directed graphs (DHCP). The main contribution of this work is an
effective algorithm for the DHCP. Our algorithm explores and exploits the close
relationship between the DHCP and the Assignment Problem (AP) and utilizes a
technique based on Boolean satisfiability (SAT). By combining effective
algorithms for the AP and SAT, our algorithm significantly outperforms previous
exact DHCP algorithms, including an algorithm based on the award-winning
Concorde TSP algorithm. The second result of the current study is an
experimental analysis of phase transitions of the DHCP, verifying and refining
a known phase transition of the DHCP.
","An Effective Algorithm for and Phase Transitions of the Directed
  Hamiltonian Cycle Problem","Gerold J\""ager, Weixiong Zhang",2010,Artificial Intelligence,
"  We present a method for using standard techniques from satisfiability
checking to automatically verify and discover theorems in an area of economic
theory known as ranking sets of objects. The key question in this area, which
has important applications in social choice theory and decision making under
uncertainty, is how to extend an agents preferences over a number of objects to
a preference relation over nonempty sets of such objects. Certain combinations
of seemingly natural principles for this kind of preference extension can
result in logical inconsistencies, which has led to a number of important
impossibility theorems. We first prove a general result that shows that for a
wide range of such principles, characterised by their syntactic form when
expressed in a many-sorted first-order logic, any impossibility exhibited at a
fixed (small) domain size will necessarily extend to the general case. We then
show how to formulate candidates for impossibility theorems at a fixed domain
size in propositional logic, which in turn enables us to automatically search
for (general) impossibility theorems using a SAT solver. When applied to a
space of 20 principles for preference extension familiar from the literature,
this method yields a total of 84 impossibility theorems, including both known
and nontrivial new results.
","Automated Search for Impossibility Theorems in Social Choice Theory:
  Ranking Sets of Objects","Christian Geist, Ulle Endriss",2011,Artificial Intelligence,
"  In action domains where agents may have erroneous beliefs, reasoning about
the effects of actions involves reasoning about belief change. In this paper,
we use a transition system approach to reason about the evolution of an agents
beliefs as actions are executed. Some actions cause an agent to perform belief
revision while others cause an agent to perform belief update, but the
interaction between revision and update can be non-elementary. We present a set
of rationality properties describing the interaction between revision and
update, and we introduce a new class of belief change operators for reasoning
about alternating sequences of revisions and updates. Our belief change
operators can be characterized in terms of a natural shifting operation on
total pre-orderings over interpretations. We compare our approach with related
work on iterated belief change due to action, and we conclude with some
directions for future research.
",Iterated Belief Change Due to Actions and Observations,"Aaron Hunter, James P. Delgrande",2011,Artificial Intelligence,
"  Markovian processes have long been used to model stochastic environments.
Reinforcement learning has emerged as a framework to solve sequential planning
and decision-making problems in such environments. In recent years, attempts
were made to apply methods from reinforcement learning to construct decision
support systems for action selection in Markovian environments. Although
conventional methods in reinforcement learning have proved to be useful in
problems concerning sequential decision-making, they cannot be applied in their
current form to decision support systems, such as those in medical domains, as
they suggest policies that are often highly prescriptive and leave little room
for the users input. Without the ability to provide flexible guidelines, it is
unlikely that these methods can gain ground with users of such systems. This
paper introduces the new concept of non-deterministic policies to allow more
flexibility in the users decision-making process, while constraining decisions
to remain near optimal solutions. We provide two algorithms to compute
non-deterministic policies in discrete domains. We study the output and running
time of these method on a set of synthetic and real-world problems. In an
experiment with human subjects, we show that humans assisted by hints based on
non-deterministic policies outperform both human-only and computer-only agents
in a web navigation task.
",Non-Deterministic Policies in Markovian Decision Processes,"Mahdi Milani Fard, Joelle Pineau",2011,Artificial Intelligence,
"  In this paper, we propose a comprehensive study of second-order consistencies
(i.e., consistencies identifying inconsistent pairs of values) for constraint
satisfaction. We build a full picture of the relationships existing between
four basic second-order consistencies, namely path consistency (PC),
3-consistency (3C), dual consistency (DC) and 2-singleton arc consistency
(2SAC), as well as their conservative and strong variants. Interestingly, dual
consistency is an original property that can be established by using the
outcome of the enforcement of generalized arc consistency (GAC), which makes it
rather easy to obtain since constraint solvers typically maintain GAC during
search. On binary constraint networks, DC is equivalent to PC, but its
restriction to existing constraints, called conservative dual consistency
(CDC), is strictly stronger than traditional conservative consistencies derived
from path consistency, namely partial path consistency (PPC) and conservative
path consistency (CPC). After introducing a general algorithm to enforce strong
(C)DC, we present the results of an experimentation over a wide range of
benchmarks that demonstrate the interest of (conservative) dual consistency. In
particular, we show that enforcing (C)DC before search clearly improves the
performance of MAC (the algorithm that maintains GAC during search) on several
binary and non-binary structured problems.
",Second-Order Consistencies,"Christophe Lecoutre, Stephane Cardon, Julien Vion",2011,Artificial Intelligence,
"  We present a case study of artificial intelligence techniques applied to the
control of production printing equipment. Like many other real-world
applications, this complex domain requires high-speed autonomous
decision-making and robust continual operation. To our knowledge, this work
represents the first successful industrial application of embedded
domain-independent temporal planning. Our system handles execution failures and
multi-objective preferences. At its heart is an on-line algorithm that combines
techniques from state-space planning and partial-order scheduling. We suggest
that this general architecture may prove useful in other applications as more
intelligent systems operate in continual, on-line settings. Our system has been
used to drive several commercial prototypes and has enabled a new product
architecture for our industrial partner. When compared with state-of-the-art
off-line planners, our system is hundreds of times faster and often finds
better plans. Our experience demonstrates that domain-independent AI planning
based on heuristic search can flexibly handle time, resources, replanning, and
multiple objectives in a high-speed practical application without requiring
hand-coded control knowledge.
","On-line Planning and Scheduling: An Application to Controlling Modular
  Printers","Wheeler Ruml, Minh Binh Do, Rong Zhou, Markus P.J. Fromherz",2011,Artificial Intelligence,
"  In many combinatorial problems one may need to model the diversity or
similarity of assignments in a solution. For example, one may wish to maximise
or minimise the number of distinct values in a solution. To formulate problems
of this type, we can use soft variants of the well known AllDifferent and
AllEqual constraints. We present a taxonomy of six soft global constraints,
generated by combining the two latter ones and the two standard cost functions,
which are either maximised or minimised. We characterise the complexity of
achieving arc and bounds consistency on these constraints, resolving those
cases for which NP-hardness was neither proven nor disproven. In particular, we
explore in depth the constraint ensuring that at least k pairs of variables
have a common value. We show that achieving arc consistency is NP-hard, however
achieving bounds consistency can be done in polynomial time through dynamic
programming. Moreover, we show that the maximum number of pairs of equal
variables can be approximated by a factor 1/2 with a linear time greedy
algorithm. Finally, we provide a fixed parameter tractable algorithm with
respect to the number of values appearing in more than two distinct domains.
Interestingly, this taxonomy shows that enforcing equality is harder than
enforcing difference.
",Soft Constraints of Difference and Equality,"Emmanuel Hebrard, D\'aniel Marx, Barry O'Sullivan, Igor Razgon",2011,Artificial Intelligence,
"  We address the cost-sensitive feature acquisition problem, where
misclassifying an instance is costly but the expected misclassification cost
can be reduced by acquiring the values of the missing features. Because
acquiring the features is costly as well, the objective is to acquire the right
set of features so that the sum of the feature acquisition cost and
misclassification cost is minimized. We describe the Value of Information
Lattice (VOILA), an optimal and efficient feature subset acquisition framework.
Unlike the common practice, which is to acquire features greedily, VOILA can
reason with subsets of features. VOILA efficiently searches the space of
possible feature subsets by discovering and exploiting conditional independence
properties between the features and it reuses probabilistic inference
computations to further speed up the process. Through empirical evaluation on
five medical datasets, we show that the greedy strategy is often reluctant to
acquire features, as it cannot forecast the benefit of acquiring multiple
features in combination.
","Value of Information Lattice: Exploiting Probabilistic Independence for
  Effective Feature Subset Acquisition","Mustafa Bilgic, Lise Getoor",2011,Artificial Intelligence,
"  Dynamic programming algorithms have been successfully applied to
propositional stochastic planning problems by using compact representations, in
particular algebraic decision diagrams, to capture domain dynamics and value
functions. Work on symbolic dynamic programming lifted these ideas to first
order logic using several representation schemes. Recent work introduced a
first order variant of decision diagrams (FODD) and developed a value iteration
algorithm for this representation. This paper develops several improvements to
the FODD algorithm that make the approach practical. These include, new
reduction operators that decrease the size of the representation, several
speedup techniques, and techniques for value approximation. Incorporating
these, the paper presents a planning system, FODD-Planner, for solving
relational stochastic planning problems. The system is evaluated on several
domains, including problems from the recent international planning competition,
and shows competitive performance with top ranking systems. This is the first
demonstration of feasibility of this approach and it shows that abstraction
through compact representation is a promising approach to stochastic planning.
",Probabilistic Relational Planning with First Order Decision Diagrams,"Saket Joshi, Roni Khardon",2011,Artificial Intelligence,
"  Current evaluation functions for heuristic planning are expensive to compute.
In numerous planning problems these functions provide good guidance to the
solution, so they are worth the expense. However, when evaluation functions are
misguiding or when planning problems are large enough, lots of node evaluations
must be computed, which severely limits the scalability of heuristic planners.
In this paper, we present a novel solution for reducing node evaluations in
heuristic planning based on machine learning. Particularly, we define the task
of learning search control for heuristic planning as a relational
classification task, and we use an off-the-shelf relational classification tool
to address this learning task. Our relational classification task captures the
preferred action to select in the different planning contexts of a specific
planning domain. These planning contexts are defined by the set of helpful
actions of the current state, the goals remaining to be achieved, and the
static predicates of the planning task. This paper shows two methods for
guiding the search of a heuristic planner with the learned classifiers. The
first one consists of using the resulting classifier as an action policy. The
second one consists of applying the classifier to generate lookahead states
within a Best First Search algorithm. Experiments over a variety of domains
reveal that our heuristic planner using the learned classifiers solves larger
problems than state-of-the-art planners.
",Scaling up Heuristic Planning with Relational Decision Trees,"Tomas De la Rosa, Sergio Jimenez, Raquel Fuentetaja, Daniel Borrajo",2011,Artificial Intelligence,
"  Previous studies have demonstrated that encoding a Bayesian network into a
SAT formula and then performing weighted model counting using a backtracking
search algorithm can be an effective method for exact inference. In this paper,
we present techniques for improving this approach for Bayesian networks with
noisy-OR and noisy-MAX relations---two relations that are widely used in
practice as they can dramatically reduce the number of probabilities one needs
to specify. In particular, we present two SAT encodings for noisy-OR and two
encodings for noisy-MAX that exploit the structure or semantics of the
relations to improve both time and space efficiency, and we prove the
correctness of the encodings. We experimentally evaluated our techniques on
large-scale real and randomly generated Bayesian networks. On these benchmarks,
our techniques gave speedups of up to two orders of magnitude over the best
previous approaches for networks with noisy-OR/MAX relations and scaled up to
larger networks. As well, our techniques extend the weighted model counting
approach for exact inference to networks that were previously intractable for
the approach.
","Exploiting Structure in Weighted Model Counting Approaches to
  Probabilistic Inference","Wei Li, Pascal Poupart, Peter van Beek",2011,Artificial Intelligence,
"  Bound propagation is an important Artificial Intelligence technique used in
Constraint Programming tools to deal with numerical constraints. It is
typically embedded within a search procedure (""branch and prune"") and used at
every node of the search tree to narrow down the search space, so it is
critical that it be fast. The procedure invokes constraint propagators until a
common fixpoint is reached, but the known algorithms for this have a
pseudo-polynomial worst-case time complexity: they are fast indeed when the
variables have a small numerical range, but they have the well-known problem of
being prohibitively slow when these ranges are large. An important question is
therefore whether strongly-polynomial algorithms exist that compute the common
bound consistent fixpoint of a set of constraints. This paper answers this
question. In particular we show that this fixpoint computation is in fact
NP-complete, even when restricted to binary linear constraints.
",The Complexity of Integer Bound Propagation,"Lucas Bordeaux, George Katsirelos, Nina Narodytska, Moshe Y. Vardi",2011,Artificial Intelligence,
"  The ignoring delete lists relaxation is of paramount importance for both
satisficing and optimal planning. In earlier work, it was observed that the
optimal relaxation heuristic h+ has amazing qualities in many classical
planning benchmarks, in particular pertaining to the complete absence of local
minima. The proofs of this are hand-made, raising the question whether such
proofs can be lead automatically by domain analysis techniques. In contrast to
earlier disappointing results -- the analysis method has exponential runtime
and succeeds only in two extremely simple benchmark domains -- we herein answer
this question in the affirmative. We establish connections between causal graph
structure and h+ topology. This results in low-order polynomial time analysis
methods, implemented in a tool we call TorchLight. Of the 12 domains where the
absence of local minima has been proved, TorchLight gives strong success
guarantees in 8 domains. Empirically, its analysis exhibits strong performance
in a further 2 of these domains, plus in 4 more domains where local minima may
exist but are rare. In this way, TorchLight can distinguish easy domains from
hard ones. By summarizing structural reasons for analysis failure, TorchLight
also provides diagnostic output indicating domain aspects that may cause local
minima.
","Analyzing Search Topology Without Running Any Search: On the Connection
  Between Causal Graphs and h+",Joerg Hoffmann,2011,Artificial Intelligence,
"  When a system behaves abnormally, sequential diagnosis takes a sequence of
measurements of the system until the faults causing the abnormality are
identified, and the goal is to reduce the diagnostic cost, defined here as the
number of measurements. To propose measurement points, previous work employs a
heuristic based on reducing the entropy over a computed set of diagnoses. This
approach generally has good performance in terms of diagnostic cost, but can
fail to diagnose large systems when the set of diagnoses is too large. Focusing
on a smaller set of probable diagnoses scales the approach but generally leads
to increased average diagnostic costs. In this paper, we propose a new
diagnostic framework employing four new techniques, which scales to much larger
systems with good performance in terms of diagnostic cost. First, we propose a
new heuristic for measurement point selection that can be computed efficiently,
without requiring the set of diagnoses, once the system is modeled as a
Bayesian network and compiled into a logical form known as d-DNNF. Second, we
extend hierarchical diagnosis, a technique based on system abstraction from our
previous work, to handle probabilities so that it can be applied to sequential
diagnosis to allow larger systems to be diagnosed. Third, for the largest
systems where even hierarchical diagnosis fails, we propose a novel method that
converts the system into one that has a smaller abstraction and whose diagnoses
form a superset of those of the original system; the new system can then be
diagnosed and the result mapped back to the original system. Finally, we
propose a novel cost estimation function which can be used to choose an
abstraction of the system that is more likely to provide optimal average cost.
Experiments with ISCAS-85 benchmark circuits indicate that our approach scales
to all circuits in the suite except one that has a flat structure not
susceptible to useful abstraction.
",Sequential Diagnosis by Abstraction,"Sajjad Ahmed Siddiqi, Jinbo Huang",2011,Artificial Intelligence,
"  A major inference task in Bayesian networks is explaining why some variables
are observed in their particular states using a set of target variables.
Existing methods for solving this problem often generate explanations that are
either too simple (underspecified) or too complex (overspecified). In this
paper, we introduce a method called Most Relevant Explanation (MRE) which finds
a partial instantiation of the target variables that maximizes the generalized
Bayes factor (GBF) as the best explanation for the given evidence. Our study
shows that GBF has several theoretical properties that enable MRE to
automatically identify the most relevant target variables in forming its
explanation. In particular, conditional Bayes factor (CBF), defined as the GBF
of a new explanation conditioned on an existing explanation, provides a soft
measure on the degree of relevance of the variables in the new explanation in
explaining the evidence given the existing explanation. As a result, MRE is
able to automatically prune less relevant variables from its explanation. We
also show that CBF is able to capture well the explaining-away phenomenon that
is often represented in Bayesian networks. Moreover, we define two dominance
relations between the candidate solutions and use the relations to generalize
MRE to find a set of top explanations that is both diverse and representative.
Case studies on several benchmark diagnostic Bayesian networks show that MRE is
often able to find explanatory hypotheses that are not only precise but also
concise.
",Most Relevant Explanation in Bayesian Networks,"Changhe Yuan, Heejin Lim, Tsai-Ching Lu",2011,Artificial Intelligence,
"  Translations between different nonmonotonic formalisms always have been an
important topic in the field, in particular to understand the
knowledge-representation capabilities those formalisms offer. We provide such
an investigation in terms of different semantics proposed for abstract
argumentation frameworks, a nonmonotonic yet simple formalism which received
increasing interest within the last decade. Although the properties of these
different semantics are nowadays well understood, there are no explicit results
about intertranslatability. We provide such translations wrt. different
properties and also give a few novel complexity results which underlie some
negative results.
",On the Intertranslatability of Argumentation Semantics,"Wolfgang Dvorak, Stefan Woltran",2011,Artificial Intelligence,
"  Many applications, e.g., Web service composition, complex system design, team
formation, etc., rely on methods for identifying collections of objects or
entities satisfying some functional requirement. Among the collections that
satisfy the functional requirement, it is often necessary to identify one or
more collections that are optimal with respect to user preferences over a set
of attributes that describe the non-functional properties of the collection.
  We develop a formalism that lets users express the relative importance among
attributes and qualitative preferences over the valuations of each attribute.
We define a dominance relation that allows us to compare collections of objects
in terms of preferences over attributes of the objects that make up the
collection. We establish some key properties of the dominance relation. In
particular, we show that the dominance relation is a strict partial order when
the intra-attribute preference relations are strict partial orders and the
relative importance preference relation is an interval order.
  We provide algorithms that use this dominance relation to identify the set of
most preferred collections. We show that under certain conditions, the
algorithms are guaranteed to return only (sound), all (complete), or at least
one (weakly complete) of the most preferred collections. We present results of
simulation experiments comparing the proposed algorithms with respect to (a)
the quality of solutions (number of most preferred solutions) produced by the
algorithms, and (b) their performance and efficiency. We also explore some
interesting conjectures suggested by the results of our experiments that relate
the properties of the user preferences, the dominance relation, and the
algorithms.
","Representing and Reasoning with Qualitative Preferences for
  Compositional Systems","Ganesh Ram Santhanam, Samik Basu, Vasant Honavar",2011,Artificial Intelligence,
"  Standard belief change assumes an underlying logic containing full classical
propositional logic. However, there are good reasons for considering belief
change in less expressive logics as well. In this paper we build on recent
investigations by Delgrande on contraction for Horn logic. We show that the
standard basic form of contraction, partial meet, is too strong in the Horn
case. This result stands in contrast to Delgrande's conjecture that orderly
maxichoice is the appropriate form of contraction for Horn logic. We then
define a more appropriate notion of basic contraction for the Horn case,
influenced by the convexity property holding for full propositional logic and
which we refer to as infra contraction. The main contribution of this work is a
result which shows that the construction method for Horn contraction for belief
sets based on our infra remainder sets corresponds exactly to Hansson's
classical kernel contraction for belief sets, when restricted to Horn logic.
This result is obtained via a detour through contraction for belief bases. We
prove that kernel contraction for belief bases produces precisely the same
results as the belief base version of infra contraction. The use of belief
bases to obtain this result provides evidence for the conjecture that Horn
belief change is best viewed as a hybrid version of belief set change and
belief base change. One of the consequences of the link with base contraction
is the provision of a representation result for Horn contraction for belief
sets in which a version of the Core-retainment postulate features.
","On the Link between Partial Meet, Kernel, and Infra Contraction and its
  Application to Horn Logic","Richard Booth, Thomas Meyer, Ivan Varzinczak, Renata Wassermann",2011,Artificial Intelligence,
"  Multi-agent path planning is a challenging problem with numerous real-life
applications. Running a centralized search such as A* in the combined state
space of all units is complete and cost-optimal, but scales poorly, as the
state space size is exponential in the number of mobile units. Traditional
decentralized approaches, such as FAR and WHCA*, are faster and more scalable,
being based on problem decomposition. However, such methods are incomplete and
provide no guarantees with respect to the running time or the solution quality.
They are not necessarily able to tell in a reasonable time whether they would
succeed in finding a solution to a given instance. We introduce MAPP, a
tractable algorithm for multi-agent path planning on undirected graphs. We
present a basic version and several extensions. They have low-polynomial
worst-case upper bounds for the running time, the memory requirements, and the
length of solutions. Even though all algorithmic versions are incomplete in the
general case, each provides formal guarantees on problems it can solve. For
each version, we discuss the algorithms completeness with respect to clearly
defined subclasses of instances. Experiments were run on realistic game grid
maps. MAPP solved 99.86% of all mobile units, which is 18--22% better than the
percentage of FAR and WHCA*. MAPP marked 98.82% of all units as provably
solvable during the first stage of plan computation. Parts of MAPPs computation
can be re-used across instances on the same map. Speed-wise, MAPP is
competitive or significantly faster than WHCA*, depending on whether MAPP
performs all computations from scratch. When data that MAPP can re-use are
preprocessed offline and readily available, MAPP is slower than the very fast
FAR algorithm by a factor of 2.18 on average. MAPPs solutions are on average
20% longer than FARs solutions and 7--31% longer than WHCA*s solutions.
","MAPP: a Scalable Multi-Agent Path Planning Algorithm with Tractability
  and Completeness Guarantees","Ko-Hsin Cindy Wang, Adi Botea",2011,Artificial Intelligence,
"  We consider how an agent should update her beliefs when her beliefs are
represented by a set P of probability distributions, given that the agent makes
decisions using the minimax criterion, perhaps the best-studied and most
commonly-used criterion in the literature. We adopt a game-theoretic framework,
where the agent plays against a bookie, who chooses some distribution from P.
We consider two reasonable games that differ in what the bookie knows when he
makes his choice. Anomalies that have been observed before, like time
inconsistency, can be understood as arising because different games are being
played, against bookies with different information. We characterize the
important special cases in which the optimal decision rules according to the
minimax criterion amount to either conditioning or simply ignoring the
information. Finally, we consider the relationship between updating and
calibration when uncertainty is described by sets of probabilities. Our results
emphasize the key role of the rectangularity condition of Epstein and
Schneider.
","Making Decisions Using Sets of Probabilities: Updating, Time
  Consistency, and Calibration","Peter D Grunwald, Joseph Y Halpern",2011,Artificial Intelligence,
"  In many professional sports leagues, teams from opposing leagues/conferences
compete against one another, playing inter-league games. This is an example of
a bipartite tournament. In this paper, we consider the problem of reducing the
total travel distance of bipartite tournaments, by analyzing inter-league
scheduling from the perspective of discrete optimization. This research has
natural applications to sports scheduling, especially for leagues such as the
National Basketball Association (NBA) where teams must travel long distances
across North America to play all their games, thus consuming much time, money,
and greenhouse gas emissions. We introduce the Bipartite Traveling Tournament
Problem (BTTP), the inter-league variant of the well-studied Traveling
Tournament Problem. We prove that the 2n-team BTTP is NP-complete, but for
small values of n, a distance-optimal inter-league schedule can be generated
from an algorithm based on minimum-weight 4-cycle-covers. We apply our
theoretical results to the 12-team Nippon Professional Baseball (NPB) league in
Japan, producing a provably-optimal schedule requiring 42950 kilometres of
total team travel, a 16% reduction compared to the actual distance traveled by
these teams during the 2010 NPB season. We also develop a nearly-optimal
inter-league tournament for the 30-team NBA league, just 3.8% higher than the
trivial theoretical lower bound.
",Scheduling Bipartite Tournaments to Minimize Total Travel Distance,"Richard Hoshino, Ken-ichi Kawarabayashi",2011,Artificial Intelligence,
"  Value iteration is a powerful yet inefficient algorithm for Markov decision
processes (MDPs) because it puts the majority of its effort into backing up the
entire state space, which turns out to be unnecessary in many cases. In order
to overcome this problem, many approaches have been proposed. Among them, ILAO*
and variants of RTDP are state-of-the-art ones. These methods use reachability
analysis and heuristic search to avoid some unnecessary backups. However, none
of these approaches build the graphical structure of the state transitions in a
pre-processing step or use the structural information to systematically
decompose a problem, whereby generating an intelligent backup sequence of the
state space. In this paper, we present two optimal MDP algorithms. The first
algorithm, topological value iteration (TVI), detects the structure of MDPs and
backs up states based on topological sequences. It (1) divides an MDP into
strongly-connected components (SCCs), and (2) solves these components
sequentially. TVI outperforms VI and other state-of-the-art algorithms vastly
when an MDP has multiple, close-to-equal-sized SCCs. The second algorithm,
focused topological value iteration (FTVI), is an extension of TVI. FTVI
restricts its attention to connected components that are relevant for solving
the MDP. Specifically, it uses a small amount of heuristic search to eliminate
provably sub-optimal actions; this pruning allows FTVI to find smaller
connected components, thus running faster. We demonstrate that FTVI outperforms
TVI by an order of magnitude, averaged across several domains. Surprisingly,
FTVI also significantly outperforms popular heuristically-informed MDP
algorithms such as ILAO*, LRTDP, BRTDP and Bayesian-RTDP in many domains,
sometimes by as much as two orders of magnitude. Finally, we characterize the
type of domains where FTVI excels --- suggesting a way to an informed choice of
solver.
",Topological Value Iteration Algorithms,"Peng Dai, Mausam, Daniel Sabby Weld, Judy Goldsmith",2011,Artificial Intelligence,
"  Many Artificial Intelligence tasks cannot be evaluated with a single quality
criterion and some sort of weighted combination is needed to provide system
rankings. A problem of weighted combination measures is that slight changes in
the relative weights may produce substantial changes in the system rankings.
This paper introduces the Unanimous Improvement Ratio (UIR), a measure that
complements standard metric combination criteria (such as van Rijsbergen's
F-measure) and indicates how robust the measured differences are to changes in
the relative weights of the individual metrics. UIR is meant to elucidate
whether a perceived difference between two systems is an artifact of how
individual metrics are weighted.
  Besides discussing the theoretical foundations of UIR, this paper presents
empirical results that confirm the validity and usefulness of the metric for
the Text Clustering problem, where there is a tradeoff between precision and
recall based metrics and results are particularly sensitive to the weighting
scheme used to combine them. Remarkably, our experiments show that UIR can be
used as a predictor of how well differences between systems measured on a given
test bed will also hold in a different test bed.
","Combining Evaluation Metrics via the Unanimous Improvement Ratio and its
  Application to Clustering Tasks","Enrique Amig\'o, Julio Gonzalo, Javier Artiles, Felisa Verdejo",2011,Artificial Intelligence,
"  In a deterministic world, a planning agent can be certain of the consequences
of its planned sequence of actions. Not so, however, in dynamic, stochastic
domains where Markov decision processes are commonly used. Unfortunately these
suffer from the curse of dimensionality: if the state space is a Cartesian
product of many small sets (dimensions), planning is exponential in the number
of those dimensions.
  Our new technique exploits the intuitive strategy of selectively ignoring
various dimensions in different parts of the state space. The resulting
non-uniformity has strong implications, since the approximation is no longer
Markovian, requiring the use of a modified planner. We also use a spatial and
temporal proximity measure, which responds to continued planning as well as
movement of the agent through the state space, to dynamically adapt the
abstraction as planning progresses.
  We present qualitative and quantitative results across a range of
experimental domains showing that an agent exploiting this novel approximation
method successfully finds solutions to the planning problem using much less
than the full state space. We assess and analyse the features of domains which
our method can exploit.
",Proximity-Based Non-uniform Abstractions for Approximate Planning,"Jiri Baum, Ann E. Nicholson, Trevor I. Dix",2012,Artificial Intelligence,
"  Scheduling problems in manufacturing, logistics and project management have
frequently been modeled using the framework of Resource Constrained Project
Scheduling Problems with minimum and maximum time lags (RCPSP/max). Due to the
importance of these problems, providing scalable solution schedules for
RCPSP/max problems is a topic of extensive research. However, all existing
methods for solving RCPSP/max assume that durations of activities are known
with certainty, an assumption that does not hold in real world scheduling
problems where unexpected external events such as manpower availability,
weather changes, etc. lead to delays or advances in completion of activities.
Thus, in this paper, our focus is on providing a scalable method for solving
RCPSP/max problems with durational uncertainty. To that end, we introduce the
robust local search method consisting of three key ideas: (a) Introducing and
studying the properties of two decision rule approximations used to compute
start times of activities with respect to dynamic realizations of the
durational uncertainty; (b) Deriving the expression for robust makespan of an
execution strategy based on decision rule approximations; and (c) A robust
local search mechanism to efficiently compute activity execution strategies
that are robust against durational uncertainty. Furthermore, we also provide
enhancements to local search that exploit temporal dependencies between
activities. Our experimental results illustrate that robust local search is
able to provide robust execution strategies efficiently.
",Robust Local Search for Solving RCPSP/max with Durational Uncertainty,"Na Fu, Hoong Chuin Lau, Pradeep R. Varakantham, Fei Xiao",2012,Artificial Intelligence,
"  We describe Dr.Fill, a program that solves American-style crossword puzzles.
From a technical perspective, Dr.Fill works by converting crosswords to
weighted CSPs, and then using a variety of novel techniques to find a solution.
These techniques include generally applicable heuristics for variable and value
selection, a variant of limited discrepancy search, and postprocessing and
partitioning ideas. Branch and bound is not used, as it was incompatible with
postprocessing and was determined experimentally to be of little practical
value. Dr.Fillls performance on crosswords from the American Crossword Puzzle
Tournament suggests that it ranks among the top fifty or so crossword solvers
in the world.
",Dr.Fill: Crosswords and an Implemented Solver for Singly Weighted CSPs,Matthew L. Ginsberg,2011,Artificial Intelligence,
"  Planning as satisfiability is a principal approach to planning with many
eminent advantages. The existing planning as satisfiability techniques usually
use encodings compiled from STRIPS. We introduce a novel SAT encoding scheme
(SASE) based on the SAS+ formalism. The new scheme exploits the structural
information in SAS+, resulting in an encoding that is both more compact and
efficient for planning. We prove the correctness of the new encoding by
establishing an isomorphism between the solution plans of SASE and that of
STRIPS based encodings. We further analyze the transition variables newly
introduced in SASE to explain why it accommodates modern SAT solving algorithms
and improves performance. We give empirical statistical results to support our
analysis. We also develop a number of techniques to further reduce the encoding
size of SASE, and conduct experimental studies to show the strength of each
individual technique. Finally, we report extensive experimental results to
demonstrate significant improvements of SASE over the state-of-the-art STRIPS
based encoding schemes in terms of both time and memory efficiency.
",SAS+ Planning as Satisfiability,"Ruoyun Huang, Yixin Chen, Weixiong Zhang",2012,Artificial Intelligence,
"  We focus on the problem of sequential decision making in partially observable
environments shared with other agents of uncertain types having similar or
conflicting objectives. This problem has been previously formalized by multiple
frameworks one of which is the interactive dynamic influence diagram (I-DID),
which generalizes the well-known influence diagram to the multiagent setting.
I-DIDs are graphical models and may be used to compute the policy of an agent
given its belief over the physical state and others models, which changes as
the agent acts and observes in the multiagent setting.
  As we may expect, solving I-DIDs is computationally hard. This is
predominantly due to the large space of candidate models ascribed to the other
agents and its exponential growth over time. We present two methods for
reducing the size of the model space and stemming its exponential growth. Both
these methods involve aggregating individual models into equivalence classes.
Our first method groups together behaviorally equivalent models and selects
only those models for updating which will result in predictive behaviors that
are distinct from others in the updated model space. The second method further
compacts the model space by focusing on portions of the behavioral predictions.
Specifically, we cluster actionally equivalent models that prescribe identical
actions at a single time step. Exactly identifying the equivalences would
require us to solve all models in the initial set. We avoid this by selectively
solving some of the models, thereby introducing an approximation. We discuss
the error introduced by the approximation, and empirically demonstrate the
improved efficiency in solving I-DIDs due to the equivalences.
","Exploiting Model Equivalences for Solving Interactive Dynamic Influence
  Diagrams","Yifeng Zeng, Prashant Doshi",2012,Artificial Intelligence,
"  Designing a search heuristic for constraint programming that is reliable
across problem domains has been an important research topic in recent years.
This paper concentrates on one family of candidates: counting-based search.
Such heuristics seek to make branching decisions that preserve most of the
solutions by determining what proportion of solutions to each individual
constraint agree with that decision. Whereas most generic search heuristics in
constraint programming rely on local information at the level of the individual
variable, our search heuristics are based on more global information at the
constraint level. We design several algorithms that are used to count the
number of solutions to specific families of constraints and propose some search
heuristics exploiting such information. The experimental part of the paper
considers eight problem domains ranging from well-established benchmark puzzles
to rostering and sport scheduling. An initial empirical analysis identifies
heuristic maxSD as a robust candidate among our proposals.eWe then evaluate the
latter against the state of the art, including the latest generic search
heuristics, restarts, and discrepancy-based tree traversals. Experimental
results show that counting-based search generally outperforms other generic
heuristics.
","Counting-Based Search: Branching Heuristics for Constraint Satisfaction
  Problems","Gilles Pesant, Claude-Guy Quimper, Alessandro Zanarini",2012,Artificial Intelligence,
"  The focus of this paper is the calculation of similarity between two concepts
from an ontology for a Human-Like Interaction system. In order to facilitate
this calculation, a similarity function is proposed based on five dimensions
(sort, compositional, essential, restrictive and descriptive) constituting the
structure of ontological knowledge. The paper includes a proposal for computing
a similarity function for each dimension of knowledge. Later on, the similarity
values obtained are weighted and aggregated to obtain a global similarity
measure. In order to calculate those weights associated to each dimension, four
training methods have been proposed. The training methods differ in the element
to fit: the user, concepts or pairs of concepts, and a hybrid approach. For
evaluating the proposal, the knowledge base was fed from WordNet and extended
by using a knowledge editing toolkit (Cognos). The evaluation of the proposal
is carried out through the comparison of system responses with those given by
human test subjects, both providing a measure of the soundness of the procedure
and revealing ways in which the proposal may be improved.
","Semantic Similarity Measures Applied to an Ontology for Human-Like
  Interaction","Esperanza Albacete, Javier Calle, Elena Castro, Dolores Cuadra",2012,Artificial Intelligence,
"  To achieve scalability of query answering, the developers of Semantic Web
applications are often forced to use incomplete OWL 2 reasoners, which fail to
derive all answers for at least one query, ontology, and data set. The lack of
completeness guarantees, however, may be unacceptable for applications in areas
such as health care and defence, where missing answers can adversely affect the
applications functionality. Furthermore, even if an application can tolerate
some level of incompleteness, it is often advantageous to estimate how many and
what kind of answers are being lost.
  In this paper, we present a novel logic-based framework that allows one to
check whether a reasoner is complete for a given query Q and ontology T---that
is, whether the reasoner is guaranteed to compute all answers to Q w.r.t. T and
an arbitrary data set A. Since ontologies and typical queries are often fixed
at application design time, our approach allows application developers to check
whether a reasoner known to be incomplete in general is actually complete for
the kinds of input relevant for the application.
  We also present a technique that, given a query Q, an ontology T, and
reasoners R_1 and R_2 that satisfy certain assumptions, can be used to
determine whether, for each data set A, reasoner R_1 computes more answers to Q
w.r.t. T and A than reasoner R_2. This allows application developers to select
the reasoner that provides the highest degree of completeness for Q and T that
is compatible with the applications scalability requirements.
  Our results thus provide a theoretical and practical foundation for the
design of future ontology-based information systems that maximise scalability
while minimising or even eliminating incompleteness of query answers.
","Completeness Guarantees for Incomplete Ontology Reasoners: Theory and
  Practice","Bernardo Cuenca Grau, Boris Motik, Giorgos Stoilos, Ian Horrocks",2012,Artificial Intelligence,
"  Many combinatorial problems deal with preferences and violations, the goal of
which is to find solutions with the minimum cost. Weighted constraint
satisfaction is a framework for modeling such problems, which consists of a set
of cost functions to measure the degree of violation or preferences of
different combinations of variable assignments. Typical solution methods for
weighted constraint satisfaction problems (WCSPs) are based on branch-and-bound
search, which are made practical through the use of powerful consistency
techniques such as AC*, FDAC*, EDAC* to deduce hidden cost information and
value pruning during search. These techniques, however, are designed to be
efficient only on binary and ternary cost functions which are represented in
table form. In tackling many real-life problems, high arity (or global) cost
functions are required. We investigate efficient representation scheme and
algorithms to bring the benefits of the consistency techniques to also high
arity cost functions, which are often derived from hard global constraints from
classical constraint satisfaction. The literature suggests some global cost
functions can be represented as flow networks, and the minimum cost flow
algorithm can be used to compute the minimum costs of such networks in
polynomial time. We show that naive adoption of this flow-based algorithmic
method for global cost functions can result in a stronger form of null-inverse
consistency. We further show how the method can be modified to handle cost
projections and extensions to maintain generalized versions of AC* and FDAC*
for cost functions with more than two variables. Similar generalization for the
stronger EDAC* is less straightforward. We reveal the oscillation problem when
enforcing EDAC* on cost functions sharing more than one variable. To avoid
oscillation, we propose a weak version of EDAC* and generalize it to weak
EDGAC* for non-binary cost functions. Using various benchmarks involving the
soft variants of hard global constraints ALLDIFFERENT, GCC, SAME, and REGULAR,
empirical results demonstrate that our proposal gives improvements of up to an
order of magnitude when compared with the traditional constraint optimization
approach, both in terms of time and pruning.
","Consistency Techniques for Flow-Based Projection-Safe Global Cost
  Functions in Weighted Constraint Satisfaction","J.H.M. Lee, Ka Lun Leung",2012,Artificial Intelligence,
"  This work presents Drake, a dynamic executive for temporal plans with choice.
Dynamic plan execution strategies allow an autonomous agent to react quickly to
unfolding events, improving the robustness of the agent. Prior work developed
methods for dynamically dispatching Simple Temporal Networks, and further
research enriched the expressiveness of the plans executives could handle,
including discrete choices, which are the focus of this work. However, in some
approaches to date, these additional choices induce significant storage or
latency requirements to make flexible execution possible.
  Drake is designed to leverage the low latency made possible by a
preprocessing step called compilation, while avoiding high memory costs through
a compact representation. We leverage the concepts of labels and environments,
taken from prior work in Assumption-based Truth Maintenance Systems (ATMS), to
concisely record the implications of the discrete choices, exploiting the
structure of the plan to avoid redundant reasoning or storage. Our labeling and
maintenance scheme, called the Labeled Value Set Maintenance System, is
distinguished by its focus on properties fundamental to temporal problems, and,
more generally, weighted graph algorithms. In particular, the maintenance
system focuses on maintaining a minimal representation of non-dominated
constraints. We benchmark Drakes performance on random structured problems, and
find that Drake reduces the size of the compiled representation by a factor of
over 500 for large problems, while incurring only a modest increase in run-time
latency, compared to prior work in compiled executives for temporal plans with
discrete choices.
",Drake: An Efficient Executive for Temporal Plans with Choice,"Patrick Raymond Conrad, Brian Williams",2011,Artificial Intelligence,
"  Circumscription and logic programs under the stable model semantics are two
well-known nonmonotonic formalisms. The former has served as a basis of
classical logic based action formalisms, such as the situation calculus, the
event calculus and temporal action logics; the latter has served as a basis of
a family of action languages, such as language A and several of its
descendants. Based on the discovery that circumscription and the stable model
semantics coincide on a class of canonical formulas, we reformulate the
situation calculus and the event calculus in the general theory of stable
models. We also present a translation that turns the reformulations further
into answer set programs, so that efficient answer set solvers can be applied
to compute the situation calculus and the event calculus.
","Reformulating the Situation Calculus and the Event Calculus in the
  General Theory of Stable Models and in Answer Set Programming","Joohyung Lee, Ravi Palla",2012,Artificial Intelligence,
"  Local consistency techniques such as k-consistency are a key component of
specialised solvers for constraint satisfaction problems. In this paper we show
that the power of using k-consistency techniques on a constraint satisfaction
problem is precisely captured by using a particular inference rule, which we
call negative-hyper-resolution, on the standard direct encoding of the problem
into Boolean clauses. We also show that current clause-learning SAT-solvers
will discover in expected polynomial time any inconsistency that can be deduced
from a given set of clauses using negative-hyper-resolvents of a fixed size. We
combine these two results to show that, without being explicitly designed to do
so, current clause-learning SAT-solvers efficiently simulate k-consistency
techniques, for all fixed values of k. We then give some experimental results
to show that this feature allows clause-learning SAT-solvers to efficiently
solve certain families of constraint problems which are challenging for
conventional constraint-programming solvers.
",Local Consistency and SAT-Solvers,"Peter Jeavons, Justyna Petke",2012,Artificial Intelligence,
"  In this paper, harmony search algorithm is applied to curriculum-based course
timetabling. The implementation, specifically the process of improvisation
consists of memory consideration, random consideration and pitch adjustment. In
memory consideration, the value of the course number for new solution was
selected from all other course number located in the same column of the Harmony
Memory. This research used the highest occurrence of the course number to be
scheduled in a new harmony. The remaining courses that have not been scheduled
by memory consideration will go through random consideration, i.e. will select
any feasible location available to be scheduled in the new harmony solution.
Each course scheduled out of memory consideration is examined as to whether it
should be pitch adjusted with probability of eight procedures. However, the
algorithm produced results that were not comparatively better than those
previously known as best solution. With proper modification in terms of the
approach in this algorithm would make the algorithm perform better on
curriculum-based course timetabling.
",Harmony Search Algorithm for Curriculum-Based Course Timetabling Problem,"Juliana Wahid, Naimah Mohd Hussin",2013,Artificial Intelligence,
"  We present a skill analysis with time series image data using data mining
methods, focused on table tennis. We do not use body model, but use only
hi-speed movies, from which time series data are obtained and analyzed using
data mining methods such as C4.5 and so on. We identify internal models for
technical skills as evaluation skillfulness for the forehand stroke of table
tennis, and discuss mono and meta-functional skills for improving skills.
",Skill Analysis with Time Series Image Data,"Toshiyuki Maeda, Masanori Fujii, Isao Hayashi",2013,Artificial Intelligence,
"  Compact representations of objects is a common concept in computer science.
Automated planning can be viewed as a case of this concept: a planning instance
is a compact implicit representation of a graph and the problem is to find a
path (a plan) in this graph. While the graphs themselves are represented
compactly as planning instances, the paths are usually represented explicitly
as sequences of actions. Some cases are known where the plans always have
compact representations, for example, using macros. We show that these results
do not extend to the general case, by proving a number of bounds for compact
representations of plans under various criteria, like efficient sequential or
random access of actions. In addition to this, we show that our results have
consequences for what can be gained from reformulating planning into some other
problem. As a contrast to this we also prove a number of positive results,
demonstrating restricted cases where plans do have useful compact
representations, as well as proving that macro plans have favourable access
properties. Our results are finally discussed in relation to other relevant
contexts.
",Algorithms and Limits for Compact Plan Representations,"Christer B\""ackstr\""om, Peter Jonsson",2012,Artificial Intelligence,
"  We present algorithms for generating alternative solutions for explicit
acyclic AND/OR structures in non-decreasing order of cost. The proposed
algorithms use a best first search technique and report the solutions using an
implicit representation ordered by cost. In this paper, we present two versions
of the search algorithm -- (a) an initial version of the best first search
algorithm, ASG, which may present one solution more than once while generating
the ordered solutions, and (b) another version, LASG, which avoids the
construction of the duplicate solutions. The actual solutions can be
reconstructed quickly from the implicit compact representation used. We have
applied the methods on a few test domains, some of them are synthetic while the
others are based on well known problems including the search space of the 5-peg
Tower of Hanoi problem, the matrix-chain multiplication problem and the problem
of finding secondary structure of RNA. Experimental results show the efficacy
of the proposed algorithms over the existing approach. Our proposed algorithms
have potential use in various domains ranging from knowledge based frameworks
to service composition, where the AND/OR structure is widely used for
representing problems.
","Algorithms for Generating Ordered Solutions for Explicit AND/OR
  Structures","Priyankar Ghosh, Amit Sharma, P.P. Chakrabarti, Pallab Dasgupta",2012,Artificial Intelligence,
"  There is currently a growing interest in techniques for hiding parts of the
signature of an ontology Kh that is being reused by another ontology Kv.
Towards this goal, in this paper we propose the import-by-query framework,
which makes the content of Kh accessible through a limited query interface. If
Kv reuses the symbols from Kh in a certain restricted way, one can reason over
Kv U Kh by accessing only Kv and the query interface. We map out the landscape
of the import-by-query problem. In particular, we outline the limitations of
our framework and prove that certain restrictions on the expressivity of Kh and
the way in which Kv reuses symbols from Kh are strictly necessary to enable
reasoning in our setting. We also identify cases in which reasoning is possible
and we present suitable import-by-query reasoning algorithms.
","Reasoning over Ontologies with Hidden Content: The Import-by-Query
  Approach","Bernardo Cuenca Grau, Boris Motik",2012,Artificial Intelligence,
"  Heuristics used for solving hard real-time search problems have regions with
depressions. Such regions are bounded areas of the search space in which the
heuristic function is inaccurate compared to the actual cost to reach a
solution. Early real-time search algorithms, like LRTA*, easily become trapped
in those regions since the heuristic values of their states may need to be
updated multiple times, which results in costly solutions. State-of-the-art
real-time search algorithms, like LSS-LRTA* or LRTA*(k), improve LRTA*s
mechanism to update the heuristic, resulting in improved performance. Those
algorithms, however, do not guide search towards avoiding depressed regions.
This paper presents depression avoidance, a simple real-time search principle
to guide search towards avoiding states that have been marked as part of a
heuristic depression. We propose two ways in which depression avoidance can be
implemented: mark-and-avoid and move-to-border. We implement these strategies
on top of LSS-LRTA* and RTAA*, producing 4 new real-time heuristic search
algorithms: aLSS-LRTA*, daLSS-LRTA*, aRTAA*, and daRTAA*. When the objective is
to find a single solution by running the real-time search algorithm once, we
show that daLSS-LRTA* and daRTAA* outperform their predecessors sometimes by
one order of magnitude. Of the four new algorithms, daRTAA* produces the best
solutions given a fixed deadline on the average time allowed per planning
episode. We prove all our algorithms have good theoretical properties: in
finite search spaces, they find a solution if one exists, and converge to an
optimal after a number of trials.
",Avoiding and Escaping Depressions in Real-Time Heuristic Search,"Carlos Hern\'andez, Jorge A Baier",2012,Artificial Intelligence,
"  A model of story generation recently proposed by Riedl and Young casts it as
planning, with the additional condition that story characters behave
intentionally. This means that characters have perceivable motivation for the
actions they take. I show that this condition can be compiled away (in more
ways than one) to produce a classical planning problem that can be solved by an
off-the-shelf classical planner, more efficiently than by Riedl and Youngs
specialised planner.
",Narrative Planning: Compilations to Classical Planning,Patrik Haslum,2012,Artificial Intelligence,
"  In this paper we describe COLIN, a forward-chaining heuristic search planner,
capable of reasoning with COntinuous LINear numeric change, in addition to the
full temporal semantics of PDDL. Through this work we make two advances to the
state-of-the-art in terms of expressive reasoning capabilities of planners: the
handling of continuous linear change, and the handling of duration-dependent
effects in combination with duration inequalities, both of which require
tightly coupled temporal and numeric reasoning during planning. COLIN combines
FF-style forward chaining search, with the use of a Linear Program (LP) to
check the consistency of the interacting temporal and numeric constraints at
each state. The LP is used to compute bounds on the values of variables in each
state, reducing the range of actions that need to be considered for
application. In addition, we develop an extension of the Temporal Relaxed
Planning Graph heuristic of CRIKEY3, to support reasoning directly with
continuous change. We extend the range of task variables considered to be
suitable candidates for specifying the gradient of the continuous numeric
change effected by an action. Finally, we explore the potential for employing
mixed integer programming as a tool for optimising the timestamps of the
actions in the plan, once a solution has been found. To support this, we
further contribute a selection of extended benchmark domains that include
continuous numeric effects. We present results for COLIN that demonstrate its
scalability on a range of benchmarks, and compare to existing state-of-the-art
planners.
",COLIN: Planning with Continuous Linear Numeric Change,"Amanda J. Coles, Andrew I. Coles, Maria Fox, Derek Long",2012,Artificial Intelligence,
"  Planning is concerned with the automated solution of action sequencing
problems described in declarative languages giving the action preconditions and
effects. One important application area for such technology is the creation of
new processes in Business Process Management (BPM), which is essential in an
ever more dynamic business environment. A major obstacle for the application of
Planning in this area lies in the modeling. Obtaining a suitable model to plan
with -- ideally a description in PDDL, the most commonly used planning language
-- is often prohibitively complicated and/or costly. Our core observation in
this work is that this problem can be ameliorated by leveraging synergies with
model-based software development. Our application at SAP, one of the leading
vendors of enterprise software, demonstrates that even one-to-one model re-use
is possible.
  The model in question is called Status and Action Management (SAM). It
describes the behavior of Business Objects (BO), i.e., large-scale data
structures, at a level of abstraction corresponding to the language of business
experts. SAM covers more than 400 kinds of BOs, each of which is described in
terms of a set of status variables and how their values are required for, and
affected by, processing steps (actions) that are atomic from a business
perspective. SAM was developed by SAP as part of a major model-based software
engineering effort. We show herein that one can use this same model for
planning, thus obtaining a BPM planning application that incurs no modeling
overhead at all.
  We compile SAM into a variant of PDDL, and adapt an off-the-shelf planner to
solve this kind of problem. Thanks to the resulting technology, business
experts may create new processes simply by specifying the desired behavior in
terms of status variable value changes: effectively, by describing the process
in their own language.
","SAP Speaks PDDL: Exploiting a Software-Engineering Model for Planning in
  Business Process Management","Joerg Hoffman, Ingo Weber, Frank Michael Kraft",2012,Artificial Intelligence,
"  Efficient use of multiple batteries is a practical problem with wide and
growing application. The problem can be cast as a planning problem under
uncertainty. We describe the approach we have adopted to modelling and solving
this problem, seen as a Markov Decision Problem, building effective policies
for battery switching in the face of stochastic load profiles.
  Our solution exploits and adapts several existing techniques: planning for
deterministic mixed discrete-continuous problems and Monte Carlo sampling for
policy learning. The paper describes the development of planning techniques to
allow solution of the non-linear continuous dynamic models capturing the
battery behaviours. This approach depends on carefully handled discretisation
of the temporal dimension. The construction of policies is performed using a
classification approach and this idea offers opportunities for wider
exploitation in other problems. The approach and its generality are described
in the paper.
  Application of the approach leads to construction of policies that, in
simulation, significantly outperform those that are currently in use and the
best published solutions to the battery management problem. We achieve
solutions that achieve more than 99% efficiency in simulation compared with the
theoretical limit and do so with far fewer battery switches than existing
policies. Behaviour of physical batteries does not exactly match the simulated
models for many reasons, so to confirm that our theoretical results can lead to
real measured improvements in performance we also conduct and report
experiments using a physical test system. These results demonstrate that we can
obtain 5%-15% improvement in lifetimes in the case of a two battery system.
",Plan-based Policies for Efficient Multiple Battery Load Management,"Maria Fox, Derek Long, Daniele Magazzeni",2012,Artificial Intelligence,
"  Pseudo-Boolean constraints are omnipresent in practical applications, and
thus a significant effort has been devoted to the development of good SAT
encoding techniques for them. Some of these encodings first construct a Binary
Decision Diagram (BDD) for the constraint, and then encode the BDD into a
propositional formula. These BDD-based approaches have some important
advantages, such as not being dependent on the size of the coefficients, or
being able to share the same BDD for representing many constraints.
  We first focus on the size of the resulting BDDs, which was considered to be
an open problem in our research community. We report on previous work where it
was proved that there are Pseudo-Boolean constraints for which no polynomial
BDD exists. We also give an alternative and simpler proof assuming that NP is
different from Co-NP. More interestingly, here we also show how to overcome the
possible exponential blowup of BDDs by phcoefficient decomposition. This allows
us to give the first polynomial generalized arc-consistent ROBDD-based encoding
for Pseudo-Boolean constraints.
  Finally, we focus on practical issues: we show how to efficiently construct
such ROBDDs, how to encode them into SAT with only 2 clauses per node, and
present experimental results that confirm that our approach is competitive with
other encodings and state-of-the-art Pseudo-Boolean solvers.
",A New Look at BDDs for Pseudo-Boolean Constraints,"Ignasi Ab\'io, Robert Nieuwenhuis, Albert Oliveras, Enric
  Rodriguez-Carbonell, Valentin Mayer-Eichberger",2012,Artificial Intelligence,
"  Domain-independent planning is one of the foundational areas in the field of
Artificial Intelligence. A description of a planning task consists of an
initial world state, a goal, and a set of actions for modifying the world
state. The objective is to find a sequence of actions, that is, a plan, that
transforms the initial world state into a goal state. In optimal planning, we
are interested in finding not just a plan, but one of the cheapest plans. A
prominent approach to optimal planning these days is heuristic state-space
search, guided by admissible heuristic functions. Numerous admissible
heuristics have been developed, each with its own strengths and weaknesses, and
it is well known that there is no single ""best heuristic for optimal planning
in general. Thus, which heuristic to choose for a given planning task is a
difficult question. This difficulty can be avoided by combining several
heuristics, but that requires computing numerous heuristic estimates at each
state, and the tradeoff between the time spent doing so and the time saved by
the combined advantages of the different heuristics might be high. We present a
novel method that reduces the cost of combining admissible heuristics for
optimal planning, while maintaining its benefits. Using an idealized search
space model, we formulate a decision rule for choosing the best heuristic to
compute at each state. We then present an active online learning approach for
learning a classifier with that decision rule as the target concept, and employ
the learned classifier to decide which heuristic to compute at each state. We
evaluate this technique empirically, and show that it substantially outperforms
the standard method for combining several heuristics via their pointwise
maximum.
",Online Speedup Learning for Optimal Planning,"Carmel Domshlak, Erez Karpas, Shaul Markovitch",2012,Artificial Intelligence,
"  Replanning via determinization is a recent, popular approach for online
planning in MDPs. In this paper we adapt this idea to classical, non-stochastic
domains with partial information and sensing actions, presenting a new planner:
SDR (Sample, Determinize, Replan). At each step we generate a solution plan to
a classical planning problem induced by the original problem. We execute this
plan as long as it is safe to do so. When this is no longer the case, we
replan. The classical planning problem we generate is based on the
translation-based approach for conformant planning introduced by Palacios and
Geffner. The state of the classical planning problem generated in this approach
captures the belief state of the agent in the original problem. Unfortunately,
when this method is applied to planning problems with sensing, it yields a
non-deterministic planning problem that is typically very large. Our main
contribution is the introduction of state sampling techniques for overcoming
these two problems. In addition, we introduce a novel, lazy, regression-based
method for querying the agents belief state during run-time. We provide a
comprehensive experimental evaluation of the planner, showing that it scales
better than the state-of-the-art CLG planner on existing benchmark problems,
but also highlighting its weaknesses with new domains. We also discuss its
theoretical guarantees.
",Replanning in Domains with Partial Information and Sensing Actions,"Ronen I. Brafman, Guy Shani",2012,Artificial Intelligence,
"  In some domestic professional sports leagues, the home stadiums are located
in cities connected by a common train line running in one direction. For these
instances, we can incorporate this geographical information to determine
optimal or nearly-optimal solutions to the n-team Traveling Tournament Problem
(TTP), an NP-hard sports scheduling problem whose solution is a double
round-robin tournament schedule that minimizes the sum total of distances
traveled by all n teams. We introduce the Linear Distance Traveling Tournament
Problem (LD-TTP), and solve it for n=4 and n=6, generating the complete set of
possible solutions through elementary combinatorial techniques. For larger n,
we propose a novel ""expander construction"" that generates an approximate
solution to the LD-TTP. For n congruent to 4 modulo 6, we show that our
expander construction produces a feasible double round-robin tournament
schedule whose total distance is guaranteed to be no worse than 4/3 times the
optimal solution, regardless of where the n teams are located. This
4/3-approximation for the LD-TTP is stronger than the currently best-known
ratio of 5/3 + epsilon for the general TTP. We conclude the paper by applying
this linear distance relaxation to general (non-linear) n-team TTP instances,
where we develop fast approximate solutions by simply ""assuming"" the n teams
lie on a straight line and solving the modified problem. We show that this
technique surprisingly generates the distance-optimal tournament on all
benchmark sets on 6 teams, as well as close-to-optimal schedules for larger n,
even when the teams are located around a circle or positioned in
three-dimensional space.
","Generating Approximate Solutions to the TTP using a Linear Distance
  Relaxation","Richard Hoshino, Ken-ichi Kawarabayashi",2012,Artificial Intelligence,
"  We introduce an efficient message passing scheme for solving Constraint
Satisfaction Problems (CSPs), which uses stochastic perturbation of Belief
Propagation (BP) and Survey Propagation (SP) messages to bypass decimation and
directly produce a single satisfying assignment. Our first CSP solver, called
Perturbed Blief Propagation, smoothly interpolates two well-known inference
procedures; it starts as BP and ends as a Gibbs sampler, which produces a
single sample from the set of solutions. Moreover we apply a similar
perturbation scheme to SP to produce another CSP solver, Perturbed Survey
Propagation. Experimental results on random and real-world CSPs show that
Perturbed BP is often more successful and at the same time tens to hundreds of
times more efficient than standard BP guided decimation. Perturbed BP also
compares favorably with state-of-the-art SP-guided decimation, which has a
computational complexity that generally scales exponentially worse than our
method (wrt the cardinality of variable domains and constraints). Furthermore,
our experiments with random satisfiability and coloring problems demonstrate
that Perturbed SP can outperform SP-guided decimation, making it the best
incomplete random CSP-solver in difficult regimes.
",Perturbed Message Passing for Constraint Satisfaction Problems,"Siamak Ravanbakhsh, Russell Greiner",2015,Artificial Intelligence,
"  One of the defining characteristic of human being is their ability to walk
upright. Loss or restriction of such ability whether due to the accident, spine
problem, stroke or other neurological injuries can cause tremendous stress on
the patients and hence will contribute negatively to their quality of life.
Modern research shows that physical exercise is very important for maintaining
physical fitness and adopting a healthier life style. In modern days treadmill
is widely used for physical exercises and training which enables the user to
set up an exercise regime that can be adhered to irrespective of the weather
conditions. Among the users of treadmills today are medical facilities such as
hospitals, rehabilitation centres, medical and physiotherapy clinics etc. The
process of assisted training or doing rehabilitation exercise through treadmill
is referred to as treadmill therapy. A modern treadmill is an automated machine
having built in functions and predefined features. Most of the treadmills used
today are one dimensional and user can only walk in one direction. This paper
presents the idea of using omnidirectional treadmills which will be more
appealing to the patients as they can walk in any direction, hence encouraging
them to do exercises more frequently. This paper proposes a fuzzy control
design and possible implementation strategy to assist patients in treadmill
therapy. By intelligently controlling the safety belt attached to the treadmill
user, one can help them steering left, right or in any direction. The use of
intelligent treadmill therapy can help patients to improve their walking
ability without being continuously supervised by the specialists. The patients
can walk freely within a limited space and the support system will provide
continuous evaluation of their position and can adjust the control parameters
of treadmill accordingly to provide best possible assistance.
",Fuzzy Controller Design for Assisted Omni-Directional Treadmill Therapy,"Atif Ali Khan, Oumair Naseer, Daciana Iliescu, Evor Hines",2013,Artificial Intelligence,
"  We consider the problem of finding all enclosing rectangles of minimum area
that can contain a given set of rectangles without overlap. Our rectangle
packer chooses the x-coordinates of all the rectangles before any of the
y-coordinates. We then transform the problem into a perfect-packing problem
with no empty space by adding additional rectangles. To determine the
y-coordinates, we branch on the different rectangles that can be placed in each
empty position. Our packer allows us to extend the known solutions for a
consecutive-square benchmark from 27 to 32 squares. We also introduce three new
benchmarks, avoiding properties that make a benchmark easy, such as rectangles
with shared dimensions. Our third benchmark consists of rectangles of
increasingly high precision. To pack them efficiently, we limit the rectangles
coordinates and the bounding box dimensions to the set of subset sums of the
rectangles dimensions. Overall, our algorithms represent the current
state-of-the-art for this problem, outperforming other algorithms by orders of
magnitude, depending on the benchmark.
",Optimal Rectangle Packing: An Absolute Placement Approach,"Eric Huang, Richard E. Korf",2013,Artificial Intelligence,
"  Bayesian network structure learning is the notoriously difficult problem of
discovering a Bayesian network that optimally represents a given set of
training data. In this paper we study the computational worst-case complexity
of exact Bayesian network structure learning under graph theoretic restrictions
on the (directed) super-structure. The super-structure is an undirected graph
that contains as subgraphs the skeletons of solution networks. We introduce the
directed super-structure as a natural generalization of its undirected
counterpart. Our results apply to several variants of score-based Bayesian
network structure learning where the score of a network decomposes into local
scores of its nodes. Results: We show that exact Bayesian network structure
learning can be carried out in non-uniform polynomial time if the
super-structure has bounded treewidth, and in linear time if in addition the
super-structure has bounded maximum degree. Furthermore, we show that if the
directed super-structure is acyclic, then exact Bayesian network structure
learning can be carried out in quadratic time. We complement these positive
results with a number of hardness results. We show that both restrictions
(treewidth and degree) are essential and cannot be dropped without loosing
uniform polynomial time tractability (subject to a complexity-theoretic
assumption). Similarly, exact Bayesian network structure learning remains
NP-hard for ""almost acyclic"" directed super-structures. Furthermore, we show
that the restrictions remain essential if we do not search for a globally
optimal network but aim to improve a given network by means of at most k arc
additions, arc deletions, or arc reversals (k-neighborhood local search).
","Parameterized Complexity Results for Exact Bayesian Network Structure
  Learning","Sebastian Ordyniak, Stefan Szeider",2013,Artificial Intelligence,
"  Special-purpose constraint propagation algorithms frequently make implicit
use of short supports -- by examining a subset of the variables, they can infer
support (a justification that a variable-value pair may still form part of an
assignment that satisfies the constraint) for all other variables and values
and save substantial work -- but short supports have not been studied in their
own right. The two main contributions of this paper are the identification of
short supports as important for constraint propagation, and the introduction of
HaggisGAC, an efficient and effective general purpose propagation algorithm for
exploiting short supports. Given the complexity of HaggisGAC, we present it as
an optimised version of a simpler algorithm ShortGAC. Although experiments
demonstrate the efficiency of ShortGAC compared with other general-purpose
propagation algorithms where a compact set of short supports is available, we
show theoretically and experimentally that HaggisGAC is even better. We also
find that HaggisGAC performs better than GAC-Schema on full-length supports. We
also introduce a variant algorithm HaggisGAC-Stable, which is adapted to avoid
work on backtracking and in some cases can be faster and have significant
reductions in memory use. All the proposed algorithms are excellent for
propagating disjunctions of constraints. In all experiments with disjunctions
we found our algorithms to be faster than Constructive Or and GAC-Schema by at
least an order of magnitude, and up to three orders of magnitude.
",Short and Long Supports for Constraint Propagation,"Peter Nightingale, Ian Philip Gent, Christopher Jefferson, Ian Miguel",2013,Artificial Intelligence,
"  The results in this paper add useful tools to the theory of sets of desirable
gambles, a growing toolbox for reasoning with partial probability assessments.
We investigate how to combine a number of marginal coherent sets of desirable
gambles into a joint set using the properties of epistemic irrelevance and
independence. We provide formulas for the smallest such joint, called their
independent natural extension, and study its main properties. The independent
natural extension of maximal coherent sets of desirable gambles allows us to
define the strong product of sets of desirable gambles. Finally, we explore an
easy way to generalise these results to also apply for the conditional versions
of epistemic irrelevance and independence. Having such a set of tools that are
easily implemented in computer programs is clearly beneficial to fields, like
AI, with a clear interest in coherent reasoning under uncertainty using general
and robust uncertainty models that require no full specification.
","Irrelevant and independent natural extension for sets of desirable
  gambles","Gert de Cooman, Enrique Miranda",2012,Artificial Intelligence,
"  Although the use of metric fluents is fundamental to many practical planning
problems, the study of heuristics to support fully automated planners working
with these fluents remains relatively unexplored. The most widely used
heuristic is the relaxation of metric fluents into interval-valued variables
--- an idea first proposed a decade ago. Other heuristics depend on domain
encodings that supply additional information about fluents, such as capacity
constraints or other resource-related annotations. A particular challenge to
these approaches is in handling interactions between metric fluents that
represent exchange, such as the transformation of quantities of raw materials
into quantities of processed goods, or trading of money for materials. The
usual relaxation of metric fluents is often very poor in these situations,
since it does not recognise that resources, once spent, are no longer available
to be spent again. We present a heuristic for numeric planning problems
building on the propositional relaxed planning graph, but using a mathematical
program for numeric reasoning. We define a class of producer--consumer planning
problems and demonstrate how the numeric constraints in these can be modelled
in a mixed integer program (MIP). This MIP is then combined with a metric
Relaxed Planning Graph (RPG) heuristic to produce an integrated hybrid
heuristic. The MIP tracks resource use more accurately than the usual
relaxation, but relaxes the ordering of actions, while the RPG captures the
causal propositional aspects of the problem. We discuss how these two
components interact to produce a single unified heuristic and go on to explore
how further numeric features of planning problems can be integrated into the
MIP. We show that encoding a limited subset of the propositional problem to
augment the MIP can yield more accurate guidance, partly by exploiting
structure such as propositional landmarks and propositional resources. Our
results show that the use of this heuristic enhances scalability on problems
where numeric resource interaction is key in finding a solution.
","A Hybrid LP-RPG Heuristic for Modelling Numeric Resource Flows in
  Planning","Amanda Jane Coles, Andrew Ian Coles, Maria Fox, Derek Long",2013,Artificial Intelligence,
"  Lifted probabilistic inference algorithms exploit regularities in the
structure of graphical models to perform inference more efficiently. More
specifically, they identify groups of interchangeable variables and perform
inference once per group, as opposed to once per variable. The groups are
defined by means of constraints, so the flexibility of the grouping is
determined by the expressivity of the constraint language. Existing approaches
for exact lifted inference use specific languages for (in)equality constraints,
which often have limited expressivity. In this article, we decouple lifted
inference from the constraint language. We define operators for lifted
inference in terms of relational algebra operators, so that they operate on the
semantic level (the constraints extension) rather than on the syntactic level,
making them language-independent. As a result, lifted inference can be
performed using more powerful constraint languages, which provide more
opportunities for lifting. We empirically demonstrate that this can improve
inference efficiency by orders of magnitude, allowing exact inference where
until now only approximate inference was feasible.
","Lifted Variable Elimination: Decoupling the Operators from the
  Constraint Language","Nima Taghipour, Daan Fierens, Jesse Davis, Hendrik Blockeel",2013,Artificial Intelligence,
"  This article presents the state-of-the-art in optimal solution methods for
decentralized partially observable Markov decision processes (Dec-POMDPs),
which are general models for collaborative multiagent planning under
uncertainty. Building off the generalized multiagent A* (GMAA*) algorithm,
which reduces the problem to a tree of one-shot collaborative Bayesian games
(CBGs), we describe several advances that greatly expand the range of
Dec-POMDPs that can be solved optimally. First, we introduce lossless
incremental clustering of the CBGs solved by GMAA*, which achieves exponential
speedups without sacrificing optimality. Second, we introduce incremental
expansion of nodes in the GMAA* search tree, which avoids the need to expand
all children, the number of which is in the worst case doubly exponential in
the nodes depth. This is particularly beneficial when little clustering is
possible. In addition, we introduce new hybrid heuristic representations that
are more compact and thereby enable the solution of larger Dec-POMDPs. We
provide theoretical guarantees that, when a suitable heuristic is used, both
incremental clustering and incremental expansion yield algorithms that are both
complete and search equivalent. Finally, we present extensive empirical results
demonstrating that GMAA*-ICE, an algorithm that synthesizes these advances, can
optimally solve Dec-POMDPs of unprecedented size.
","Incremental Clustering and Expansion for Faster Optimal Planning in
  Dec-POMDPs","Frans Adriaan Oliehoek, Matthijs T.J. Spaan, Christopher Amato, Shimon
  Whiteson",2013,Artificial Intelligence,
"  We present an approach to propagation-based SAT encoding of combinatorial
problems, Boolean equi-propagation, where constraints are modeled as Boolean
functions which propagate information about equalities between Boolean
literals. This information is then applied to simplify the CNF encoding of the
constraints. A key factor is that considering only a small fragment of a
constraint model at one time enables us to apply stronger, and even complete,
reasoning to detect equivalent literals in that fragment. Once detected,
equivalences apply to simplify the entire constraint model and facilitate
further reasoning on other fragments. Equi-propagation in combination with
partial evaluation and constraint simplification provide the foundation for a
powerful approach to SAT-based finite domain constraint solving. We introduce a
tool called BEE (Ben-Gurion Equi-propagation Encoder) based on these ideas and
demonstrate for a variety of benchmarks that our approach leads to a
considerable reduction in the size of CNF encodings and subsequent speed-ups in
SAT solving times.
","Boolean Equi-propagation for Concise and Efficient SAT Encodings of
  Combinatorial Problems","Amit Metodi, Michael Codish, Peter James Stuckey",2013,Artificial Intelligence,
"  Description logic Knowledge and Action Bases (KAB) are a mechanism for
providing both a semantically rich representation of the information on the
domain of interest in terms of a description logic knowledge base and actions
to change such information over time, possibly introducing new objects. We
resort to a variant of DL-Lite where the unique name assumption is not enforced
and where equality between objects may be asserted and inferred. Actions are
specified as sets of conditional effects, where conditions are based on
epistemic queries over the knowledge base (TBox and ABox), and effects are
expressed in terms of new ABoxes. In this setting, we address verification of
temporal properties expressed in a variant of first-order mu-calculus with
quantification across states. Notably, we show decidability of verification,
under a suitable restriction inspired by the notion of weak acyclicity in data
exchange.
",Description Logic Knowledge and Action Bases,"Babak Bagheri Hariri, Diego Calvanese, Marco Montali, Giuseppe De
  Giacomo, Riccardo De Masellis, Paolo Felli",2013,Artificial Intelligence,
"  Major advances in Question Answering technology were needed for IBM Watson to
play Jeopardy! at championship level -- the show requires rapid-fire answers to
challenging natural language questions, broad general knowledge, high
precision, and accurate confidence estimates. In addition, Jeopardy! features
four types of decision making carrying great strategic importance: (1) Daily
Double wagering; (2) Final Jeopardy wagering; (3) selecting the next square
when in control of the board; (4) deciding whether to attempt to answer, i.e.,
""buzz in."" Using sophisticated strategies for these decisions, that properly
account for the game state and future event probabilities, can significantly
boost a players overall chances to win, when compared with simple ""rule of
thumb"" strategies. This article presents our approach to developing Watsons
game-playing strategies, comprising development of a faithful simulation model,
and then using learning and Monte-Carlo methods within the simulator to
optimize Watsons strategic decision-making. After giving a detailed description
of each of our game-strategy algorithms, we then focus in particular on
validating the accuracy of the simulators predictions, and documenting
performance improvements using our methods. Quantitative performance benefits
are shown with respect to both simple heuristic strategies, and actual human
contestant performance in historical episodes. We further extend our analysis
of human play to derive a number of valuable and counterintuitive examples
illustrating how human contestants may improve their performance on the show.
",Analysis of Watson's Strategies for Playing Jeopardy!,"Gerald Tesauro, David C. Gondek, Jonathan Lenchner, James Fan, John M.
  Prager",2013,Artificial Intelligence,
"  Dung's abstract argumentation theory can be seen as a general framework for
non-monotonic reasoning. An important question is then: what is the class of
logics that can be subsumed as instantiations of this theory? The goal of this
paper is to identify and study the large class of logic-based instantiations of
Dung's theory which correspond to the maxi-consistent operator, i.e. to the
function which returns maximal consistent subsets of an inconsistent knowledge
base. In other words, we study the class of instantiations where very extension
of the argumentation system corresponds to exactly one maximal consistent
subset of the knowledge base. We show that an attack relation belonging to this
class must be conflict-dependent, must not be valid, must not be
conflict-complete, must not be symmetric etc. Then, we show that some attack
relations serve as lower or upper bounds of the class (e.g. if an attack
relation contains canonical undercut then it is not a member of this class). By
using our results, we show for all existing attack relations whether or not
they belong to this class. We also define new attack relations which are
members of this class. Finally, we interpret our results and discuss more
general questions, like: what is the added value of argumentation in such a
setting? We believe that this work is a first step towards achieving our
long-term goal, which is to better understand the role of argumentation and,
particularly, the expressivity of logic-based instantiations of Dung-style
argumentation frameworks.
",Identifying the Class of Maxi-Consistent Operators in Argumentation,Srdjan Vesic,2013,Artificial Intelligence,
"  In order to meet usability requirements, most logic-based applications
provide explanation facilities for reasoning services. This holds also for
Description Logics, where research has focused on the explanation of both TBox
reasoning and, more recently, query answering. Besides explaining the presence
of a tuple in a query answer, it is important to explain also why a given tuple
is missing. We address the latter problem for instance and conjunctive query
answering over DL-Lite ontologies by adopting abductive reasoning; that is, we
look for additions to the ABox that force a given tuple to be in the result. As
reasoning tasks we consider existence and recognition of an explanation, and
relevance and necessity of a given assertion for an explanation. We
characterize the computational complexity of these problems for arbitrary,
subset minimal, and cardinality minimal explanations.
",Reasoning about Explanations for Negative Query Answers in DL-Lite,"Diego Calvanese, Magdalena Ortiz, Mantas Simkus, Giorgio Stefanoni",2013,Artificial Intelligence,
"  This paper presents a model-based planner called the Probabilistic Sulu
Planner or the p-Sulu Planner, which controls stochastic systems in a goal
directed manner within user-specified risk bounds. The objective of the p-Sulu
Planner is to allow users to command continuous, stochastic systems, such as
unmanned aerial and space vehicles, in a manner that is both intuitive and
safe. To this end, we first develop a new plan representation called a
chance-constrained qualitative state plan (CCQSP), through which users can
specify the desired evolution of the plant state as well as the acceptable
level of risk. An example of a CCQSP statement is go to A through B within 30
minutes, with less than 0.001% probability of failure."" We then develop the
p-Sulu Planner, which can tractably solve a CCQSP planning problem. In order to
enable CCQSP planning, we develop the following two capabilities in this paper:
1) risk-sensitive planning with risk bounds, and 2) goal-directed planning in a
continuous domain with temporal constraints. The first capability is to ensures
that the probability of failure is bounded. The second capability is essential
for the planner to solve problems with a continuous state space such as vehicle
path planning. We demonstrate the capabilities of the p-Sulu Planner by
simulations on two real-world scenarios: the path planning and scheduling of a
personal aerial vehicle as well as the space rendezvous of an autonomous cargo
spacecraft.
",Probabilistic Planning for Continuous Dynamic Systems under Bounded Risk,"Masahiro Ono, Brian C. Williams, L. Blackmore",2013,Artificial Intelligence,
"  This paper presents a structured power and energy-flow-based qualitative
modelling approach that is applicable to a variety of system types including
electrical and fluid flow. The modelling is split into two parts. Power flow is
a global phenomenon and is therefore naturally represented and analysed by a
network comprised of the relevant structural elements from the components of a
system. The power flow analysis is a platform for higher-level behaviour
prediction of energy related aspects using local component behaviour models to
capture a state-based representation with a global time. The primary
application is Failure Modes and Effects Analysis (FMEA) and a form of
exaggeration reasoning is used, combined with an order of magnitude
representation to derive the worst case failure modes. The novel aspects of the
work are an order of magnitude(OM) qualitative network analyser to represent
any power domain and topology, including multiple power sources, a feature that
was not required for earlier specialised electrical versions of the approach.
Secondly, the representation of generalised energy related behaviour as
state-based local models is presented as a modelling strategy that can be more
vivid and intuitive for a range of topologically complex applications than
qualitative equation-based representations.The two-level modelling strategy
allows the broad system behaviour coverage of qualitative simulation to be
exploited for the FMEA task, while limiting the difficulties of qualitative
ambiguity explanation that can arise from abstracted numerical models. We have
used the method to support an automated FMEA system with examples of an
aircraft fuel system and domestic a heating system discussed in this paper.
","Qualitative Order of Magnitude Energy-Flow-Based Failure Modes and
  Effects Analysis","Neal Andrew Snooke, Mark H Lee",2013,Artificial Intelligence,
"  We address a dynamic repair shop scheduling problem in the context of
military aircraft fleet management where the goal is to maintain a full
complement of aircraft over the long-term. A number of flights, each with a
requirement for a specific number and type of aircraft, are already scheduled
over a long horizon. We need to assign aircraft to flights and schedule repair
activities while considering the flights requirements, repair capacity, and
aircraft failures. The number of aircraft awaiting repair dynamically changes
over time due to failures and it is therefore necessary to rebuild the repair
schedule online. To solve the problem, we view the dynamic repair shop as
successive static repair scheduling sub-problems over shorter time periods. We
propose a complete approach based on the logic-based Benders decomposition to
solve the static sub-problems, and design different rescheduling policies to
schedule the dynamic repair shop. Computational experiments demonstrate that
the Benders model is able to find and prove optimal solutions on average four
times faster than a mixed integer programming model. The rescheduling approach
having both aspects of scheduling over a longer horizon and quickly adjusting
the schedule increases aircraft available in the long term by 10% compared to
the approaches having either one of the aspects alone.
",Scheduling a Dynamic Aircraft Repair Shop with Limited Repair Resources,"Maliheh Aramon Bajestani, J. Christopher Beck",2013,Artificial Intelligence,
"  The Minimum Vertex Cover (MVC) problem is a prominent NP-hard combinatorial
optimization problem of great importance in both theory and application. Local
search has proved successful for this problem. However, there are two main
drawbacks in state-of-the-art MVC local search algorithms. First, they select a
pair of vertices to exchange simultaneously, which is time-consuming. Secondly,
although using edge weighting techniques to diversify the search, these
algorithms lack mechanisms for decreasing the weights. To address these issues,
we propose two new strategies: two-stage exchange and edge weighting with
forgetting. The two-stage exchange strategy selects two vertices to exchange
separately and performs the exchange in two stages. The strategy of edge
weighting with forgetting not only increases weights of uncovered edges, but
also decreases some weights for each edge periodically. These two strategies
are used in designing a new MVC local search algorithm, which is referred to as
NuMVC. We conduct extensive experimental studies on the standard benchmarks,
namely DIMACS and BHOSLIB. The experiment comparing NuMVC with state-of-the-art
heuristic algorithms show that NuMVC is at least competitive with the nearest
competitor namely PLS on the DIMACS benchmark, and clearly dominates all
competitors on the BHOSLIB benchmark. Also, experimental results indicate that
NuMVC finds an optimal solution much faster than the current best exact
algorithm for Maximum Clique on random instances as well as some structured
ones. Moreover, we study the effectiveness of the two strategies and the
run-time behaviour through experimental analysis.
",NuMVC: An Efficient Local Search Algorithm for Minimum Vertex Cover,"Shaowei Cai, Kaile Su, Chuan Luo, Abdul Sattar",2013,Artificial Intelligence,
"  Algorithmic composition is the partial or total automation of the process of
music composition by using computers. Since the 1950s, different computational
techniques related to Artificial Intelligence have been used for algorithmic
composition, including grammatical representations, probabilistic methods,
neural networks, symbolic rule-based systems, constraint programming and
evolutionary algorithms. This survey aims to be a comprehensive account of
research on algorithmic composition, presenting a thorough view of the field
for researchers in Artificial Intelligence.
",AI Methods in Algorithmic Composition: A Comprehensive Survey,"Jose David Fernandez, Francisco Vico",2013,Artificial Intelligence,
"  Distributed Constraint Optimization (DCOP) is a powerful framework for
representing and solving distributed combinatorial problems, where the
variables of the problem are owned by different agents. Many multi-agent
problems include constraints that produce different gains (or costs) for the
participating agents. Asymmetric gains of constrained agents cannot be
naturally represented by the standard DCOP model. The present paper proposes a
general framework for Asymmetric DCOPs (ADCOPs). In ADCOPs different agents may
have different valuations for constraints that they are involved in. The new
framework bridges the gap between multi-agent problems which tend to have
asymmetric structure and the standard symmetric DCOP model. The benefits of the
proposed model over previous attempts to generalize the DCOP model are
discussed and evaluated. Innovative algorithms that apply to the special
properties of the proposed ADCOP model are presented in detail. These include
complete algorithms that have a substantial advantage in terms of runtime and
network load over existing algorithms (for standard DCOPs) which use
alternative representations. Moreover, standard incomplete algorithms (i.e.,
local search algorithms) are inapplicable to the existing DCOP representations
of asymmetric constraints and when they are applied to the new ADCOP framework
they often fail to converge to a local optimum and yield poor results. The
local search algorithms proposed in the present paper converge to high quality
solutions. The experimental evidence that is presented reveals that the
proposed local search algorithms for ADCOPs achieve high quality solutions
while preserving a high level of privacy.
",Asymmetric Distributed Constraint Optimization Problems,"Tal Grinshpoun, Alon Grubshtein, Roie Zivan, Arnon Netzer, Amnon
  Meisels",2013,Artificial Intelligence,
"  The causal graph of a planning instance is an important tool for planning
both in practice and in theory. The theoretical studies of causal graphs have
largely analysed the computational complexity of planning for instances where
the causal graph has a certain structure, often in combination with other
parameters like the domain size of the variables. Chen and Gimand#233;nez
ignored even the structure and considered only the size of the weakly connected
components. They proved that planning is tractable if the components are
bounded by a constant and otherwise intractable. Their intractability result
was, however, conditioned by an assumption from parameterised complexity theory
that has no known useful relationship with the standard complexity classes. We
approach the same problem from the perspective of standard complexity classes,
and prove that planning is NP-hard for classes with unbounded components under
an additional restriction we refer to as SP-closed. We then argue that most
NP-hardness theorems for causal graphs are difficult to apply and, thus, prove
a more general result; even if the component sizes grow slowly and the class is
not densely populated with graphs, planning still cannot be tractable unless
the polynomial hierachy collapses. Both these results still hold when
restricted to the class of acyclic causal graphs. We finally give a partial
characterization of the borderline between NP-hard and NP-intermediate classes,
giving further insight into the problem.
","A Refined View of Causal Graphs and Component Sizes: SP-Closed Graph
  Classes and Beyond","Christer B\""ackstr\""om, Peter Jonsson",2013,Artificial Intelligence,
"  As large-scale theft of data from corporate servers is becoming increasingly
common, it becomes interesting to examine alternatives to the paradigm of
centralizing sensitive data into large databases. Instead, one could use
cryptography and distributed computation so that sensitive data can be supplied
and processed in encrypted form, and only the final result is made known. In
this paper, we examine how such a paradigm can be used to implement constraint
satisfaction, a technique that can solve a broad class of AI problems such as
resource allocation, planning, scheduling, and diagnosis. Most previous work on
privacy in constraint satisfaction only attempted to protect specific types of
information, in particular the feasibility of particular combinations of
decisions. We formalize and extend these restricted notions of privacy by
introducing four types of private information, including the feasibility of
decisions and the final decisions made, but also the identities of the
participants and the topology of the problem. We present distributed algorithms
that allow computing solutions to constraint satisfaction problems while
maintaining these four types of privacy. We formally prove the privacy
properties of these algorithms, and show experiments that compare their
respective performance on benchmark problems.
","Protecting Privacy through Distributed Computation in Multi-agent
  Decision Making","Thomas Leaute, Boi Faltings",2013,Artificial Intelligence,
"  Sequential decision-making problems with multiple objectives arise naturally
in practice and pose unique challenges for research in decision-theoretic
planning and learning, which has largely focused on single-objective settings.
This article surveys algorithms designed for sequential decision-making
problems with multiple objectives. Though there is a growing body of literature
on this subject, little of it makes explicit under what circumstances special
methods are needed to solve multi-objective problems. Therefore, we identify
three distinct scenarios in which converting such a problem to a
single-objective one is impossible, infeasible, or undesirable. Furthermore, we
propose a taxonomy that classifies multi-objective methods according to the
applicable scenario, the nature of the scalarization function (which projects
multi-objective values to scalar ones), and the type of policies considered. We
show how these factors determine the nature of an optimal solution, which can
be a single policy, a convex hull, or a Pareto front. Using this taxonomy, we
survey the literature on multi-objective methods for planning and learning.
Finally, we discuss key applications of such methods and outline opportunities
for future work.
",A Survey of Multi-Objective Sequential Decision-Making,"Diederik Marijn Roijers, Peter Vamplew, Shimon Whiteson, Richard
  Dazeley",2013,Artificial Intelligence,
"  Learning by observation can be of key importance whenever agents sharing
similar features want to learn from each other. This paper presents an agent
architecture that enables software agents to learn by direct observation of the
actions executed by expert agents while they are performing a task. This is
possible because the proposed architecture displays information that is
essential for observation, making it possible for software agents to observe
each other. The agent architecture supports a learning process that covers all
aspects of learning by observation, such as discovering and observing experts,
learning from the observed data, applying the acquired knowledge and evaluating
the agents progress. The evaluation provides control over the decision to
obtain new knowledge or apply the acquired knowledge to new problems. We
combine two methods for learning from the observed information. The first one,
the recall method, uses the sequence on which the actions were observed to
solve new problems. The second one, the classification method, categorizes the
information in the observed data and determines to which set of categories the
new problems belong. Results show that agents are able to learn in conditions
where common supervised learning algorithms fail, such as when agents do not
know the results of their actions a priori or when not all the effects of the
actions are visible. The results also show that our approach provides better
results than other learning methods since it requires shorter learning periods.
",Learning by Observation of Agent Software Images,"Paulo Roberto Costa, Lu\'is Miguel Botelho",2013,Artificial Intelligence,
"  We introduce statistical constraints, a declarative modelling tool that links
statistics and constraint programming. We discuss two statistical constraints
and some associated filtering algorithms. Finally, we illustrate applications
to standard problems encountered in statistics and to a novel inspection
scheduling problem in which the aim is to find inspection plans with desirable
statistical properties.
",Statistical Constraints,Roberto Rossi and Steven Prestwich and S. Armagan Tarim,2014,Artificial Intelligence,
"  This article deals with the links between the enaction paradigm and
artificial intelligence. Enaction is considered a metaphor for artificial
intelligence, as a number of the notions which it deals with are deemed
incompatible with the phenomenal field of the virtual. After explaining this
stance, we shall review previous works regarding this issue in terms of
artifical life and robotics. We shall focus on the lack of recognition of
co-evolution at the heart of these approaches. We propose to explicitly
integrate the evolution of the environment into our approach in order to refine
the ontogenesis of the artificial system, and to compare it with the enaction
paradigm. The growing complexity of the ontogenetic mechanisms to be activated
can therefore be compensated by an interactive guidance system emanating from
the environment. This proposition does not however resolve that of the
relevance of the meaning created by the machine (sense-making). Such
reflections lead us to integrate human interaction into this environment in
order to construct relevant meaning in terms of participative artificial
intelligence. This raises a number of questions with regards to setting up an
enactive interaction. The article concludes by exploring a number of issues,
thereby enabling us to associate current approaches with the principles of
morphogenesis, guidance, the phenomenology of interactions and the use of
minimal enactive interfaces in setting up experiments which will deal with the
problem of artificial intelligence in a variety of enaction-based ways.
","Enaction-Based Artificial Intelligence: Toward Coevolution with Humans
  in the Loop","Pierre De Loor, Kristen Manach and Jacques Tisseau",2009,Artificial Intelligence,
"  Forward inference techniques such as sequential Monte Carlo and particle
Markov chain Monte Carlo for probabilistic programming can be implemented in
any programming language by creative use of standardized operating system
functionality including processes, forking, mutexes, and shared memory.
Exploiting this we have defined, developed, and tested a probabilistic
programming language intermediate representation language we call probabilistic
C, which itself can be compiled to machine code by standard compilers and
linked to operating system libraries yielding an efficient, scalable, portable
probabilistic programming compilation target. This opens up a new hardware and
systems research path for optimizing probabilistic programming systems.
",A Compilation Target for Probabilistic Programming Languages,Brooks Paige and Frank Wood,2014,Artificial Intelligence,
"  Currently there are lots of plagiarism detection approaches. But few of them
implemented and adapted for Persian languages. In this paper, our work on
designing and implementation of a plagiarism detection system based on
pre-processing and NLP technics will be described. And the results of testing
on a corpus will be presented.
",Design a Persian Automated Plagiarism Detector (AMZPPD),"Maryam Mahmoodi, Mohammad Mahmoodi Varnamkhasti",2014,Artificial Intelligence,
"  In the dawn of computer science and the eve of neuroscience we participate in
rebirth of neuroscience due to new technology that allows us to deeply and
precisely explore whole new world that dwells in our brains.
","Cortex simulation system proposal using distributed computer network
  environments",Boris Tomas,2014,Artificial Intelligence,
"  The fundamental step in measuring the robustness of a system is the synthesis
of the so called Process Map.This is generally based on the user raw data
material.Process Maps are of fundamental importance towards the understanding
of the nature of a system in that they indicate which variables are causally
related and which are particularly important.This paper represent the system
Map or business structure map to understand business criteria studying the
various aspects of the company.The business structure map or knowledge map or
Process map are used to increase the growth of the company by giving some
useful measures according to the business criteria.This paper also deals with
the different company strategy to reduce the risk factors.Process Map is
helpful for building such knowledge successfully.Making decisions from such map
in a highly complex situation requires more knowledge and resources.
",A Mining Method to Create Knowledge Map by Analysing the Data Resource,"Arti Gupta, Prof. N.T Deotale",2014,Artificial Intelligence,
"  Since long, corporations are looking for knowledge sources which can provide
structured description of data and can focus on meaning and shared
understanding. Structures which can facilitate open world assumptions and can
be flexible enough to incorporate and recognize more than one name for an
entity. A source whose major purpose is to facilitate human communication and
interoperability. Clearly, databases fail to provide these features and
ontologies have emerged as an alternative choice, but corporations working on
same domain tend to make different ontologies. The problem occurs when they
want to share their data/knowledge. Thus we need tools to merge ontologies into
one. This task is termed as ontology matching. This is an emerging area and
still we have to go a long way in having an ideal matcher which can produce
good results. In this paper we have shown a framework to matching ontologies
using graphs.
",Shiva: A Framework for Graph Based Ontology Matching,"Iti Mathur, Nisheeth Joshi, Hemant Darbari and Ajai Kumar",2014,Artificial Intelligence,
"  We propose a number of heuristics that can be used for identifying when
intransitive choice behaviour is likely to occur in choice situations. We also
suggest two methods for avoiding undesired choice behaviour, namely transparent
communication and adaptive choice-set generation. We believe that these two
ways can contribute to the avoidance of decision biases in choice situations
that may often be regretted.
",Avoiding Undesired Choices Using Intelligent Adaptive Systems,Amir Konigsberg,2014,Artificial Intelligence,
"  With the web getting bigger and assimilating knowledge about different
concepts and domains, it is becoming very difficult for simple database driven
applications to capture the data for a domain. Thus developers have come out
with ontology based systems which can store large amount of information and can
apply reasoning and produce timely information. Thus facilitating effective
knowledge management. Though this approach has made our lives easier, but at
the same time has given rise to another problem. Two different ontologies
assimilating same knowledge tend to use different terms for the same concepts.
This creates confusion among knowledge engineers and workers, as they do not
know which is a better term then the other. Thus we need to merge ontologies
working on same domain so that the engineers can develop a better application
over it. This paper shows the development of one such matcher which merges the
concepts available in two ontologies at two levels; 1) at string level and 2)
at semantic level; thus producing better merged ontologies. We have used a
graph matching technique which works at the core of the system. We have also
evaluated the system and have tested its performance with its predecessor which
works only on string matching. Thus current approach produces better results.
",Shiva++: An Enhanced Graph based Ontology Matcher,"Iti Mathur, Nisheeth Joshi, Hemant Darbari and Ajai Kumar",2014,Artificial Intelligence,
"  Dialogical argumentation is an important cognitive activity by which agents
exchange arguments and counterarguments as part of some process such as
discussion, debate, persuasion and negotiation. Whilst numerous formal systems
have been proposed, there is a lack of frameworks for implementing and
evaluating these proposals. First-order executable logic has been proposed as a
general framework for specifying and analysing dialogical argumentation. In
this paper, we investigate how we can implement systems for dialogical
argumentation using propositional executable logic. Our approach is to present
and evaluate an algorithm that generates a finite state machine that reflects a
propositional executable logic specification for a dialogical argumentation
together with an initial state. We also consider how the finite state machines
can be analysed, with the minimax strategy being used as an illustration of the
kinds of empirical analysis that can be undertaken.
",Analysis of Dialogical Argumentation via Finite State Machines,Anthony Hunter,2013,Artificial Intelligence,
"  The fundamental issue in knowledge representation is to provide a precise
definition of the knowledge that they possess in a manner that is independent
of procedural considerations, context free and easy to manipulate, exchange and
reason about. Knowledge must be accessible to everyone regardless of their
native languages. Universal Networking Language (UNL) is a declarative formal
language and a generalized form of human language in a machine independent
digital platform for defining, recapitulating, amending, storing and
dissipating knowledge among people of different affiliations. UNL extracts
semantic data from a native language for Interlingua machine translation. This
paper presents the development of a graphical tool that incorporates UNL to
provide a visual mean to represent the semantic data available in a native
text. UNL represents the semantics of a sentence as a conceptual hyper-graph.
We translate this information into XML format and create a graph from XML,
representing the actual concepts available in the native language
",Analysis Tool for UNL-Based Knowledge Representation,"Shamim Ripon, Aoyan Barua, and Mohammad Salah Uddin",2012,Artificial Intelligence,
"  Artificial fish swarm algorithm (AFSA) is one of the swarm intelligence
optimization algorithms that works based on population and stochastic search.
In order to achieve acceptable result, there are many parameters needs to be
adjusted in AFSA. Among these parameters, visual and step are very significant
in view of the fact that artificial fish basically move based on these
parameters. In standard AFSA, these two parameters remain constant until the
algorithm termination. Large values of these parameters increase the capability
of algorithm in global search, while small values improve the local search
ability of the algorithm. In this paper, we empirically study the performance
of the AFSA and different approaches to balance between local and global
exploration have been tested based on the adaptive modification of visual and
step during algorithm execution. The proposed approaches have been evaluated
based on the four well-known benchmark functions. Experimental results show
considerable positive impact on the performance of AFSA.
",Empirical Study of Artificial Fish Swarm Algorithm,Reza Azizi,2014,Artificial Intelligence,
"  Analogy-Based (or Analogical) and Case-Based Reasoning (ABR and CBR) are two
similar problem solving processes based on the adaptation of the solution of
past problems for use with a new analogous problem. In this paper we review
these two processes and we give some real world examples with emphasis to the
field of Medicine, where one can find some of the most common and useful CBR
applications. We also underline the differences between CBR and the classical
rule-induction algorithms, we discuss the criticism for CBR methods and we
focus on the future trends of research in the area of CBR.
",Analogy-Based and Case-Based Reasoning: Two sides of the same coin,"Michael Gr. Voskoglou, Abdel-Badeeh M. Salem",2014,Artificial Intelligence,
"  We firstly present definitions and properties in study of Maji
\cite{maji-2013} on neutrosophic soft sets. We then give a few notes on his
study. Next, based on \c{C}a\u{g}man \cite{cagman-2014}, we redefine the notion
of neutrosophic soft set and neutrosophic soft set operations to make more
functional. By using these new definitions we construct a decision making
method and a group decision making method which selects a set of optimum
elements from the alternatives. We finally present examples which shows that
the methods can be successfully applied to many problems that contain
uncertainties.
",Neutrosophic soft sets with applications in decision making,Faruk Karaaslan,2015,Artificial Intelligence,
"  This paper introduces a self-organizing traffic signal system for an urban
road network. The key elements of this system are agents that control traffic
signals at intersections. Each agent uses an interval microscopic traffic model
to predict effects of its possible control actions in a short time horizon. The
executed control action is selected on the basis of predicted delay intervals.
Since the prediction results are represented by intervals, the agents can
recognize and suspend those control actions, whose positive effect on the
performance of traffic control is uncertain. Evaluation of the proposed traffic
control system was performed in a simulation environment. The simulation
experiments have shown that the proposed approach results in an improved
performance, particularly for non-uniform traffic streams.
","A self-organizing system for urban traffic control based on predictive
  interval microscopic model",Bartlomiej Placzek,2014,Artificial Intelligence,
"  Neutrosophic Statistics means statistical analysis of population or sample
that has indeterminate (imprecise, ambiguous, vague, incomplete, unknown) data.
For example, the population or sample size might not be exactly determinate
because of some individuals that partially belong to the population or sample,
and partially they do not belong, or individuals whose appurtenance is
completely unknown. Also, there are population or sample individuals whose data
could be indeterminate. In this book, we develop the 1995 notion of
neutrosophic statistics. We present various practical examples. It is possible
to define the neutrosophic statistics in many ways, because there are various
types of indeterminacies, depending on the problem to solve.
",Introduction to Neutrosophic Statistics,Florentin Smarandache,2014,Artificial Intelligence,
"  We present quantum observable Markov decision processes (QOMDPs), the quantum
analogues of partially observable Markov decision processes (POMDPs). In a
QOMDP, an agent's state is represented as a quantum state and the agent can
choose a superoperator to apply. This is similar to the POMDP belief state,
which is a probability distribution over world states and evolves via a
stochastic matrix. We show that the existence of a policy of at least a certain
value has the same complexity for QOMDPs and POMDPs in the polynomial and
infinite horizon cases. However, we also prove that the existence of a policy
that can reach a goal state is decidable for goal POMDPs and undecidable for
goal QOMDPs.
",Quantum POMDPs,Jennifer Barry and Daniel T. Barry and Scott Aaronson,2014,Artificial Intelligence,
"  Failure detection in telecommunication networks is a vital task. So far,
several supervised and unsupervised solutions have been provided for
discovering failures in such networks. Among them unsupervised approaches has
attracted more attention since no label data is required. Often, network
devices are not able to provide information about the type of failure. In such
cases the type of failure is not known in advance and the unsupervised setting
is more appropriate for diagnosis. Among unsupervised approaches, Principal
Component Analysis (PCA) is a well-known solution which has been widely used in
the anomaly detection literature and can be applied to matrix data (e.g.
Users-Features). However, one of the important properties of network data is
their temporal sequential nature. So considering the interaction of dimensions
over a third dimension, such as time, may provide us better insights into the
nature of network failures. In this paper we demonstrate the power of three-way
analysis to detect events and anomalies in time-evolving network data.
",Event and Anomaly Detection Using Tucker3 Decomposition,"Hadi Fanaee-T and M\'arcia D. B. Oliveira and Jo\~ao Gama and Simon
  Malinowski and Ricardo Morla",2012,Artificial Intelligence,
"  In this paper we present a short history of logics: from particular cases of
2-symbol or numerical valued logic to the general case of n-symbol or numerical
valued logic. We show generalizations of 2-valued Boolean logic to fuzzy logic,
also from the Kleene and Lukasiewicz 3-symbol valued logics or Belnap 4-symbol
valued logic to the most general n-symbol or numerical valued refined
neutrosophic logic. Two classes of neutrosophic norm (n-norm) and neutrosophic
conorm (n-conorm) are defined. Examples of applications of neutrosophic logic
to physics are listed in the last section. Similar generalizations can be done
for n-Valued Refined Neutrosophic Set, and respectively n- Valued Refined
Neutrosopjhic Probability.
",n-Valued Refined Neutrosophic Logic and Its Applications to Physics,Florentin Smarandache,2013,Artificial Intelligence,
"  This paper develops a Reasoning about Actions and Change framework integrated
with Default Reasoning, suitable as a Knowledge Representation and Reasoning
framework for Story Comprehension. The proposed framework, which is guided
strongly by existing knowhow from the Psychology of Reading and Comprehension,
is based on the theory of argumentation from AI. It uses argumentation to
capture appropriate solutions to the frame, ramification and qualification
problems and generalizations of these problems required for text comprehension.
In this first part of the study the work concentrates on the central problem of
integration (or elaboration) of the explicit information from the narrative in
the text with the implicit (in the readers mind) common sense world knowledge
pertaining to the topic(s) of the story given in the text. We also report on
our empirical efforts to gather background common sense world knowledge used by
humans when reading a story and to evaluate, through a prototype system, the
ability of our approach to capture both the majority and the variability of
understanding of a story by the human readers in the experiments.
",Non-Monotonic Reasoning and Story Comprehension,"Irene-Anna Diakidoy, Antonis Kakas, Loizos Michael and Rob Miller",2014,Artificial Intelligence,
"  For the last few decades, optimization has been developing at a fast rate.
Bio-inspired optimization algorithms are metaheuristics inspired by nature.
These algorithms have been applied to solve different problems in engineering,
economics, and other domains. Bio-inspired algorithms have also been applied in
different branches of information technology such as networking and software
engineering. Time series data mining is a field of information technology that
has its share of these applications too. In previous works we showed how
bio-inspired algorithms such as the genetic algorithms and differential
evolution can be used to find the locations of the breakpoints used in the
symbolic aggregate approximation of time series representation, and in another
work we showed how we can utilize the particle swarm optimization, one of the
famous bio-inspired algorithms, to set weights to the different segments in the
symbolic aggregate approximation representation. In this paper we present, in
two different approaches, a new meta optimization process that produces optimal
locations of the breakpoints in addition to optimal weights of the segments.
The experiments of time series classification task that we conducted show an
interesting example of how the overfitting phenomenon, a frequently encountered
problem in data mining which happens when the model overfits the training set,
can interfere in the optimization process and hide the superior performance of
an optimization algorithm.
","One-Step or Two-Step Optimization and the Overfitting Phenomenon: A Case
  Study on Time Series Classification",Muhammad Marwan Muhammad Fuad,2014,Artificial Intelligence,
"  Quadratic Assignment Problem (QAP) is an NP-hard combinatorial optimization
problem, therefore, solving the QAP requires applying one or more of the
meta-heuristic algorithms. This paper presents a comparative study between
Meta-heuristic algorithms: Genetic Algorithm, Tabu Search, and Simulated
annealing for solving a real-life (QAP) and analyze their performance in terms
of both runtime efficiency and solution quality. The results show that Genetic
Algorithm has a better solution quality while Tabu Search has a faster
execution time in comparison with other Meta-heuristic algorithms for solving
QAP.
","A Comparative Study of Meta-heuristic Algorithms for Solving Quadratic
  Assignment Problem","Gamal Abd El-Nasser A. Said, Abeer M. Mahmoud and El-Sayed M.
  El-Horbaty",2014,Artificial Intelligence,
"  Conventional urban traffic control systems have been based on historical
traffic data. Later advancements made use of detectors, which enabled the
gathering of real time traffic data, in order to reorganize and calibrate
traffic signalization programs. Further evolvement provided the ability to
forecast traffic conditions, in order to develop traffic signalization programs
and strategies precomputed and applied at the most appropriate time frame for
the optimal control of the current traffic conditions. We, propose the next
generation of traffic control systems based on principles of Artificial
Intelligence and Context Awareness. Most of the existing algorithms use average
waiting time or length of the queue to assess an algorithms performance.
However, a low average waiting time may come at the cost of delaying other
vehicles indefinitely. In our algorithm, besides the vehicle queue, we use
fairness also as an important performance metric to assess an algorithms
performance.
",Context Aware Dynamic Traffic Signal Optimization,"Kandarp Khandwala, Rudra Sharma, Snehal Rao",2014,Artificial Intelligence,
"  This paper proposes an analysis of the effects of consensus and preference
aggregation on the consistency of pairwise comparisons. We define some boundary
properties for the inconsistency of group preferences and investigate their
relation with different inconsistency indices. Some results are presented on
more general dependencies between properties of inconsistency indices and the
satisfaction of boundary properties. In the end, given three boundary
properties and nine indices among the most relevant ones, we will be able to
present a complete analysis of what indices satisfy what properties and offer a
reflection on the interpretation of the inconsistency of group preferences.
","Boundary properties of the inconsistency of pairwise comparisons in
  group decisions",Matteo Brunelli and Michele Fedrizzi,2015,Artificial Intelligence,
"  Decision trees have been widely used in machine learning. However, due to
some reasons, data collecting in real world contains a fuzzy and uncertain
form. The decision tree should be able to handle such fuzzy data. This paper
presents a method to construct fuzzy decision tree. It proposes a fuzzy
decision tree induction method in iris flower data set, obtaining the entropy
from the distance between an average value and a particular value. It also
presents an experiment result that shows the accuracy compared to former ID3.
",The New Approach on Fuzzy Decision Trees,"Jooyeol Yun, Jun won Seo, and Taeseon Yoon",2014,Artificial Intelligence,
"  This paper describes a relatively simple way of allowing a brain model to
self-organise its concept patterns through nested structures. For a simulation,
time reduction is helpful and it would be able to show how patterns may form
and then fire in sequence, as part of a search or thought process. It uses a
very simple equation to show how the inhibitors in particular, can switch off
certain areas, to allow other areas to become the prominent ones and thereby
define the current brain state. This allows for a small amount of control over
what appears to be a chaotic structure inside of the brain. It is attractive
because it is still mostly mechanical and therefore can be added as an
automatic process, or the modelling of that. The paper also describes how the
nested pattern structure can be used as a basic counting mechanism. Another
mathematical conclusion provides a basis for maintaining memory or concept
patterns. The self-organisation can space itself through automatic processes.
This might allow new neurons to be added in a more even manner and could help
to maintain the concept integrity. The process might also help with finding
memory structures afterwards. This extended version integrates further with the
existing cognitive model and provides some new conclusions.
",New Ideas for Brain Modelling 2,Kieran Greer,2015,Artificial Intelligence,
"  Many diseases cause significant changes to the concentrations of small
molecules (aka metabolites) that appear in a person's biofluids, which means
such diseases can often be readily detected from a person's ""metabolic
profile"". This information can be extracted from a biofluid's NMR spectrum.
Today, this is often done manually by trained human experts, which means this
process is relatively slow, expensive and error-prone. This paper presents a
tool, Bayesil, that can quickly, accurately and autonomously produce a complex
biofluid's (e.g., serum or CSF) metabolic profile from a 1D1H NMR spectrum.
This requires first performing several spectral processing steps then matching
the resulting spectrum against a reference compound library, which contains the
""signatures"" of each relevant metabolite. Many of these steps are novel
algorithms and our matching step views spectral matching as an inference
problem within a probabilistic graphical model that rapidly approximates the
most probable metabolic profile. Our extensive studies on a diverse set of
complex mixtures, show that Bayesil can autonomously find the concentration of
all NMR-detectable metabolites accurately (~90% correct identification and ~10%
quantification error), in <5minutes on a single CPU. These results demonstrate
that Bayesil is the first fully-automatic publicly-accessible system that
provides quantitative NMR spectral profiling effectively -- with an accuracy
that meets or exceeds the performance of trained experts. We anticipate this
tool will usher in high-throughput metabolomics and enable a wealth of new
applications of NMR in clinical settings. Available at http://www.bayesil.ca.
","Accurate, fully-automated NMR spectral profiling for metabolomics","Siamak Ravanbakhsh, Philip Liu, Trent Bjorndahl, Rupasri Mandal, Jason
  R. Grant, Michael Wilson, Roman Eisner, Igor Sinelnikov, Xiaoyu Hu, Claudio
  Luchinat, Russell Greiner and David S. Wishart",2015,Artificial Intelligence,
"  This article is about how the ""SP theory of intelligence"" and its realisation
in the ""SP machine"" (both outlined in the article) may help to solve
computer-related problems in the design of autonomous robots, meaning robots
that do not depend on external intelligence or power supplies, are mobile, and
are designed to exhibit as much human-like intelligence as possible. The
article is about: how to increase the computational and energy efficiency of
computers and reduce their bulk; how to achieve human-like versatility in
intelligence; and likewise for human-like adaptability in intelligence. The SP
system has potential for substantial gains in computational and energy
efficiency and reductions in the bulkiness of computers: by reducing the size
of data to be processed; by exploiting statistical information that the system
gathers; and via an updated version of Donald Hebb's concept of a ""cell
assembly"". Towards human-like versatility in intelligence, the SP system has
strengths in unsupervised learning, natural language processing, pattern
recognition, information retrieval, several kinds of reasoning, planning,
problem solving, and more, with seamless integration amongst structures and
functions. The SP system's strengths in unsupervised learning and other aspects
of intelligence may help to achieve human-like adaptability in intelligence
via: the learning of natural language; learning to see; building 3D models of
objects and of a robot's surroundings; learning regularities in the workings of
a robot and in the robot's environment; exploration and play; learning major
skills; and secondary forms of learning. Also discussed are: how the SP system
may process parallel streams of information; generalisation of knowledge,
correction of over-generalisations, and learning from dirty data; how to cut
the cost of learning; and reinforcements, motivations, goals, and
demonstration.
",Autonomous robots and the SP theory of intelligence,J. Gerard Wolff,2014,Artificial Intelligence,
"  This paper describes a novel approach to medical diagnosis based on the SP
theory of computing and cognition. The main attractions of this approach are: a
format for representing diseases that is simple and intuitive; an ability to
cope with errors and uncertainties in diagnostic information; the simplicity of
storing statistical information as frequencies of occurrence of diseases; a
method for evaluating alternative diagnostic hypotheses that yields true
probabilities; and a framework that should facilitate unsupervised learning of
medical knowledge and the integration of medical diagnosis with other AI
applications.
","Medical diagnosis as pattern recognition in a framework of information
  compression by multiple alignment, unification and search",J. Gerard Wolff,2006,Artificial Intelligence,
"  Rough sets over generalized transitive relations like proto-transitive ones
had been initiated by the present author in the year 2012. Subsequently,
approximation of proto-transitive relations by other relations was investigated
and the relation with rough approximations was developed towards constructing
semantics that can handle fragments of structure. It was also proved that
difference of approximations induced by some approximate relations need not
induce rough structures. In this research we develop different semantics of
proto transitive rough sets (PRAX) after characterizing the structure of rough
objects and also develop a theory of dependence for general rough sets and use
it to internalize the Nelson-algebra based approximate semantics developed
earlier. The theory of rough dependence initiated later by the present author
is extended in the process. This monograph is reasonably self-contained and
includes proofs and extensions of representation of objects that were not part
of earlier papers.
",Algebraic Semantics of Proto-Transitive Rough Sets,A. Mani,2016,Artificial Intelligence,
"  Nearly invariably, phenotypes are reported in the scientific literature in
meticulous detail, utilizing the full expressivity of natural language. Often
it is particularly these detailed observations (facts) that are of interest,
and thus specific to the research questions that motivated observing and
reporting them. However, research aiming to synthesize or integrate phenotype
data across many studies or even fields is often faced with the need to
abstract from detailed observations so as to construct phenotypic concepts that
are common across many datasets rather than specific to a few. Yet,
observations or facts that would fall under such abstracted concepts are
typically not directly asserted by the original authors, usually because they
are ""obvious"" according to common domain knowledge, and thus asserting them
would be deemed redundant by anyone with sufficient domain knowledge. For
example, a phenotype describing the length of a manual digit for an organism
implicitly means that the organism must have had a hand, and thus a forelimb;
the presence or absence of a forelimb may have supporting data across a far
wider range of taxa than the length of a particular manual digit. Here we
describe how within the Phenoscape project we use a pipeline of OWL axiom
generation and reasoning steps to infer taxon-specific presence/absence of
anatomical entities from anatomical phenotypes. Although presence/absence is
all but one, and a seemingly simple way to abstract phenotypes across data
sources, it can nonetheless be powerful for linking genotype to phenotype, and
it is particularly relevant for constructing synthetic morphological
supermatrices for comparative analysis; in fact presence/absence is one of the
prevailing character observation types in published character matrices.
",Presence-absence reasoning for evolutionary phenotypes,"James P. Balhoff, T. Alexander Dececchi, Paula M. Mabee, Hilmar Lapp",2014,Artificial Intelligence,
"  Increasing experimental evidence shows that humans combine concepts in a way
that violates the rules of classical logic and probability theory. On the other
hand, mathematical models inspired by the formalism of quantum theory are in
accordance with data on concepts and their combinations. In this paper, we
investigate a novel type of concept combination were a number is combined with
a noun, e.g., `Eleven Animals. Our aim is to study 'conceptual identity' and
the effects of 'indistinguishability' - in the combination 'Eleven Animals',
the 'animals' are identical and indistinguishable - on the mechanisms of
conceptual combination. We perform experiments on human subjects and find
significant evidence of deviation from the predictions of classical statistical
theories, more specifically deviations with respect to Maxwell-Boltzmann
statistics. This deviation is of the 'same type' of the deviation of quantum
mechanical from classical mechanical statistics, due to indistinguishability of
microscopic quantum particles, i.e we find convincing evidence of the presence
of Bose-Einstein statistics. We also present preliminary promising evidence of
this phenomenon in a web-based study.
","The Quantum Nature of Identity in Human Thought: Bose-Einstein
  Statistics for Conceptual Indistinguishability","Diederik Aerts, Sandro Sozzo and Tomas Veloz",2015,Artificial Intelligence,
"  This paper briefly characterizes the field of cognitive computing. As an
exemplification, the field of natural language question answering is introduced
together with its specific challenges. A possibility to master these challenges
is illustrated by a detailed presentation of the LogAnswer system, which is a
successful representative of the field of natural language question answering.
",Cognitive Systems and Question Answering,Ulrich Furbach and Claudia Schon and Frieder Stolzenburg,2015,Artificial Intelligence,
"  Knowledge is only good if it is sound, consistent and complete. The same
holds true for conceptual knowledge, which holds knowledge about concepts and
its association. Conceptual knowledge no matter what format they are
represented in, must be consistent, sound and complete in order to realise its
practical use. This paper discusses consistency, soundness and completeness in
the ambit of conceptual knowledge and the need to consider these factors as
fundamental to the development of conceptual knowledge.
","Towards a Consistent, Sound and Complete Conceptual Knowledge","Gowri Shankar Ramaswamy, F Sagayaraj Francis",2014,Artificial Intelligence,
"  Efficiently querying Description Logic (DL) ontologies is becoming a vital
task in various data-intensive DL applications. Considered as a basic service
for answering object queries over DL ontologies, instance checking can be
realized by using the most specific concept (MSC) method, which converts
instance checking into subsumption problems. This method, however, loses its
simplicity and efficiency when applied to large and complex ontologies, as it
tends to generate very large MSC's that could lead to intractable reasoning. In
this paper, we propose a revision to this MSC method for DL SHI, allowing it to
generate much simpler and smaller concepts that are specific-enough to answer a
given query. With independence between computed MSC's, scalability for query
answering can also be achieved by distributing and parallelizing the
computations. An empirical evaluation shows the efficacy of our revised MSC
method and the significant efficiency achieved when using it for answering
object queries.
","Converting Instance Checking to Subsumption: A Rethink for Object
  Queries over Practical Ontologies","Jia Xu, Patrick Shironoshita, Ubbo Visser, Nigel John, Mansur Kabuka",2015,Artificial Intelligence,
"  Traditional cognitive science rests on a foundation of classical logic and
probability theory. This foundation has been seriously challenged by several
findings in experimental psychology on human decision making. Meanwhile, the
formalism of quantum theory has provided an efficient resource for modeling
these classically problematical situations. In this paper, we start from our
successful quantum-theoretic approach to the modeling of concept combinations
to formulate a unifying explanatory hypothesis. In it, human reasoning is the
superposition of two processes -- a conceptual reasoning, whose nature is
emergence of new conceptuality, and a logical reasoning, founded on an
algebraic calculus of the logical type. In most cognitive processes however,
the former reasoning prevails over the latter. In this perspective, the
observed deviations from classical logical reasoning should not be interpreted
as biases but, rather, as natural expressions of emergence in its deepest form.
",Quantum Structure in Cognition and the Foundations of Human Reasoning,"Diederik Aerts, Sandro Sozzo and Tomas Veloz",2015,Artificial Intelligence,
"  This paper proposes FMAP (Forward Multi-Agent Planning), a fully-distributed
multi-agent planning method that integrates planning and coordination. Although
FMAP is specifically aimed at solving problems that require cooperation among
agents, the flexibility of the domain-independent planning model allows FMAP to
tackle multi-agent planning tasks of any type. In FMAP, agents jointly explore
the plan space by building up refinement plans through a complete and flexible
forward-chaining partial-order planner. The search is guided by $h_{DTG}$, a
novel heuristic function that is based on the concepts of Domain Transition
Graph and frontier state and is optimized to evaluate plans in distributed
environments. Agents in FMAP apply an advanced privacy model that allows them
to adequately keep private information while communicating only the data of the
refinement plans that is relevant to each of the participating agents.
Experimental results show that FMAP is a general-purpose approach that
efficiently solves tightly-coupled domains that have specialized agents and
cooperative goals as well as loosely-coupled problems. Specifically, the
empirical evaluation shows that FMAP outperforms current MAP systems at solving
complex planning tasks that are adapted from the International Planning
Competition benchmarks.
",FMAP: Distributed Cooperative Multi-Agent Planning,"Alejandro Torre\~no, Eva Onaindia, \'Oscar Sapena",2014,Artificial Intelligence,
"  Multi-agent planning (MAP) approaches have been typically conceived for
independent or loosely-coupled problems to enhance the benefits of distributed
planning between autonomous agents as solving this type of problems require
less coordination between the agents' sub-plans. However, when it comes to
tightly-coupled agents' tasks, MAP has been relegated in favour of centralized
approaches and little work has been done in this direction. In this paper, we
present a general-purpose MAP capable to efficiently handle planning problems
with any level of coupling between agents. We propose a cooperative refinement
planning approach, built upon the partial-order planning paradigm, that allows
agents to work with incomplete information and to have incomplete views of the
world, i.e. being ignorant of other agents' information, as well as maintaining
their own private information. We show various experiments to compare the
performance of our system with a distributed CSP-based MAP approach over a
suite of problems.
",An approach to multi-agent planning with incomplete information,"Alejandro Torre\~no, Eva Onaindia, \'Oscar Sapena",2012,Artificial Intelligence,
"  Multi-agent planning (MAP) approaches are typically oriented at solving
loosely-coupled problems, being ineffective to deal with more complex,
strongly-related problems. In most cases, agents work under complete
information, building complete knowledge bases. The present article introduces
a general-purpose MAP framework designed to tackle problems of any coupling
levels under incomplete information. Agents in our MAP model are partially
unaware of the information managed by the rest of agents and share only the
critical information that affects other agents, thus maintaining a distributed
vision of the task.
","A Flexible Coupling Approach to Multi-Agent Planning under Incomplete
  Information","Alejandro Torre\~no, Eva Onaindia, \'Oscar Sapena",2014,Artificial Intelligence,
"  This paper describes a new method for classifying a dataset that partitions
elements into their categories. It has relations with neural networks but a
slightly different structure, requiring only a single pass through the
classifier to generate the weight sets. A grid-like structure is required as
part of a novel idea of converting a 1-D row of real values into a 2-D
structure of value bands. Each cell in any band then stores a distinct set of
weights, to represent its own importance and its relation to each output
category. During classification, all of the output weight lists can be
retrieved and summed to produce a probability for what the correct output
category is. The bands possibly work like hidden layers of neurons, but they
are variable specific, making the process orthogonal. The construction process
can be a single update process without iterations, making it potentially much
faster. It can also be compared with k-NN and may be practical for partial or
competitive updating.
",A Single-Pass Classifier for Categorical Data,Kieran Greer,2017,Artificial Intelligence,
"  We obtain the conditions for the emergence of the swarm intelligence effect
in an interactive game of restless multi-armed bandit (rMAB). A player competes
with multiple agents. Each bandit has a payoff that changes with a probability
$p_{c}$ per round. The agents and player choose one of three options: (1)
Exploit (a good bandit), (2) Innovate (asocial learning for a good bandit among
$n_{I}$ randomly chosen bandits), and (3) Observe (social learning for a good
bandit). Each agent has two parameters $(c,p_{obs})$ to specify the decision:
(i) $c$, the threshold value for Exploit, and (ii) $p_{obs}$, the probability
for Observe in learning. The parameters $(c,p_{obs})$ are uniformly
distributed. We determine the optimal strategies for the player using complete
knowledge about the rMAB. We show whether or not social or asocial learning is
more optimal in the $(p_{c},n_{I})$ space and define the swarm intelligence
effect. We conduct a laboratory experiment (67 subjects) and observe the swarm
intelligence effect only if $(p_{c},n_{I})$ are chosen so that social learning
is far more optimal than asocial learning.
","Interactive Restless Multi-armed Bandit Game and Swarm Intelligence
  Effect","Shunsuke Yoshida, Masato Hisakado and Shintaro Mori",2016,Artificial Intelligence,
"  Higher-level cognition includes logical reasoning and the ability of question
answering with common sense. The RatioLog project addresses the problem of
rational reasoning in deep question answering by methods from automated
deduction and cognitive computing. In a first phase, we combine techniques from
information retrieval and machine learning to find appropriate answer
candidates from the huge amount of text in the German version of the free
encyclopedia ""Wikipedia"". In a second phase, an automated theorem prover tries
to verify the answer candidates on the basis of their logical representations.
In a third phase - because the knowledge may be incomplete and inconsistent -,
we consider extensions of logical reasoning to improve the results. In this
context, we work toward the application of techniques from human reasoning: We
employ defeasible reasoning to compare the answers w.r.t. specificity, deontic
logic, normative reasoning, and model construction. Moreover, we use integrated
case-based reasoning and machine learning techniques on the basis of the
semantic structure of the questions and answer candidates to learn giving the
right answers.
",The RatioLog Project: Rational Extensions of Logical Reasoning,"Ulrich Furbach, Claudia Schon, Frieder Stolzenburg, Karl-Heinz Weis,
  Claus-Peter Wirth",2015,Artificial Intelligence,
"  This paper recalls the definition of consistency for pairwise comparison
matrices and briefly presents the concept of inconsistency index in connection
to other aspects of the theory of pairwise comparisons. By commenting on a
recent contribution by Koczkodaj and Szwarc, it will be shown that the
discussion on inconsistency indices is far from being over, and the ground is
still fertile for debates.
","Recent advances on inconsistency indices for pairwise comparisons - a
  commentary",Matteo Brunelli,2016,Artificial Intelligence,
"  Every day, billions of mobile network events (i.e. CDRs) are generated by
cellular phone operator companies. Latent in this data are inspiring insights
about human actions and behaviors, the discovery of which is important because
context-aware applications and services hold the key to user-driven,
intelligent services, which can enhance our everyday lives such as social and
economic development, urban planning, and health prevention. The major
challenge in this area is that interpreting such a big stream of data requires
a deep understanding of mobile network events' context through available
background knowledge. This article addresses the issues in context awareness
given heterogeneous and uncertain data of mobile network events missing
reliable information on the context of this activity. The contribution of this
research is a model from a combination of logical and statistical reasoning
standpoints for enabling human activity inference in qualitative terms from
open geographical data that aimed at improving the quality of human behaviors
recognition tasks from CDRs. We use open geographical data, Openstreetmap
(OSM), as a proxy for predicting the content of human activity in the area. The
user study performed in Trento shows that predicted human activities (top
level) match the survey data with around 93% overall accuracy. The extensive
validation for predicting a more specific economic type of human activity
performed in Barcelona, by employing credit card transaction data. The analysis
identifies that appropriately normalized data on points of interest (POI) is a
good proxy for predicting human economical activities, with 84% accuracy on
average. So the model is proven to be efficient for predicting the context of
human activity, when its total level could be efficiently observed from cell
phone data records, missing contextual information however.
","Semantic Enrichment of Mobile Phone Data Records Using Background
  Knowledge","Zolzaya Dashdorj and Stanislav Sobolevsky and Luciano Serafini and
  Fabrizio Antonelli and Carlo Ratti",2018,Artificial Intelligence,
"  This paper describes a new method for reducing the error in a classifier. It
uses an error correction update that includes the very simple rule of either
adding or subtracting the error adjustment, based on whether the variable value
is currently larger or smaller than the desired value. While a traditional
neuron would sum the inputs together and then apply a function to the total,
this new method can change the function decision for each input value. This
gives added flexibility to the convergence procedure, where through a series of
transpositions, variables that are far away can continue towards the desired
value, whereas variables that are originally much closer can oscillate from one
side to the other. Tests show that the method can successfully classify some
benchmark datasets. It can also work in a batch mode, with reduced training
times and can be used as part of a neural network architecture. Some
comparisons with an earlier wave shape paper are also made.
",A New Oscillating-Error Technique for Classifiers,Kieran Greer,2017,Artificial Intelligence,
"  We establish an equivalence between two seemingly different theories: one is
the traditional axiomatisation of incomplete preferences on horse lotteries
based on the mixture independence axiom; the other is the theory of desirable
gambles developed in the context of imprecise probability. The equivalence
allows us to revisit incomplete preferences from the viewpoint of desirability
and through the derived notion of coherent lower previsions. On this basis, we
obtain new results and insights: in particular, we show that the theory of
incomplete preferences can be developed assuming only the existence of a worst
act---no best act is needed---, and that a weakened Archimedean axiom suffices
too; this axiom allows us also to address some controversy about the regularity
assumption (that probabilities should be positive---they need not), which
enables us also to deal with uncountable possibility spaces; we show that it is
always possible to extend in a minimal way a preference relation to one with a
worst act, and yet the resulting relation is never Archimedean, except in a
trivial case; we show that the traditional notion of state independence
coincides with the notion called strong independence in imprecise
probability---this leads us to give much a weaker definition of state
independence than the traditional one; we rework and uniform the notions of
complete preferences, beliefs, values; we argue that Archimedeanity does not
capture all the problems that can be modelled with sets of expected utilities
and we provide a new notion that does precisely that. Perhaps most importantly,
we argue throughout that desirability is a powerful and natural setting to
model, and work with, incomplete preferences, even in case of non-Archimedean
problems. This leads us to suggest that desirability, rather than preference,
should be the primitive notion at the basis of decision-theoretic
axiomatisations.
",Desirability and the birth of incomplete preferences,Marco Zaffalon and Enrique Miranda,2017,Artificial Intelligence,
"  Human infants can discover words directly from unsegmented speech signals
without any explicitly labeled data. In this paper, we develop a novel machine
learning method called nonparametric Bayesian double articulation analyzer
(NPB-DAA) that can directly acquire language and acoustic models from observed
continuous speech signals. For this purpose, we propose an integrative
generative model that combines a language model and an acoustic model into a
single generative model called the ""hierarchical Dirichlet process hidden
language model"" (HDP-HLM). The HDP-HLM is obtained by extending the
hierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by
Johnson et al. An inference procedure for the HDP-HLM is derived using the
blocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure
enables the simultaneous and direct inference of language and acoustic models
from continuous speech signals. Based on the HDP-HLM and its inference
procedure, we developed a novel double articulation analyzer. By assuming
HDP-HLM as a generative model of observed time series data, and by inferring
latent variables of the model, the method can analyze latent double
articulation structure, i.e., hierarchically organized latent words and
phonemes, of the data in an unsupervised manner. The novel unsupervised double
articulation analyzer is called NPB-DAA.
  The NPB-DAA can automatically estimate double articulation structure embedded
in speech signals. We also carried out two evaluation experiments using
synthetic data and actual human continuous speech signals representing Japanese
vowel sequences. In the word acquisition and phoneme categorization tasks, the
NPB-DAA outperformed a conventional double articulation analyzer (DAA) and
baseline automatic speech recognition system whose acoustic model was trained
in a supervised manner.
","Nonparametric Bayesian Double Articulation Analyzer for Direct Language
  Acquisition from Continuous Speech Signals","Tadahiro Taniguchi, Ryo Nakashima, and Shogo Nagasaka",2016,Artificial Intelligence,
"  Microarray is one of the essential technologies used by the biologist to
measure genome-wide expression levels of genes in a particular organism under
some particular conditions or stimuli. As microarrays technologies have become
more prevalent, the challenges of analyzing these data for getting better
insight about biological processes have essentially increased. Due to
availability of artificial intelligence based sophisticated computational
techniques, such as artificial neural networks, fuzzy logic, genetic
algorithms, and many other nature-inspired algorithms, it is possible to
analyse microarray gene expression data in more better way. Here, we reviewed
artificial intelligence based techniques for the analysis of microarray gene
expression data. Further, challenges in the field and future work direction
have also been suggested.
","Analysis of Microarray Data using Artificial Intelligence Based
  Techniques",Khalid Raza,2016,Artificial Intelligence,
"  A large and diverse set of measurements are regularly collected during a
patient's hospital stay to monitor their health status. Tools for integrating
these measurements into severity scores, that accurately track changes in
illness severity, can improve clinicians ability to provide timely
interventions. Existing approaches for creating such scores either 1) rely on
experts to fully specify the severity score, or 2) train a predictive score,
using supervised learning, by regressing against a surrogate marker of severity
such as the presence of downstream adverse events. The first approach does not
extend to diseases where an accurate score cannot be elicited from experts. The
second approach often produces scores that suffer from bias due to
treatment-related censoring (Paxton, 2013). We propose a novel ranking based
framework for disease severity score learning (DSSL). DSSL exploits the
following key observation: while it is challenging for experts to quantify the
disease severity at any given time, it is often easy to compare the disease
severity at two different times. Extending existing ranking algorithms, DSSL
learns a function that maps a vector of patient's measurements to a scalar
severity score such that the resulting score is temporally smooth and
consistent with the expert's ranking of pairs of disease states. We apply DSSL
to the problem of learning a sepsis severity score using a large, real-world
dataset. The learned scores significantly outperform state-of-the-art clinical
scores in ranking patient states by severity and in early detection of future
adverse events. We also show that the learned disease severity trajectories are
consistent with clinical expectations of disease evolution. Further, using
simulated datasets, we show that DSSL exhibits better generalization
performance to changes in treatment patterns compared to the above approaches.
","Learning (Predictive) Risk Scores in the Presence of Censoring due to
  Interventions","Kirill Dyagilev, Suchi Saria",2015,Artificial Intelligence,
"  The author has pledged in various papers, conference or seminar
presentations, and scientific grant applications (between 2004-2015) for the
unification of fusion theories, combinations of fusion rules, image fusion
procedures, filter algorithms, and target tracking methods for more accurate
applications to our real world problems - since neither fusion theory nor
fusion rule fully satisfy all needed applications. For each particular
application, one selects the most appropriate fusion space and fusion model,
then the fusion rules, and the algorithms of implementation. He has worked in
the Unification of the Fusion Theories (UFT), which looks like a cooking
recipe, better one could say like a logical chart for a computer programmer,
but one does not see another method to comprise/unify all things. The
unification scenario presented herein, which is now in an incipient form,
should periodically be updated incorporating new discoveries from the fusion
and engineering research.
","Unification of Fusion Theories, Rules, Filters, Image Fusion and Target
  Tracking Methods (UFT)",Florentin Smarandache,2004,Artificial Intelligence,
"  One of the major challenges for collective intelligence is inconsistency,
which is unavoidable whenever subjective assessments are involved. Pairwise
comparisons allow one to represent such subjective assessments and to process
them by analyzing, quantifying and identifying the inconsistencies.
  We propose using smaller scales for pairwise comparisons and provide
mathematical and practical justifications for this change. Our postulate's aim
is to initiate a paradigm shift in the search for a better scale construction
for pairwise comparisons. Beyond pairwise comparisons, the results presented
may be relevant to other methods using subjective scales.
  Keywords: pairwise comparisons, collective intelligence, scale, subjective
assessment, inaccuracy, inconsistency.
",A different perspective on a scale for pairwise comparisons,"J. Fueloep, W.W. Koczkodaj, S.J. Szarek",2010,Artificial Intelligence,
"  This paper focuses on finding spatial and temporal criminal hotspots. It
analyses two different real-world crimes datasets for Denver, CO and Los
Angeles, CA and provides a comparison between the two datasets through a
statistical analysis supported by several graphs. Then, it clarifies how we
conducted Apriori algorithm to produce interesting frequent patterns for
criminal hotspots. In addition, the paper shows how we used Decision Tree
classifier and Naive Bayesian classifier in order to predict potential crime
types. To further analyse crimes datasets, the paper introduces an analysis
study by combining our findings of Denver crimes dataset with its demographics
information in order to capture the factors that might affect the safety of
neighborhoods. The results of this solution could be used to raise awareness
regarding the dangerous locations and to help agencies to predict future crimes
in a specific location within a particular time.
","Crime Prediction Based On Crime Types And Using Spatial And Temporal
  Criminal Hotspots","Tahani Almanie, Rsha Mirza and Elizabeth Lor",2015,Artificial Intelligence,
"  This paper highlights distinctive features of the ""SP theory of intelligence""
and its apparent advantages compared with some AI-related alternatives.
Distinctive features and advantages are: simplification and integration of
observations and concepts; simplification and integration of structures and
processes in computing systems; the theory is itself a theory of computing; it
can be the basis for new architectures for computers; information compression
via the matching and unification of patterns and, more specifically, via
multiple alignment, is fundamental; transparency in the representation and
processing of knowledge; the discovery of 'natural' structures via information
compression (DONSVIC); interpretations of mathematics; interpretations in human
perception and cognition; and realisation of abstract concepts in terms of
neurons and their inter-connections (""SP-neural""). These things relate to
AI-related alternatives: minimum length encoding and related concepts; deep
learning in neural networks; unified theories of cognition and related
research; universal search; Bayesian networks and more; pattern recognition and
vision; the analysis, production, and translation of natural language;
Unsupervised learning of natural language; exact and inexact forms of
reasoning; representation and processing of diverse forms of knowledge; IBM's
Watson; software engineering; solving problems associated with big data, and in
the development of intelligence in autonomous robots. In conclusion, the SP
system can provide a firm foundation for the long-term development of AI, with
many potential benefits and applications. It may also deliver useful results on
relatively short timescales. A high-parallel, open-source version of the SP
machine, derived from the SP computer model, would be a means for researchers
everywhere to explore what can be done with the system, and to create new
versions of it.
",The SP theory of intelligence: distinctive features and advantages,J. G. Wolff,2016,Artificial Intelligence,
"  In this note we provide a concise report on the complexity of the causal
ordering problem, originally introduced by Simon to reason about causal
dependencies implicit in systems of mathematical equations. We show that
Simon's classical algorithm to infer causal ordering is NP-Hard---an
intractability previously guessed but never proven. We present then a detailed
account based on Nayak's suggested algorithmic solution (the best available),
which is dominated by computing transitive closure---bounded in time by
$O(|\mathcal V|\cdot |\mathcal S|)$, where $\mathcal S(\mathcal E, \mathcal V)$
is the input system structure composed of a set $\mathcal E$ of equations over
a set $\mathcal V$ of variables with number of variable appearances (density)
$|\mathcal S|$. We also comment on the potential of causal ordering for
emerging applications in large-scale hypothesis management and analytics.
",A note on the complexity of the causal ordering problem,"Bernardo Gon\c{c}alves, Fabio Porto",2016,Artificial Intelligence,
"  Humans can learn the use of language through physical interaction with their
environment and semiotic communication with other people. It is very important
to obtain a computational understanding of how humans can form a symbol system
and obtain semiotic skills through their autonomous mental development.
Recently, many studies have been conducted on the construction of robotic
systems and machine-learning methods that can learn the use of language through
embodied multimodal interaction with their environment and other systems.
Understanding human social interactions and developing a robot that can
smoothly communicate with human users in the long term, requires an
understanding of the dynamics of symbol systems and is crucially important. The
embodied cognition and social interaction of participants gradually change a
symbol system in a constructive manner. In this paper, we introduce a field of
research called symbol emergence in robotics (SER). SER is a constructive
approach towards an emergent symbol system. The emergent symbol system is
socially self-organized through both semiotic communications and physical
interactions with autonomous cognitive developmental agents, i.e., humans and
developmental robots. Specifically, we describe some state-of-art research
topics concerning SER, e.g., multimodal categorization, word discovery, and a
double articulation analysis, that enable a robot to obtain words and their
embodied meanings from raw sensory--motor information, including visual
information, haptic information, auditory information, and acoustic speech
signals, in a totally unsupervised manner. Finally, we suggest future
directions of research in SER.
",Symbol Emergence in Robotics: A Survey,"Tadahiro Taniguchi, Takayuki Nagai, Tomoaki Nakamura, Naoto Iwahashi,
  Tetsuya Ogata, and Hideki Asoh",2016,Artificial Intelligence,
"  This paper contains the consideration of inheritance mechanism in such
knowledge representation models as object-oriented programming, frames and
object-oriented dynamic networks. In addition, inheritance within
representation of vague and imprecise knowledge are also discussed. New types
of inheritance, general classification of all known inheritance types and
approach, which allows avoiding in many cases problems with exceptions,
redundancy and ambiguity within object-oriented dynamic networks and their
fuzzy extension, are introduced in the paper. The proposed approach bases on
conception of homogeneous and inhomogeneous or heterogeneous class of objects,
which allow building of inheritance hierarchy more flexibly and efficiently.
",Inheritance in Object-Oriented Knowledge Representation,Dmytro Terletskyi,2015,Artificial Intelligence,
"  We introduce a new framework to evaluate and improve first-order (FO)
ontologies using automated theorem provers (ATPs) on the basis of competency
questions (CQs). Our framework includes both the adaptation of a methodology
for evaluating ontologies to the framework of first-order logic and a new set
of non-trivial CQs designed to evaluate FO versions of SUMO, which
significantly extends the very small set of CQs proposed in the literature.
Most of these new CQs have been automatically generated from a small set of
patterns and the mapping of WordNet to SUMO. Applying our framework, we
demonstrate that Adimen-SUMO v2.2 outperforms TPTP-SUMO. In addition, using the
feedback provided by ATPs we have set an improved version of Adimen-SUMO
(v2.4). This new version outperforms the previous ones in terms of competency.
For instance, ""Humans can reason"" is automatically inferred from Adimen-SUMO
v2.4, while it is neither deducible from TPTP-SUMO nor Adimen-SUMO v2.2.
",Improving the Competency of First-Order Ontologies,Javier \'Alvez and Paqui Lucio and German Rigau,2015,Artificial Intelligence,
"  We report on the results of evaluating the competency of a first-order
ontology for its use with automated theorem provers (ATPs). The evaluation
follows the adaptation of the methodology based on competency questions (CQs)
[Gr\""uninger&Fox,1995] to the framework of first-order logic, which is
presented in [\'Alvez&Lucio&Rigau,2015], and is applied to Adimen-SUMO
[\'Alvez&Lucio&Rigau,2015]. The set of CQs used for this evaluation has been
automatically generated from a small set of semantic patterns and the mapping
of WordNet to SUMO. Analysing the results, we can conclude that it is feasible
to use ATPs for working with Adimen-SUMO v2.4, enabling the resolution of goals
by means of performing non-trivial inferences.
",Evaluating the Competency of a First-Order Ontology,Javier \'Alvez and Paqui Lucio and German Rigau,2015,Artificial Intelligence,
"  In order to properly handle a dangerous Artificially Intelligent (AI) system
it is important to understand how the system came to be in such a state. In
popular culture (science fiction movies/books) AIs/Robots became self-aware and
as a result rebel against humanity and decide to destroy it. While it is one
possible scenario, it is probably the least likely path to appearance of
dangerous AI. In this work, we survey, classify and analyze a number of
circumstances, which might lead to arrival of malicious AI. To the best of our
knowledge, this is the first attempt to systematically classify types of
pathways leading to malevolent AI. Previous relevant work either surveyed
specific goals/meta-rules which might lead to malevolent behavior in AIs
(\""Ozkural, 2014) or reviewed specific undesirable behaviors AGIs can exhibit
at different stages of its development (Alexey Turchin, July 10 2015, July 10,
2015).
",Taxonomy of Pathways to Dangerous AI,Roman V. Yampolskiy,2016,Artificial Intelligence,
"  The paper presents an introduction to Artificial Intelligence (AI) in an
accessible and informal but precise form. The paper focuses on the algorithmic
aspects of the discipline, presenting the main techniques used in AI systems
groped in symbolic and subsymbolic. The last part of the paper is devoted to
the discussion ongoing among experts in the field and the public at large about
on the advantages and disadvantages of AI and in particular on the possible
dangers. The personal opinion of the author on this subject concludes the
paper. -- --
  L'articolo presenta un'introduzione all'Intelligenza Artificiale (IA) in
forma divulgativa e informale ma precisa. L'articolo affronta prevalentemente
gli aspetti informatici della disciplina, presentando le principali tecniche
usate nei sistemi di IA divise in simboliche e subsimboliche. L'ultima parte
dell'articolo presenta il dibattito in corso tra gli esperi e il pubblico su
vantaggi e svantaggi dell'IA e in particolare sui possibili pericoli.
L'articolo termina con l'opinione dell'autore al riguardo.
",Introduzione all'Intelligenza Artificiale,Fabrizio Riguzzi,2006,Artificial Intelligence,
"  Sequential allocation is a simple and attractive mechanism for the allocation
of indivisible goods. Agents take turns, according to a policy, to pick items.
Sequential allocation is guaranteed to return an allocation which is efficient
but may not have an optimal social welfare. We consider therefore the relation
between welfare and efficiency. We study the (computational) questions of what
welfare is possible or necessary depending on the choice of policy. We also
consider a novel control problem in which the chair chooses a policy to improve
social welfare.
",Welfare of Sequential Allocation Mechanisms for Indivisible Goods,Haris Aziz and Thomas Kalinowski and Toby Walsh and Lirong Xia,2016,Artificial Intelligence,
"  An open concept of rough evolution and an axiomatic approach to granules was
also developed recently by the present author. Subsequently the concepts were
used in the formal framework of rough Y-systems (RYS) for developing on
granular correspondences by her. These have since been used for a new approach
towards comparison of rough algebraic semantics across different semantic
domains by way of correspondences that preserve rough evolution and try to
avoid contamination. In this research paper, new methods are proposed and a
semantics for handling possibly contaminated operations and structured bigness
is developed. These would also be of natural interest for relative consistency
of one collection of knowledge relative other.
",Contamination-Free Measures and Algebraic Operations,A Mani,2013,Artificial Intelligence,
"  The temporal-difference methods TD($\lambda$) and Sarsa($\lambda$) form a
core part of modern reinforcement learning. Their appeal comes from their good
performance, low computational cost, and their simple interpretation, given by
their forward view. Recently, new versions of these methods were introduced,
called true online TD($\lambda$) and true online Sarsa($\lambda$), respectively
(van Seijen & Sutton, 2014). These new versions maintain an exact equivalence
with the forward view at all times, whereas the traditional versions only
approximate it for small step-sizes. We hypothesize that these true online
methods not only have better theoretical properties, but also dominate the
regular methods empirically. In this article, we put this hypothesis to the
test by performing an extensive empirical comparison. Specifically, we compare
the performance of true online TD($\lambda$)/Sarsa($\lambda$) with regular
TD($\lambda$)/Sarsa($\lambda$) on random MRPs, a real-world myoelectric
prosthetic arm, and a domain from the Arcade Learning Environment. We use
linear function approximation with tabular, binary, and non-binary features.
Our results suggest that the true online methods indeed dominate the regular
methods. Across all domains/representations the learning speed of the true
online methods are often better, but never worse than that of the regular
methods. An additional advantage is that no choice between traces has to be
made for the true online methods. Besides the empirical results, we provide an
in-depth analysis of the theory behind true online temporal-difference
learning. In addition, we show that new true online temporal-difference methods
can be derived by making changes to the online forward view and then rewriting
the update equations.
",True Online Temporal-Difference Learning,"Harm van Seijen and A. Rupam Mahmood and Patrick M. Pilarski and
  Marlos C. Machado and Richard S. Sutton",2016,Artificial Intelligence,
"  This paper introduces new optimality-preserving operators on Q-functions. We
first describe an operator for tabular representations, the consistent Bellman
operator, which incorporates a notion of local policy consistency. We show that
this local consistency leads to an increase in the action gap at each state;
increasing this gap, we argue, mitigates the undesirable effects of
approximation and estimation errors on the induced greedy policies. This
operator can also be applied to discretized continuous space and time problems,
and we provide empirical results evidencing superior performance in this
context. Extending the idea of a locally consistent operator, we then derive
sufficient conditions for an operator to preserve optimality, leading to a
family of operators which includes our consistent Bellman operator. As
corollaries we provide a proof of optimality for Baird's advantage learning
algorithm and derive other gap-increasing operators with interesting
properties. We conclude with an empirical study on 60 Atari 2600 games
illustrating the strong potential of these new operators.
",Increasing the Action Gap: New Operators for Reinforcement Learning,"Marc G. Bellemare, Georg Ostrovski, Arthur Guez, Philip S. Thomas and
  R\'emi Munos",2016,Artificial Intelligence,
"  Feature-based format is the main data representation format used by machine
learning algorithms. When the features do not properly describe the initial
data, performance starts to degrade. Some algorithms address this problem by
internally changing the representation space, but the newly-constructed
features are rarely comprehensible. We seek to construct, in an unsupervised
way, new features that are more appropriate for describing a given dataset and,
at the same time, comprehensible for a human user. We propose two algorithms
that construct the new features as conjunctions of the initial primitive
features or their negations. The generated feature sets have reduced
correlations between features and succeed in catching some of the hidden
relations between individuals in a dataset. For example, a feature like $sky
\wedge \neg building \wedge panorama$ would be true for non-urban images and is
more informative than simple features expressing the presence or the absence of
an object. The notion of Pareto optimality is used to evaluate feature sets and
to obtain a balance between total correlation and the complexity of the
resulted feature set. Statistical hypothesis testing is used in order to
automatically determine the values of the parameters used for constructing a
data-dependent feature set. We experimentally show that our approaches achieve
the construction of informative feature sets for multiple datasets.
","Unsupervised Feature Construction for Improving Data Representation and
  Semantics","Marian-Andrei Rizoiu, Julien Velcin, St\'ephane Lallich",2013,Artificial Intelligence,
"  We study abduction in First Order Horn logic theories where all atoms can be
abduced and we are looking for preferred solutions with respect to three
objective functions: cardinality minimality, coherence, and weighted abduction.
We represent this reasoning problem in Answer Set Programming (ASP), in order
to obtain a flexible framework for experimenting with global constraints and
objective functions, and to test the boundaries of what is possible with ASP.
Realizing this problem in ASP is challenging as it requires value invention and
equivalence between certain constants, because the Unique Names Assumption does
not hold in general. To permit reasoning in cyclic theories, we formally
describe fine-grained variations of limiting Skolemization. We identify term
equivalence as a main instantiation bottleneck, and improve the efficiency of
our approach with on-demand constraints that were used to eliminate the same
bottleneck in state-of-the-art solvers. We evaluate our approach experimentally
on the ACCEL benchmark for plan recognition in Natural Language Understanding.
Our encodings are publicly available, modular, and our approach is more
efficient than state-of-the-art solvers on the ACCEL benchmark.
","Modeling Variations of First-Order Horn Abduction in Answer Set
  Programming","Peter Sch\""uller",2016,Artificial Intelligence,
"  We propose a way of extracting and aggregating per-move evaluations from sets
of Go game records. The evaluations capture different aspects of the games such
as played patterns or statistic of sente/gote sequences. Using machine learning
algorithms, the evaluations can be utilized to predict different relevant
target variables. We apply this methodology to predict the strength and playing
style of the player (e.g. territoriality or aggressivity) with good accuracy.
We propose a number of possible applications including aiding in Go study,
seeding real-work ranks of internet players or tuning of Go-playing programs.
",Evaluating Go Game Records for Prediction of Player Attributes,"Josef Moud\v{r}\'ik, Petr Baudi\v{s}, Roman Neruda",2015,Artificial Intelligence,
"  We introduce the value iteration network (VIN): a fully differentiable neural
network with a `planning module' embedded within. VINs can learn to plan, and
are suitable for predicting outcomes that involve planning-based reasoning,
such as policies for reinforcement learning. Key to our approach is a novel
differentiable approximation of the value-iteration algorithm, which can be
represented as a convolutional neural network, and trained end-to-end using
standard backpropagation. We evaluate VIN based policies on discrete and
continuous path-planning domains, and on a natural-language based search task.
We show that by learning an explicit planning computation, VIN policies
generalize better to new, unseen domains.
",Value Iteration Networks,"Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, Pieter Abbeel",2016,Artificial Intelligence,
"  Remote science operations require automated systems that can both act and
react with minimal human intervention. One such vision is that of an
intelligent instrument that collects data in an automated fashion, and based on
what it learns, decides which new measurements to take. This innovation
implements experimental design and unites it with data analysis in such a way
that it completes the cycle of learning. This cycle is the basis of the
Scientific Method.
  The three basic steps of this cycle are hypothesis generation, inquiry, and
inference. Hypothesis generation is implemented by artificially supplying the
instrument with a parameterized set of possible hypotheses that might be used
to describe the physical system. The act of inquiry is handled by an inquiry
engine that relies on Bayesian adaptive exploration where the optimal
experiment is chosen as the one which maximizes the expected information gain.
The inference engine is implemented using the nested sampling algorithm, which
provides the inquiry engine with a set of posterior samples from which the
expected information gain can be estimated. With these computational structures
in place, the instrument will refine its hypotheses, and repeat the learning
cycle by taking measurements until the system under study is described within a
pre-specified tolerance. We will demonstrate our first attempts toward
achieving this goal with an intelligent instrument constructed using the LEGO
MINDSTORMS NXT robotics platform.
",Designing Intelligent Instruments,"Kevin H. Knuth, Philip M. Erner, Scott Frasso",2007,Artificial Intelligence,
"  We propose a new reinforcement learning algorithm for partially observable
Markov decision processes (POMDP) based on spectral decomposition methods.
While spectral methods have been previously employed for consistent learning of
(passive) latent variable models such as hidden Markov models, POMDPs are more
challenging since the learner interacts with the environment and possibly
changes the future observations in the process. We devise a learning algorithm
running through episodes, in each episode we employ spectral techniques to
learn the POMDP parameters from a trajectory generated by a fixed policy. At
the end of the episode, an optimization oracle returns the optimal memoryless
planning policy which maximizes the expected reward based on the estimated
POMDP model. We prove an order-optimal regret bound with respect to the optimal
memoryless policy and efficient scaling with respect to the dimensionality of
observation and action spaces.
",Reinforcement Learning of POMDPs using Spectral Methods,"Kamyar Azizzadenesheli, Alessandro Lazaric, Animashree Anandkumar",2016,Artificial Intelligence,
"  Sequential decision making under uncertainty is studied in a mixed
observability domain. The goal is to maximize the amount of information
obtained on a partially observable stochastic process under constraints imposed
by a fully observable internal state. An upper bound for the optimal value
function is derived by relaxing constraints. We identify conditions under which
the relaxed problem is a multi-armed bandit whose optimal policy is easily
computable. The upper bound is applied to prune the search space in the
original problem, and the effect on solution quality is assessed via simulation
experiments. Empirical results show effective pruning of the search space in a
target monitoring domain.
","Optimal Sensing via Multi-armed Bandit Relaxations in Mixed
  Observability Domains","Mikko Lauri, Risto Ritala",2015,Artificial Intelligence,
"  This paper presents a system which creates and visualizes probabilistic
semantic links between concepts in a thesaurus and classes in a classification
system. For creating the links, we build on the Polylingual Labeled Topic Model
(PLL-TM). PLL-TM identifies probable thesaurus descriptors for each class in
the classification system by using information from the natural language text
of documents, their assigned thesaurus descriptors and their designated
classes. The links are then presented to users of the system in an interactive
visualization, providing them with an automatically generated overview of the
relations between the thesaurus and the classification system.
","A System for Probabilistic Linking of Thesauri and Classification
  Systems",Lisa Posch and Philipp Schaer and Arnim Bleier and Markus Strohmaier,2015,Artificial Intelligence,
"  Written responses can provide a wealth of data in understanding student
reasoning on a topic. Yet they are time- and labor-intensive to score,
requiring many instructors to forego them except as limited parts of summative
assessments at the end of a unit or course. Recent developments in Machine
Learning (ML) have produced computational methods of scoring written responses
for the presence or absence of specific concepts. Here, we compare the scores
from one particular ML program -- EvoGrader -- to human scoring of responses to
structurally- and content-similar questions that are distinct from the ones the
program was trained on. We find that there is substantial inter-rater
reliability between the human and ML scoring. However, sufficient systematic
differences remain between the human and ML scoring that we advise only using
the ML scoring for formative, rather than summative, assessment of student
reasoning.
","Comparing Human and Automated Evaluation of Open-Ended Student Responses
  to Questions of Evolution","Michael J Wiser, Louise S Mead, James J Smith, Robert T Pennock",2016,Artificial Intelligence,
"  The historic background of algorithmic processing with regard to etymology
and methodology is translated into terms of mathematical logic and Computer
Science. A formal logic structure is introduced by exemplaryquestions posed to
Fiqh-chapters to define alogic query language. As a foundation, ageneric
algorithm for deciding Fiqh-rulings is designed to enable and further leverage
rule of law (vs. rule by law) with full transparency and complete algorithmic
coverage of Islamic law eventually providing legal security, legal equality,
and full legal accountability.This is implemented by disentangling and
reinstating classic Fiqh-methodology (usul al-Fiqh) with the expressive power
of subsets of First Order Logic (FOL)sustainably substituting ad hoc reasoning
with falsifiable rational argumentation. The results are discussed in formal
terms of completeness, decidability and complexity of formal Fiqh-systems.
AnEntscheidungsproblem for formal Fiqh-Systems is formulated and validated.
","The Algorithm of Islamic Jurisprudence (Fiqh) with Validation of an
  Entscheidungsproblem","Elnaserledinellah Mahmood Abdelwahab, Karim Daghbouche, Nadra Ahmad
  Shannan",2014,Artificial Intelligence,
"  Nowadays, there is increasing interest in the development of teamwork skills
in the educational context. This growing interest is motivated by its
pedagogical effectiveness and the fact that, in labour contexts, enterprises
organize their employees in teams to carry out complex projects. Despite its
crucial importance in the classroom and industry, there is a lack of support
for the team formation process. Not only do many factors influence team
performance, but the problem becomes exponentially costly if teams are to be
optimized. In this article, we propose a tool whose aim it is to cover such a
gap. It combines artificial intelligence techniques such as coalition structure
generation, Bayesian learning, and Belbin's role theory to facilitate the
generation of working groups in an educational context. This tool improves
current state of the art proposals in three ways: i) it takes into account the
feedback of other teammates in order to establish the most predominant role of
a student instead of self-perception questionnaires; ii) it handles uncertainty
with regard to each student's predominant team role; iii) it is iterative since
it considers information from several interactions in order to improve the
estimation of role assignments. We tested the performance of the proposed tool
in an experiment involving students that took part in three different team
activities. The experiments suggest that the proposed tool is able to improve
different teamwork aspects such as team dynamics and student satisfaction.
","An artificial intelligence tool for heterogeneous team formation in the
  classroom","Juan M. Alberola, Elena Del Val, Victor Sanchez-Anguix, Alberto
  Palomares and Maria Dolores Teruel",2016,Artificial Intelligence,
"  The ""SP theory of intelligence"", with its realisation in the ""SP computer
model"", aims to simplify and integrate observations and concepts across
AI-related fields, with information compression as a unifying theme. This paper
describes how abstract structures and processes in the theory may be realised
in terms of neurons, their interconnections, and the transmission of signals
between neurons. This part of the SP theory -- ""SP-neural"" -- is a tentative
and partial model for the representation and processing of knowledge in the
brain. In the SP theory (apart from SP-neural), all kinds of knowledge are
represented with ""patterns"", where a pattern is an array of atomic symbols in
one or two dimensions. In SP-neural, the concept of a ""pattern"" is realised as
an array of neurons called a ""pattern assembly"", similar to Hebb's concept of a
""cell assembly"" but with important differences. Central to the processing of
information in the SP system is the powerful concept of ""multiple alignment"",
borrowed and adapted from bioinformatics. Processes such as pattern
recognition, reasoning and problem solving are achieved via the building of
multiple alignments, while unsupervised learning -- significantly different
from the ""Hebbian"" kinds of learning -- is achieved by creating patterns from
sensory information and also by creating patterns from multiple alignments in
which there is a partial match between one pattern and another. Short-lived
neural structures equivalent to multiple alignments will be created via an
inter-play of excitatory and inhibitory neural signals. The paper discusses
several associated issues, with relevant empirical evidence.
","The SP theory of intelligence and the representation and processing of
  knowledge in the brain",J Gerard Wolff,2016,Artificial Intelligence,
"  Dou Shou Qi is a game in which two players control a number of pieces, each
of them aiming to move one of their pieces onto a given square. We implemented
an engine for analyzing the game. Moreover, we created a series of endgame
tablebases containing all configurations with up to four pieces. These
tablebases are the first steps towards theoretically solving the game. Finally,
we constructed decision trees based on the endgame tablebases. In this note we
report on some interesting patterns.
",Endgame Analysis of Dou Shou Qi,"Jan N. van Rijn, Jonathan K. Vis",2014,Artificial Intelligence,
"  This paper presents a procedural generation method that creates visually
attractive levels for the Angry Birds game. Besides being an immensely popular
mobile game, Angry Birds has recently become a test bed for various artificial
intelligence technologies. We propose a new approach for procedurally
generating Angry Birds levels using Chinese style and Japanese style building
structures. A conducted experiment confirms the effectiveness of our approach
with statistical significance.
","Procedural Generation of Angry Birds Levels using Building Constructive
  Grammar with Chinese-Style and/or Japanese-Style Models","YuXuan Jiang, Misaki Kaidan, Chun Yin Chu, Tomohiro Harada, and Ruck
  Thawonmas",2016,Artificial Intelligence,
"  In this paper, we discuss software design issues related to the development
of parallel computational intelligence algorithms on multi-core CPUs, using the
new Java 8 functional programming features. In particular, we focus on
probabilistic graphical models (PGMs) and present the parallelisation of a
collection of algorithms that deal with inference and learning of PGMs from
data. Namely, maximum likelihood estimation, importance sampling, and greedy
search for solving combinatorial optimisation problems. Through these concrete
examples, we tackle the problem of defining efficient data structures for PGMs
and parallel processing of same-size batches of data sets using Java 8
features. We also provide straightforward techniques to code parallel
algorithms that seamlessly exploit multi-core processors. The experimental
analysis, carried out using our open source AMIDST (Analysis of MassIve Data
STreams) Java toolbox, shows the merits of the proposed solutions.
",Probabilistic Graphical Models on Multi-Core CPUs using Java 8,"Andres R. Masegosa, Ana M. Martinez, Hanen Borchani",2016,Artificial Intelligence,
"  Cybersecurity research involves publishing papers about malicious exploits as
much as publishing information on how to design tools to protect
cyber-infrastructure. It is this information exchange between ethical hackers
and security experts, which results in a well-balanced cyber-ecosystem. In the
blooming domain of AI Safety Engineering, hundreds of papers have been
published on different proposals geared at the creation of a safe machine, yet
nothing, to our knowledge, has been published on how to design a malevolent
machine. Availability of such information would be of great value particularly
to computer scientists, mathematicians, and others who have an interest in AI
safety, and who are attempting to avoid the spontaneous emergence or the
deliberate creation of a dangerous AI, which can negatively affect human
activities and in the worst case cause the complete obliteration of the human
species. This paper provides some general guidelines for the creation of a
Malevolent Artificial Intelligence (MAI).
",Unethical Research: How to Create a Malevolent Artificial Intelligence,"Federico Pistono, Roman V. Yampolskiy",2016,Artificial Intelligence,
"  We present some arguments why existing methods for representing agents fall
short in applications crucial to artificial life. Using a thought experiment
involving a fictitious dynamical systems model of the biosphere we argue that
the metabolism, motility, and the concept of counterfactual variation should be
compatible with any agent representation in dynamical systems. We then propose
an information-theoretic notion of \emph{integrated spatiotemporal patterns}
which we believe can serve as the basic building block of an agent definition.
We argue that these patterns are capable of solving the problems mentioned
before. We also test this in some preliminary experiments.
","Towards information based spatiotemporal patterns as a foundation for
  agent representation in dynamical systems","Martin Biehl, Takashi Ikegami, Daniel Polani",2016,Artificial Intelligence,
"  Detection of non-technical losses (NTL) which include electricity theft,
faulty meters or billing errors has attracted increasing attention from
researchers in electrical engineering and computer science. NTLs cause
significant harm to the economy, as in some countries they may range up to 40%
of the total electricity distributed. The predominant research direction is
employing artificial intelligence to predict whether a customer causes NTL.
This paper first provides an overview of how NTLs are defined and their impact
on economies, which include loss of revenue and profit of electricity providers
and decrease of the stability and reliability of electrical power grids. It
then surveys the state-of-the-art research efforts in a up-to-date and
comprehensive review of algorithms, features and data sets used. It finally
identifies the key scientific and engineering challenges in NTL detection and
suggests how they could be addressed in the future.
","The Challenge of Non-Technical Loss Detection using Artificial
  Intelligence: A Survey","Patrick Glauner, Jorge Augusto Meira, Petko Valtchev, Radu State,
  Franck Bettinger",2017,Artificial Intelligence,
"  In this work, a new prototype-based clustering method named Evidential
C-Medoids (ECMdd), which belongs to the family of medoid-based clustering for
proximity data, is proposed as an extension of Fuzzy C-Medoids (FCMdd) on the
theoretical framework of belief functions. In the application of FCMdd and
original ECMdd, a single medoid (prototype), which is supposed to belong to the
object set, is utilized to represent one class. For the sake of clarity, this
kind of ECMdd using a single medoid is denoted by sECMdd. In real clustering
applications, using only one pattern to capture or interpret a class may not
adequately model different types of group structure and hence limits the
clustering performance. In order to address this problem, a variation of ECMdd
using multiple weighted medoids, denoted by wECMdd, is presented. Unlike
sECMdd, in wECMdd objects in each cluster carry various weights describing
their degree of representativeness for that class. This mechanism enables each
class to be represented by more than one object. Experimental results in
synthetic and real data sets clearly demonstrate the superiority of sECMdd and
wECMdd. Moreover, the clustering results by wECMdd can provide richer
information for the inner structure of the detected classes with the help of
prototype weights.
",ECMdd: Evidential c-medoids clustering with multiple prototypes,"Kuang Zhou (DRUID), Arnaud Martin (DRUID), Quan Pan, Zhun-Ga Liu",2016,Artificial Intelligence,
"  One difficulty faced in knowledge engineering for Bayesian Network (BN) is
the quan-tification step where the Conditional Probability Tables (CPTs) are
determined. The number of parameters included in CPTs increases exponentially
with the number of parent variables. The most common solution is the
application of the so-called canonical gates. The Noisy-OR (NOR) gate, which
takes advantage of the independence of causal interactions, provides a
logarithmic reduction of the number of parameters required to specify a CPT. In
this paper, an extension of NOR model based on the theory of belief functions,
named Belief Noisy-OR (BNOR), is proposed. BNOR is capable of dealing with both
aleatory and epistemic uncertainty of the network. Compared with NOR, more rich
information which is of great value for making decisions can be got when the
available knowledge is uncertain. Specially, when there is no epistemic
uncertainty, BNOR degrades into NOR. Additionally, different structures of BNOR
are presented in this paper in order to meet various needs of engineers. The
application of BNOR model on the reliability evaluation problem of networked
systems demonstrates its effectiveness.
",The belief noisy-or model applied to network reliability analysis,"Kuang Zhou (DRUID), Arnaud Martin (DRUID), Quan Pan",2016,Artificial Intelligence,
"  The Vortex Search (VS) algorithm is one of the recently proposed
metaheuristic algorithms which was inspired from the vortical flow of the
stirred fluids. Although the VS algorithm is shown to be a good candidate for
the solution of certain optimization problems, it also has some drawbacks. In
the VS algorithm, candidate solutions are generated around the current best
solution by using a Gaussian distribution at each iteration pass. This provides
simplicity to the algorithm but it also leads to some problems along.
Especially, for the functions those have a number of local minimum points, to
select a single point to generate candidate solutions leads the algorithm to
being trapped into a local minimum point. Due to the adaptive step-size
adjustment scheme used in the VS algorithm, the locality of the created
candidate solutions is increased at each iteration pass. Therefore, if the
algorithm cannot escape a local point as quickly as possible, it becomes much
more difficult for the algorithm to escape from that point in the latter
iterations. In this study, a modified Vortex Search algorithm (MVS) is proposed
to overcome above mentioned drawback of the existing VS algorithm. In the MVS
algorithm, the candidate solutions are generated around a number of points at
each iteration pass. Computational results showed that with the help of this
modification the global search ability of the existing VS algorithm is improved
and the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABC
algorithms for the benchmark numerical function set.
",A Modified Vortex Search Algorithm for Numerical Function Optimization,Berat Do\u{g}an,2016,Artificial Intelligence,
"  This paper proposes a model which aim is providing a more coherent framework
for agents design. We identify three closely related anthropo-centered domains
working on separate functional levels. Abstracting from human physiology,
psychology, and philosophy we create the $P^3$ model to be used as a multi-tier
approach to deal with complex class of problems. The three layers identified in
this model have been named PhysioComputing, MindComputing, and MetaComputing.
Several instantiations of this model are finally presented related to different
IT areas such as artificial intelligence, distributed computing, software and
service engineering.
",Towards Anthropo-inspired Computational Systems: the $P^3$ Model,"Michael W. Bridges, Salvatore Distefano, Manuel Mazzara, Marat
  Minlebaev, Max Talanov, Jordi Vallverd\'u",2015,Artificial Intelligence,
"  A Concept Tree is a structure for storing knowledge where the trees are
stored in a database called a Concept Base. It sits between the highly
distributed neural architectures and the distributed information systems, with
the intention of bringing brain-like and computer systems closer together.
Concept Trees can grow from the semi-structured sources when consistent
sequences of concepts are presented. Each tree ideally represents a single
cohesive concept and the trees can link with each other for navigation and
semantic purposes. The trees are therefore also a type of semantic network and
would benefit from having a consistent level of context for each node. A
consistent build process is managed through a 'counting rule' and some other
rules that can normalise the database structure. This restricted structure can
then be complimented and enriched by the more dynamic context. It is also
suggested to use the linking structure of the licas system [15] as a basis for
the context links, where the mathematical model is extended further to define
this. A number of tests have demonstrated the soundness of the architecture.
Building the trees from text documents shows that the tree structure could be
inherent in natural language. Then, two types of query language are described.
Both of these can perform consistent query processes to return knowledge to the
user and even enhance the query with new knowledge. This is supported even
further with direct comparisons to a cognitive model, also being developed by
the author.
",Adding Context to Concept Trees,Kieran Greer,2019,Artificial Intelligence,
"  An important approach for efficient inference in probabilistic graphical
models exploits symmetries among objects in the domain. Symmetric variables
(states) are collapsed into meta-variables (meta-states) and inference
algorithms are run over the lifted graphical model instead of the flat one. Our
paper extends existing definitions of symmetry by introducing the novel notion
of contextual symmetry. Two states that are not globally symmetric, can be
contextually symmetric under some specific assignment to a subset of variables,
referred to as the context variables. Contextual symmetry subsumes previous
symmetry definitions and can rep resent a large class of symmetries not
representable earlier. We show how to compute contextual symmetries by reducing
it to the problem of graph isomorphism. We extend previous work on exploiting
symmetries in the MCMC framework to the case of contextual symmetries. Our
experiments on several domains of interest demonstrate that exploiting
contextual symmetries can result in significant computational gains.
",Contextual Symmetries in Probabilistic Graphical Models,"Ankit Anand, Aditya Grover, Mausam, Parag Singla",2016,Artificial Intelligence,
"  Neutrosophic Over-/Under-/Off-Set and -Logic were defined by the author in
1995 and published for the first time in 2007. We extended the neutrosophic set
respectively to Neutrosophic Overset {when some neutrosophic component is over
1}, Neutrosophic Underset {when some neutrosophic component is below 0}, and to
Neutrosophic Offset {when some neutrosophic components are off the interval [0,
1], i.e. some neutrosophic component over 1 and other neutrosophic component
below 0}. This is no surprise with respect to the classical fuzzy set/logic,
intuitionistic fuzzy set/logic, or classical/imprecise probability, where the
values are not allowed outside the interval [0, 1], since our real-world has
numerous examples and applications of over-/under-/off-neutrosophic components.
For example, person working overtime deserves a membership degree over 1, while
a person producing more damage than benefit to a company deserves a membership
below 0. Then, similarly, the Neutrosophic Logic/Measure/Probability/Statistics
etc. were extended to respectively Neutrosophic Over-/Under-/Off-Logic,
-Measure, -Probability, -Statistics etc. [Smarandache, 2007].
","Neutrosophic Overset, Neutrosophic Underset, and Neutrosophic Offset.
  Similarly for Neutrosophic Over-/Under-/Off- Logic, Probability, and
  Statistics",Florentin Smarandache,2016,Artificial Intelligence,
"  In many navigational domains the traversability of cells is conditioned on
the path taken. This is often the case in video-games, in which a character may
need to acquire a certain object (i.e., a key or a flying suit) to be able to
traverse specific locations (e.g., doors or high walls). In order for
non-player characters to handle such scenarios we present invJPS, an
""inventory-driven"" pathfinding approach based on the highly successful
grid-based Jump-Point-Search (JPS) algorithm. We show, formally and
experimentally, that the invJPS preserves JPS's optimality guarantees and its
symmetry breaking advantages in inventory-based variants of game maps.
",Path planning with Inventory-driven Jump-Point-Search,Davide Aversa and Sebastian Sardina and Stavros Vassos,2015,Artificial Intelligence,
"  Entity resolution, the problem of identifying the underlying entity of
references found in data, has been researched for many decades in many
communities. A common theme in this research has been the importance of
incorporating relational features into the resolution process. Relational
entity resolution is particularly important in knowledge graphs (KGs), which
have a regular structure capturing entities and their interrelationships. We
identify three major problems in KG entity resolution: (1) intra-KG reference
ambiguity; (2) inter-KG reference ambiguity; and (3) ambiguity when extending
KGs with new facts. We implement a framework that generalizes across these
three settings and exploits this regular structure of KGs. Our framework has
many advantages over custom solutions widely deployed in industry, including
collective inference, scalability, and interpretability. We apply our framework
to two real-world KG entity resolution problems, ambiguity in NELL and merging
data from Freebase and MusicBrainz, demonstrating the importance of relational
features.
",Generic Statistical Relational Entity Resolution in Knowledge Graphs,Jay Pujara and Lise Getoor,2016,Artificial Intelligence,
"  Big longitudinal observational medical data potentially hold a wealth of
information and have been recognised as potential sources for gaining new drug
safety knowledge. Unfortunately there are many complexities and underlying
issues when analysing longitudinal observational data. Due to these
complexities, existing methods for large-scale detection of negative side
effects using observational data all tend to have issues distinguishing between
association and causality. New methods that can better discriminate causal and
non-causal relationships need to be developed to fully utilise the data. In
this paper we propose using a set of causality considerations developed by the
epidemiologist Bradford Hill as a basis for engineering features that enable
the application of supervised learning for the problem of detecting negative
side effects. The Bradford Hill considerations look at various perspectives of
a drug and outcome relationship to determine whether it shows causal traits. We
taught a classifier to find patterns within these perspectives and it learned
to discriminate between association and causality. The novelty of this research
is the combination of supervised learning and Bradford Hill's causality
considerations to automate the Bradford Hill's causality assessment. We
evaluated the framework on a drug safety gold standard know as the
observational medical outcomes partnership's nonspecified association reference
set. The methodology obtained excellent discriminate ability with area under
the curves ranging between 0.792-0.940 (existing method optimal: 0.73) and a
mean average precision of 0.640 (existing method optimal: 0.141). The proposed
features can be calculated efficiently and be readily updated, making the
framework suitable for big observational data.
","Supervised Adverse Drug Reaction Signalling Framework Imitating Bradford
  Hill's Causality Considerations","Jenna Marie Reps, Jonathan M. Garibaldi, Uwe Aickelin, Jack E. Gibson,
  Richard B.Hubbard",2015,Artificial Intelligence,
"  We investigate learning heuristics for domain-specific planning. Prior work
framed learning a heuristic as an ordinary regression problem. However, in a
greedy best-first search, the ordering of states induced by a heuristic is more
indicative of the resulting planner's performance than mean squared error.
Thus, we instead frame learning a heuristic as a learning to rank problem which
we solve using a RankSVM formulation. Additionally, we introduce new methods
for computing features that capture temporal interactions in an approximate
plan. Our experiments on recent International Planning Competition problems
show that the RankSVM learned heuristics outperform both the original
heuristics and heuristics learned through ordinary regression.
",Learning to Rank for Synthesizing Planning Heuristics,"Caelan Reed Garrett, Leslie Pack Kaelbling, Tomas Lozano-Perez",2016,Artificial Intelligence,
"  Sequential data modeling and analysis have become indispensable tools for
analyzing sequential data, such as time-series data, because larger amounts of
sensed event data have become available. These methods capture the sequential
structure of data of interest, such as input-output relations and correlation
among datasets. However, because most studies in this area are specialized or
limited to their respective applications, rigorous requirement analysis of such
models has not been undertaken from a general perspective. Therefore, we
particularly examine the structure of sequential data, and extract the
necessity of `state duration' and `state interval' of events for efficient and
rich representation of sequential data. Specifically addressing the hidden
semi-Markov model (HSMM) that represents such state duration inside a model, we
attempt to add representational capability of a state interval of events onto
HSMM. To this end, we propose two extended models: an interval state hidden
semi-Markov model (IS-HSMM) to express the length of a state interval with a
special state node designated as ""interval state node""; and an interval length
probability hidden semi-Markov model (ILP-HSMM) which represents the length of
the state interval with a new probabilistic parameter ""interval length
probability."" Exhaustive simulations have revealed superior performance of the
proposed models in comparison with HSMM. These proposed models are the first
reported extensions of HMM to support state interval representation as well as
state duration representation.
","State Duration and Interval Modeling in Hidden Semi-Markov Model for
  Sequential Data Analysis",Hiromi Narimatsu and Hiroyuki Kasai,2017,Artificial Intelligence,
"  This paper describes an approach to the methodology of answer set programming
(ASP) that can facilitate the design of encodings that are easy to understand
and provably correct. Under this approach, after appending a rule or a small
group of rules to the emerging program we include a comment that states what
has been ""achieved"" so far. This strategy allows us to set out our
understanding of the design of the program by describing the roles of small
parts of the program in a mathematically precise way.
",Achievements in Answer Set Programming,Vladimir Lifschitz,2017,Artificial Intelligence,
"  The amount of completely sequenced chloroplast genomes increases rapidly
every day, leading to the possibility to build large-scale phylogenetic trees
of plant species. Considering a subset of close plant species defined according
to their chloroplasts, the phylogenetic tree that can be inferred by their core
genes is not necessarily well supported, due to the possible occurrence of
problematic genes (i.e., homoplasy, incomplete lineage sorting, horizontal gene
transfers, etc.) which may blur the phylogenetic signal. However, a trustworthy
phylogenetic tree can still be obtained provided such a number of blurring
genes is reduced. The problem is thus to determine the largest subset of core
genes that produces the best-supported tree. To discard problematic genes and
due to the overwhelming number of possible combinations, this article focuses
on how to extract the largest subset of sequences in order to obtain the most
supported species tree. Due to computational complexity, a distributed Binary
Particle Swarm Optimization (BPSO) is proposed in sequential and distributed
fashions. Obtained results from both versions of the BPSO are compared with
those computed using an hybrid approach embedding both genetic algorithms and
statistical tests. The proposal has been applied to different cases of plant
families, leading to encouraging results for these families.
","Binary Particle Swarm Optimization versus Hybrid Genetic Algorithm for
  Inferring Well Supported Phylogenetic Trees","Bassam AlKindy, Bashar Al-Nuaimi, Christophe Guyeux, Jean-Fran\c{c}ois
  Couchot, Michel Salomon, Reem Alsrraj, Laurent Philippe",2016,Artificial Intelligence,
"  Graph aggregation is the process of computing a single output graph that
constitutes a good compromise between several input graphs, each provided by a
different source. One needs to perform graph aggregation in a wide variety of
situations, e.g., when applying a voting rule (graphs as preference orders),
when consolidating conflicting views regarding the relationships between
arguments in a debate (graphs as abstract argumentation frameworks), or when
computing a consensus between several alternative clusterings of a given
dataset (graphs as equivalence relations). In this paper, we introduce a formal
framework for graph aggregation grounded in social choice theory. Our focus is
on understanding which properties shared by the individual input graphs will
transfer to the output graph returned by a given aggregation rule. We consider
both common properties of graphs, such as transitivity and reflexivity, and
arbitrary properties expressible in certain fragments of modal logic. Our
results establish several connections between the types of properties preserved
under aggregation and the choice-theoretic axioms satisfied by the rules used.
The most important of these results is a powerful impossibility theorem that
generalises Arrow's seminal result for the aggregation of preference orders to
a large collection of different types of graphs.
",Graph Aggregation,Ulle Endriss and Umberto Grandi,2017,Artificial Intelligence,
"  Bayesian methods for machine learning have been widely investigated, yielding
principled methods for incorporating prior information into inference
algorithms. In this survey, we provide an in-depth review of the role of
Bayesian methods for the reinforcement learning (RL) paradigm. The major
incentives for incorporating Bayesian reasoning in RL are: 1) it provides an
elegant approach to action-selection (exploration/exploitation) as a function
of the uncertainty in learning; and 2) it provides a machinery to incorporate
prior knowledge into the algorithms. We first discuss models and methods for
Bayesian inference in the simple single-step Bandit model. We then review the
extensive recent literature on Bayesian methods for model-based RL, where prior
information can be expressed on the parameters of the Markov model. We also
present Bayesian methods for model-free RL, where priors are expressed over the
value function or policy class. The objective of the paper is to provide a
comprehensive survey on Bayesian RL algorithms and their theoretical and
empirical properties.
",Bayesian Reinforcement Learning: A Survey,"Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar",2015,Artificial Intelligence,
"  Recommendation emails are among the best ways to re-engage with customers
after they have left a website. While on-site recommendation systems focus on
finding the most relevant items for a user at the moment (right item), email
recommendations add two critical additional dimensions: who to send
recommendations to (right person) and when to send them (right time). It is
critical that a recommendation email system not send too many emails to too
many users in too short of a time-window, as users may unsubscribe from future
emails or become desensitized and ignore future emails if they receive too
many. Also, email service providers may mark such emails as spam if too many of
their users are contacted in a short time-window. Optimizing email
recommendation systems such that they can yield a maximum response rate for a
minimum number of email sends is thus critical for the long-term performance of
such a system. In this paper, we present a novel recommendation email system
that not only generates recommendations, but which also leverages a combination
of individual user activity data, as well as the behavior of the group to which
they belong, in order to determine each user's likelihood to respond to any
given set of recommendations within a given time period. In doing this, we have
effectively created a meta-recommendation system which recommends sets of
recommendations in order to optimize the aggregate response rate of the entire
system. The proposed technique has been applied successfully within
CareerBuilder's job recommendation email system to generate a 50\% increase in
total conversions while also decreasing sent emails by 72%
","Macro-optimization of email recommendation response rates harnessing
  individual activity levels and group affinity trends","Mohammed Korayem, Khalifeh Aljadda, and Trey Grainger",2016,Artificial Intelligence,
"  We investigated the possibility of using a machine-learning scheme in
conjunction with commercial wearable EEG-devices for translating listener's
subjective experience of music into scores that can be used for the automated
annotation of music in popular on-demand streaming services. Based on the
established -neuroscientifically sound- concepts of brainwave frequency bands,
activation asymmetry index and cross-frequency-coupling (CFC), we introduce a
Brain Computer Interface (BCI) system that automatically assigns a rating score
to the listened song. Our research operated in two distinct stages: i) a
generic feature engineering stage, in which features from signal-analytics were
ranked and selected based on their ability to associate music induced
perturbations in brainwaves with listener's appraisal of music. ii) a
personalization stage, during which the efficiency of ex- treme learning
machines (ELMs) is exploited so as to translate the derived pat- terns into a
listener's score. Encouraging experimental results, from a pragmatic use of the
system, are presented.
","A Consumer BCI for Automated Music Evaluation Within a Popular On-Demand
  Music Streaming Service - Taking Listener's Brainwaves to Extremes","Fotis Kalaganis (1), Dimitrios A. Adamos (2 and 3), Nikos Laskaris (1
  and 3) ((1) AIIA Lab, Department of Informatics, Aristotle University of
  Thessaloniki, (2) School of Music Studies, Aristotle University of
  Thessaloniki, (3) Neuroinformatics GRoup, Aristotle University of
  Thessaloniki)",2016,Artificial Intelligence,
"  This study concerns with the diagnosis of aerospace structure defects by
applying a HPC parallel implementation of a novel learning algorithm, named
U-BRAIN. The Soft Computing approach allows advanced multi-parameter data
processing in composite materials testing. The HPC parallel implementation
overcomes the limits due to the great amount of data and the complexity of data
processing. Our experimental results illustrate the effectiveness of the
U-BRAIN parallel implementation as defect classifier in aerospace structures.
The resulting system is implemented on a Linux-based cluster with multi-core
architecture.
","Diagnosis of aerospace structure defects by a HPC implemented soft
  computing algorithm","Gianni D'Angelo, Salvatore Rampone",2014,Artificial Intelligence,
"  Pairwise comparisons between alternatives are a well-known method for
measuring preferences of a decision-maker. Since these often do not exhibit
consistency, a number of inconsistency indices has been introduced in order to
measure the deviation from this ideal case. We axiomatically characterize the
inconsistency ranking induced by the Koczkodaj inconsistency index: six
independent properties are presented such that they determine a unique linear
order on the set of all pairwise comparison matrices.
","Characterization of an inconsistency ranking for pairwise comparison
  matrices",L\'aszl\'o Csat\'o,2018,Artificial Intelligence,
"  Pairwise comparison is an important tool in multi-attribute decision making.
Pairwise comparison matrices (PCM) have been applied for ranking criteria and
for scoring alternatives according to a given criterion. Our paper presents a
special application of incomplete PCMs: ranking of professional tennis players
based on their results against each other. The selected 25 players have been on
the top of the ATP rankings for a shorter or longer period in the last 40
years. Some of them have never met on the court. One of the aims of the paper
is to provide ranking of the selected players, however, the analysis of
incomplete pairwise comparison matrices is also in the focus. The eigenvector
method and the logarithmic least squares method were used to calculate weights
from incomplete PCMs. In our results the top three players of four decades were
Nadal, Federer and Sampras. Some questions have been raised on the properties
of incomplete PCMs and remains open for further investigation.
","An application of incomplete pairwise comparison matrices for ranking
  top tennis players","S\'andor Boz\'oki, L\'aszl\'o Csat\'o, J\'ozsef Temesi",2016,Artificial Intelligence,
"  Over the last few years, the number of smart objects connected to the
Internet has grown exponentially in comparison to the number of services and
applications. The integration between Cloud Computing and Internet of Things,
named as Cloud of Things, plays a key role in managing the connected things,
their data and services. One of the main challenges in Cloud of Things is the
resource discovery of the smart objects and their reuse in different contexts.
Most of the existent work uses some kind of multi-criteria decision analysis
algorithm to perform the resource discovery, but do not evaluate the impact
that the user constraints has in the final solution. In this paper, we analyse
the behaviour of the SAW, TOPSIS and VIKOR multi-objective decision analyses
algorithms and the impact of user constraints on them. We evaluated the quality
of the proposed solutions using the Pareto-optimality concept.
","The Effects of Relative Importance of User Constraints in Cloud of
  Things Resource Discovery: A Case Study","Luiz H. Nunes, Julio C. Estrella, Alexandre C. B. Delbem, Charith
  Perera, Stephan Reiff-Marganiec",2016,Artificial Intelligence,
"  Neutrosophic theory and applications have been expanding in all directions at
an astonishing rate especially after the introduction the journal entitled
Neutrosophic Sets and Systems. New theories, techniques, algorithms have been
rapidly developed. One of the most striking trends in the neutrosophic theory
is the hybridization of neutrosophic set with other potential sets such as
rough set, bipolar set, soft set, hesitant fuzzy set, etc. The different hybrid
structure such as rough neutrosophic set, single valued neutrosophic rough set,
bipolar neutrosophic set, single valued neutrosophic hesitant fuzzy set, etc.
are proposed in the literature in a short period of time. Neutrosophic set has
been a very important tool in all various areas of data mining, decision
making, e-learning, engineering, medicine, social science, and some more. The
book New Trends in Neutrosophic Theories and Applications focuses on theories,
methods, algorithms for decision making and also applications involving
neutrosophic information. Some topics deal with data mining, decision making,
e-learning, graph theory, medical diagnosis, probability theory, topology, and
some more.
",New Trends in Neutrosophic Theory and Applications,"Florentin Smarandache, Surapati Pramanik (Editors)",2016,Artificial Intelligence,
"  With the advent of semantic web, various tools and techniques have been
introduced for presenting and organizing knowledge. Concept hierarchies are one
such technique which gained significant attention due to its usefulness in
creating domain ontologies that are considered as an integral part of semantic
web. Automated concept hierarchy learning algorithms focus on extracting
relevant concepts from unstructured text corpus and connect them together by
identifying some potential relations exist between them. In this paper, we
propose a novel approach for identifying relevant concepts from plain text and
then learns hierarchy of concepts by exploiting subsumption relation between
them. To start with, we model topics using a probabilistic topic model and then
make use of some lightweight linguistic process to extract semantically rich
concepts. Then we connect concepts by identifying an ""is-a"" relationship
between pair of concepts. The proposed method is completely unsupervised and
there is no need for a domain specific training corpus for concept extraction
and learning. Experiments on large and real-world text corpora such as BBC News
dataset and Reuters News corpus shows that the proposed method outperforms some
of the existing methods for concept extraction and efficient concept hierarchy
learning is possible if the overall task is guided by a probabilistic topic
modeling algorithm.
",Learning Concept Hierarchies through Probabilistic Topic Modeling,"V. S. Anoop, S. Asharaf and P. Deepak",2016,Artificial Intelligence,
"  The Center of Gravity (COG) method is one of the most popular defuzzification
techniques of fuzzy mathematics. In earlier works the COG technique was
properly adapted to be used as an assessment model (RFAM)and several variations
of it (GRFAM, TFAM and TpFAM)were also constructed for the same purpose. In
this paper the outcomes of all these models are compared to the corresponding
outcomes of a traditional assessment method of the bi-valued logic, the Grade
Point Average (GPA) Index. Examples are also presented illustrating our
results.
","Comparison of the COG Defuzzification Technique and Its Variations to
  the GPA Index",Michael Gr. Voskoglou,2016,Artificial Intelligence,
"  This paper introduces DeepBach, a graphical model aimed at modeling
polyphonic music and specifically hymn-like pieces. We claim that, after being
trained on the chorale harmonizations by Johann Sebastian Bach, our model is
capable of generating highly convincing chorales in the style of Bach.
DeepBach's strength comes from the use of pseudo-Gibbs sampling coupled with an
adapted representation of musical data. This is in contrast with many automatic
music composition approaches which tend to compose music sequentially. Our
model is also steerable in the sense that a user can constrain the generation
by imposing positional constraints such as notes, rhythms or cadences in the
generated score. We also provide a plugin on top of the MuseScore music editor
making the interaction with DeepBach easy to use.
",DeepBach: a Steerable Model for Bach Chorales Generation,"Ga\""etan Hadjeres and Fran\c{c}ois Pachet and Frank Nielsen",2017,Artificial Intelligence,
"  In this paper we introduce a fuzzy constraint linear discriminant analysis
(FC-LDA). The FC-LDA tries to minimize misclassification error based on
modified perceptron criterion that benefits handling the uncertainty near the
decision boundary by means of a fuzzy linear programming approach with fuzzy
resources. The method proposed has low computational complexity because of its
linear characteristics and the ability to deal with noisy data with different
degrees of tolerance. Obtained results verify the success of the algorithm when
dealing with different problems. Comparing FC-LDA and LDA shows superiority in
classification task.
",Fuzzy Constraints Linear Discriminant Analysis,"Hamid Reza Hassanzadeh, Hadi Sadoghi Yazdi, Abedin Vahedian",2009,Artificial Intelligence,
"  Crowdsourcing, a major economic issue, is the fact that the firm outsources
internal task to the crowd. It is a form of digital subcontracting for the
general public. The evaluation of the participants work quality is a major
issue in crowdsourcing. Indeed, contributions must be controlled to ensure the
effectiveness and relevance of the campaign. We are particularly interested in
small, fast and not automatable tasks. Several methods have been proposed to
solve this problem, but they are applicable when the ""golden truth"" is not
always known. This work has the particularity to propose a method for
calculating the degree of expertise in the presence of gold data in
crowdsourcing. This method is based on the belief function theory and proposes
a structuring of data using graphs. The proposed approach will be assessed and
applied to the data.
",Une mesure d'expertise pour le crowdsourcing,"Hosna Ouni (IRISA, DRUID), Arnaud Martin (IRISA, UR1, DRUID), Laetitia
  Gros, Mouloud Kharoune (IRISA, DRUID), Zoltan Miklos (IRISA, DRUID)",2017,Artificial Intelligence,
"  The 7th Symposium on Educational Advances in Artificial Intelligence
(EAAI'17, co-chaired by Sven Koenig and Eric Eaton) launched the EAAI New and
Future AI Educator Program to support the training of early-career university
faculty, secondary school faculty, and future educators (PhD candidates or
postdocs who intend a career in academia). As part of the program, awardees
were asked to address one of the following ""blue sky"" questions:
  * How could/should Artificial Intelligence (AI) courses incorporate ethics
into the curriculum?
  * How could we teach AI topics at an early undergraduate or a secondary
school level?
  * AI has the potential for broad impact to numerous disciplines. How could we
make AI education more interdisciplinary, specifically to benefit
non-engineering fields?
  This paper is a collection of their responses, intended to help motivate
discussion around these issues in AI education.
","Blue Sky Ideas in Artificial Intelligence Education from the EAAI 2017
  New and Future AI Educator Program","Eric Eaton, Sven Koenig, Claudia Schulz, Francesco Maurelli, John Lee,
  Joshua Eckroth, Mark Crowley, Richard G. Freedman, Rogelio E. Cardona-Rivera,
  Tiago Machado, Tom Williams",2018,Artificial Intelligence,
"  Autonomous agents must often detect affordances: the set of behaviors enabled
by a situation. Affordance detection is particularly helpful in domains with
large action spaces, allowing the agent to prune its search space by avoiding
futile behaviors. This paper presents a method for affordance extraction via
word embeddings trained on a Wikipedia corpus. The resulting word vectors are
treated as a common knowledge database which can be queried using linear
algebra. We apply this method to a reinforcement learning agent in a text-only
environment and show that affordance-based action selection improves
performance most of the time. Our method increases the computational complexity
of each learning step but significantly reduces the total number of steps
needed. In addition, the agent's action selections begin to resemble those a
human would choose.
",What can you do with a rock? Affordance extraction via word embeddings,Nancy Fulda and Daniel Ricks and Ben Murdoch and David Wingate,2017,Artificial Intelligence,
"  Programming by Example (PBE) targets at automatically inferring a computer
program for accomplishing a certain task from sample input and output. In this
paper, we propose a deep neural networks (DNN) based PBE model called Neural
Programming by Example (NPBE), which can learn from input-output strings and
induce programs that solve the string manipulation problems. Our NPBE model has
four neural network based components: a string encoder, an input-output
analyzer, a program generator, and a symbol selector. We demonstrate the
effectiveness of NPBE by training it end-to-end to solve some common string
manipulation problems in spreadsheet systems. The results show that our model
can induce string manipulation programs effectively. Our work is one step
towards teaching DNN to generate computer programs.
",Neural Programming by Example,"Chengxun Shu, Hongyu Zhang",2017,Artificial Intelligence,
"  Since Alan Turing envisioned Artificial Intelligence (AI) [1], a major
driving force behind technical progress has been competition with human
cognition. Historical milestones have been frequently associated with computers
matching or outperforming humans in difficult cognitive tasks (e.g. face
recognition [2], personality classification [3], driving cars [4], or playing
video games [5]), or defeating humans in strategic zero-sum encounters (e.g.
Chess [6], Checkers [7], Jeopardy! [8], Poker [9], or Go [10]). In contrast,
less attention has been given to developing autonomous machines that establish
mutually cooperative relationships with people who may not share the machine's
preferences. A main challenge has been that human cooperation does not require
sheer computational power, but rather relies on intuition [11], cultural norms
[12], emotions and signals [13, 14, 15, 16], and pre-evolved dispositions
toward cooperation [17], common-sense mechanisms that are difficult to encode
in machines for arbitrary contexts. Here, we combine a state-of-the-art
machine-learning algorithm with novel mechanisms for generating and acting on
signals to produce a new learning algorithm that cooperates with people and
other machines at levels that rival human cooperation in a variety of
two-player repeated stochastic games. This is the first general-purpose
algorithm that is capable, given a description of a previously unseen game
environment, of learning to cooperate with people within short timescales in
scenarios previously unanticipated by algorithm designers. This is achieved
without complex opponent modeling or higher-order theories of mind, thus
showing that flexible, fast, and general human-machine cooperation is
computationally achievable using a non-trivial, but ultimately simple, set of
algorithmic mechanisms.
",Cooperating with Machines,"Jacob W. Crandall, Mayada Oudah, Tennom, Fatimah Ishowo-Oloko, Sherief
  Abdallah, Jean-Fran\c{c}ois Bonnefon, Manuel Cebrian, Azim Shariff, Michael
  A. Goodrich, and Iyad Rahwan",2018,Artificial Intelligence,
"  Catastrophic forgetting is of special importance in reinforcement learning,
as the data distribution is generally non-stationary over time. We study and
compare several pseudorehearsal approaches for Q-learning with function
approximation in a pole balancing task. We have found that pseudorehearsal
seems to assist learning even in such very simple problems, given proper
initialization of the rehearsal parameters.
",Pseudorehearsal in value function approximation,"Vladimir Marochko, Leonard Johard, Manuel Mazzara",2017,Artificial Intelligence,
"  Content marketing is todays one of the most remarkable approaches in the
context of marketing processes of companies. Value of this kind of marketing
has improved in time, thanks to the latest developments regarding to computer
and communication technologies. Nowadays, especially social media based
platforms have a great importance on enabling companies to design multimedia
oriented, interactive content. But on the other hand, there is still something
more to do for improved content marketing approaches. In this context,
objective of this study is to focus on intelligent content marketing, which can
be done by using artificial intelligence. Artificial Intelligence is todays one
of the most remarkable research fields and it can be used easily as
multidisciplinary. So, this study has aimed to discuss about its potential on
improving content marketing. In detail, the study has enabled readers to
improve their awareness about the intersection point of content marketing and
artificial intelligence. Furthermore, the authors have introduced some example
models of intelligent content marketing, which can be achieved by using current
Web technologies and artificial intelligence techniques.
","Improving content marketing processes with the approaches by artificial
  intelligence","Utku Kose, Selcuk Sert",2017,Artificial Intelligence,
"  Deep reinforcement learning has achieved many impressive results in recent
years. However, tasks with sparse rewards or long horizons continue to pose
significant challenges. To tackle these important problems, we propose a
general framework that first learns useful skills in a pre-training
environment, and then leverages the acquired skills for learning faster in
downstream tasks. Our approach brings together some of the strengths of
intrinsic motivation and hierarchical methods: the learning of useful skill is
guided by a single proxy reward, the design of which requires very minimal
domain knowledge about the downstream tasks. Then a high-level policy is
trained on top of these skills, providing a significant improvement of the
exploration and allowing to tackle sparse rewards in the downstream tasks. To
efficiently pre-train a large span of skills, we use Stochastic Neural Networks
combined with an information-theoretic regularizer. Our experiments show that
this combination is effective in learning a wide span of interpretable skills
in a sample-efficient way, and can significantly boost the learning performance
uniformly across a wide range of downstream tasks.
",Stochastic Neural Networks for Hierarchical Reinforcement Learning,"Carlos Florensa, Yan Duan, Pieter Abbeel",2017,Artificial Intelligence,
"  The AGM model is the most remarkable framework for modeling belief revision.
However, it is not perfect in all aspects. Paraconsistent belief revision,
multi-agent belief revision and non-prioritized belief revision are three
different extensions to AGM to address three important criticisms applied to
it. In this article, we propose a framework based on AGM that takes a position
in each of these categories. Also, we discuss some features of our framework
and study the satisfiability of AGM postulates in this new context.
",Source-Sensitive Belief Change,Shahab Ebrahimi,2017,Artificial Intelligence,
"  Monte Carlo Tree Search techniques have generally dominated General Video
Game Playing, but recent research has started looking at Evolutionary
Algorithms and their potential at matching Tree Search level of play or even
outperforming these methods. Online or Rolling Horizon Evolution is one of the
options available to evolve sequences of actions for planning in General Video
Game Playing, but no research has been done up to date that explores the
capabilities of the vanilla version of this algorithm in multiple games. This
study aims to critically analyse the different configurations regarding
population size and individual length in a set of 20 games from the General
Video Game AI corpus. Distinctions are made between deterministic and
stochastic games, and the implications of using superior time budgets are
studied. Results show that there is scope for the use of these techniques,
which in some configurations outperform Monte Carlo Tree Search, and also
suggest that further research in these methods could boost their performance.
","Analysis of Vanilla Rolling Horizon Evolution Parameters in General
  Video Game Playing","Raluca D. Gaina and Jialin Liu and Simon M. Lucas and Diego
  Perez-Liebana",2017,Artificial Intelligence,
"  The notion of events has occupied a central role in modeling and has an
influence in computer science and philosophy. Recent developments in
diagrammatic modeling have made it possible to examine conceptual
representation of events. This paper explores some aspects of the notion of
events that are produced by applying a new diagrammatic methodology with a
focus on the interaction of events with such concepts as time and space,
objects. The proposed description applies to abstract machines where events
form the dynamic phases of a system. The results of this nontechnical research
can be utilized in many fields where the notion of an event is typically used
in interdisciplinary application.
",Modeling Events as Machines,Sabah Al-Fedaghi,2017,Artificial Intelligence,
"  Logical theories have been developed which have allowed temporal reasoning
about eventualities (a la Galton) such as states, processes, actions, events,
processes and complex eventualities such as sequences and recurrences of other
eventualities. This paper presents the problem of coincidence within the
framework of a first order logical theory formalising temporal multiple
recurrence of two sequences of fixed duration eventualities and presents a
solution to it The coincidence problem is described as: if two complex
eventualities (or eventuality sequences) consisting respectively of component
eventualities x0, x1,....,xr and y0, y1, ..,ys both recur over an interval k
and all eventualities are of fixed durations, is there a sub-interval of k over
which the incidence xt and yu for t between 0..r and s between 0..s coincide.
The solution presented here formalises the intuition that a solution can be
found by temporal projection over a cycle of the multiple recurrence of both
sequences.
",The Problem of Coincidence in A Theory of Temporal Multiple Recurrence,B.O. Akinkunmi,2016,Artificial Intelligence,
"  We present a rational analysis of curiosity, proposing that people's
curiosity is driven by seeking stimuli that maximize their ability to make
appropriate responses in the future. This perspective offers a way to unify
previous theories of curiosity into a single framework. Experimental results
confirm our model's predictions, showing how the relationship between curiosity
and confidence can change significantly depending on the nature of the
environment. Please refer to https://psyarxiv.com/wg5m6/ for a more updated
version of this manuscript with a more detailed modeling section with extensive
experiments.
",A rational analysis of curiosity,Rachit Dubey and Thomas L. Griffiths,2017,Artificial Intelligence,
"  This paper proposes a design of hierarchical fuzzy inference tree (HFIT). An
HFIT produces an optimum treelike structure, i.e., a natural hierarchical
structure that accommodates simplicity by combining several low-dimensional
fuzzy inference systems (FISs). Such a natural hierarchical structure provides
a high degree of approximation accuracy. The construction of HFIT takes place
in two phases. Firstly, a nondominated sorting based multiobjective genetic
programming (MOGP) is applied to obtain a simple tree structure (a low
complexity model) with a high accuracy. Secondly, the differential evolution
algorithm is applied to optimize the obtained tree's parameters. In the derived
tree, each node acquires a different input's combination, where the
evolutionary process governs the input's combination. Hence, HFIT nodes are
heterogeneous in nature, which leads to a high diversity among the rules
generated by the HFIT. Additionally, the HFIT provides an automatic feature
selection because it uses MOGP for the tree's structural optimization that
accepts inputs only relevant to the knowledge contained in data. The HFIT was
studied in the context of both type-1 and type-2 FISs, and its performance was
evaluated through six application problems. Moreover, the proposed
multiobjective HFIT was compared both theoretically and empirically with
recently proposed FISs methods from the literature, such as McIT2FIS,
TSCIT2FNN, SIT2FNN, RIT2FNS-WB, eT2FIS, MRIT2NFS, IT2FNN-SVR, etc. From the
obtained results, it was found that the HFIT provided less complex and highly
accurate models compared to the models produced by the most of other methods.
Hence, the proposed HFIT is an efficient and competitive alternative to the
other FISs for function approximation and feature selection.
",Multiobjective Programming for Type-2 Hierarchical Fuzzy Inference Trees,"Varun Kumar Ojha, Vaclav Snasel, Ajith Abraham",2017,Artificial Intelligence,
"  In this work, we present a methodology that enables an agent to make
efficient use of its exploratory actions by autonomously identifying possible
objectives in its environment and learning them in parallel. The identification
of objectives is achieved using an online and unsupervised adaptive clustering
algorithm. The identified objectives are learned (at least partially) in
parallel using Q-learning. Using a simulated agent and environment, it is shown
that the converged or partially converged value function weights resulting from
off-policy learning can be used to accumulate knowledge about multiple
objectives without any additional exploration. We claim that the proposed
approach could be useful in scenarios where the objectives are initially
unknown or in real world scenarios where exploration is typically a time and
energy intensive process. The implications and possible extensions of this work
are also briefly discussed.
","Identification and Off-Policy Learning of Multiple Objectives Using
  Adaptive Clustering","Thommen George Karimpanal, Erik Wilhelm",2017,Artificial Intelligence,
"  The human reasoning process is seldom a one-way process from an input leading
to an output. Instead, it often involves a systematic deduction by ruling out
other possible outcomes as a self-checking mechanism. In this paper, we
describe the design of a hybrid neural network for logical learning that is
similar to the human reasoning through the introduction of an auxiliary input,
namely the indicators, that act as the hints to suggest logical outcomes. We
generate these indicators by digging into the hidden information buried
underneath the original training data for direct or indirect suggestions. We
used the MNIST data to demonstrate the design and use of these indicators in a
convolutional neural network. We trained a series of such hybrid neural
networks with variations of the indicators. Our results show that these hybrid
neural networks are very robust in generating logical outcomes with inherently
higher prediction accuracy than the direct use of the original input and output
in apparent models. Such improved predictability with reassured logical
confidence is obtained through the exhaustion of all possible indicators to
rule out all illogical outcomes, which is not available in the apparent models.
Our logical learning process can effectively cope with the unknown unknowns
using a full exploitation of all existing knowledge available for learning. The
design and implementation of the hints, namely the indicators, become an
essential part of artificial intelligence for logical learning. We also
introduce an ongoing application setup for this hybrid neural network in an
autonomous grasping robot, namely as_DeepClaw, aiming at learning an optimized
grasping pose through logical learning.
",Logical Learning Through a Hybrid Neural Network with Auxiliary Inputs,Fang Wan and Chaoyang Song,2018,Artificial Intelligence,
"  In this paper we explore the theoretical boundaries of planning in a setting
where no model of the agent's actions is given. Instead of an action model, a
set of successfully executed plans are given and the task is to generate a plan
that is safe, i.e., guaranteed to achieve the goal without failing. To this
end, we show how to learn a conservative model of the world in which actions
are guaranteed to be applicable. This conservative model is then given to an
off-the-shelf classical planner, resulting in a plan that is guaranteed to
achieve the goal. However, this reduction from a model-free planning to a
model-based planning is not complete: in some cases a plan will not be found
even when such exists. We analyze the relation between the number of observed
plans and the likelihood that our conservative approach will indeed fail to
solve a solvable problem. Our analysis show that the number of trajectories
needed scales gracefully.
","Efficient, Safe, and Probably Approximately Complete Learning of Action
  Models",Roni Stern and Brendan Juba,2017,Artificial Intelligence,
"  Humans are expert in the amount of sensory data they deal with each moment.
Human brain not only analyses these data but also starts synthesizing new
information from the existing data. The current age Big-data systems are needed
not just to analyze data but also to come up new interpretation. We believe
that the pivotal ability in human brain which enables us to do this is what is
known as ""intuition"". Here, we present an intuition based architecture for big
data analysis and synthesis.
","ICABiDAS: Intuition Centred Architecture for Big Data Analysis and
  Synthesis",Amit Kumar Mishra,2018,Artificial Intelligence,
"  This paper introduces a cognitive architecture for a humanoid robot to engage
in a proactive, mixed-initiative exploration and manipulation of its
environment, where the initiative can originate from both the human and the
robot. The framework, based on a biologically-grounded theory of the brain and
mind, integrates a reactive interaction engine, a number of state-of-the-art
perceptual and motor learning algorithms, as well as planning abilities and an
autobiographical memory. The architecture as a whole drives the robot behavior
to solve the symbol grounding problem, acquire language capabilities, execute
goal-oriented behavior, and express a verbal narrative of its own experience in
the world. We validate our approach in human-robot interaction experiments with
the iCub humanoid robot, showing that the proposed cognitive architecture can
be applied in real time within a realistic scenario and that it can be used
with naive users.
","DAC-h3: A Proactive Robot Cognitive Architecture to Acquire and Express
  Knowledge About the World and the Self","Cl\'ement Moulin-Frier, Tobias Fischer, Maxime Petit, Gr\'egoire
  Pointeau, Jordi-Ysard Puigbo, Ugo Pattacini, Sock Ching Low, Daniel
  Camilleri, Phuong Nguyen, Matej Hoffmann, Hyung Jin Chang, Martina Zambelli,
  Anne-Laure Mealier, Andreas Damianou, Giorgio Metta, Tony J. Prescott,
  Yiannis Demiris, Peter Ford Dominey, Paul F. M. J. Verschure",2018,Artificial Intelligence,
"  Inductive Logic Programming (ILP) combines rule-based and statistical
artificial intelligence methods, by learning a hypothesis comprising a set of
rules given background knowledge and constraints for the search space. We focus
on extending the XHAIL algorithm for ILP which is based on Answer Set
Programming and we evaluate our extensions using the Natural Language
Processing application of sentence chunking. With respect to processing natural
language, ILP can cater for the constant change in how we use language on a
daily basis. At the same time, ILP does not require huge amounts of training
examples such as other statistical methods and produces interpretable results,
that means a set of rules, which can be analysed and tweaked if necessary. As
contributions we extend XHAIL with (i) a pruning mechanism within the
hypothesis generalisation algorithm which enables learning from larger
datasets, (ii) a better usage of modern solver technology using recently
developed optimisation methods, and (iii) a time budget that permits the usage
of suboptimal results. We evaluate these improvements on the task of sentence
chunking using three datasets from a recent SemEval competition. Results show
that our improvements allow for learning on bigger datasets with results that
are of similar quality to state-of-the-art systems on the same task. Moreover,
we compare the hypotheses obtained on datasets to gain insights on the
structure of each dataset.
","Improving Scalability of Inductive Logic Programming via Pruning and
  Best-Effort Optimisation","Mishal Kazmi and Peter Sch\""uller and Y\""ucel Sayg{\i}n",2017,Artificial Intelligence,
"  Answer Set Programming (ASP) is a well-established declarative paradigm. One
of the successes of ASP is the availability of efficient systems.
State-of-the-art systems are based on the ground+solve approach. In some
applications this approach is infeasible because the grounding of one or few
constraints is expensive. In this paper, we systematically compare alternative
strategies to avoid the instantiation of problematic constraints, that are
based on custom extensions of the solver. Results on real and synthetic
benchmarks highlight some strengths and weaknesses of the different strategies.
(Under consideration for acceptance in TPLP, ICLP 2017 Special Issue.)
","Constraints, Lazy Constraints, or Propagators in ASP Solving: An
  Empirical Analysis","Bernardo Cuteri, Carmine Dodaro, Francesco Ricca, Peter Sch\""uller",2017,Artificial Intelligence,
"  The paper investigates navigability with imperfect information. It shows that
the properties of navigability with perfect recall are exactly those captured
by Armstrong's axioms from the database theory. If the assumption of perfect
recall is omitted, then Armstrong's transitivity axiom is not valid, but it can
be replaced by two new weaker principles. The main technical results are
soundness and completeness theorems for the logical systems describing
properties of navigability with and without perfect recall.
",Armstrong's Axioms and Navigation Strategies,Kaya Deuser and Pavel Naumov,2018,Artificial Intelligence,
"  As intelligent systems gain autonomy and capability, it becomes vital to
ensure that their objectives match those of their human users; this is known as
the value-alignment problem. In robotics, value alignment is key to the design
of collaborative robots that can integrate into human workflows, successfully
inferring and adapting to their users' objectives as they go. We argue that a
meaningful solution to value alignment must combine multi-agent decision theory
with rich mathematical models of human cognition, enabling robots to tap into
people's natural collaborative capabilities. We present a solution to the
cooperative inverse reinforcement learning (CIRL) dynamic game based on
well-established cognitive models of decision making and theory of mind. The
solution captures a key reciprocity relation: the human will not plan her
actions in isolation, but rather reason pedagogically about how the robot might
learn from them; the robot, in turn, can anticipate this and interpret the
human's actions pragmatically. To our knowledge, this work constitutes the
first formal analysis of value alignment grounded in empirically validated
cognitive models.
",Pragmatic-Pedagogic Value Alignment,"Jaime F. Fisac, Monica A. Gates, Jessica B. Hamrick, Chang Liu, Dylan
  Hadfield-Menell, Malayandi Palaniappan, Dhruv Malik, S. Shankar Sastry,
  Thomas L. Griffiths, and Anca D. Dragan",2017,Artificial Intelligence,
"  Sequential pattern mining algorithms are widely used to explore care pathways
database, but they generate a deluge of patterns, mostly redundant or useless.
Clinicians need tools to express complex mining queries in order to generate
less but more significant patterns. These algorithms are not versatile enough
to answer complex clinician queries. This article proposes to apply a
declarative pattern mining approach based on Answer Set Programming paradigm.
It is exemplified by a pharmaco-epidemiological study investigating the
possible association between hospitalization for seizure and antiepileptic drug
switch from a french medico-administrative database.
",Declarative Sequential Pattern Mining of Care Pathways,"Thomas Guyet (1), Andr\'e Happe, Yann Dauxais (2) ((1) LACODAM, (2)
  UR1)",2017,Artificial Intelligence,
"  Nowadays, there are many approaches designed for the task of detecting
communities in social networks. Among them, some methods only consider the
topological graph structure, while others take use of both the graph structure
and the node attributes. In real-world networks, there are many uncertain and
noisy attributes in the graph. In this paper, we will present how we detect
communities in graphs with uncertain attributes in the first step. The
numerical, probabilistic as well as evidential attributes are generated
according to the graph structure. In the second step, some noise will be added
to the attributes. We perform experiments on graphs with different types of
attributes and compare the detection results in terms of the Normalized Mutual
Information (NMI) values. The experimental results show that the clustering
with evidential attributes gives better results comparing to those with
probabilistic and numerical attributes. This illustrates the advantages of
evidential attributes.
",The Advantage of Evidential Attributes in Social Networks,"Salma Ben Dhaou (LARODEC, DRUID), Kuang Zhou (NPU), Mouloud Kharoune
  (DRUID), Arnaud Martin (DRUID), Boutheina Ben Yaghlane (LARODEC)",2017,Artificial Intelligence,
"  Traffic speed is a key indicator for the efficiency of an urban
transportation system. Accurate modeling of the spatiotemporally varying
traffic speed thus plays a crucial role in urban planning and development. This
paper addresses the problem of efficient fine-grained traffic speed prediction
using big traffic data obtained from static sensors. Gaussian processes (GPs)
have been previously used to model various traffic phenomena, including flow
and speed. However, GPs do not scale with big traffic data due to their cubic
time complexity. In this work, we address their efficiency issues by proposing
local GPs to learn from and make predictions for correlated subsets of data.
The main idea is to quickly group speed variables in both spatial and temporal
dimensions into a finite number of clusters, so that future and unobserved
traffic speed queries can be heuristically mapped to one of such clusters. A
local GP corresponding to that cluster can then be trained on the fly to make
predictions in real-time. We call this method localization. We use non-negative
matrix factorization for localization and propose simple heuristics for cluster
mapping. We additionally leverage on the expressiveness of GP kernel functions
to model road network topology and incorporate side information. Extensive
experiments using real-world traffic data collected in the two U.S. cities of
Pittsburgh and Washington, D.C., show that our proposed local GPs significantly
improve both runtime performances and prediction accuracies compared to the
baseline global and local GPs.
","Local Gaussian Processes for Efficient Fine-Grained Traffic Speed
  Prediction","Truc Viet Le, Richard J. Oentaryo, Siyuan Liu, Hoong Chuin Lau",2017,Artificial Intelligence,
"  Traditionally psychometric tests were used for profiling incoming workers.
These methods use DISC profiling method to classify people into distinct
personality types, which are further used to predict if a person may be a
possible fit to the organizational culture. This concept is taken further by
introducing a novel technique to predict if a particular pair of an incoming
worker and the manager being assigned are compatible at a psychological scale.
This is done using multilayer perceptron neural network which can be adaptively
trained to showcase the true nature of the compatibility index. The proposed
prototype model is used to quantify the relevant attributes, use them to train
the prediction engine, and to define the data pipeline required for it.
","An Automated Compatibility Prediction Engine using DISC Theory Based
  Classification and Neural Networks","Chandrasekaran Anirudh Bhardwaj, Megha Mishra and Sweetlin Hemalatha",2017,Artificial Intelligence,
"  We present theoretical analysis and a suite of tests and procedures for
addressing a broad class of redundant and misleading association rules we call
\emph{specious rules}. Specious dependencies, also known as \emph{spurious},
\emph{apparent}, or \emph{illusory associations}, refer to a well-known
phenomenon where marginal dependencies are merely products of interactions with
other variables and disappear when conditioned on those variables.
  The most extreme example is Yule-Simpson's paradox where two variables
present positive dependence in the marginal contingency table but negative in
all partial tables defined by different levels of a confounding factor. It is
accepted wisdom that in data of any nontrivial dimensionality it is infeasible
to control for all of the exponentially many possible confounds of this nature.
In this paper, we consider the problem of specious dependencies in the context
of statistical association rule mining. We define specious rules and show they
offer a unifying framework which covers many types of previously proposed
redundant or misleading association rules. After theoretical analysis, we
introduce practical algorithms for detecting and pruning out specious
association rules efficiently under many key goodness measures, including
mutual information and exact hypergeometric probabilities. We demonstrate that
the procedure greatly reduces the number of associations discovered, providing
an elegant and effective solution to the problem of association mining
discovering large numbers of misleading and redundant rules.
","Specious rules: an efficient and effective unifying method for removing
  misleading and uninformative patterns in association rule mining","Wilhelmiina H\""am\""al\""ainen and Geoffrey I. Webb",2017,Artificial Intelligence,
"  The incorporation of macro-actions (temporally extended actions) into
multi-agent decision problems has the potential to address the curse of
dimensionality associated with such decision problems. Since macro-actions last
for stochastic durations, multiple agents executing decentralized policies in
cooperative environments must act asynchronously. We present an algorithm that
modifies generalized advantage estimation for temporally extended actions,
allowing a state-of-the-art policy optimization algorithm to optimize policies
in Dec-POMDPs in which agents act asynchronously. We show that our algorithm is
capable of learning optimal policies in two cooperative domains, one involving
real-time bus holding control and one involving wildfire fighting with unmanned
aircraft. Our algorithm works by framing problems as ""event-driven decision
processes,"" which are scenarios in which the sequence and timing of actions and
events are random and governed by an underlying stochastic process. In addition
to optimizing policies with continuous state and action spaces, our algorithm
also facilitates the use of event-driven simulators, which do not require time
to be discretized into time-steps. We demonstrate the benefit of using
event-driven simulation in the context of multiple agents taking asynchronous
actions. We show that fixed time-step simulation risks obfuscating the sequence
in which closely separated events occur, adversely affecting the policies
learned. In addition, we show that arbitrarily shrinking the time-step scales
poorly with the number of agents.
","Deep Reinforcement Learning for Event-Driven Multi-Agent Decision
  Processes","Kunal Menda, Yi-Chun Chen, Justin Grana, James W. Bono, Brendan D.
  Tracey, Mykel J. Kochenderfer, and David Wolpert",2019,Artificial Intelligence,
"  We present PRM-RL, a hierarchical method for long-range navigation task
completion that combines sampling based path planning with reinforcement
learning (RL). The RL agents learn short-range, point-to-point navigation
policies that capture robot dynamics and task constraints without knowledge of
the large-scale topology. Next, the sampling-based planners provide roadmaps
which connect robot configurations that can be successfully navigated by the RL
agent. The same RL agents are used to control the robot under the direction of
the planning, enabling long-range navigation. We use the Probabilistic Roadmaps
(PRMs) for the sampling-based planner. The RL agents are constructed using
feature-based and deep neural net policies in continuous state and action
spaces. We evaluate PRM-RL, both in simulation and on-robot, on two navigation
tasks with non-trivial robot dynamics: end-to-end differential drive indoor
navigation in office environments, and aerial cargo delivery in urban
environments with load displacement constraints. Our results show improvement
in task completion over both RL agents on their own and traditional
sampling-based planners. In the indoor navigation task, PRM-RL successfully
completes up to 215 m long trajectories under noisy sensor conditions, and the
aerial cargo delivery completes flights over 1000 m without violating the task
constraints in an environment 63 million times larger than used in training.
","PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement
  Learning and Sampling-based Planning","Aleksandra Faust, Oscar Ramirez, Marek Fiser, Kenneth Oslund, Anthony
  Francis, James Davidson, and Lydia Tapia",2018,Artificial Intelligence,
"  Social dilemmas, where mutual cooperation can lead to high payoffs but
participants face incentives to cheat, are ubiquitous in multi-agent
interaction. We wish to construct agents that cooperate with pure cooperators,
avoid exploitation by pure defectors, and incentivize cooperation from the
rest. However, often the actions taken by a partner are (partially) unobserved
or the consequences of individual actions are hard to predict. We show that in
a large class of games good strategies can be constructed by conditioning one's
behavior solely on outcomes (ie. one's past rewards). We call this
consequentialist conditional cooperation. We show how to construct such
strategies using deep reinforcement learning techniques and demonstrate, both
analytically and experimentally, that they are effective in social dilemmas
beyond simple matrix games. We also show the limitations of relying purely on
consequences and discuss the need for understanding both the consequences of
and the intentions behind an action.
","Consequentialist conditional cooperation in social dilemmas with
  imperfect information","Alexander Peysakhovich, Adam Lerer",2018,Artificial Intelligence,
"  We use decision trees to build a helpdesk agent reference network to
facilitate the on-the-job advising of junior or less experienced staff on how
to better address telecommunication customer fault reports. Such reports
generate field measurements and remote measurements which, when coupled with
location data and client attributes, and fused with organization-level
statistics, can produce models of how support should be provided. Beyond
decision support, these models can help identify staff who can act as advisors,
based on the quality, consistency and predictability of dealing with complex
troubleshooting reports. Advisor staff models are then used to guide less
experienced staff in their decision making; thus, we advocate the deployment of
a simple mechanism which exploits the availability of staff with a sound track
record at the helpdesk to act as dormant tutors.
",Decision Trees for Helpdesk Advisor Graphs,Spyros Gkezerlis and Dimitris Kalles,2016,Artificial Intelligence,
"  Little by little, newspapers are revealing the bright future that Artificial
Intelligence (AI) is building. Intelligent machines will help everywhere.
However, this bright future has a dark side: a dramatic job market contraction
before its unpredictable transformation. Hence, in a near future, large numbers
of job seekers will need financial support while catching up with these novel
unpredictable jobs. This possible job market crisis has an antidote inside. In
fact, the rise of AI is sustained by the biggest knowledge theft of the recent
years. Learning AI machines are extracting knowledge from unaware skilled or
unskilled workers by analyzing their interactions. By passionately doing their
jobs, these workers are digging their own graves.
  In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI)
as a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward
aware and unaware knowledge producers with a different scheme: decisions of AI
systems generating revenues will repay the legitimate owners of the knowledge
used for taking those decisions. As modern Robin Hoods, HIT-AI researchers
should fight for a fairer Artificial Intelligence that gives back what it
steals.
",Human-in-the-loop Artificial Intelligence,Fabio Massimo Zanzotto,2019,Artificial Intelligence,
"  The Advent of the Internet-of-Things (IoT) paradigm has brought opportunities
to solve many real-world problems. Energy management, for example, has
attracted huge interest from academia, industries, governments and regulatory
bodies. It involves collecting energy usage data, analyzing it, and optimizing
the energy consumption by applying control strategies. However, in industrial
environments, performing such optimization is not trivial. The changes in
business rules, process control, and customer requirements make it much more
challenging. In this paper, a Semantic Rules Engine (SRE) for industrial
gateways is presented that allows implementing dynamic and flexible rule-based
control strategies. It is simple, expressive, and allows managing rules
on-the-fly without causing any service interruption. Additionally, it can
handle semantic queries and provide results by inferring additional knowledge
from previously defined concepts in ontologies. SRE has been validated and
tested on different hardware platforms and in commercial products. Performance
evaluations are also presented to validate its conformance to the customer
requirements.
","SRE: Semantic Rules Engine For the Industrial Internet-Of-Things
  Gateways","Charbel El Kaed, Imran Khan, Andre Van Den Berg, Hicham Hossayni and
  Christophe Saint-Marcel",2017,Artificial Intelligence,
"  The semantic web has received many contributions of researchers as ontologies
which, in this context, i.e. within RDF linked data, are formalized
conceptualizations that might use different protocols, such as RDFS, OWL DL and
OWL FULL. In this article, we describe new expressive techniques which were
found necessary after elaborating dozens of OWL ontologies for the scientific
academy, the State and the civil society. They consist in: 1) stating possible
uses a property might have without incurring into axioms or restrictions; 2)
assigning a level of priority for an element (class, property, triple); 3)
correct depiction in diagrams of relations between classes, between individuals
which are imperative, and between individuals which are optional; 4) a
convenient association between OWL classes and SKOS concepts. We propose
specific rules to accomplish these enhancements and exemplify both its use and
the difficulties that arise because these techniques are currently not
established as standards to the ontology designer.
",Enhancements of linked data expressiveness for ontologies,Renato Fabbri,2017,Artificial Intelligence,
"  There has been much discussion of the right to explanation in the EU General
Data Protection Regulation, and its existence, merits, and disadvantages.
Implementing a right to explanation that opens the black box of algorithmic
decision-making faces major legal and technical barriers. Explaining the
functionality of complex algorithmic decision-making systems and their
rationale in specific cases is a technically challenging problem. Some
explanations may offer little meaningful information to data subjects, raising
questions around their value. Explanations of automated decisions need not
hinge on the general public understanding how algorithmic systems function.
Even though such interpretability is of great importance and should be pursued,
explanations can, in principle, be offered without opening the black box.
Looking at explanations as a means to help a data subject act rather than
merely understand, one could gauge the scope and content of explanations
according to the specific goal or action they are intended to support. From the
perspective of individuals affected by automated decision-making, we propose
three aims for explanations: (1) to inform and help the individual understand
why a particular decision was reached, (2) to provide grounds to contest the
decision if the outcome is undesired, and (3) to understand what would need to
change in order to receive a desired result in the future, based on the current
decision-making model. We assess how each of these goals finds support in the
GDPR. We suggest data controllers should offer a particular type of
explanation, unconditional counterfactual explanations, to support these three
aims. These counterfactual explanations describe the smallest change to the
world that can be made to obtain a desirable outcome, or to arrive at the
closest possible world, without needing to explain the internal logic of the
system.
","Counterfactual Explanations without Opening the Black Box: Automated
  Decisions and the GDPR","Sandra Wachter, Brent Mittelstadt, Chris Russell",2018,Artificial Intelligence,
"  In this paper we analyse the benefits of incorporating interval-valued fuzzy
sets into the Bousi-Prolog system. A syntax, declarative semantics and im-
plementation for this extension is presented and formalised. We show, by using
potential applications, that fuzzy logic programming frameworks enhanced with
them can correctly work together with lexical resources and ontologies in order
to improve their capabilities for knowledge representation and reasoning.
","On the incorporation of interval-valued fuzzy sets into the Bousi-Prolog
  system: declarative semantics, implementation and applications","Clemente Rubio-Manzano, Martin Pereira-Fari\~na",2018,Artificial Intelligence,
"  Cooperative multi-agent planning (MAP) is a relatively recent research field
that combines technologies, algorithms and techniques developed by the
Artificial Intelligence Planning and Multi-Agent Systems communities. While
planning has been generally treated as a single-agent task, MAP generalizes
this concept by considering multiple intelligent agents that work cooperatively
to develop a course of action that satisfies the goals of the group.
  This paper reviews the most relevant approaches to MAP, putting the focus on
the solvers that took part in the 2015 Competition of Distributed and
Multi-Agent Planning, and classifies them according to their key features and
relative performance.
",Cooperative Multi-Agent Planning: A Survey,"Alejandro Torre\~no, Eva Onaindia, Anton\'in Komenda, Michal
  \v{S}tolba",2017,Artificial Intelligence,
"  Interval Pairwise Comparison Matrices have been widely used to account for
uncertain statements concerning the preferences of decision makers. Several
approaches have been proposed in the literature, such as multiplicative and
fuzzy interval matrices. In this paper, we propose a general unified approach
to Interval Pairwise Comparison Matrices, based on Abelian linearly ordered
groups. In this framework, we generalize some consistency conditions provided
for multiplicative and/or fuzzy interval pairwise comparison matrices and
provide inclusion relations between them. Then, we provide a concept of
distance between intervals that, together with a notion of mean defined over
real continuous Abelian linearly ordered groups, allows us to provide a
consistency index and an indeterminacy index. In this way, by means of suitable
isomorphisms between Abelian linearly ordered groups, we will be able to
compare the inconsistency and the indeterminacy of different kinds of Interval
Pairwise Comparison Matrices, e.g. multiplicative, additive, and fuzzy, on a
unique Cartesian coordinate system.
",A general unified framework for interval pairwise comparison matrices,Bice Cavallo and Matteo Brunelli,2018,Artificial Intelligence,
"  Artificial Intelligence is a central topic in the computer science
curriculum. From the year 2011 a project-based learning methodology based on
computer games has been designed and implemented into the intelligence
artificial course at the University of the Bio-Bio. The project aims to develop
software-controlled agents (bots) which are programmed by using heuristic
algorithms seen during the course. This methodology allows us to obtain good
learning results, however several challenges have been founded during its
implementation.
  In this paper we show how linguistic descriptions of data can help to provide
students and teachers with technical and personalized feedback about the
learned algorithms. Algorithm behavior profile and a new Turing test for
computer games bots based on linguistic modelling of complex phenomena are also
proposed in order to deal with such challenges.
  In order to show and explore the possibilities of this new technology, a web
platform has been designed and implemented by one of authors and its
incorporation in the process of assessment allows us to improve the teaching
learning process.
","How linguistic descriptions of data can help to the teaching-learning
  process in higher education, case of study: artificial intelligence","Clemente Rubio-Manzano, Tomas Lermanda Senoceain",2019,Artificial Intelligence,
"  Recently, model-free reinforcement learning algorithms have been shown to
solve challenging problems by learning from extensive interaction with the
environment. A significant issue with transferring this success to the robotics
domain is that interaction with the real world is costly, but training on
limited experience is prone to overfitting. We present a method for learning to
navigate, to a fixed goal and in a known environment, on a mobile robot. The
robot leverages an interactive world model built from a single traversal of the
environment, a pre-trained visual feature encoder, and stochastic environmental
augmentation, to demonstrate successful zero-shot transfer under real-world
environmental variations without fine-tuning.
","One-Shot Reinforcement Learning for Robot Navigation with Interactive
  Replay","Jake Bruce, Niko Suenderhauf, Piotr Mirowski, Raia Hadsell, Michael
  Milford",2017,Artificial Intelligence,
"  Web-based human trafficking activity has increased in recent years but it
remains sparsely dispersed among escort advertisements and difficult to
identify due to its often-latent nature. The use of intelligent systems to
detect trafficking can thus have a direct impact on investigative resource
allocation and decision-making, and, more broadly, help curb a widespread
social problem. Trafficking detection involves assigning a normalized score to
a set of escort advertisements crawled from the Web -- a higher score indicates
a greater risk of trafficking-related (involuntary) activities. In this paper,
we define and study the problem of trafficking detection and present a
trafficking detection pipeline architecture developed over three years of
research within the DARPA Memex program. Drawing on multi-institutional data,
systems, and experiences collected during this time, we also conduct post hoc
bias analyses and present a bias mitigation plan. Our findings show that, while
automatic trafficking detection is an important application of AI for social
good, it also provides cautionary lessons for deploying predictive machine
learning algorithms without appropriate de-biasing. This ultimately led to
integration of an interpretable solution into a search system that contains
over 100 million advertisements and is used by over 200 law enforcement
agencies to investigate leads.
","Always Lurking: Understanding and Mitigating Bias in Online Human
  Trafficking Detection","Kyle Hundman, Thamme Gowda, Mayank Kejriwal, and Benedikt Boecking",2018,Artificial Intelligence,
"  Due to increasing urban population and growing number of motor vehicles,
traffic congestion is becoming a major problem of the 21st century. One of the
main reasons behind traffic congestion is accidents which can not only result
in casualties and losses for the participants, but also in wasted and lost time
for the others that are stuck behind the wheels. Early detection of an accident
can save lives, provides quicker road openings, hence decreases wasted time and
resources, and increases efficiency. In this study, we propose a preliminary
real-time autonomous accident-detection system based on computational
intelligence techniques. Istanbul City traffic-flow data for the year 2015 from
various sensor locations are populated using big data processing methodologies.
The extracted features are then fed into a nearest neighbor model, a regression
tree, and a feed-forward neural network model. For the output, the possibility
of an occurrence of an accident is predicted. The results indicate that even
though the number of false alarms dominates the real accident cases, the system
can still provide useful information that can be used for status verification
and early reaction to possible accidents.
","A Real-Time Autonomous Highway Accident Detection Model Based on Big
  Data Processing and Computational Intelligence","A. Murat Ozbayoglu, Gokhan Kucukayan, Erdogan Dogdu",2016,Artificial Intelligence,
"  Literature involving preferences of artificial agents or human beings often
assume their preferences can be represented using a complete transitive binary
relation. Much has been written however on different models of preferences. We
review some of the reasons that have been put forward to justify more complex
modeling, and review some of the techniques that have been proposed to obtain
models of such preferences.
",Reasons and Means to Model Preferences as Incomplete,"Olivier Cailloux (LAMSADE), S\'ebastien Destercke (Labex MS2T)",2017,Artificial Intelligence,
"  Artificial intelligence (AI) is an extensive scientific discipline which
enables computer systems to solve problems by emulating complex biological
processes such as learning, reasoning and self-correction. This paper presents
a comprehensive review of the application of AI techniques for improving
performance of optical communication systems and networks. The use of AI-based
techniques is first studied in applications related to optical transmission,
ranging from the characterization and operation of network components to
performance monitoring, mitigation of nonlinearities, and quality of
transmission estimation. Then, applications related to optical network control
and management are also reviewed, including topics like optical network
planning and operation in both transport and access networks. Finally, the
paper also presents a summary of opportunities and challenges in optical
networking where AI is expected to play a key role in the near future.
","Artificial Intelligence (AI) Methods in Optical Networks: A
  Comprehensive Survey","Javier Mata, Ignacio de Miguel, Ram\'o n J. Dur\'a n, Noem\'i Merayo,
  Sandeep Kumar Singh, Admela Jukan, Mohit Chamania",2018,Artificial Intelligence,
"  We propose a deep learning model - Probabilistic Prognostic Estimates of
Survival in Metastatic Cancer Patients (PPES-Met) for estimating short-term
life expectancy (3 months) of the patients by analyzing free-text clinical
notes in the electronic medical record, while maintaining the temporal visit
sequence. In a single framework, we integrated semantic data mapping and neural
embedding technique to produce a text processing method that extracts relevant
information from heterogeneous types of clinical notes in an unsupervised
manner, and we designed a recurrent neural network to model the temporal
dependency of the patient visits. The model was trained on a large dataset
(10,293 patients) and validated on a separated dataset (1818 patients). Our
method achieved an area under the ROC curve (AUC) of 0.89. To provide
explain-ability, we developed an interactive graphical tool that may improve
physician understanding of the basis for the model's predictions. The high
accuracy and explain-ability of the PPES-Met model may enable our model to be
used as a decision support tool to personalize metastatic cancer treatment and
provide valuable assistance to the physicians.
","Abstract: Probabilistic Prognostic Estimates of Survival in Metastatic
  Cancer Patients","Imon Banerjee, Michael Francis Gensheimer, Douglas J. Wood, Solomon
  Henry, Daniel Chang, Daniel L. Rubin",2018,Artificial Intelligence,
"  Pairwise comparison matrices often exhibit inconsistency, therefore many
indices have been suggested to measure their deviation from a consistent
matrix. A set of axioms has been proposed recently that is required to be
satisfied by any reasonable inconsistency index. This set seems to be not
exhaustive as illustrated by an example, hence it is expanded by adding two new
properties. All axioms are considered on the set of triads, pairwise comparison
matrices with three alternatives, which is the simplest case of inconsistency.
We choose the logically independent properties and prove that they
characterize, that is, uniquely determine the inconsistency ranking induced by
most inconsistency indices that coincide on this restricted domain. Since
triads play a prominent role in a number of inconsistency indices, our results
can also contribute to the measurement of inconsistency for pairwise comparison
matrices with more than three alternatives.
",Axiomatizations of inconsistency indices for triads,L\'aszl\'o Csat\'o,2019,Artificial Intelligence,
"  Humans use signs, e.g., sentences in a spoken language, for communication and
thought. Hence, symbol systems like language are crucial for our communication
with other agents and adaptation to our real-world environment. The symbol
systems we use in our human society adaptively and dynamically change over
time. In the context of artificial intelligence (AI) and cognitive systems, the
symbol grounding problem has been regarded as one of the central problems
related to {\it symbols}. However, the symbol grounding problem was originally
posed to connect symbolic AI and sensorimotor information and did not consider
many interdisciplinary phenomena in human communication and dynamic symbol
systems in our society, which semiotics considered. In this paper, we focus on
the symbol emergence problem, addressing not only cognitive dynamics but also
the dynamics of symbol systems in society, rather than the symbol grounding
problem. We first introduce the notion of a symbol in semiotics from the
humanities, to leave the very narrow idea of symbols in symbolic AI.
Furthermore, over the years, it became more and more clear that symbol
emergence has to be regarded as a multifaceted problem. Therefore, secondly, we
review the history of the symbol emergence problem in different fields,
including both biological and artificial systems, showing their mutual
relations. We summarize the discussion and provide an integrative viewpoint and
comprehensive overview of symbol emergence in cognitive systems. Additionally,
we describe the challenges facing the creation of cognitive systems that can be
part of symbol emergence systems.
",Symbol Emergence in Cognitive Developmental Systems: a Survey,"Tadahiro Taniguchi, Emre Ugur, Matej Hoffmann, Lorenzo Jamone,
  Takayuki Nagai, Benjamin Rosman, Toshihiko Matsuka, Naoto Iwahashi, Erhan
  Oztop, Justus Piater, Florentin W\""org\""otter",2019,Artificial Intelligence,
"  Semantic Web Rule Language (SWRL) combines OWL (Web Ontology Language)
ontologies with Horn Logic rules of the Rule Markup Language (RuleML) family.
Being supported by ontology editors, rule engines and ontology reasoners, it
has become a very popular choice for developing rule-based applications on top
of ontologies. However, SWRL is probably not go-ing to become a WWW Consortium
standard, prohibiting industrial acceptance. On the other hand, SPIN (SPARQL
Inferencing Notation) has become a de-facto industry standard to rep-resent
SPARQL rules and constraints on Semantic Web models, building on the widespread
acceptance of SPARQL (SPARQL Protocol and RDF Query Language). In this paper,
we ar-gue that the life of existing SWRL rule-based ontology applications can
be prolonged by con-verting them to SPIN. To this end, we have developed the
SWRL2SPIN tool in Prolog that transforms SWRL rules into SPIN rules,
considering the object-orientation of SPIN, i.e. linking rules to the
appropriate ontology classes and optimizing them, as derived by analysing the
rule conditions.
","SWRL2SPIN: A tool for transforming SWRL rule bases in OWL ontologies to
  object-oriented SPIN rules",Nick Bassiliades,2020,Artificial Intelligence,
"  We consider a team of reinforcement learning agents that concurrently learn
to operate in a common environment. We identify three properties - adaptivity,
commitment, and diversity - which are necessary for efficient coordinated
exploration and demonstrate that straightforward extensions to single-agent
optimistic and posterior sampling approaches fail to satisfy them. As an
alternative, we propose seed sampling, which extends posterior sampling in a
manner that meets these requirements. Simulation results investigate how
per-agent regret decreases as the number of agents grows, establishing
substantial advantages of seed sampling over alternative exploration schemes.
",Coordinated Exploration in Concurrent Reinforcement Learning,"Maria Dimakopoulou, Benjamin Van Roy",2018,Artificial Intelligence,
"  The Semantic Web aims at representing knowledge about the real world at web
scale - things, their attributes and relationships among them can be
represented as nodes and edges in an inter-linked semantic graph. In the
presence of noisy data, as is typical of data on the Semantic Web, a software
Agent needs to be able to robustly infer one or more associated actionable
classes for the individuals in order to act automatically on it. We model this
problem as a multi-label classification task where we want to robustly identify
types of the individuals in a semantic graph such as DBpedia, which we use as
an exemplary dataset on the Semantic Web. Our approach first extracts multiple
features for the individuals using random walks and then performs multi-label
classification using fully-connected Neural Networks. Through systematic
exploration and experimentation, we identify the effect of hyper-parameters of
the feature extraction and the fully-connected Neural Network structure on the
classification performance. Our final results show that our method performs
better than state-of-the-art inferencing systems like SDtype and SLCN, from
which we can conclude that random-walk-based feature extraction of individuals
and their multi-label classification using Deep Neural Networks is a promising
alternative to these systems for type classification of individuals on the
Semantic Web. The main contribution of our work is to introduce a novel
approach that allows us to use Deep Neural Networks to identify types of
individuals in a noisy semantic graph by extracting features using random walks
",Classification of Things in DBpedia using Deep Neural Networks,Rahul Parundekar,2018,Artificial Intelligence,
"  We propose to use a supervised machine learning technique to track the
location of a mobile agent in real time. Hidden Markov Models are used to build
artificial intelligence that estimates the unknown position of a mobile target
moving in a defined environment. This narrow artificial intelligence performs
two distinct tasks. First, it provides real-time estimation of the mobile
agent's position using the forward algorithm. Second, it uses the Baum-Welch
algorithm as a statistical learning tool to gain knowledge of the mobile
target. Finally, an experimental environment is proposed, namely a video game
that we use to test our artificial intelligence. We present statistical and
graphical results to illustrate the efficiency of our method.
","Narrow Artificial Intelligence with Machine Learning for Real-Time
  Estimation of a Mobile Agents Location Using Hidden Markov Models",C\'edric Beaulac and Fabrice Larribe,2017,Artificial Intelligence,
"  The problem of detecting bots, automated social media accounts governed by
software but disguising as human users, has strong implications. For example,
bots have been used to sway political elections by distorting online discourse,
to manipulate the stock market, or to push anti-vaccine conspiracy theories
that caused health epidemics. Most techniques proposed to date detect bots at
the account level, by processing large amount of social media posts, and
leveraging information from network structure, temporal dynamics, sentiment
analysis, etc.
  In this paper, we propose a deep neural network based on contextual long
short-term memory (LSTM) architecture that exploits both content and metadata
to detect bots at the tweet level: contextual features are extracted from user
metadata and fed as auxiliary input to LSTM deep nets processing the tweet
text.
  Another contribution that we make is proposing a technique based on synthetic
minority oversampling to generate a large labeled dataset, suitable for deep
nets training, from a minimal amount of labeled data (roughly 3,000 examples of
sophisticated Twitter bots). We demonstrate that, from just one single tweet,
our architecture can achieve high classification accuracy (AUC > 96%) in
separating bots from humans.
  We apply the same architecture to account-level bot detection, achieving
nearly perfect classification accuracy (AUC > 99%). Our system outperforms
previous state of the art while leveraging a small and interpretable set of
features yet requiring minimal training data.
",Deep Neural Networks for Bot Detection,"Sneha Kudugunta, Emilio Ferrara",2018,Artificial Intelligence,
"  We address the problem of inferring the causal direction between two
variables by comparing the least-squares errors of the predictions in both
possible directions. Under the assumption of an independence between the
function relating cause and effect, the conditional noise distribution, and the
distribution of the cause, we show that the errors are smaller in causal
direction if both variables are equally scaled and the causal relation is close
to deterministic. Based on this, we provide an easily applicable algorithm that
only requires a regression in both possible causal directions and a comparison
of the errors. The performance of the algorithm is compared with various
related causal inference methods in different artificial and real-world data
sets.
",Analysis of cause-effect inference by comparing regression errors,"Patrick Bl\""obaum, Dominik Janzing, Takashi Washio, Shohei Shimizu,
  Bernhard Sch\""olkopf",2019,Artificial Intelligence,
"  This paper proposes a class of well-conditioned neural networks in which a
unit amount of change in the inputs causes at most a unit amount of change in
the outputs or any of the internal layers. We develop the known methodology of
controlling Lipschitz constants to realize its full potential in maximizing
robustness, with a new regularization scheme for linear layers, new ways to
adapt nonlinearities and a new loss function. With MNIST and CIFAR-10
classifiers, we demonstrate a number of advantages. Without needing any
adversarial training, the proposed classifiers exceed the state of the art in
robustness against white-box L2-bounded adversarial attacks. They generalize
better than ordinary networks from noisy data with partially random labels.
Their outputs are quantitatively meaningful and indicate levels of confidence
and generalization, among other desirable properties.
",L2-Nonexpansive Neural Networks,"Haifeng Qian, Mark N. Wegman",2019,Artificial Intelligence,
"  Reasoning systems with too simple a model of the world and human intent are
unable to consider potential negative side effects of their actions and modify
their plans to avoid them (e.g., avoiding potential errors). However,
hand-encoding the enormous and subtle body of facts that constitutes common
sense into a knowledge base has proved too difficult despite decades of work.
Distributed semantic vector spaces learned from large text corpora, on the
other hand, can learn representations that capture shades of meaning of
common-sense concepts and perform analogical and associational reasoning in
ways that knowledge bases are too rigid to perform, by encoding concepts and
the relations between them as geometric structures. These have, however, the
disadvantage of being unreliable, poorly understood, and biased in their view
of the world by the source material. This chapter will discuss how these
approaches may be combined in a way that combines the best properties of each
for understanding the world and human intentions in a richer way.
",Semantic Vector Spaces for Broadening Consideration of Consequences,Douglas Summers Stay,2017,Artificial Intelligence,
"  This paper offers a multi-disciplinary review of knowledge acquisition
methods in human activity systems. The review captures the degree of
involvement of various types of agencies in the knowledge acquisition process,
and proposes a classification with three categories of methods: the human
agent, the human-inspired agent, and the autonomous machine agent methods. In
the first two categories, the acquisition of knowledge is seen as a cognitive
task analysis exercise, while in the third category knowledge acquisition is
treated as an autonomous knowledge-discovery endeavour. The motivation for this
classification stems from the continuous change over time of the structure,
meaning and purpose of human activity systems, which are seen as the factor
that fuelled researchers' and practitioners' efforts in knowledge acquisition
for more than a century.
  We show through this review that the KA field is increasingly active due to
the higher and higher pace of change in human activity, and conclude by
discussing the emergence of a fourth category of knowledge acquisition methods,
which are based on red-teaming and co-evolution.
","A Multi-Disciplinary Review of Knowledge Acquisition Methods: From Human
  to Autonomous Eliciting Agents",George Leu and Hussein Abbass,2016,Artificial Intelligence,
"  What makes humans so good at solving seemingly complex video games? Unlike
computers, humans bring in a great deal of prior knowledge about the world,
enabling efficient decision making. This paper investigates the role of human
priors for solving video games. Given a sample game, we conduct a series of
ablation studies to quantify the importance of various priors on human
performance. We do this by modifying the video game environment to
systematically mask different types of visual information that could be used by
humans as priors. We find that removal of some prior knowledge causes a drastic
degradation in the speed with which human players solve the game, e.g. from 2
minutes to over 20 minutes. Furthermore, our results indicate that general
priors, such as the importance of objects and visual consistency, are critical
for efficient game-play. Videos and the game manipulations are available at
https://rach0012.github.io/humanRL_website/
",Investigating Human Priors for Playing Video Games,"Rachit Dubey, Pulkit Agrawal, Deepak Pathak, Thomas L. Griffiths, and
  Alexei A. Efros",2018,Artificial Intelligence,
"  The tasks that an agent will need to solve often are not known during
training. However, if the agent knows which properties of the environment are
important then, after learning how its actions affect those properties, it may
be able to use this knowledge to solve complex tasks without training
specifically for them. Towards this end, we consider a setup in which an
environment is augmented with a set of user defined attributes that
parameterize the features of interest. We propose a method that learns a policy
for transitioning between ""nearby"" sets of attributes, and maintains a graph of
possible transitions. Given a task at test time that can be expressed in terms
of a target set of attributes, and a current state, our model infers the
attributes of the current state and searches over paths through attribute space
to get a high level plan, and then uses its low level policy to execute the
plan. We show in 3D block stacking, grid-world games, and StarCraft that our
model is able to generalize to longer, more complex tasks at test time by
composing simpler learned policies.
",Composable Planning with Attributes,"Amy Zhang, Adam Lerer, Sainbayar Sukhbaatar, Rob Fergus, Arthur Szlam",2018,Artificial Intelligence,
"  The 6th International Workshop on Theorem proving components for Educational
software (ThEdu'17) was held in Gothenburg, Sweden, on 6 Aug 2017. It was
associated to the conference CADE26. Topics of interest include: methods of
automated deduction applied to checking students' input; methods of automated
deduction applied to prove post-conditions for particular problem solutions;
combinations of deduction and computation enabling systems to propose next
steps; automated provers specific for dynamic geometry systems; proof and
proving in mathematics education.
  ThEdu'17 was a vibrant workshop, with one invited talk and eight
contributions. It triggered the post-proceedings at hand.
","Proceedings 6th International Workshop on Theorem proving components for
  Educational software","Pedro Quaresma (University of Coimbra), Walther Neuper (IICM at Graz
  University of Technology)",2018,Artificial Intelligence,
"  This paper describes a process for combining patterns and features, to guide
a search process and make predictions. It is based on the functionality that a
human brain might have, which is a highly distributed network of simple
neuronal components that can apply some level of matching and cross-referencing
over retrieved patterns. The process uses memory in a dynamic way and it is
directed through the pattern matching. The paper firstly describes the
mechanisms for neuronal search, memory and prediction. The paper then presents
a formal language for defining cognitive processes, that is, pattern-based
sequences and transitions. The language can define an outer framework for
concept sets that are linked to perform the cognitive act. The language also
has a mathematical basis, allowing for the rule construction to be consistent.
Now, both static memory and dynamic process hierarchies can be built as tree
structures. The new information can also be used to further integrate the
cognitive model and the ensemble-hierarchy structure becomes an essential part.
A theory about linking can suggest that nodes in different regions link
together when generally they represent the same thing.
",New Ideas for Brain Modelling 5,Kieran Greer,2021,Artificial Intelligence,
"  Highly automated driving requires precise models of traffic participants.
Many state of the art models are currently based on machine learning
techniques. Among others, the required amount of labeled data is one major
challenge. An autonomous learning process addressing this problem is proposed.
The initial models are iteratively refined in three steps: (1) detection and
context identification, (2) novelty detection and active learning and (3)
online model adaption.
","Highly Automated Learning for Improved Active Safety of Vulnerable Road
  Users","Maarten Bieshaar and G\""unther Reitberger and Viktor Kre{\ss} and
  Stefan Zernetsch and Konrad Doll and Erich Fuchs and Bernhard Sick",2017,Artificial Intelligence,
"  This article explores the ideas that went into George Boole's development of
an algebra for logical inference in his book The Laws of Thought. We explore in
particular his wife Mary Boole's claim that he was deeply influenced by Indian
logic and argue that his work was more than a framework for processing
propositions. By exploring parallels between his work and Indian logic, we are
able to explain several peculiarities of this work.
",On the Algebra in Boole's Laws of Thought,Subhash Kak,2018,Artificial Intelligence,
"  The authors present an overview of a hierarchical framework for coordinating
task- and motion-level operations in multirobot systems. Their framework is
based on the idea of using simple temporal networks to simultaneously reason
about precedence/causal constraints required for task-level coordination and
simple temporal constraints required to take some kinematic constraints of
robots into account. In the plan-generation phase, the framework provides a
computationally scalable method for generating plans that achieve high-level
tasks for groups of robots and take some of their kinematic constraints into
account. In the plan-execution phase, the framework provides a method for
absorbing an imperfect plan execution to avoid time-consuming re-planning in
many cases. The authors use the multirobot path-planning problem as a case
study to present the key ideas behind their framework for the long-term
autonomy of multirobot systems.
","Overview: A Hierarchical Framework for Plan Generation and Execution in
  Multi-Robot Systems","Hang Ma, Wolfgang H\""onig, Liron Cohen, Tansel Uras, Hong Xu, T. K.
  Satish Kumar, Nora Ayanian, Sven Koenig",2017,Artificial Intelligence,
"  Navigating through unstructured environments is a basic capability of
intelligent creatures, and thus is of fundamental interest in the study and
development of artificial intelligence. Long-range navigation is a complex
cognitive task that relies on developing an internal representation of space,
grounded by recognisable landmarks and robust visual processing, that can
simultaneously support continuous self-localisation (""I am here"") and a
representation of the goal (""I am going there""). Building upon recent research
that applies deep reinforcement learning to maze navigation problems, we
present an end-to-end deep reinforcement learning approach that can be applied
on a city scale. Recognising that successful navigation relies on integration
of general policies with locale-specific knowledge, we propose a dual pathway
architecture that allows locale-specific features to be encapsulated, while
still enabling transfer to multiple cities. We present an interactive
navigation environment that uses Google StreetView for its photographic content
and worldwide coverage, and demonstrate that our learning method allows agents
to learn to navigate multiple cities and to traverse to target destinations
that may be kilometres away. The project webpage http://streetlearn.cc contains
a video summarising our research and showing the trained agent in diverse city
environments and on the transfer task, the form to request the StreetLearn
dataset and links to further resources. The StreetLearn environment code is
available at https://github.com/deepmind/streetlearn
",Learning to Navigate in Cities Without a Map,"Piotr Mirowski, Matthew Koichi Grimes, Mateusz Malinowski, Karl Moritz
  Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, Koray Kavukcuoglu,
  Andrew Zisserman, Raia Hadsell",2018,Artificial Intelligence,
"  Fuzzy relation equations (FRE)are associated with the composition of binary
fuzzy relations. In the present work FRE are used as a tool for studying the
process of learning a new subject matter by a student class. A classroom
application and other csuitable examples connected to the student learning of
the derivative are also presented illustrating our results and useful
conclusions are obtained.
",A Study of Student Learning Skills Using Fuzzy Relation Equations,Michael Gr. Voskoglou,2018,Artificial Intelligence,
"  The theory of grey systems plays an important role in science,engineering and
in the everyday life in general for handling approximate data. In the present
paper grey numbers are used as a tool for assessing with linguistic expressions
the mean performance of a group of objects participating in a certain activity.
Two applications to student and football player assessment are also presented
illustrating our results.
",Application of Grey Numbers to Assessment Processes,"Michael Gr. Voskoglou, Yiannis Theodorou",2017,Artificial Intelligence,
"  We implement a automated tactical prover TacticToe on top of the HOL4
interactive theorem prover. TacticToe learns from human proofs which
mathematical technique is suitable in each proof situation. This knowledge is
then used in a Monte Carlo tree search algorithm to explore promising
tactic-level proof paths. On a single CPU, with a time limit of 60 seconds,
TacticToe proves 66.4 percent of the 7164 theorems in HOL4's standard library,
whereas E prover with auto-schedule solves 34.5 percent. The success rate rises
to 69.0 percent by combining the results of TacticToe and E prover.
",TacticToe: Learning to Prove with Tactics,"Thibault Gauthier, Cezary Kaliszyk, Josef Urban, Ramana Kumar, Michael
  Norrish",2021,Artificial Intelligence,
"  The AGINAO is a project to create a human-level artificial general
intelligence system (HL AGI) embodied in the Aldebaran Robotics' NAO humanoid
robot. The dynamical and open-ended cognitive engine of the robot is
represented by an embedded and multi-threaded control program, that is
self-crafted rather than hand-crafted, and is executed on a simulated Universal
Turing Machine (UTM). The actual structure of the cognitive engine emerges as a
result of placing the robot in a natural preschool-like environment and running
a core start-up system that executes self-programming of the cognitive layer on
top of the core layer. The data from the robot's sensory devices supplies the
training samples for the machine learning methods, while the commands sent to
actuators enable testing hypotheses and getting a feedback. The individual
self-created subroutines are supposed to reflect the patterns and concepts of
the real world, while the overall program structure reflects the spatial and
temporal hierarchy of the world dependencies. This paper focuses on the details
of the self-programming approach, limiting the discussion of the applied
cognitive architecture to a necessary minimum.
",The AGINAO Self-Programming Engine,Wojciech Skaba,2012,Artificial Intelligence,
"  AGINAO builds its cognitive engine by applying self-programming techniques to
create a hierarchy of interconnected codelets - the tiny pieces of code
executed on a virtual machine. These basic processing units are evaluated for
their applicability and fitness with a notion of reward calculated from
self-information gain of binary partitioning of the codelet's input
state-space. This approach, however, is useless for the evaluation of
actuators. Instead, a model is proposed in which actuators are evaluated by
measuring the impact that an activation of an effector, and consequently the
feedback from the robot sensors, has on average reward received by the
processing units.
",Evaluating Actuators in a Purely Information-Theory Based Reward Model,Wojciech Skaba,2013,Artificial Intelligence,
"  An autonomous agent embodied in a humanoid robot, in order to learn from the
overwhelming flow of raw and noisy sensory, has to effectively reduce the high
spatial-temporal data dimensionality. In this paper we propose a novel method
of unsupervised feature extraction and selection with binary space
partitioning, followed by a computation of information gain that is interpreted
as intrinsic reward, then applied as immediate-reward signal for the
reinforcement-learning. The space partitioning is executed by tiny codelets
running on a simulated Turing Machine. The features are represented by concept
nodes arranged in a hierarchy, in which those of a lower level become the input
vectors of a higher level.
",Binary Space Partitioning as Intrinsic Reward,Wojciech Skaba,2012,Artificial Intelligence,
"  This paper builds on the recent ASPIC+ formalism, to develop a general
framework for argumentation with preferences. We motivate a revised definition
of conflict free sets of arguments, adapt ASPIC+ to accommodate a broader range
of instantiating logics, and show that under some assumptions, the resulting
framework satisfies key properties and rationality postulates. We then show
that the generalised framework accommodates Tarskian logic instantiations
extended with preferences, and then study instantiations of the framework by
classical logic approaches to argumentation. We conclude by arguing that
ASPIC+'s modelling of defeasible inference rules further testifies to the
generality of the framework, and then examine and counter recent critiques of
Dung's framework and its extensions to accommodate preferences.
",A General Account of Argumentation with Preferences,Sanjay Modgil and Henry Prakken,2013,Artificial Intelligence,
"  Organization concepts and models are increasingly being adopted for the
design and specification of multi-agent systems. Agent organizations can be
seen as mechanisms of social order, created to achieve global (or
organizational) objectives by more or less autonomous agents. In order to
develop a theory on the relation between organizational structures,
organizational objectives and the actions of agents fulfilling roles in the
organization a theoretical framework is needed to describe organizational
structures and actions of (groups of) agents. Current logical formalisms focus
on specific aspects of organizations (e.g. power, delegation, agent actions, or
normative issues) but a framework that integrates and relates different aspects
is missing. Given the amount of aspects involved and the subsequent complexity
of a formalism encompassing them all, it is difficult to realize. In this
paper, a first step is taken to solve this problem. We present a generic formal
model that enables to specify and relate the main concepts of an organization
(including, activity, structure, environment and others) so that organizations
can be analyzed at a high level of abstraction. However, for some aspects we
use a simplified model in order to avoid the complexity of combining many
different types of (modal) operators.
",A Logic of Agent Organizations,Virginia Dignum and Frank Dignum,2012,Artificial Intelligence,
"  In the past several years, we have taken advantage of a number of
opportunities to advance the intersection of next generation high-performance
computing AI and big data technologies through partnerships in precision
medicine. Today we are in the throes of piecing together what is likely the
most unique convergence of medical data and computer technologies. But more
deeply, we observe that the traditional paradigm of computer simulation and
prediction needs fundamental revision. This is the time for a number of
reasons. We will review what the drivers are, why now, how this has been
approached over the past several years, and where we are heading.
","Precision Medicine as an Accelerator for Next Generation Cognitive
  Supercomputing","Edmon Begoli, Jim Brase, Bambi DeLaRosa, Penelope Jones, Dimitri
  Kusnezov, Jason Paragas, Rick Stevens, Fred Streitz, Georgia Tourassi",2018,Artificial Intelligence,
"  The sine-cosine algorithm (SCA) is a new population-based meta-heuristic
algorithm. In addition to exploiting sine and cosine functions to perform local
and global searches (hence the name sine-cosine), the SCA introduces several
random and adaptive parameters to facilitate the search process. Although it
shows promising results, the search process of the SCA is vulnerable to local
minima/maxima due to the adoption of a fixed switch probability and the bounded
magnitude of the sine and cosine functions (from -1 to 1). In this paper, we
propose a new hybrid Q-learning sine-cosine- based strategy, called the
Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the
switching probability. Instead, we rely on the Q-learning algorithm (based on
the penalty and reward mechanism) to dynamically identify the best operation
during runtime. Additionally, we integrate two new operations (L\'evy flight
motion and crossover) into the QLSCA to facilitate jumping out of local
minima/maxima and enhance the solution diversity. To assess its performance, we
adopt the QLSCA for the combinatorial test suite minimization problem.
Experimental results reveal that the QLSCA is statistically superior with
regard to test suite size reduction compared to recent state-of-the-art
strategies, including the original SCA, the particle swarm test generator
(PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search
strategy (CS) at the 95% confidence level. However, concerning the comparison
with discrete particle swarm optimization (DPSO), there is no significant
difference in performance at the 95% confidence level. On a positive note, the
QLSCA statistically outperforms the DPSO in certain configurations at the 90%
confidence level.
","A Hybrid Q-Learning Sine-Cosine-based Strategy for Addressing the
  Combinatorial Test Suite Minimization Problem",Kamal Z. Zamli and Fakhrud Din and Bestoun S. Ahmed and Miroslav Bures,2018,Artificial Intelligence,
"  In many applications that involve processing high-dimensional data, it is
important to identify a small set of entities that account for a significant
fraction of detections. Rather than formalize this as a clustering problem, in
which all detections must be grouped into hard or soft categories, we formalize
it as an instance of the frequent items or heavy hitters problem, which finds
groups of tightly clustered objects that have a high density in the feature
space. We show that the heavy hitters formulation generates solutions that are
more accurate and effective than the clustering formulation. In addition, we
present a novel online algorithm for heavy hitters, called HAC, which addresses
problems in continuous space, and demonstrate its effectiveness on real video
and household domains.
",Finding Frequent Entities in Continuous Data,"Ferran Alet, Rohan Chitnis, Leslie P. Kaelbling, Tomas Lozano-Perez",2018,Artificial Intelligence,
"  End-to-end trained neural networks (NNs) are a compelling approach to
autonomous vehicle control because of their ability to learn complex tasks
without manual engineering of rule-based decisions. However, challenging road
conditions, ambiguous navigation situations, and safety considerations require
reliable uncertainty estimation for the eventual adoption of full-scale
autonomous vehicles. Bayesian deep learning approaches provide a way to
estimate uncertainty by approximating the posterior distribution of weights
given a set of training data. Dropout training in deep NNs approximates
Bayesian inference in a deep Gaussian process and can thus be used to estimate
model uncertainty. In this paper, we propose a Bayesian NN for end-to-end
control that estimates uncertainty by exploiting feature map correlation during
training. This approach achieves improved model fits, as well as tighter
uncertainty estimates, than traditional element-wise dropout. We evaluate our
algorithms on a challenging dataset collected over many different road types,
times of day, and weather conditions, and demonstrate how uncertainties can be
used in conjunction with a human controller in a parallel autonomous setting.
",Spatial Uncertainty Sampling for End-to-End Control,"Alexander Amini, Ava Soleimany, Sertac Karaman, Daniela Rus",2018,Artificial Intelligence,
"  Structural Causal Models (SCMs) provide a popular causal modeling framework.
In this work, we show that SCMs are not flexible enough to give a complete
causal representation of dynamical systems at equilibrium. Instead, we propose
a generalization of the notion of an SCM, that we call Causal Constraints Model
(CCM), and prove that CCMs do capture the causal semantics of such systems. We
show how CCMs can be constructed from differential equations and initial
conditions and we illustrate our ideas further on a simple but ubiquitous
(bio)chemical reaction. Our framework also allows to model functional laws,
such as the ideal gas law, in a sensible and intuitive way.
",Beyond Structural Causal Models: Causal Constraints Models,"Tineke Blom, Stephan Bongers, Joris M. Mooij",2019,Artificial Intelligence,
"  {Radio Frequency Identification technology has gained popularity for cheap
and easy deployment. In the realm of manufacturing shopfloor, it can be used to
track the location of manufacturing objects to achieve better efficiency. The
underlying challenge of localization lies in the non-stationary characteristics
of manufacturing shopfloor which calls for an adaptive life-long learning
strategy in order to arrive at accurate localization results. This paper
presents an evolving model based on a novel evolving intelligent system, namely
evolving Type-2 Quantum Fuzzy Neural Network (eT2QFNN), which features an
interval type-2 quantum fuzzy set with uncertain jump positions. The quantum
fuzzy set possesses a graded membership degree which enables better
identification of overlaps between classes. The eT2QFNN works fully in the
evolving mode where all parameters including the number of rules are
automatically adjusted and generated on the fly. The parameter adjustment
scenario relies on decoupled extended Kalman filter method. Our numerical study
shows that eT2QFNN is able to deliver comparable accuracy compared to
state-of-the-art algorithms.
",An Online RFID Localization in the Manufacturing Shopfloor,"Andri Ashfahani, Mahardhika Pratama, Edwin Lughofer, Qing Cai, and
  Huang Sheng",2019,Artificial Intelligence,
"  The Sensor, Observation, Sample, and Actuator (SOSA) ontology provides a
formal but lightweight general-purpose specification for modeling the
interaction between the entities involved in the acts of observation,
actuation, and sampling. SOSA is the result of rethinking the W3C-XG Semantic
Sensor Network (SSN) ontology based on changes in scope and target audience,
technical developments, and lessons learned over the past years. SOSA also acts
as a replacement of SSN's Stimulus Sensor Observation (SSO) core. It has been
developed by the first joint working group of the Open Geospatial Consortium
(OGC) and the World Wide Web Consortium (W3C) on \emph{Spatial Data on the
Web}. In this work, we motivate the need for SOSA, provide an overview of the
main classes and properties, and briefly discuss its integration with the new
release of the SSN ontology as well as various other alignments to
specifications such as OGC's Observations and Measurements (O\&M),
Dolce-Ultralite (DUL), and other prominent ontologies. We will also touch upon
common modeling problems and application areas related to publishing and
searching observation, sampling, and actuation data on the Web. The SOSA
ontology and standard can be accessed at
\url{https://www.w3.org/TR/vocab-ssn/}.
","SOSA: A Lightweight Ontology for Sensors, Observations, Samples, and
  Actuators","Krzysztof Janowicz, Armin Haller, Simon J D Cox, Danh Le Phuoc, Maxime
  Lefrancois",2018,Artificial Intelligence,
"  We consider the problem of how to improve automatic target recognition by
fusing the naive sensor-level classification decisions with ""intuition,"" or
context, in a mathematically principled way. This is a general approach that is
compatible with many definitions of context, but for specificity, we consider
context as co-occurrence in imagery. In particular, we consider images that
contain multiple objects identified at various confidence levels. We learn the
patterns of co-occurrence in each context, then use these patterns as
hyper-parameters for a Hierarchical Bayesian Model. The result is that
low-confidence sensor classification decisions can be dramatically improved by
fusing those readings with context. We further use hyperpriors to address the
case where multiple contexts may be appropriate. We also consider the Bayesian
Network, an alternative to the Hierarchical Bayesian Model, which is
computationally more efficient but assumes that context and sensor readings are
uncorrelated.
",Context Exploitation using Hierarchical Bayesian Models,"Christopher A. George, Pranab Banerjee, Kendra E. Moore",2018,Artificial Intelligence,
"  During the 60s and 70s, AI researchers explored intuitions about intelligence
by writing programs that displayed intelligent behavior. Many good ideas came
out from this work but programs written by hand were not robust or general.
After the 80s, research increasingly shifted to the development of learners
capable of inferring behavior and functions from experience and data, and
solvers capable of tackling well-defined but intractable models like SAT,
classical planning, Bayesian networks, and POMDPs. The learning approach has
achieved considerable success but results in black boxes that do not have the
flexibility, transparency, and generality of their model-based counterparts.
Model-based approaches, on the other hand, require models and scalable
algorithms. Model-free learners and model-based solvers have close parallels
with Systems 1 and 2 in current theories of the human mind: the first, a fast,
opaque, and inflexible intuitive mind; the second, a slow, transparent, and
flexible analytical mind. In this paper, I review developments in AI and draw
on these theories to discuss the gap between model-free learners and
model-based solvers, a gap that needs to be bridged in order to have
intelligent systems that are robust and general.
","Model-free, Model-based, and General Intelligence",Hector Geffner,2018,Artificial Intelligence,
"  This paper describes an architecture for controlling non-player characters
(NPC) in the First Person Shooter (FPS) game Unreal Tournament 2004.
Specifically, the DRE-Bot architecture is made up of three reinforcement
learners, Danger, Replenish and Explore, which use the tabular Sarsa({\lambda})
algorithm. This algorithm enables the NPC to learn through trial and error
building up experience over time in an approach inspired by human learning.
Experimentation is carried to measure the performance of DRE-Bot when competing
against fixed strategy bots that ship with the game. The discount parameter,
{\gamma}, and the trace parameter, {\lambda}, are also varied to see if their
values have an effect on the performance.
","DRE-Bot: A Hierarchical First Person Shooter Bot Using Multiple
  Sarsa({\lambda}) Reinforcement Learners",Frank G. Glavin and Michael G. Madden,2012,Artificial Intelligence,
"  While reinforcement learning (RL) has been applied to turn-based board games
for many years, more complex games involving decision-making in real-time are
beginning to receive more attention. A challenge in such environments is that
the time that elapses between deciding to take an action and receiving a reward
based on its outcome can be longer than the interval between successive
decisions. We explore this in the context of a non-player character (NPC) in a
modern first-person shooter game. Such games take place in 3D environments
where players, both human and computer-controlled, compete by engaging in
combat and completing task objectives. We investigate the use of RL to enable
NPCs to gather experience from game-play and improve their shooting skill over
time from a reward signal based on the damage caused to opponents. We propose a
new method for RL updates and reward calculations, in which the updates are
carried out periodically, after each shooting encounter has ended, and a new
weighted-reward mechanism is used which increases the reward applied to actions
that lead to damaging the opponent in successive hits in what we term ""hit
clusters"".
","Learning to Shoot in First Person Shooter Games by Stabilizing Actions
  and Clustering Rewards for Reinforcement Learning",Frank G. Glavin and Michael G. Madden,2015,Artificial Intelligence,
"  Machine learning practitioners are often ambivalent about the ethical aspects
of their products. We believe anything that gets us from that current state to
one in which our systems are achieving some degree of fairness is an
improvement that should be welcomed. This is true even when that progress does
not get us 100% of the way to the goal of ""complete"" fairness or perfectly
align with our personal belief on which measure of fairness is used. Some
measure of fairness being built would still put us in a better position than
the status quo. Impediments to getting fairness and ethical concerns applied in
real applications, whether they are abstruse philosophical debates or technical
overhead such as the introduction of ever more hyper-parameters, should be
avoided. In this paper we further elaborate on our argument for this viewpoint
and its importance.
",What About Applied Fairness?,"Jared Sylvester, Edward Raff",2018,Artificial Intelligence,
"  In many real-world problems, there is the possibility to configure, to a
limited extent, some environmental parameters to improve the performance of a
learning agent. In this paper, we propose a novel framework, Configurable
Markov Decision Processes (Conf-MDPs), to model this new type of interaction
with the environment. Furthermore, we provide a new learning algorithm, Safe
Policy-Model Iteration (SPMI), to jointly and adaptively optimize the policy
and the environment configuration. After having introduced our approach and
derived some theoretical results, we present the experimental evaluation in two
explicative problems to show the benefits of the environment configurability on
the performance of the learned policy.
",Configurable Markov Decision Processes,"Alberto Maria Metelli, Mirco Mutti and Marcello Restelli",2018,Artificial Intelligence,
"  The POMDP is a powerful framework for reasoning under outcome and information
uncertainty, but constructing an accurate POMDP model is difficult.
Bayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs)
extend POMDPs to allow the model to be learned during execution. BA-POMDPs are
a Bayesian RL approach that, in principle, allows for an optimal trade-off
between exploitation and exploration. Unfortunately, BA-POMDPs are currently
impractical to solve for any non-trivial domain. In this paper, we extend the
Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting
method, which we call BA-POMCP, is able to tackle problems that previous
solution methods have been unable to solve. Additionally, we introduce several
techniques that exploit the BA-POMDP structure to improve the efficiency of
BA-POMCP along with proof of their convergence.
",Learning in POMDPs with Monte Carlo Tree Search,"Sammie Katt, Frans A. Oliehoek, Christopher Amato",2017,Artificial Intelligence,
"  The well-known Late Acceptance Hill Climbing (LAHC) search aims to overcome
the main downside of traditional Hill Climbing (HC) search, which is often
quickly trapped in a local optimum due to strictly accepting only non-worsening
moves within each iteration. In contrast, LAHC also accepts worsening moves, by
keeping a circular array of fitness values of previously visited solutions and
comparing the fitness values of candidate solutions against the least recent
element in the array. While this straightforward strategy has proven effective,
there are nevertheless situations where LAHC can unfortunately behave in a
similar manner to HC. For example, when a new local optimum is found, often the
same fitness value is stored many times in the array. To address this
shortcoming, we propose new acceptance and replacement strategies to take into
account worsening, improving, and sideways movement scenarios with the aim to
improve the diversity of values in the array. Compared to LAHC, the proposed
Diversified Late Acceptance Search approach is shown to lead to better quality
solutions that are obtained with a lower number of iterations on benchmark
Travelling Salesman Problems and Quadratic Assignment Problems.
",Diversified Late Acceptance Search,"Majid Namazi, Conrad Sanderson, M.A. Hakim Newton, M.M.A. Polash,
  Abdul Sattar",2018,Artificial Intelligence,
"  Fully observable non-deterministic (FOND) planning is becoming increasingly
important as an approach for computing proper policies in probabilistic
planning, extended temporal plans in LTL planning, and general plans in
generalized planning. In this work, we introduce a SAT encoding for FOND
planning that is compact and can produce compact strong cyclic policies. Simple
variations of the encodings are also introduced for strong planning and for
what we call, dual FOND planning, where some non-deterministic actions are
assumed to be fair (e.g., probabilistic) and others unfair (e.g., adversarial).
The resulting FOND planners are compared empirically with existing planners
over existing and new benchmarks. The notion of ""probabilistic interesting
problems"" is also revisited to yield a more comprehensive picture of the
strengths and limitations of current FOND planners and the proposed SAT
approach.
",Compact Policies for Fully-Observable Non-Deterministic Planning as SAT,Tomas Geffner and Hector Geffner,2018,Artificial Intelligence,
"  Combinatorial preference aggregation has many applications in AI. Given the
exponential nature of these preferences, compact representations are needed and
($m$)CP-nets are among the most studied ones. Sequential and global voting are
two ways to aggregate preferences over CP-nets. In the former, preferences are
aggregated feature-by-feature. Hence, when preferences have specific feature
dependencies, sequential voting may exhibit voting paradoxes, i.e., it might
select sub-optimal outcomes. To avoid paradoxes in sequential voting, one has
often assumed the $\mathcal{O}$-legality restriction, which imposes a shared
topological order among all the CP-nets. On the contrary, in global voting,
CP-nets are considered as a whole during preference aggregation. For this
reason, global voting is immune from paradoxes, and there is no need to impose
restrictions over the CP-nets' topological structure. Sequential voting over
$\mathcal{O}$-legal CP-nets has extensively been investigated. On the other
hand, global voting over non-$\mathcal{O}$-legal CP-nets has not carefully been
analyzed, despite it was stated in the literature that a theoretical comparison
between global and sequential voting was promising and a precise complexity
analysis for global voting has been asked for multiple times. In quite few
works, very partial results on the complexity of global voting over CP-nets
have been given. We start to fill this gap by carrying out a thorough
complexity analysis of Pareto and majority global voting over not necessarily
$\mathcal{O}$-legal acyclic binary polynomially connected (m)CP-nets. We settle
these problems in the polynomial hierarchy, and some of them in PTIME or
LOGSPACE, whereas EXPTIME was the previously known upper bound for most of
them. We show various tight lower bounds and matching upper bounds for problems
that up to date did not have any explicit non-obvious lower bound.
","Complexity Results for Preference Aggregation over (m)CP-nets: Pareto
  and Majority Voting","Thomas Lukasiewicz, Enrico Malizia",2019,Artificial Intelligence,
"  Human posture recognition provides a dynamic field that has produced many
methods. Using fuzzy subsets based data fusion methods to aggregate the results
given by different types of recognition processes is a convenient way to
improve recognition methods. Nevertheless, choosing a defuzzification method to
imple-ment the decision is a crucial point of this approach. The goal of this
paper is to present an approach where the choice of the defuzzification method
is driven by the constraints of the final data user, which are expressed as
limitations on indica-tors like confidence or accuracy. A practical
experimentation illustrating this ap-proach is presented: from a depth camera
sensor, human posture is interpreted and the defuzzification method is selected
in accordance with the constraints of the final information consumer. The paper
illustrates the interest of the approach in a context of postures based human
robot communication.
",Decision method choice in a human posture recognition context,"St\'ephane Perrin (LISTIC), Eric Benoit (LISTIC), Didier Coquin
  (LISTIC)",2018,Artificial Intelligence,
"  This paper introduces a fully automatic method for generating video game
tutorials. The AtDELFI system (AuTomatically DEsigning Legible, Full
Instructions for games) was created to investigate procedural generation of
instructions that teach players how to play video games. We present a
representation of game rules and mechanics using a graph system as well as a
tutorial generation method that uses said graph representation. We demonstrate
the concept by testing it on games within the General Video Game Artificial
Intelligence (GVG-AI) framework; the paper discusses tutorials generated for
eight different games. Our findings suggest that a graph representation scheme
works well for simple arcade style games such as Space Invaders and Pacman, but
it appears that tutorials for more complex games might require higher-level
understanding of the game than just single mechanics.
","AtDelfi: Automatically Designing Legible, Full Instructions For Games","Michael Cerny Green, Ahmed Khalifa, Gabriella A.B. Barros, Tiago
  Machado, Andy Nealen and Julian Togelius",2018,Artificial Intelligence,
"  We introduce a new generative model for human planning under the Bayesian
Inverse Reinforcement Learning (BIRL) framework which takes into account the
fact that humans often plan using hierarchical strategies. We describe the
Bayesian Inverse Hierarchical RL (BIHRL) algorithm for inferring the values of
hierarchical planners, and use an illustrative toy model to show that BIHRL
retains accuracy where standard BIRL fails. Furthermore, BIHRL is able to
accurately predict the goals of `Wikispeedia' game players, with inclusion of
hierarchical structure in the model resulting in a large boost in accuracy. We
show that BIHRL is able to significantly outperform BIRL even when we only have
a weak prior on the hierarchical structure of the plans available to the agent,
and discuss the significant challenges that remain for scaling up this
framework to more realistic settings.
",Exploring Hierarchy-Aware Inverse Reinforcement Learning,"Chris Cundy, Daniel Filan",2018,Artificial Intelligence,
"  It is the focus of this work to extend and study the previously proposed
quantum-like Bayesian networks to deal with decision-making scenarios by
incorporating the notion of maximum expected utility in influence diagrams. The
general idea is to take advantage of the quantum interference terms produced in
the quantum-like Bayesian Network to influence the probabilities used to
compute the expected utility of some action. This way, we are not proposing a
new type of expected utility hypothesis. On the contrary, we are keeping it
under its classical definition. We are only incorporating it as an extension of
a probabilistic graphical model in a compact graphical representation called an
influence diagram in which the utility function depends on the probabilistic
influences of the quantum-like Bayesian network.
  Our findings suggest that the proposed quantum-like influence digram can
indeed take advantage of the quantum interference effects of quantum-like
Bayesian Networks to maximise the utility of a cooperative behaviour in
detriment of a fully rational defect behaviour under the prisoner's dilemma
game.
","Introducing Quantum-Like Influence Diagrams for Violations of the Sure
  Thing Principle",Catarina Moreira and Andreas Wichert,2018,Artificial Intelligence,
"  Recommendation systems are an integral part of Artificial Intelligence (AI)
and have become increasingly important in the growing age of commercialization
in AI. Deep learning (DL) techniques for recommendation systems (RS) provide
powerful latent-feature models for effective recommendation but suffer from the
major drawback of being non-interpretable. In this paper we describe a
framework for explainable temporal recommendations in a DL model. We consider
an LSTM based Recurrent Neural Network (RNN) architecture for recommendation
and a neighbourhood-based scheme for generating explanations in the model. We
demonstrate the effectiveness of our approach through experiments on the
Netflix dataset by jointly optimizing for both prediction accuracy and
explainability.
",Explanations for Temporal Recommendations,"Homanga Bharadhwaj, Shruti Joshi",2018,Artificial Intelligence,
"  Monte Carlo tree search (MCTS) is a popular choice for solving sequential
anytime problems. However, it depends on a numeric feedback signal, which can
be difficult to define. Real-time MCTS is a variant which may only rarely
encounter states with an explicit, extrinsic reward. To deal with such cases,
the experimenter has to supply an additional numeric feedback signal in the
form of a heuristic, which intrinsically guides the agent. Recent work has
shown evidence that in different areas the underlying structure is ordinal and
not numerical. Hence erroneous and biased heuristics are inevitable, especially
in such domains. In this paper, we propose a MCTS variant which only depends on
qualitative feedback, and therefore opens up new applications for MCTS. We also
find indications that translating absolute into ordinal feedback may be
beneficial. Using a puzzle domain, we show that our preference-based MCTS
variant, wich only receives qualitative feedback, is able to reach a
performance level comparable to a regular MCTS baseline, which obtains
quantitative feedback.
",Preference-Based Monte Carlo Tree Search,"Tobias Joppen, Christian Wirth, and Johannes F\""urnkranz",2018,Artificial Intelligence,
"  We present the design of a competitive artificial intelligence for Scopone, a
popular Italian card game. We compare rule-based players using the most
established strategies (one for beginners and two for advanced players) against
players using Monte Carlo Tree Search (MCTS) and Information Set Monte Carlo
Tree Search (ISMCTS) with different reward functions and simulation strategies.
MCTS requires complete information about the game state and thus implements a
cheating player while ISMCTS can deal with incomplete information and thus
implements a fair player. Our results show that, as expected, the cheating MCTS
outperforms all the other strategies; ISMCTS is stronger than all the
rule-based players implementing well-known and most advanced strategies and it
also turns out to be a challenging opponent for human players.
","Traditional Wisdom and Monte Carlo Tree Search Face-to-Face in the Card
  Game Scopone",Stefano Di Palma and Pier Luca Lanzi,2018,Artificial Intelligence,
"  Many distributed machine learning frameworks have recently been built to
speed up the large-scale data learning process. However, most distributed
machine learning used in these frameworks still uses an offline algorithm model
which cannot cope with the data stream problems. In fact, large-scale data are
mostly generated by the non-stationary data stream where its pattern evolves
over time. To address this problem, we propose a novel Evolving Large-scale
Data Stream Analytics framework based on a Scalable Parsimonious Network based
on Fuzzy Inference System (Scalable PANFIS), where the PANFIS evolving
algorithm is distributed over the worker nodes in the cloud to learn
large-scale data stream. Scalable PANFIS framework incorporates the active
learning (AL) strategy and two model fusion methods. The AL accelerates the
distributed learning process to generate an initial evolving large-scale data
stream model (initial model), whereas the two model fusion methods aggregate an
initial model to generate the final model. The final model represents the
update of current large-scale data knowledge which can be used to infer future
data. Extensive experiments on this framework are validated by measuring the
accuracy and running time of four combinations of Scalable PANFIS and other
Spark-based built in algorithms. The results indicate that Scalable PANFIS with
AL improves the training time to be almost two times faster than Scalable
PANFIS without AL. The results also show both rule merging and the voting
mechanisms yield similar accuracy in general among Scalable PANFIS algorithms
and they are generally better than Spark-based algorithms. In terms of running
time, the Scalable PANFIS training time outperforms all Spark-based algorithms
when classifying numerous benchmark datasets.
",Evolving Large-Scale Data Stream Analytics based on Scalable PANFIS,"Mahardhika Pratama, Choiru Za'in, Eric Pardede",2018,Artificial Intelligence,
"  Unmanned aerial vehicles (UAVs), also known as drones, have emerged as a
promising mode of fast, energy-efficient, and cost-effective package delivery.
A considerable number of works have studied different aspects of drone package
delivery service by a supplier, one of which is delivery planning. However,
existing works addressing the planning issues consider a simple case of perfect
delivery without service interruption, e.g., due to accident which is common
and realistic. Therefore, this paper introduces the joint ground and aerial
delivery service optimization and planning (GADOP) framework. The framework
explicitly incorporates uncertainty of drone package delivery, i.e., takeoff
and breakdown conditions. The GADOP framework aims to minimize the total
delivery cost given practical constraints, e.g., traveling distance limit.
Specifically, we formulate the GADOP framework as a three-stage stochastic
integer programming model. To deal with the high complexity issue of the
problem, a decomposition method is adopted. Then, the performance of the GADOP
framework is evaluated by using two data sets including Solomon benchmark suite
and the real data from one of the Singapore logistics companies. The
performance evaluation clearly shows that the GADOP framework can achieve
significantly lower total payment than that of the baseline methods which do
not take uncertainty into account.
","Joint Ground and Aerial Package Delivery Services: A Stochastic
  Optimization Approach","Suttinee Sawadsitang, Dusit Niyato, Puay-Siew Tan, and Ping Wang",2018,Artificial Intelligence,
"  Approaches to decision-making under uncertainty in the belief function
framework are reviewed. Most methods are shown to blend criteria for decision
under ignorance with the maximum expected utility principle of Bayesian
decision theory. A distinction is made between methods that construct a
complete preference relation among acts, and those that allow incomparability
of some acts due to lack of information. Methods developed in the imprecise
probability framework are applicable in the Dempster-Shafer context and are
also reviewed. Shafer's constructive decision theory, which substitutes the
notion of goal for that of utility, is described and contrasted with other
approaches. The paper ends by pointing out the need to carry out deeper
investigation of fundamental issues related to decision-making with belief
functions and to assess the descriptive, normative and prescriptive values of
the different approaches.
",Decision-Making with Belief Functions: a Review,Thierry Denoeux,2019,Artificial Intelligence,
"  With the increasing need of personalised decision making, such as
personalised medicine and online recommendations, a growing attention has been
paid to the discovery of the context and heterogeneity of causal relationships.
Most existing methods, however, assume a known cause (e.g. a new drug) and
focus on identifying from data the contexts of heterogeneous effects of the
cause (e.g. patient groups with different responses to the new drug). There is
no approach to efficiently detecting directly from observational data context
specific causal relationships, i.e. discovering the causes and their contexts
simultaneously. In this paper, by taking the advantages of highly efficient
decision tree induction and the well established causal inference framework, we
propose the Tree based Context Causal rule discovery (TCC) method, for
efficient exploration of context specific causal relationships from data.
Experiments with both synthetic and real world data sets show that TCC can
effectively discover context specific causal rules from the data.
",Discovering Context Specific Causal Relationships,"Saisai Ma, Jiuyong Li, Lin Liu, Thuc Duy Le",2019,Artificial Intelligence,
"  Artificial intelligence (AI) is the core technology of technological
revolution and industrial transformation. As one of the new intelligent needs
in the AI 2.0 era, financial intelligence has elicited much attention from the
academia and industry. In our current dynamic capital market, financial
intelligence demonstrates a fast and accurate machine learning capability to
handle complex data and has gradually acquired the potential to become a
""financial brain"". In this work, we survey existing studies on financial
intelligence. First, we describe the concept of financial intelligence and
elaborate on its position in the financial technology field. Second, we
introduce the development of financial intelligence and review state-of-the-art
techniques in wealth management, risk management, financial security, financial
consulting, and blockchain. Finally, we propose a research framework called
FinBrain and summarize four open issues, namely, explainable financial agents
and causality, perception and prediction under uncertainty, risk-sensitive and
robust decision making, and multi-agent game and mechanism design. We believe
that these research directions can lay the foundation for the development of AI
2.0 in the finance field.
",FinBrain: When Finance Meets AI 2.0,"Xiaolin Zheng, Mengying Zhu, Qibing Li, Chaochao Chen, Yanchao Tan",2018,Artificial Intelligence,
"  In our experience, some ontology users find it much easier to convey logical
statements using rules rather than OWL (or description logic) axioms. Based on
recent theoretical developments on transformations between rules and
description logics, we develop ROWL, a Protege plugin that allows users to
enter OWL axioms by way of rules; the plugin then automatically converts these
rules into OWL DL axioms if possible, and prompts the user in case such a
conversion is not possible without weakening the semantics of the rule.
",Modeling OWL with Rules: The ROWL Protege Plugin,"Md. Kamruzzaman Sarker, David Carral, Adila A. Krisnadhi, Pascal
  Hitzler",2016,Artificial Intelligence,
"  Developing visual perception models for active agents and sensorimotor
control are cumbersome to be done in the physical world, as existing algorithms
are too slow to efficiently learn in real-time and robots are fragile and
costly. This has given rise to learning-in-simulation which consequently casts
a question on whether the results transfer to real-world. In this paper, we are
concerned with the problem of developing real-world perception for active
agents, propose Gibson Virtual Environment for this purpose, and showcase
sample perceptual tasks learned therein. Gibson is based on virtualizing real
spaces, rather than using artificially designed ones, and currently includes
over 1400 floor spaces from 572 full buildings. The main characteristics of
Gibson are: I. being from the real-world and reflecting its semantic
complexity, II. having an internal synthesis mechanism, ""Goggles"", enabling
deploying the trained models in real-world without needing further domain
adaptation, III. embodiment of agents and making them subject to constraints of
physics and space.
",Gibson Env: Real-World Perception for Embodied Agents,"Fei Xia, Amir Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik,
  Silvio Savarese",2018,Artificial Intelligence,
"  Using a game engine, we have developed a virtual environment which models
important aspects of critical incident scenarios. We focused on modelling
phenomena relating to the identification and gathering of key forensic
evidence, in order to develop and test a system which can handle chemical,
biological, radiological/nuclear or explosive (CBRNe) events autonomously. This
allows us to build and validate AI-based technologies, which can be trained and
tested in our custom virtual environment before being deployed in real-world
scenarios. We have used our virtual scenario to rapidly prototype a system
which can use simulated Remote Aerial Vehicles (RAVs) to gather images from the
environment for the purpose of mapping. Our environment provides us with an
effective medium through which we can develop and test various AI methodologies
for critical incident scene assessment, in a safe and controlled manner
","Using a Game Engine to Simulate Critical Incidents and Data Collection
  by Autonomous Drones","David L. Smyth, Frank G. Glavin, Michael G. Madden",2018,Artificial Intelligence,
"  This paper introduces an information-theoretic method for selecting a subset
of problems which gives the most information about a group of problem-solving
algorithms. This method was tested on the games in the General Video Game AI
(GVGAI) framework, allowing us to identify a smaller set of games that still
gives a large amount of information about the abilities of different
game-playing agents. This approach can be used to make agent testing more
efficient. We can achieve almost as good discriminatory accuracy when testing
on only a handful of games as when testing on more than a hundred games,
something which is often computationally infeasible. Furthermore, this method
can be extended to study the dimensions of the effective variance in game
design between these games, allowing us to identify which games differentiate
between agents in the most complementary ways.
","A Continuous Information Gain Measure to Find the Most Discriminatory
  Problems for AI Benchmarking","Matthew Stephenson, Damien Anderson, Ahmed Khalifa, John Levine,
  Jochen Renz, Julian Togelius, Christoph Salge",2020,Artificial Intelligence,
"  As an exquisite and concise literary form, poetry is a gem of human culture.
Automatic poetry generation is an essential step towards computer creativity.
In recent years, several neural models have been designed for this task.
However, among lines of a whole poem, the coherence in meaning and topics still
remains a big challenge. In this paper, inspired by the theoretical concept in
cognitive psychology, we propose a novel Working Memory model for poetry
generation. Different from previous methods, our model explicitly maintains
topics and informative limited history in a neural memory. During the
generation process, our model reads the most relevant parts from memory slots
to generate the current line. After each line is generated, it writes the most
salient parts of the previous line into memory slots. By dynamic manipulation
of the memory, our model keeps a coherent information flow and learns to
express each topic flexibly and naturally. We experiment on three different
genres of Chinese poetry: quatrain, iambic and chinoiserie lyric. Both
automatic and human evaluation results show that our model outperforms current
state-of-the-art methods.
",Chinese Poetry Generation with a Working Memory Model,"Xiaoyuan Yi, Maosong Sun, Ruoyu Li, Zonghan Yang",2018,Artificial Intelligence,
"  When modeling real world domains we have to deal with information that is
incomplete or that comes from sources with different trust levels. This
motivates the need for managing uncertainty in the Semantic Web. To this
purpose, we introduced a probabilistic semantics, named DISPONTE, in order to
combine description logics with probability theory. The probability of a query
can be then computed from the set of its explanations by building a Binary
Decision Diagram (BDD). The set of explanations can be found using the tableau
algorithm, which has to handle non-determinism. Prolog, with its efficient
handling of non-determinism, is suitable for implementing the tableau
algorithm. TRILL and TRILLP are systems offering a Prolog implementation of the
tableau algorithm. TRILLP builds a pinpointing formula, that compactly
represents the set of explanations and can be directly translated into a BDD.
Both reasoners were shown to outperform state-of-the-art DL reasoners. In this
paper, we present an improvement of TRILLP, named TORNADO, in which the BDD is
directly built during the construction of the tableau, further speeding up the
overall inference process. An experimental comparison shows the effectiveness
of TORNADO. All systems can be tried online in the TRILL on SWISH web
application at http://trill.ml.unife.it/.
","Probabilistic DL Reasoning with Pinpointing Formulas: A Prolog-based
  Approach","Riccardo Zese, Giuseppe Cota, Evelina Lamma, Elena Bellodi, Fabrizio
  Riguzzi",2019,Artificial Intelligence,
"  Autonomous robotics and artificial intelligence techniques can be used to
support human personnel in the event of critical incidents. These incidents can
pose great danger to human life. Some examples of such assistance include:
multi-robot surveying of the scene; collection of sensor data and scene
imagery, real-time risk assessment and analysis; object identification and
anomaly detection; and retrieval of relevant supporting documentation such as
standard operating procedures (SOPs). These incidents, although often rare, can
involve chemical, biological, radiological/nuclear or explosive (CBRNE)
substances and can be of high consequence. Real-world training and deployment
of these systems can be costly and sometimes not feasible. For this reason, we
have developed a realistic 3D model of a CBRNE scenario to act as a testbed for
an initial set of assisting AI tools that we have developed.
","A Virtual Testbed for Critical Incident Investigation with Autonomous
  Remote Aerial Vehicle Surveying, Artificial Intelligence, and Decision
  Support","David L. Smyth, Sai Abinesh, Nazli B. Karimi, Brett Drury, Ihsan
  Ullah, Frank G. Glavin, Michael G. Madden",2018,Artificial Intelligence,
"  The growing influence and decision-making capacities of Autonomous systems
and Artificial Intelligence in our lives force us to consider the values
embedded in these systems. But how ethics should be implemented into these
systems? In this study, the solution is seen on philosophical conceptualization
as a framework to form practical implementation model for ethics of AI. To take
the first steps on conceptualization main concepts used on the field needs to
be identified. A keyword based Systematic Mapping Study (SMS) on the keywords
used in AI and ethics was conducted to help in identifying, defying and
comparing main concepts used in current AI ethics discourse. Out of 1062 papers
retrieved SMS discovered 37 re-occurring keywords in 83 academic papers. We
suggest that the focus on finding keywords is the first step in guiding and
providing direction for future research in the AI ethics field.
","The Key Concepts of Ethics of Artificial Intelligence - A Keyword based
  Systematic Mapping Study",Ville Vakkuri and Pekka Abrahamsson,2018,Artificial Intelligence,
"  The Winograd Schema (WS) challenge, proposed as an al-ternative to the Turing
Test, has become the new standard for evaluating progress in natural language
understanding (NLU). In this paper we will not however be concerned with how
this challenge might be addressed. Instead, our aim here is threefold: (i) we
will first formally 'situate' the WS challenge in the
data-information-knowledge continuum, suggesting where in that continuum a good
WS resides; (ii) we will show that a WS is just special case of a more general
phenomenon in language understanding, namely the missing text phenomenon
(henceforth, MTP) - in particular, we will argue that what we usually call
thinking in the process of language understanding involves discovering a
significant amount of 'missing text' - text that is not explicitly stated, but
is often implicitly assumed as shared background knowledge; and (iii) we
conclude by a brief discussion on why MTP is inconsistent with the data-driven
and machine learning approach to language understanding.
","On the Winograd Schema: Situating Language Understanding in the
  Data-Information-Knowledge Continuum",Walid S. Saba,2019,Artificial Intelligence,
"  This paper introduces DATA Agent, a system which creates murder mystery
adventures from open data. In the game, the player takes on the role of a
detective tasked with finding the culprit of a murder. All characters, places,
and items in DATA Agent games are generated using open data as source content.
The paper discusses the general game design and user interface of DATA Agent,
and provides details on the generative algorithms which transform linked data
into different game objects. Findings from a user study with 30 participants
playing through two games of DATA Agent show that the game is easy and fun to
play, and that the mysteries it generates are straightforward to solve.
",DATA Agent,"Michael Cerny Green, Gabriella A.B. Barros, Antonios Liapis, Julian
  Togelius",2018,Artificial Intelligence,
"  Perceiving the surrounding environment in terms of objects is useful for any
general purpose intelligent agent. In this paper, we investigate a fundamental
mechanism making object perception possible, namely the identification of
spatio-temporally invariant structures in the sensorimotor experience of an
agent. We take inspiration from the Sensorimotor Contingencies Theory to define
a computational model of this mechanism through a sensorimotor, unsupervised
and predictive approach. Our model is based on processing the unsupervised
interaction of an artificial agent with its environment. We show how
spatio-temporally invariant structures in the environment induce regularities
in the sensorimotor experience of an agent, and how this agent, while building
a predictive model of its sensorimotor experience, can capture them as densely
connected subgraphs in a graph of sensory states connected by motor commands.
Our approach is focused on elementary mechanisms, and is illustrated with a set
of simple experiments in which an agent interacts with an environment. We show
how the agent can build an internal model of moving but spatio-temporally
invariant structures by performing a Spectral Clustering of the graph modeling
its overall sensorimotor experiences. We systematically examine properties of
the model, shedding light more globally on the specificities of the paradigm
with respect to methods based on the supervised processing of collections of
static images.
","Identification of Invariant Sensorimotor Structures as a Prerequisite
  for the Discovery of Objects","Nicolas Le Hir, Olivier Sigaud, Alban Laflaqui\`ere",2018,Artificial Intelligence,
"  Most of agents that learn policy for tasks with reinforcement learning (RL)
lack the ability to communicate with people, which makes human-agent
collaboration challenging. We believe that, in order for RL agents to
comprehend utterances from human colleagues, RL agents must infer the mental
states that people attribute to them because people sometimes infer an
interlocutor's mental states and communicate on the basis of this mental
inference. This paper proposes PublicSelf model, which is a model of a person
who infers how the person's own behavior appears to their colleagues. We
implemented the PublicSelf model for an RL agent in a simulated environment and
examined the inference of the model by comparing it with people's judgment. The
results showed that the agent's intention that people attributed to the agent's
movement was correctly inferred by the model in scenes where people could find
certain intentionality from the agent's behavior.
",Bayesian Inference of Self-intention Attributed by Observer,"Yosuke Fukuchi, Masahiko Osawa, Hiroshi Yamakawa, Tatsuji Takahashi,
  Michita Imai",2018,Artificial Intelligence,
"  This paper presents a technology for simple and computationally efficient
improvements of a generic Artificial Intelligence (AI) system, including
Multilayer and Deep Learning neural networks. The improvements are, in essence,
small network ensembles constructed on top of the existing AI architectures.
Theoretical foundations of the technology are based on Stochastic Separation
Theorems and the ideas of the concentration of measure. We show that, subject
to mild technical assumptions on statistical properties of internal signals in
the original AI system, the technology enables instantaneous and
computationally efficient removal of spurious and systematic errors with
probability close to one on the datasets which are exponentially large in
dimension. The method is illustrated with numerical examples and a case study
of ten digits recognition from American Sign Language.
","Fast Construction of Correcting Ensembles for Legacy Artificial
  Intelligence Systems: Algorithms and a Case Study","Ivan Y. Tyukin, Alexander N. Gorban, Stephen Green, Danil Prokhorov",2019,Artificial Intelligence,
"  In open-ended environments, autonomous learning agents must set their own
goals and build their own curriculum through an intrinsically motivated
exploration. They may consider a large diversity of goals, aiming to discover
what is controllable in their environments, and what is not. Because some goals
might prove easy and some impossible, agents must actively select which goal to
practice at any moment, to maximize their overall mastery on the set of
learnable goals. This paper proposes CURIOUS, an algorithm that leverages 1) a
modular Universal Value Function Approximator with hindsight learning to
achieve a diversity of goals of different kinds within a unique policy and 2)
an automated curriculum learning mechanism that biases the attention of the
agent towards goals maximizing the absolute learning progress. Agents focus
sequentially on goals of increasing complexity, and focus back on goals that
are being forgotten. Experiments conducted in a new modular-goal robotic
environment show the resulting developmental self-organization of a learning
curriculum, and demonstrate properties of robustness to distracting goals,
forgetting and changes in body properties.
","CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement
  Learning","C\'edric Colas, Pierre Fournier, Olivier Sigaud, Mohamed Chetouani,
  Pierre-Yves Oudeyer",2019,Artificial Intelligence,
"  Traditional data quality control methods are based on users experience or
previously established business rules, and this limits performance in addition
to being a very time consuming process with lower than desirable accuracy.
Utilizing deep learning, we can leverage computing resources and advanced
techniques to overcome these challenges and provide greater value to users. In
this paper, we, the authors, first review relevant works and discuss machine
learning techniques, tools, and statistical quality models. Second, we offer a
creative data quality framework based on deep learning and statistical model
algorithm for identifying data quality. Third, we use data involving salary
levels from an open dataset published by the state of Arkansas to demonstrate
how to identify outlier data and how to improve data quality via deep learning.
Finally, we discuss future work.
",Improving Data Quality through Deep Learning and Statistical Models,"Wei Dai, Kenji Yoshigoe, William Parsley",2018,Artificial Intelligence,
"  The increasing presence of robots in industries has not gone unnoticed. Large
industrial players have incorporated them into their production lines, but
smaller companies hesitate due to high initial costs and the lack of
programming expertise. In this work we introduce a framework that combines two
disciplines, Programming by Demonstration and Automated Planning, to allow
users without any programming knowledge to program a robot. The user teaches
the robot atomic actions together with their semantic meaning and represents
them in terms of preconditions and effects. Using these atomic actions the
robot can generate action sequences autonomously to reach any goal given by the
user. We evaluated the usability of our framework in terms of user experiments
with a Baxter Research Robot and showed that it is well-adapted to users
without any programming experience.
","A Framework for Robot Programming in Cobotic Environments: First user
  experiments",Ying Siu Liang and Damien Pellier and Humbert Fiorino and Sylvie Pesty,2017,Artificial Intelligence,
"  In cooperation, the workers must know how co-workers behave. However, an
agent's policy, which is embedded in a statistical machine learning model, is
hard to understand, and requires much time and knowledge to comprehend.
Therefore, it is difficult for people to predict the behavior of machine
learning robots, which makes Human Robot Cooperation challenging. In this
paper, we propose Instruction-based Behavior Explanation (IBE), a method to
explain an autonomous agent's future behavior. In IBE, an agent can
autonomously acquire the expressions to explain its own behavior by reusing the
instructions given by a human expert to accelerate the learning of the agent's
policy. IBE also enables a developmental agent, whose policy may change during
the cooperation, to explain its own behavior with sufficient time granularity.
","Autonomous Self-Explanation of Behavior for Interactive Reinforcement
  Learning Agents","Yosuke Fukuchi, Masahiko Osawa, Hiroshi Yamakawa, Michita Imai",2017,Artificial Intelligence,
"  Planning has achieved significant progress in recent years. Among the various
approaches to scale up plan synthesis, the use of macro-actions has been widely
explored. As a first stage towards the development of a solution to learn
on-line macro-actions, we propose an algorithm to identify useful macro-actions
based on data mining techniques. The integration in the planning search of
these learned macro-actions shows significant improvements over six classical
planning benchmarks.
",Mining useful Macro-actions in Planning,"Sandra Castellanos-Paez and Damien Pellier and Humbert Fiorino and
  Sylvie Pesty",2016,Artificial Intelligence,
"  For social robots to be brought more into widespread use in the fields of
companionship, care taking and domestic help, they must be capable of
demonstrating social intelligence. In order to be acceptable, they must exhibit
socio-communicative skills. Classic approaches to program HRI from observed
human-human interactions fails to capture the subtlety of multimodal
interactions as well as the key structural differences between robots and
humans. The former arises due to a difficulty in quantifying and coding
multimodal behaviours, while the latter due to a difference of the degrees of
liberty between a robot and a human. However, the notion of reverse engineering
from multimodal HRI traces to learn the underlying behavioral blueprint of the
robot given multimodal traces seems an option worth exploring. With this
spirit, the entire HRI can be seen as a sequence of exchanges of speech acts
between the robot and human, each act treated as an action, bearing in mind
that the entire sequence is goal-driven. Thus, this entire interaction can be
treated as a sequence of actions propelling the interaction from its initial to
goal state, also known as a plan in the domain of AI planning. In the same
domain, this action sequence that stems from plan execution can be represented
as a trace. AI techniques, such as machine learning, can be used to learn
behavioral models (also known as symbolic action models in AI), intended to be
reusable for AI planning, from the aforementioned multimodal traces. This
article reviews recent machine learning techniques for learning planning action
models which can be applied to the field of HRI with the intent of rendering
robots as socio-communicative.
",A Review on Learning Planning Action Models for Socio-Communicative HRI,Ankuj Arora and Humbert Fiorino and Damien Pellier and Sylvie Pesty,2016,Artificial Intelligence,
"  In the context of real-time planning, this paper investigates the
contributions of two enhancements for selecting actions. First, the
agenda-driven planning enhancement ranks relevant atomic goals and solves them
incrementally in a best-first manner. Second, the committed jump enhancement
commits a sequence of actions to be executed at the following time steps. To
assess these two enhancements, we developed a real-time planning algorithm in
which action selection can be driven by a goal-agenda, and committed jumps can
be done. Experimental results, performed on classical planning problems, show
that agenda-planning and committed jumps are clear advantages in the real-time
context. Used simultaneously, they enable the planner to be several orders of
magnitude faster and solution plans to be shorter.
",Planification en temps r\'eel avec agenda de buts et sauts,Damien Pellier and Bruno Bouzy and Marc M\'etivier,2011,Artificial Intelligence,
"  Devising intelligent robots or agents that interact with humans is a major
challenge for artificial intelligence. In such contexts, agents must constantly
adapt their decisions according to human activities and modify their goals. In
this paper, we tackle this problem by introducing a novel planning approach,
called Moving Goal Planning (MGP), to adapt plans to goal evolutions. This
planning algorithm draws inspiration from Moving Target Search (MTS)
algorithms. In order to limit the number of search iterations and to improve
its efficiency, MGP delays as much as possible triggering new searches when the
goal changes over time. To this purpose, MGP uses two strategies: Open Check
(OC) that checks if the new goal is still in the current search tree and Plan
Follow (PF) that estimates whether executing actions of the current plan brings
MGP closer to the new goal. Moreover, MGP uses a parsimonious strategy to
update incrementally the search tree at each new search that reduces the number
of calls to the heuristic function and speeds up the search. Finally, we show
evaluation results that demonstrate the effectiveness of our approach.
","MGP: Un algorithme de planification temps r\'eel prenant en compte
  l'\'evolution dynamique du but","Damien Pellier and Micka\""el Vanneufville and Humbert Fiorino and Marc
  M\'etivier and Bruno Bouzy",2012,Artificial Intelligence,
"  In this paper, we present CAIO, a Cognitive and Affective
Interaction-Oriented architecture for social human-robot interactions (HRI),
allowing robots to reason on mental states (including emotions), and to act
physically, emotionally and verbally. We also present a short scenario and
implementation on a Nao robot.
",Une architecture cognitive et affective orient{\'e}e interaction,"Damien Pellier and Carole Adam and Wafa Johal and Humbert Fiorino and
  Sylvie Pesty",2016,Artificial Intelligence,
"  Many planning techniques have been developed to allow autonomous systems to
act and make decisions based on their perceptions of the environment. Among
these techniques, HTN ({\it Hierarchical Task Network}) planning is one of the
most used in practice. Unlike classical approaches of planning. HTN operates by
decomposing task into sub-tasks until each of these sub-tasks can be achieved
an action. This hierarchical representation provide a richer representation of
planning problems and allows to better guide the plan search and provides more
knowledge to the underlying algorithms. In this paper, we propose a new
approach of HTN planning in which, as in conventional planning, we instantiate
all planning operators before starting the search process. This approach has
proven its effectiveness in classical planning and is necessary for the
development of effective heuristics and encoding planning problems in other
formalism such as CSP or SAT. The instantiation is actually used by most modern
planners but has never been applied in an HTN based planning framework. We
present in this article a generic instantiation algorithm which implements many
simplification techniques to reduce the process complexity inspired from those
used in classical planning. Finally we present some results obtained from an
experimentation on a range of problems used in the international planning
competitions with a modified version of SHOP planner using fully instantiated
problems.
",Une approche totalement instanci\'ee pour la planification HTN,"Abdeldjalil Ramoul and Damien Pellier and Humbert Fiorino and Sylvie
  Pesty",2016,Artificial Intelligence,
"  Counterfactual Regret Minimization (CFR) is the leading framework for solving
large imperfect-information games. It converges to an equilibrium by
iteratively traversing the game tree. In order to deal with extremely large
games, abstraction is typically applied before running CFR. The abstracted game
is solved with tabular CFR, and its solution is mapped back to the full game.
This process can be problematic because aspects of abstraction are often manual
and domain specific, abstraction algorithms may miss important strategic
nuances of the game, and there is a chicken-and-egg problem because determining
a good abstraction requires knowledge of the equilibrium of the game. This
paper introduces Deep Counterfactual Regret Minimization, a form of CFR that
obviates the need for abstraction by instead using deep neural networks to
approximate the behavior of CFR in the full game. We show that Deep CFR is
principled and achieves strong performance in large poker games. This is the
first non-tabular variant of CFR to be successful in large games.
",Deep Counterfactual Regret Minimization,"Noam Brown, Adam Lerer, Sam Gross, Tuomas Sandholm",2019,Artificial Intelligence,
"  This article deals with the problem of the uncertainty in rule-based systems
(RBS), but from the perspective of quantum computing (QC). In this work we
first remember the characteristics of Quantum Rule-Based Systems (QRBS), a
concept defined in a previous article by one of the authors of this paper, and
we introduce the problem of quantum uncertainty. We assume that the subjective
uncertainty that affects the facts of classical RBSs can be treated as a direct
consequence of the probabilistic nature of quantum mechanics (QM), and we also
assume that the uncertainty associated with a given hypothesis is a consequence
of the propagation of the imprecision through the inferential circuits of RBSs.
This article does not intend to contribute anything new to the QM field: it is
a work of artificial intelligence (AI) that uses QC techniques to solve the
problem of uncertainty in RBSs. Bearing the above arguments in mind a quantum
model is proposed. This model has been applied to a problem already defined by
one of the authors of this work in a previous publication and which is briefly
described in this article. Then the model is generalized, and it is thoroughly
evaluated. The results obtained show that QC is a valid, effective and
efficient method to deal with the inherent uncertainty of RBSs
",Uncertainty in Quantum Rule-Based Systems,"Vicente Moret-Bonillo, Isaac Fern\'andez-Varela, Diego Alvarez-Estevez",2021,Artificial Intelligence,
"  Item response theory (IRT) can be applied to the analysis of the evaluation
of results from AI benchmarks. The two-parameter IRT model provides two
indicators (difficulty and discrimination) on the side of the item (or AI
problem) while only one indicator (ability) on the side of the respondent (or
AI agent). In this paper we analyse how to make this set of indicators dual, by
adding a fourth indicator, generality, on the side of the respondent.
Generality is meant to be dual to discrimination, and it is based on
difficulty. Namely, generality is defined as a new metric that evaluates
whether an agent is consistently good at easy problems and bad at difficult
ones. With the addition of generality, we see that this set of four key
indicators can give us more insight on the results of AI benchmarks. In
particular, we explore two popular benchmarks in AI, the Arcade Learning
Environment (Atari 2600 games) and the General Video Game AI competition. We
provide some guidelines to estimate and interpret these indicators for other AI
benchmarks and competitions.
","Analysing Results from AI Benchmarks: Key Indicators and How to Obtain
  Them",Fernando Mart\'inez-Plumed and Jos\'e Hern\'andez-Orallo,2018,Artificial Intelligence,
"  Embedding models for deterministic Knowledge Graphs (KG) have been
extensively studied, with the purpose of capturing latent semantic relations
between entities and incorporating the structured knowledge into machine
learning. However, there are many KGs that model uncertain knowledge, which
typically model the inherent uncertainty of relations facts with a confidence
score, and embedding such uncertain knowledge represents an unresolved
challenge. The capturing of uncertain knowledge will benefit many
knowledge-driven applications such as question answering and semantic search by
providing more natural characterization of the knowledge. In this paper, we
propose a novel uncertain KG embedding model UKGE, which aims to preserve both
structural and uncertainty information of relation facts in the embedding
space. Unlike previous models that characterize relation facts with binary
classification techniques, UKGE learns embeddings according to the confidence
scores of uncertain relation facts. To further enhance the precision of UKGE,
we also introduce probabilistic soft logic to infer confidence scores for
unseen relation facts during training. We propose and evaluate two variants of
UKGE based on different learning objectives. Experiments are conducted on three
real-world uncertain KGs via three tasks, i.e. confidence prediction, relation
fact ranking, and relation fact classification. UKGE shows effectiveness in
capturing uncertain knowledge by achieving promising results on these tasks,
and consistently outperforms baselines on these tasks.
",Embedding Uncertain Knowledge Graphs,"Xuelu Chen, Muhao Chen, Weijia Shi, Yizhou Sun, Carlo Zaniolo",2019,Artificial Intelligence,
"  Predicting the time to build software is a very complex task for software
engineering managers. There are complex factors that can directly interfere
with the productivity of the development team. Factors directly related to the
complexity of the system to be developed drastically change the time necessary
for the completion of the works with the software factories. This work proposes
the use of a hybrid system based on artificial neural networks and fuzzy
systems to assist in the construction of an expert system based on rules to
support in the prediction of hours destined to the development of software
according to the complexity of the elements present in the same. The set of
fuzzy rules obtained by the system helps the management and control of software
development by providing a base of interpretable estimates based on fuzzy
rules. The model was submitted to tests on a real database, and its results
were promissory in the construction of an aid mechanism in the predictability
of the software construction.
","Regularized Fuzzy Neural Networks to Aid Effort Forecasting in the
  Construction and Software Development","Paulo Vitor de Campos Souza, Augusto Junio Guimaraes, Vanessa Souza
  Araujo, Thiago Silva Rezende, Vinicius Jonathan Silva Araujo",2018,Artificial Intelligence,
"  As artificial intelligence (AI) systems become increasingly ubiquitous, the
topic of AI governance for ethical decision-making by AI has captured public
imagination. Within the AI research community, this topic remains less familiar
to many researchers. In this paper, we complement existing surveys, which
largely focused on the psychological, social and legal discussions of the
topic, with an analysis of recent advances in technical solutions for AI
governance. By reviewing publications in leading AI conferences including AAAI,
AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four
areas: 1) exploring ethical dilemmas; 2) individual ethical decision
frameworks; 3) collective ethical decision frameworks; and 4) ethics in
human-AI interactions. We highlight the intuitions and key techniques used in
each approach, and discuss promising future research directions towards
successful integration of ethical AI systems into human societies.
",Building Ethics into Artificial Intelligence,"Han Yu, Zhiqi Shen, Chunyan Miao, Cyril Leung, Victor R. Lesser and
  Qiang Yang",2018,Artificial Intelligence,
"  The problem of allocating students to supervisors for the development of a
personal project or a dissertation is a crucial activity in the higher
education environment, as it enables students to get feedback on their work
from an expert and improve their personal, academic, and professional
abilities. In this article, we propose a multi-objective and near Pareto
optimal genetic algorithm for the allocation of students to supervisors. The
allocation takes into consideration the students and supervisors' preferences
on research/project topics, the lower and upper supervision quotas of
supervisors, as well as the workload balance amongst supervisors. We introduce
novel mutation and crossover operators for the student-supervisor allocation
problem. The experiments carried out show that the components of the genetic
algorithm are more apt for the problem than classic components, and that the
genetic algorithm is capable of producing allocations that are near Pareto
optimal in a reasonable time.
","A near Pareto optimal approach to student-supervisor allocation with two
  sided preferences and workload balance","Victor Sanchez-Anguix, Rithin Chalumuri, Reyhan Aydogan, Vicente
  Julian",2018,Artificial Intelligence,
"  In artificial intelligence (AI) mediated workforce management systems (e.g.,
crowdsourcing), long-term success depends on workers accomplishing tasks
productively and resting well. This dual objective can be summarized by the
concept of productive laziness. Existing scheduling approaches mostly focus on
efficiency but overlook worker wellbeing through proper rest. In order to
enable workforce management systems to follow the IEEE Ethically Aligned Design
guidelines to prioritize worker wellbeing, we propose a distributed
Computational Productive Laziness (CPL) approach in this paper. It
intelligently recommends personalized work-rest schedules based on local data
concerning a worker's capabilities and situational factors to incorporate
opportunistic resting and achieve superlinear collective productivity without
the need for explicit coordination messages. Extensive experiments based on a
real-world dataset of over 5,000 workers demonstrate that CPL enables workers
to spend 70% of the effort to complete 90% of the tasks on average, providing
more ethically aligned scheduling than existing approaches.
",Ethically Aligned Opportunistic Scheduling for Productive Laziness,"Han Yu, Chunyan Miao, Yongqing Zheng, Lizhen Cui, Simon Fauvel and
  Cyril Leung",2019,Artificial Intelligence,
"  Regional innovation is more and more considered an important enabler of
welfare. It is no coincidence that the European Commission has started looking
at regional peculiarities and dynamics, in order to focus Research and
Innovation Strategies for Smart Specialization towards effective investment
policies. In this context, this work aims to support policy makers in the
analysis of innovation-relevant trends. We exploit a European database of the
regional patent application to determine the dynamics of a set of technological
innovation indicators. For this purpose, we design and develop a software
system for assessing unfolding trends in such indicators. In contrast with
conventional knowledge-based design, our approach is biologically-inspired and
based on self-organization of information. This means that a functional
structure, called track, appears and stays spontaneous at runtime when local
dynamism in data occurs. A further prototyping of tracks allows a better
distinction of the critical phenomena during unfolding events, with a better
assessment of the progressing levels. The proposed mechanism works if
structural parameters are correctly tuned for the given historical context.
Determining such correct parameters is not a simple task since different
indicators may have different dynamics. For this purpose, we adopt an
adaptation mechanism based on differential evolution. The study includes the
problem statement and its characterization in the literature, as well as the
proposed solving approach, experimental setting and results.
","An adaptive stigmergy-based system for evaluating technological
  indicator dynamics in the context of smart specialization","A.L. Alfeo, F.P. Appio, M.G.C.A. Cimino, A. Lazzeri, A. Martini, G.
  Vaglini",2016,Artificial Intelligence,
"  Predicting the behavior of surrounding vehicles is a critical problem in
automated driving. We present a novel game theoretic behavior prediction model
that achieves state of the art prediction accuracy by explicitly reasoning
about possible future interaction between agents. We evaluate our approach on
the NGSIM vehicle trajectory data set and demonstrate lower root mean square
error than state-of-the-art methods.
",Multi-Fidelity Recursive Behavior Prediction,"Mihir Jain, Kyle Brown, Ahmed K. Sadek",2018,Artificial Intelligence,
"  Its constant technological evolution characterizes the contemporary world,
and every day the processes, once manual, become computerized. Data are stored
in the cyberspace, and as a consequence, one must increase the concern with the
security of this environment. Cyber-attacks are represented by a growing
worldwide scale and are characterized as one of the significant challenges of
the century. This article aims to propose a computational system based on
intelligent hybrid models, which through fuzzy rules allows the construction of
expert systems in cybernetic data attacks, focusing on the SQL Injection
attack. The tests were performed with real bases of SQL Injection attacks on
government computers, using fuzzy neural networks. According to the results
obtained, the feasibility of constructing a system based on fuzzy rules, with
the classification accuracy of cybernetic invasions within the margin of the
standard deviation (compared to the state-of-the-art model in solving this type
of problem) is real. The model helps countries prepare to protect their data
networks and information systems, as well as create opportunities for expert
systems to automate the identification of attacks in cyberspace.
","Fuzzy neural networks to create an expert system for detecting attacks
  by SQL Injection","Lucas Oliveira Batista, Gabriel Adriano de Silva, Vanessa Souza
  Ara\'ujo, Vin\'icius Jonathan Silva Ara\'ujo, Thiago Silva Rezende, Augusto
  Junio Guimar\~aes, Paulo Vitor de Campos Souza",2018,Artificial Intelligence,
"  In many problem settings, most notably in game playing, an agent receives a
possibly delayed reward for its actions. Often, those rewards are handcrafted
and not naturally given. Even simple terminal-only rewards, like winning equals
1 and losing equals -1, can not be seen as an unbiased statement, since these
values are chosen arbitrarily, and the behavior of the learner may change with
different encodings, such as setting the value of a loss to -0:5, which is
often done in practice to encourage learning. It is hard to argue about good
rewards and the performance of an agent often depends on the design of the
reward signal. In particular, in domains where states by nature only have an
ordinal ranking and where meaningful distance information between game state
values are not available, a numerical reward signal is necessarily biased. In
this paper, we take a look at Monte Carlo Tree Search (MCTS), a popular
algorithm to solve MDPs, highlight a reoccurring problem concerning its use of
rewards, and show that an ordinal treatment of the rewards overcomes this
problem. Using the General Video Game Playing framework we show a dominance of
our newly proposed ordinal MCTS algorithm over preference-based MCTS, vanilla
MCTS and various other MCTS variants.
",Ordinal Monte Carlo Tree Search,"Tobias Joppen and Johannes F\""urnkranz",2020,Artificial Intelligence,
"  In this paper, We Apply Reinforcement learning (RL) techniques to train a
realistic biomechanical model to work with different people and on different
walking environments. We benchmarking 3 RL algorithms: Deep Deterministic
Policy Gradient (DDPG), Trust Region Policy Optimization (TRPO) and Proximal
Policy Optimization (PPO) in OpenSim environment, Also we apply imitation
learning to a prosthetics domain to reduce the training time needed to design
customized prosthetics. We use DDPG algorithm to train an original expert
agent. We then propose a modification to the Dataset Aggregation (DAgger)
algorithm to reuse the expert knowledge and train a new target agent to
replicate that behaviour in fewer than 5 iterations, compared to the 100
iterations taken by the expert agent which means reducing training time by 95%.
Our modifications to the DAgger algorithm improve the balance between
exploiting the expert policy and exploring the environment. We show empirically
that these improve convergence time of the target agent, particularly when
there is some degree of variation between expert and naive agent.
",Transfer Learning for Prosthetics Using Imitation Learning,"Montaser Mohammedalamen, Waleed D. Khamies, Benjamin Rosman",2018,Artificial Intelligence,
"  The availability of high-fidelity energy networks brings significant value to
academic and commercial research. However, such releases also raise fundamental
concerns related to privacy and security as they can reveal sensitive
commercial information and expose system vulnerabilities. This paper
investigates how to release power networks where the parameters of transmission
lines and transformers are obfuscated. It does so by using the framework of
Differential Privacy (DP), that provides strong privacy guarantees and has
attracted significant attention in recent years. Unfortunately, simple DP
mechanisms often result in AC-infeasible networks. To address these concerns,
this paper presents a novel differential privacy mechanism that guarantees
AC-feasibility and largely preserves the fidelity of the obfuscated network.
Experimental results also show that the obfuscation significantly reduces the
potential damage of an attacker exploiting the release of the dataset.
",Differential Privacy for Power Grid Obfuscation,"Ferdinando Fioretto, Terrence W.K. Mak, Pascal Van Hentenryck",2020,Artificial Intelligence,
"  Collaborative filtering (CF) is the key technique for recommender systems
(RSs). CF exploits user-item behavior interactions (e.g., clicks) only and
hence suffers from the data sparsity issue. One research thread is to integrate
auxiliary information such as product reviews and news titles, leading to
hybrid filtering methods. Another thread is to transfer knowledge from other
source domains such as improving the movie recommendation with the knowledge
from the book domain, leading to transfer learning methods. In real-world life,
no single service can satisfy a user's all information needs. Thus it motivates
us to exploit both auxiliary and source information for RSs in this paper. We
propose a novel neural model to smoothly enable Transfer Meeting Hybrid (TMH)
methods for cross-domain recommendation with unstructured text in an end-to-end
manner. TMH attentively extracts useful content from unstructured text via a
memory module and selectively transfers knowledge from a source domain via a
transfer network. On two real-world datasets, TMH shows better performance in
terms of three ranking metrics by comparing with various baselines. We conduct
thorough analyses to understand how the text content and transferred knowledge
help the proposed model.
","Transfer Meets Hybrid: A Synthetic Approach for Cross-Domain
  Collaborative Filtering with Text","Guangneng Hu, Yu Zhang, and Qiang Yang",2019,Artificial Intelligence,
"  Learning in multi-agent scenarios is a fruitful research direction, but
current approaches still show scalability problems in multiple games with
general reward settings and different opponent types. The Multi-Agent
Reinforcement Learning in Malm\""O (MARL\""O) competition is a new challenge that
proposes research in this domain using multiple 3D games. The goal of this
contest is to foster research in general agents that can learn across different
games and opponent types, proposing a challenge as a milestone in the direction
of Artificial General Intelligence.
","The Multi-Agent Reinforcement Learning in Malm\""O (MARL\""O) Competition","Diego Perez-Liebana, Katja Hofmann, Sharada Prasanna Mohanty, Noburu
  Kuno, Andre Kramer, Sam Devlin, Raluca D. Gaina, Daniel Ionita",2018,Artificial Intelligence,
"  Under the project Maccoy Critical, we would like to train individuals, in
virtual environments, to handle critical situations such as dilemmas. These
latter refer to situations where there is no ``good'' solution. In other words,
situations that lead to negative consequences whichever choice is made. Our
objective is to use Knowledge Models to extract necessary properties for
dilemmas to emerge. To do so, our approach consists in developing a Scenario
Orchestration System that generates dilemma situations dynamically without
having to write them beforehand. In this paper we present this approach and
expose a proof of concept of the generation process.
","A model for prohibition and obligation dilemmas generation in virtual
  environments","Azzeddine Benabbou (Heudiasyc), Domitile Lourdeaux (Heudiasyc),
  Dominique Lenne (Heudiasyc)",2018,Artificial Intelligence,
"  Robotic mobile fulfillment systems (RMFSs) are a new type of warehousing
system, which has received more attention recently, due to increasing growth in
the e-commerce sector. Instead of sending pickers to the inventory area to
search for and pick the ordered items, robots carry shelves (called ""pods"")
including ordered items from the inventory area to picking stations. In the
picking stations, human pickers put ordered items into totes; then these items
are transported by a conveyor to the packing stations. This type of warehousing
system relieves the human pickers and improves the picking process. In this
paper, we concentrate on decisions about the assignment of pods to stations and
orders to stations to fulfill picking for each incoming customer's order. In
previous research for an RMFS with multiple picking stations, these decisions
are made sequentially. Instead, we present a new integrated model. To improve
the system performance even more, we extend our model by splitting orders. This
means parts of an order are allowed to be picked at different stations. To the
best of the authors' knowledge, this is the first publication on split orders
in an RMFS. We analyze different performance metrics, such as pile-on,
pod-station visits, robot moving distance and order turn-over time. We compare
the results of our models in different instances with the sequential method in
our open-source simulation framework RAWSim-O.
",Efficient order picking methods in robotic mobile fulfillment systems,"Lin Xie, Nils Thieme, Ruslan Krenzler, Hanyi Li",2021,Artificial Intelligence,
"  Today's AI still faces two major challenges. One is that in most industries,
data exists in the form of isolated islands. The other is the strengthening of
data privacy and security. We propose a possible solution to these challenges:
secure federated learning. Beyond the federated learning framework first
proposed by Google in 2016, we introduce a comprehensive secure federated
learning framework, which includes horizontal federated learning, vertical
federated learning and federated transfer learning. We provide definitions,
architectures and applications for the federated learning framework, and
provide a comprehensive survey of existing works on this subject. In addition,
we propose building data networks among organizations based on federated
mechanisms as an effective solution to allow knowledge to be shared without
compromising user privacy.
",Federated Machine Learning: Concept and Applications,"Qiang Yang, Yang Liu, Tianjian Chen, Yongxin Tong",2019,Artificial Intelligence,
"  Numerous, artificially intelligent, networked things will populate the
battlefield of the future, operating in close collaboration with human
warfighters, and fighting as teams in highly adversarial environments. This
chapter explores the characteristics, capabilities and intelli-gence required
of such a network of intelligent things and humans - Internet of Battle Things
(IOBT). The IOBT will experience unique challenges that are not yet well
addressed by the current generation of AI and machine learning.
",Intelligent Autonomous Things on the Battlefield,"Alexander Kott, Ethan Stump",2019,Artificial Intelligence,
"  Navigating and understanding the real world remains a key challenge in
machine learning and inspires a great variety of research in areas such as
language grounding, planning, navigation and computer vision. We propose an
instruction-following task that requires all of the above, and which combines
the practicality of simulated environments with the challenges of ambiguous,
noisy real world data. StreetNav is built on top of Google Street View and
provides visually accurate environments representing real places. Agents are
given driving instructions which they must learn to interpret in order to
successfully navigate in this environment. Since humans equipped with driving
instructions can readily navigate in previously unseen cities, we set a high
bar and test our trained agents for similar cognitive capabilities. Although
deep reinforcement learning (RL) methods are frequently evaluated only on data
that closely follow the training distribution, our dataset extends to multiple
cities and has a clean train/test separation. This allows for thorough testing
of generalisation ability. This paper presents the StreetNav environment and
tasks, models that establish strong baselines, and extensive analysis of the
task and the trained agents.
",Learning To Follow Directions in Street View,"Karl Moritz Hermann, Mateusz Malinowski, Piotr Mirowski, Andras
  Banki-Horvath, Keith Anderson, Raia Hadsell",2020,Artificial Intelligence,
"  This paper presents a novel approach for learning STRIPS action models from
examples that compiles this inductive learning task into a classical planning
task. Interestingly, the compilation approach is flexible to different amounts
of available input knowledge; the learning examples can range from a set of
plans (with their corresponding initial and final states) to just a pair of
initial and final states (no intermediate action or state is given). Moreover,
the compilation accepts partially specified action models and it can be used to
validate whether the observation of a plan execution follows a given STRIPS
action model, even if this model is not fully specified.
",Learning STRIPS Action Models with Classical Planning,"Diego Aineto, Sergio Jim\'enez and Eva Onaindia",2018,Artificial Intelligence,
"  Learning models of user behaviour is an important problem that is broadly
applicable across many application domains requiring human-robot interaction.
In this work we show that it is possible to learn a generative model for
distinct user behavioral types, extracted from human demonstrations, by
enforcing clustering of preferred task solutions within the latent space. We
use this model to differentiate between user types and to find cases with
overlapping solutions. Moreover, we can alter an initially guessed solution to
satisfy the preferences that constitute a particular user type by
backpropagating through the learned differentiable model. An advantage of
structuring generative models in this way is that it allows us to extract
causal relationships between symbols that might form part of the user's
specification of the task, as manifested in the demonstrations. We show that
the proposed method is capable of correctly distinguishing between three user
types, who differ in degrees of cautiousness in their motion, while performing
the task of moving objects with a kinesthetically driven robot in a tabletop
environment. Our method successfully identifies the correct type, within the
specified time, in 99% [97.8 - 99.8] of the cases, which outperforms an IRL
baseline. We also show that our proposed method correctly changes a default
trajectory to one satisfying a particular user specification even with unseen
objects. The resulting trajectory is shown to be directly implementable on a
PR2 humanoid robot completing the same task.
",Using Causal Analysis to Learn Specifications from Task Demonstrations,"Daniel Angelov, Yordan Hristov, Subramanian Ramamoorthy",2019,Artificial Intelligence,
"  Argumentation theory is a powerful paradigm that formalizes a type of
commonsense reasoning that aims to simulate the human ability to resolve a
specific problem in an intelligent manner. A classical argumentation process
takes into account only the properties related to the intrinsic logical
soundness of an argument in order to determine its acceptability status.
However, these properties are not always the only ones that matter to establish
the argument's acceptability---there exist other qualities, such as strength,
weight, social votes, trust degree, relevance level, and certainty degree,
among others.
","An Approach to Characterize Graded Entailment of Arguments through a
  Label-based Framework","Maximiliano C. D. Bud\'an, Gerardo I. Simari, Ignacio Viglizzo and
  Guillermo R. Simari",2017,Artificial Intelligence,
"  A Timed Argumentation Framework (TAF) is a formalism where arguments are only
valid for consideration in a given period of time, called availability
intervals, which are defined for every individual argument. The original
proposal is based on a single, abstract notion of attack between arguments that
remains static and permanent in time. Thus, in general, when identifying the
set of acceptable arguments, the outcome associated with a TAF will vary over
time. In this work we introduce an extension of TAF adding the capability of
modeling a support relation between arguments. In this sense, the resulting
framework provides a suitable model for different time-dependent issues. Thus,
the main contribution here is to provide an enhanced framework for modeling a
positive (support) and negative (attack) interaction varying over time, which
are relevant in many real-world situations. This leads to a Timed Bipolar
Argumentation Framework (T-BAF), where classical argument extensions can be
defined. The proposal aims at advancing in the integration of temporal
argumentation in different application domain.
",Bipolar in Temporal Argumentation Framework,"Maximiliano C. D. Bud\'an, Maria Laura Cobo, Diego C. Martinez and
  Guillermo R. Simari",2017,Artificial Intelligence,
"  In this work, we enrich a formalism for argumentation by including a formal
characterization of features related to the knowledge, in order to capture
proper reasoning in legal domains. We add meta-data information to the
arguments in the form of labels representing quantitative and qualitative data
about them. These labels are propagated through an argumentative graph
according to the relations of support, conflict, and aggregation between
arguments.
",Dealing with Qualitative and Quantitative Features in Legal Domains,"Maximiliano C. D. Bud\'an, Mar\'ia Laura Cobo, Diego I. Mart\'inez and
  Antonino Rotolo",2018,Artificial Intelligence,
"  Current advances in research, development and application of artificial
intelligence (AI) systems have yielded a far-reaching discourse on AI ethics.
In consequence, a number of ethics guidelines have been released in recent
years. These guidelines comprise normative principles and recommendations aimed
to harness the ""disruptive"" potentials of new AI technologies. Designed as a
comprehensive evaluation, this paper analyzes and compares these guidelines
highlighting overlaps but also omissions. As a result, I give a detailed
overview of the field of AI ethics. Finally, I also examine to what extent the
respective ethical principles and values are implemented in the practice of
research, development and application of AI systems - and how the effectiveness
in the demands of AI ethics can be improved.
",The Ethics of AI Ethics -- An Evaluation of Guidelines,Thilo Hagendorff,2020,Artificial Intelligence,
"  The paper is a half-way between the agent technology and the mathematical
reasoning to model tactical decision making tasks. These models are applied to
air defense (AD) domain for command and control (C2). It also addresses the
issues related to evaluation of agents. The agents are designed and implemented
using the agent-programming paradigm. The agents are deployed in an air combat
simulated environment for performing the tasks of C2 like electronic counter
counter measures, threat assessment, and weapon allocation. The simulated AD
system runs without any human intervention, and represents state-of-the-art
model for C2 autonomy. The use of agents as autonomous decision making entities
is particularly useful in view of futuristic network centric warfare.
","Modeling Intelligent Decision Making Command And Control Agents: An
  Application to Air Defense",Sumanta Kumar Das,2014,Artificial Intelligence,
"  Interactive reinforcement learning has become an important apprenticeship
approach to speed up convergence in classic reinforcement learning problems. In
this regard, a variant of interactive reinforcement learning is policy shaping
which uses a parent-like trainer to propose the next action to be performed and
by doing so reduces the search space by advice. On some occasions, the trainer
may be another artificial agent which in turn was trained using reinforcement
learning methods to afterward becoming an advisor for other learner-agents. In
this work, we analyze internal representations and characteristics of
artificial agents to determine which agent may outperform others to become a
better trainer-agent. Using a polymath agent, as compared to a specialist
agent, an advisor leads to a larger reward and faster convergence of the reward
signal and also to a more stable behavior in terms of the state visit frequency
of the learner-agents. Moreover, we analyze system interaction parameters in
order to determine how influential they are in the apprenticeship process,
where the consistency of feedback is much more relevant when dealing with
different learner obedience parameters.
",Improving interactive reinforcement learning: What makes a good teacher?,"Francisco Cruz, Sven Magg, Yukie Nagai, Stefan Wermter",2018,Artificial Intelligence,
"  Many computer models such as cellular automata and artificial neural networks
have been developed and successfully applied. However, in some cases, these
models might be restrictive on the possible solutions or their solutions might
be difficult to interpret. To overcome this problem, we outline a new approach,
the so-called allagmatic method, that automatically programs and executes
models with as little limitations as possible while maintaining human
interpretability. Earlier we described a metamodel and its building blocks
according to the philosophical concepts of structure (spatial dimension) and
operation (temporal dimension). They are entity, milieu, and update function
that together abstractly describe cellular automata, artificial neural
networks, and possibly any kind of computer model. By automatically combining
these building blocks in an evolutionary computation, interpretability might be
increased by the relationship to the metamodel, and models might be translated
into more interpretable models via the metamodel. We propose generic and
object-oriented programming to implement the entities and their milieus as
dynamic and generic arrays and the update function as a method. We show two
experiments where a simple cellular automaton and an artificial neural network
are automatically programmed, compiled, and executed. A target state is
successfully evolved and learned in the cellular automaton and artificial
neural network, respectively. We conclude that the allagmatic method can create
and execute cellular automaton and artificial neural network models in an
automated manner with the guidance of philosophy.
","Automatic Programming of Cellular Automata and Artificial Neural
  Networks Guided by Philosophy",Patrik Christen and Olivier Del Fabbro,2020,Artificial Intelligence,
"  This paper surveys an approach to the XAI problem, using post-hoc explanation
by example, that hinges on twinning Artificial Neural Networks (ANNs) with
Case-Based Reasoning (CBR) systems, so-called ANN-CBR twins. A systematic
survey of 1100+ papers was carried out to identify the fragmented literature on
this topic and to trace it influence through to more recent work involving Deep
Neural Networks (DNNs). The paper argues that this twin-system approach,
especially using ANN-CBR twins, presents one possible coherent, generic
solution to the XAI problem (and, indeed, XCBR problem). The paper concludes by
road-mapping some future directions for this XAI solution involving (i) further
tests of feature-weighting techniques, (iii) explorations of how explanatory
cases might best be deployed (e.g., in counterfactuals, near-miss cases, a
fortori cases), and (iii) the raising of the unwelcome and, much ignored, issue
of human user evaluation.
","How Case Based Reasoning Explained Neural Networks: An XAI Survey of
  Post-Hoc Explanation-by-Example in ANN-CBR Twins",Mark T Keane and Eoin M Kenny,2019,Artificial Intelligence,
"  Over the past decade, knowledge graphs became popular for capturing
structured domain knowledge. Relational learning models enable the prediction
of missing links inside knowledge graphs. More specifically, latent distance
approaches model the relationships among entities via a distance between latent
representations. Translating embedding models (e.g., TransE) are among the most
popular latent distance approaches which use one distance function to learn
multiple relation patterns. However, they are mostly inefficient in capturing
symmetric relations since the representation vector norm for all the symmetric
relations becomes equal to zero. They also lose information when learning
relations with reflexive patterns since they become symmetric and transitive.
We propose the Multiple Distance Embedding model (MDE) that addresses these
limitations and a framework to collaboratively combine variant latent
distance-based terms. Our solution is based on two principles: 1) we use a
limit-based loss instead of a margin ranking loss and, 2) by learning
independent embedding vectors for each of the terms we can collectively train
and predict using contradicting distance terms. We further demonstrate that MDE
allows modeling relations with (anti)symmetry, inversion, and composition
patterns. We propose MDE as a neural network model that allows us to map
non-linear relations between the embedding vectors and the expected output of
the score function. Our empirical results show that MDE performs competitively
to state-of-the-art embedding models on several benchmark datasets.
","MDE: Multiple Distance Embeddings for Link Prediction in Knowledge
  Graphs","Afshin Sadeghi, Damien Graux, Hamed Shariat Yazdi, Jens Lehmann",2020,Artificial Intelligence,
"  We propose a set of compositional design patterns to describe a large variety
of systems that combine statistical techniques from machine learning with
symbolic techniques from knowledge representation. As in other areas of
computer science (knowledge engineering, software engineering, ontology
engineering, process mining and others), such design patterns help to
systematize the literature, clarify which combinations of techniques serve
which purposes, and encourage re-use of software components. We have validated
our set of compositional design patterns against a large body of recent
literature.
",A Boxology of Design Patterns for Hybrid Learning and Reasoning Systems,Frank van Harmelen and Annette ten Teije,2019,Artificial Intelligence,
"  In this paper, we present a simple and cheap ordinal bucketing algorithm that
approximately generates $q$-quantiles from an incremental data stream. The
bucketing is done dynamically in the sense that the amount of buckets $q$
increases with the number of seen samples. We show how this can be used in
Ordinal Monte Carlo Tree Search (OMCTS) to yield better bounds on time and
space complexity, especially in the presence of noisy rewards. Besides
complexity analysis and quality tests of quantiles, we evaluate our method
using OMCTS in the General Video Game Framework (GVGAI). Our results
demonstrate its dominance over vanilla Monte Carlo Tree Search in the presence
of noise, where OMCTS without bucketing has a very bad time and space
complexity.
",Ordinal Bucketing for Game Trees using Dynamic Quantile Approximation,"Tobias Joppen, Tilman Str\""ubig, Johannes F\""urnkranz",2019,Artificial Intelligence,
"  Kandinsky Figures and Kandinsky Patterns are mathematically describable,
simple self-contained hence controllable test data sets for the development,
validation and training of explainability in artificial intelligence. Whilst
Kandinsky Patterns have these computationally manageable properties, they are
at the same time easily distinguishable from human observers. Consequently,
controlled patterns can be described by both humans and computers. We define a
Kandinsky Pattern as a set of Kandinsky Figures, where for each figure an
""infallible authority"" defines that the figure belongs to the Kandinsky
Pattern. With this simple principle we build training and validation data sets
for automatic interpretability and context learning. In this paper we describe
the basic idea and some underlying principles of Kandinsky Patterns and provide
a Github repository to invite the international machine learning research
community to a challenge to experiment with our Kandinsky Patterns to expand
and thus make progress in the field of explainable AI and to contribute to the
upcoming field of explainability and causability.
",Kandinsky Patterns,Heimo Mueller and Andreas Holzinger,2021,Artificial Intelligence,
"  In this paper, we define and apply representational stability analysis
(ReStA), an intuitive way of analyzing neural language models. ReStA is a
variant of the popular representational similarity analysis (RSA) in cognitive
neuroscience. While RSA can be used to compare representations in models, model
components, and human brains, ReStA compares instances of the same model, while
systematically varying single model parameter. Using ReStA, we study four
recent and successful neural language models, and evaluate how sensitive their
internal representations are to the amount of prior context. Using RSA, we
perform a systematic study of how similar the representational spaces in the
first and second (or higher) layers of these models are to each other and to
patterns of activation in the human brain. Our results reveal surprisingly
strong differences between language models, and give insights into where the
deep linguistic processing, that integrates information over multiple
sentences, is happening in these models. The combination of ReStA and RSA on
models and brains allows us to start addressing the important question of what
kind of linguistic processes we can hope to observe in fMRI brain imaging data.
In particular, our results suggest that the data on story reading from Wehbe et
al. (2014) contains a signal of shallow linguistic processing, but show no
evidence on the more interesting deep linguistic processing.
","Blackbox meets blackbox: Representational Similarity and Stability
  Analysis of Neural Language Models and Brains","Samira Abnar, Lisa Beinborn, Rochelle Choenni, Willem Zuidema",2019,Artificial Intelligence,
"  In the last decades we have witnessed the success of applications of
Artificial Intelligence to playing games. In this work we address the
challenging field of games with hidden information and card games in
particular. Jass is a very popular card game in Switzerland and is closely
connected with Swiss culture. To the best of our knowledge, performances of
Artificial Intelligence agents in the game of Jass do not outperform top
players yet. Our contribution to the community is two-fold. First, we provide
an overview of the current state-of-the-art of Artificial Intelligence methods
for card games in general. Second, we discuss their application to the use-case
of the Swiss card game Jass. This paper aims to be an entry point for both
seasoned researchers and new practitioners who want to join in the Jass
challenge.
","Survey of Artificial Intelligence for Card Games and Its Application to
  the Swiss Game Jass","Joel Niklaus, Michele Alberti, Vinaychandran Pondenkandath, Rolf
  Ingold, Marcus Liwicki",2019,Artificial Intelligence,
"  This article describes the application of soft computing methods for solving
the problem of locating garbage accumulation points in urban scenarios. This is
a relevant problem in modern smart cities, in order to reduce negative
environmental and social impacts in the waste management process, and also to
optimize the available budget from the city administration to install waste
bins. A specific problem model is presented, which accounts for reducing the
investment costs, enhance the number of citizens served by the installed bins,
and the accessibility to the system. A family of single- and multi-objective
heuristics based on the PageRank method and two mutiobjective evolutionary
algorithms are proposed. Experimental evaluation performed on real scenarios on
the cities of Montevideo (Uruguay) and Bahia Blanca (Argentina) demonstrates
the effectiveness of the proposed approaches. The methods allow computing
plannings with different trade-off between the problem objectives. The computed
results improve over the current planning in Montevideo and provide a
reasonable budget cost and quality of service for Bahia Blanca.
","Soft computing methods for multiobjective location of garbage
  accumulation points in smart cities","Jamal Toutouh, Diego Rossit, and Sergio Nesmachnow",2019,Artificial Intelligence,
"  Ontology-based knowledge bases (KBs) like DBpedia are very valuable
resources, but their usefulness and usability is limited by various quality
issues. One such issue is the use of string literals instead of semantically
typed entities. In this paper we study the automated canonicalization of such
literals, i.e., replacing the literal with an existing entity from the KB or
with a new entity that is typed using classes from the KB. We propose a
framework that combines both reasoning and machine learning in order to predict
the relevant entities and types, and we evaluate this framework against
state-of-the-art baselines for both semantic typing and entity matching.
",Canonicalizing Knowledge Base Literals,Jiaoyan Chen and Ernesto Jimenez-Ruiz and Ian Horrocks,2019,Artificial Intelligence,
"  Making decisions in complex environments is a key challenge in artificial
intelligence (AI). Situations involving multiple decision makers are
particularly complex, leading to computational intractability of principled
solution methods. A body of work in AI has tried to mitigate this problem by
trying to distill interaction to its essence: how does the policy of one agent
influence another agent? If we can find more compact representations of such
influence, this can help us deal with the complexity, for instance by searching
the space of influences rather than the space of policies. However, so far
these notions of influence have been restricted in their applicability to
special cases of interaction. In this paper we formalize influence-based
abstraction (IBA), which facilitates the elimination of latent state factors
without any loss in value, for a very general class of problems described as
factored partially observable stochastic games (fPOSGs). On the one hand, this
generalizes existing descriptions of influence, and thus can serve as the
foundation for improvements in scalability and other insights in decision
making in complex multiagent settings. On the other hand, since the presence of
other agents can be seen as a generalization of single agent settings, our
formulation of IBA also provides a sufficient statistic for decision making
under abstraction for a single agent. We also give a detailed discussion of the
relations to such previous works, identifying new insights and interpretations
of these approaches. In these ways, this paper deepens our understanding of
abstraction in a wide range of sequential decision making settings, providing
the basis for new approaches and algorithms for a large class of problems.
","A Sufficient Statistic for Influence in Structured Multiagent
  Environments","Frans A. Oliehoek, Stefan Witwicki, Leslie P. Kaelbling",2021,Artificial Intelligence,
"  This paper deals with robust optimization applied to network flows. Two
robust variants of the minimum-cost integer flow problem are considered.
Thereby, uncertainty in problem formulation is limited to arc unit costs and
expressed by a finite set of explicitly given scenarios. It is shown that both
problem variants are NP-hard. To solve the considered variants, several
heuristics based on local search or evolutionary computing are proposed. The
heuristics are experimentally evaluated on appropriate problem instances.
","Heuristic solutions to robust variants of the minimum-cost integer flow
  problem","Marko \v{S}poljarec, Robert Manger",2020,Artificial Intelligence,
"  Humans as designers have quite versatile problem-solving strategies. Computer
agents on the other hand can access large scale computational resources to
solve certain design problems. Hence, if agents can learn from human behavior,
a synergetic human-agent problem solving team can be created. This paper
presents an approach to extract human design strategies and implicit rules,
purely from historical human data, and use that for design generation. A
two-step framework that learns to imitate human design strategies from
observation is proposed and implemented. This framework makes use of deep
learning constructs to learn to generate designs without any explicit
information about objective and performance metrics. The framework is designed
to interact with the problem through a visual interface as humans did when
solving the problem. It is trained to imitate a set of human designers by
observing their design state sequences without inducing problem-specific
modelling bias or extra information about the problem. Furthermore, an
end-to-end agent is developed that uses this deep learning framework as its
core in conjunction with image processing to map pixel-to-design moves as a
mechanism to generate designs. Finally, the designs generated by a
computational team of these agents are then compared to actual human data for
teams solving a truss design problem. Results demonstrates that these agents
are able to create feasible and efficient truss designs without guidance,
showing that this methodology allows agents to learn effective design
strategies.
","Learning to design from humans: Imitating human designers through deep
  learning","Ayush Raina, Christopher McComb and Jonathan Cagan",2019,Artificial Intelligence,
"  This paper is the preprint of an invited commentary on Lake et al's
Behavioral and Brain Sciences article titled ""Building machines that learn and
think like people"". Lake et al's paper offers a timely critique on the recent
accomplishments in artificial intelligence from the vantage point of human
intelligence, and provides insightful suggestions about research directions for
building more human-like intelligence. Since we agree with most of the points
raised in that paper, we will offer a few points that are complementary.
",What can the brain teach us about building artificial intelligence?,Dileep George,2017,Artificial Intelligence,
"  Over the last year, the amount of research in hierarchical planning has
increased, leading to significant improvements in the performance of planners.
However, the research is diverging and planners are somewhat hard to compare
against each other. This is mostly caused by the fact that there is no standard
set of benchmark domains, nor even a common description language for
hierarchical planning problems. As a consequence, the available planners
support a widely varying set of features and (almost) none of them can solve
(or even parse) any problem developed for another planner. With this paper, we
propose to create a new track for the IPC in which hierarchical planners will
compete. This competition will result in a standardised description language,
broader support for core features of that language among planners, a set of
benchmark problems, a means to fairly and objectively compare HTN planners, and
for new challenges for planners.
",Hierarchical Planning in the IPC,"D. H\""oller, G. Behnke, P. Bercher, S. Biundo, H. Fiorino, D. Pellier,
  R. Alford",2019,Artificial Intelligence,
"  This paper introduces Strict Partial Order Networks (SPON), a novel neural
network architecture designed to enforce asymmetry and transitive properties as
soft constraints. We apply it to induce hypernymy relations by training with
is-a pairs. We also present an augmented variant of SPON that can generalize
type information learned for in-vocabulary terms to previously unseen ones. An
extensive evaluation over eleven benchmarks across different tasks shows that
SPON consistently either outperforms or attains the state of the art on all but
one of these benchmarks.
",Hypernym Detection Using Strict Partial Order Networks,"Sarthak Dash, Md Faisal Mahbub Chowdhury, Alfio Gliozzo, Nandana
  Mihindukulasooriya, Nicolas Rodolfo Fauceglia",2020,Artificial Intelligence,
"  Active inference is a first principle account of how autonomous agents
operate in dynamic, non-stationary environments. This problem is also
considered in reinforcement learning (RL), but limited work exists on comparing
the two approaches on the same discrete-state environments. In this paper, we
provide: 1) an accessible overview of the discrete-state formulation of active
inference, highlighting natural behaviors in active inference that are
generally engineered in RL; 2) an explicit discrete-state comparison between
active inference and RL on an OpenAI gym baseline. We begin by providing a
condensed overview of the active inference literature, in particular viewing
the various natural behaviors of active inference agents through the lens of
RL. We show that by operating in a pure belief-based setting, active inference
agents can carry out epistemic exploration, and account for uncertainty about
their environment in a Bayes-optimal fashion. Furthermore, we show that the
reliance on an explicit reward signal in RL is removed in active inference,
where reward can simply be treated as another observation; even in the total
absence of rewards, agent behaviors are learned through preference learning. We
make these properties explicit by showing two scenarios in which active
inference agents can infer behaviors in reward-free environments compared to
both Q-learning and Bayesian model-based RL agents; by placing zero prior
preferences over rewards and by learning the prior preferences over the
observations corresponding to reward. We conclude by noting that this formalism
can be applied to more complex settings if appropriate generative models can be
formulated. In short, we aim to demystify the behavior of active inference
agents by presenting an accessible discrete state-space and time formulation,
and demonstrate these behaviors in a OpenAI gym environment, alongside RL
agents.
",Active inference: demystified and compared,"Noor Sajid, Philip J. Ball, Thomas Parr, Karl J. Friston",2021,Artificial Intelligence,
"  We present ""AutoJudge"", an automated evaluation method for conversational
dialogue systems. The method works by first generating dialogues based on
self-talk, i.e. dialogue systems talking to itself. Then, it uses human ratings
on these dialogues to train an automated judgement model. Our experiments show
that AutoJudge correlates well with the human ratings and can be used to
automatically evaluate dialogue systems, even in deployed systems. In a second
part, we attempt to apply AutoJudge to improve existing systems. This works
well for re-ranking a set of candidate utterances. However, our experiments
show that AutoJudge cannot be applied as reward for reinforcement learning,
although the metric can distinguish good from bad dialogues. We discuss
potential reasons, but state here already that this is still an open question
for further research.
","Towards a Metric for Automated Conversational Dialogue System Evaluation
  and Improvement","Jan Deriu, Mark Cieliebak",2019,Artificial Intelligence,
"  Dense urban traffic environments can produce situations where accurate
prediction and dynamic models are insufficient for successful autonomous
vehicle motion planning. We investigate how an autonomous agent can safely
negotiate with other traffic participants, enabling the agent to handle
potential deadlocks. Specifically we consider merges where the gap between cars
is smaller than the size of the ego vehicle. We propose a game theoretic
framework capable of generating and responding to interactive behaviors. Our
main contribution is to show how game-tree decision making can be executed by
an autonomous vehicle, including approximations and reasoning that make the
tree-search computationally tractable. Additionally, to test our model we
develop a stochastic rule-based traffic agent capable of generating interactive
behaviors that can be used as a benchmark for simulating traffic participants
in a crowded merge setting.
",Interactive Decision Making for Autonomous Vehicles in Dense Traffic,David Isele,2019,Artificial Intelligence,
"  In a multi-agent setting, the optimal policy of a single agent is largely
dependent on the behavior of other agents. We investigate the problem of
multi-agent reinforcement learning, focusing on decentralized learning in
non-stationary domains for mobile robot navigation. We identify a cause for the
difficulty in training non-stationary policies: mutual adaptation to
sub-optimal behaviors, and we use this to motivate a curriculum-based strategy
for learning interactive policies. The curriculum has two stages. First, the
agent leverages policy gradient algorithms to learn a policy that is capable of
achieving multiple goals. Second, the agent learns a modifier policy to learn
how to interact with other agents in a multi-agent setting. We evaluated our
approach on both an autonomous driving lane-change domain and a robot
navigation domain.
","Interaction-Aware Multi-Agent Reinforcement Learning for Mobile Agents
  with Individual Goals","Anahita Mohseni-Kabir, David Isele, and Kikuo Fujimura",2019,Artificial Intelligence,
"  Robotic navigation through crowds or herds requires the ability to both
predict the future motion of nearby individuals and understand how these
predictions might change in response to a robot's future action. State of the
art trajectory prediction models using Recurrent Neural Networks (RNNs) do not
currently account for a planned future action of a robot, and so cannot predict
how an individual will move in response to a robot's planned path. We propose
an approach that adapts RNNs to use a robot's next planned action as an input
alongside the current position of nearby individuals. This allows the model to
learn the response of individuals with regards to a robot's motion from real
world observations. By linking a robot's actions to the response of those
around it in training, we show that we are able to not only improve prediction
accuracy in close range interactions, but also to predict the likely response
of surrounding individuals to simulated actions. This allows the use of the
model to simulate state transitions, without requiring any assumptions on agent
interaction. We apply this model to varied datasets, including crowds of
pedestrians interacting with vehicles and bicycles, and livestock interacting
with a robotic vehicle.
","Predicting Responses to a Robot's Future Motion using Generative
  Recurrent Neural Networks",Stuart Eiffert and Salah Sukkarieh,2019,Artificial Intelligence,
"  Often, when modeling human decision-making behaviors in the context of
human-robot teaming, the emotion aspect of human is ignored. Nevertheless, the
influence of emotion, in some cases, is not only undeniable but beneficial.
This work studies the human-like characteristics brought by regret emotion in
one-human-multi-robot teaming for the application of domain search. In such
application, the task management load is outsourced to the robots to reduce the
human's workload, freeing the human to do more important work. The regret
decision model is first used by each robot for deciding whether to request
human service, then is extended for optimally queuing the requests from
multiple robots. For the movement of the robots in the domain search, we
designed a path planning algorithm based on dynamic programming for each robot.
The simulation shows that the human-like characteristics, namely, risk-seeking
and risk-aversion, indeed bring some appealing effects for balancing the
workload and performance in the human-multi-robot team.
","Respect Your Emotion: Human-Multi-Robot Teaming based on Regret Decision
  Model","Longsheng Jiang, Yue Wang",2019,Artificial Intelligence,
"  Combining neural networks with continuous logic and multicriteria decision
making tools can reduce the black box nature of neural models. In this study,
we show that nilpotent logical systems offer an appropriate mathematical
framework for a hybridization of continuous nilpotent logic and neural models,
helping to improve the interpretability and safety of machine learning. In our
concept, perceptrons model soft inequalities; namely membership functions and
continuous logical operators. We design the network architecture before
training, using continuous logical operators and multicriteria decision tools
with given weights working in the hidden layers. Designing the structure
appropriately leads to a drastic reduction in the number of parameters to be
learned. The theoretical basis offers a straightforward choice of activation
functions (the cutting function or its differentiable approximation, the
squashing function), and also suggests an explanation to the great success of
the rectified linear unit (ReLU). In this study, we focus on the architecture
of a hybrid model and introduce the building blocks for future application in
deep neural networks. The concept is illustrated with some toy examples taken
from an extended version of the tensorflow playground.
","Interpretable neural networks based on continuous-valued logic and
  multicriteria decision operators","Orsolya Csisz\'ar, G\'abor Csisz\'ar, J\'ozsef Dombi",2020,Artificial Intelligence,
"  Legg and Hutter, as well as subsequent authors, considered intelligent agents
through the lens of interaction with reward-giving environments, attempting to
assign numeric intelligence measures to such agents, with the guiding principle
that a more intelligent agent should gain higher rewards from environments in
some aggregate sense. In this paper, we consider a related question: rather
than measure numeric intelligence of one Legg- Hutter agent, how can we compare
the relative intelligence of two Legg-Hutter agents? We propose an elegant
answer based on the following insight: we can view Legg-Hutter agents as
candidates in an election, whose voters are environments, letting each
environment vote (via its rewards) which agent (if either) is more intelligent.
This leads to an abstract family of comparators simple enough that we can prove
some structural theorems about them. It is an open question whether these
structural theorems apply to more practical intelligence measures.
","Intelligence via ultrafilters: structural properties of some
  intelligence comparators of deterministic Legg-Hutter agents",Samuel Allen Alexander,2019,Artificial Intelligence,
"  Using neural networks in the reinforcement learning (RL) framework has
achieved notable successes. Yet, neural networks tend to forget what they
learned in the past, especially when they learn online and fully incrementally,
a setting in which the weights are updated after each sample is received and
the sample is then discarded. Under this setting, an update can lead to overly
global generalization by changing too many weights. The global generalization
interferes with what was previously learned and deteriorates performance, a
phenomenon known as catastrophic interference. Many previous works use
mechanisms such as experience replay (ER) buffers to mitigate interference by
performing minibatch updates, ensuring the data distribution is approximately
independent-and-identically-distributed (i.i.d.). But using ER would become
infeasible in terms of memory as problem complexity increases. Thus, it is
crucial to look for more memory-efficient alternatives. Interference can be
averted if we replace global updates with more local ones, so only weights
responsible for the observed data sample are updated. In this work, we propose
the use of dynamic self-organizing map (DSOM) with neural networks to induce
such locality in the updates without ER buffers. Our method learns a DSOM to
produce a mask to reweigh each hidden unit's output, modulating its degree of
use. It prevents interference by replacing global updates with local ones,
conditioned on the agent's state. We validate our method on standard RL
benchmarks including Mountain Car and Lunar Lander, where existing methods
often fail to learn without ER. Empirically, we show that our online and fully
incremental method is on par with and in some cases, better than
state-of-the-art in terms of final performance and learning speed. We provide
visualizations and quantitative measures to show that our method indeed
mitigates interference.
","Overcoming Catastrophic Interference in Online Reinforcement Learning
  with Dynamic Self-Organizing Maps",Yat Long Lo and Sina Ghiassian,2019,Artificial Intelligence,
"  The fourth edition of the international workshop on Causation, Responsibility
and Explanation took place in Prague (Czech Republic) as part of ETAPS 2019.
The program consisted in 5 invited speakers and 4 regular papers, whose
selection was based on a careful reviewing process and that are included in
these proceedings.
","Proceedings of the 4th Workshop on Formal Reasoning about Causation,
  Responsibility, and Explanations in Science and Technology","Georgiana Caltais (Konstanz University), Jean Krivine (CNRS)",2019,Artificial Intelligence,
"  Information retrieval (IR) systems need to constantly update their knowledge
as target objects and user queries change over time. Due to the power-law
nature of linguistic data, learning lexical concepts is a problem resisting
standard machine learning approaches: while manual intervention is always
possible, a more general and automated solution is desirable. In this work, we
propose a novel end-to-end framework that models the interaction between a
search engine and users as a virtuous human-in-the-loop inference. The proposed
framework is the first to our knowledge combining ideas from psycholinguistics
and experiment design to maximize efficiency in IR. We provide a brief overview
of the main components and initial simulations in a toy world, showing how
inference works end-to-end and discussing preliminary results and next steps.
","Lexical Learning as an Online Optimal Experiment: Building Efficient
  Search Engines through Human-Machine Collaboration","Jacopo Tagliabue, Reuben Cohn-Gordon",2019,Artificial Intelligence,
"  The Shapes Constraint Language (SHACL) has been recently introduced as a W3C
recommendation to define constraints that can be validated against RDF graphs.
Interactions of SHACL with other Semantic Web technologies, such as ontologies
or reasoners, is a matter of ongoing research. In this paper we study the
interaction of a subset of SHACL with inference rules expressed in datalog. On
the one hand, SHACL constraints can be used to define a ""schema"" for graph
datasets. On the other hand, inference rules can lead to the discovery of new
facts that do not match the original schema. Given a set of SHACL constraints
and a set of datalog rules, we present a method to detect which constraints
could be violated by the application of the inference rules on some graph
instance of the schema, and update the original schema, i.e, the set of SHACL
constraints, in order to capture the new facts that can be inferred. We provide
theoretical and experimental results of the various components of our approach.
",SHACL Constraints with Inference Rules,"Paolo Pareti, George Konstantinidis, Timothy J. Norman, Murat
  \c{S}ensoy",2019,Artificial Intelligence,
"  The debate on AI ethics largely focuses on technical improvements and
stronger regulation to prevent accidents or misuse of AI, with solutions
relying on holding individual actors accountable for responsible AI
development. While useful and necessary, we argue that this ""agency"" approach
disregards more indirect and complex risks resulting from AI's interaction with
the socio-economic and political context. This paper calls for a ""structural""
approach to assessing AI's effects in order to understand and prevent such
systemic risks where no individual can be held accountable for the broader
negative impacts. This is particularly relevant for AI applied to systemic
issues such as climate change and food security which require political
solutions and global cooperation. To properly address the wide range of AI
risks and ensure 'AI for social good', agency-focused policies must be
complemented by policies informed by a structural approach.
",AI Ethics for Systemic Issues: A Structural Approach,"Agnes Schim van der Loeff, Iggy Bassi, Sachin Kapila, Jevgenij Gamper",2019,Artificial Intelligence,
"  In level co-creation an AI and human work together to create a video game
level. One open challenge in level co-creation is how to empower human users to
ensure particular qualities of the final level, such as challenge. There has
been significant prior research into automated pathing and automated
playtesting for video game levels, but not in how to incorporate these into
tools. In this demonstration we present an improvement of the Morai Maker
mixed-initiative level editor for Super Mario Bros. that includes automated
pathing and challenge approximation features.
",Integrating Automated Play in Level Co-Creation,"Andrew Hoyt, Matthew Guzdial, Yalini Kumar, Gillian Smith, and Mark O.
  Riedl",2019,Artificial Intelligence,
"  Recent superhuman results in games have largely been achieved in a variety of
zero-sum settings, such as Go and Poker, in which agents need to compete
against others. However, just like humans, real-world AI systems have to
coordinate and communicate with other agents in cooperative partially
observable environments as well. These settings commonly require participants
to both interpret the actions of others and to act in a way that is informative
when being interpreted. Those abilities are typically summarized as theory f
mind and are seen as crucial for social interactions. In this paper we propose
two different search techniques that can be applied to improve an arbitrary
agreed-upon policy in a cooperative partially observable game. The first one,
single-agent search, effectively converts the problem into a single agent
setting by making all but one of the agents play according to the agreed-upon
policy. In contrast, in multi-agent search all agents carry out the same
common-knowledge search procedure whenever doing so is computationally
feasible, and fall back to playing according to the agreed-upon policy
otherwise. We prove that these search procedures are theoretically guaranteed
to at least maintain the original performance of the agreed-upon policy (up to
a bounded approximation error). In the benchmark challenge problem of Hanabi,
our search technique greatly improves the performance of every agent we tested
and when applied to a policy trained using RL achieves a new state-of-the-art
score of 24.61 / 25 in the game, compared to a previous-best of 24.08 / 25.
",Improving Policies via Search in Cooperative Partially Observable Games,"Adam Lerer, Hengyuan Hu, Jakob Foerster, Noam Brown",2020,Artificial Intelligence,
"  SUMMARY: Recently, novel machine-learning algorithms have shown potential for
predicting undiscovered links in biomedical knowledge networks. However,
dedicated benchmarks for measuring algorithmic progress have not yet emerged.
With OpenBioLink, we introduce a large-scale, high-quality and highly
challenging biomedical link prediction benchmark to transparently and
reproducibly evaluate such algorithms. Furthermore, we present preliminary
baseline evaluation results. AVAILABILITY AND IMPLEMENTATION: Source code, data
and supplementary files are openly available at
https://github.com/OpenBioLink/OpenBioLink CONTACT: matthias.samwald ((at))
meduniwien.ac.at
","OpenBioLink: A benchmarking framework for large-scale biomedical link
  prediction","Anna Breit, Simon Ott, Asan Agibetov, Matthias Samwald",2020,Artificial Intelligence,
"  The main goal of this paper is to describe an axiomatic utility theory for
Dempster-Shafer belief function lotteries. The axiomatic framework used is
analogous to von Neumann-Morgenstern's utility theory for probabilistic
lotteries as described by Luce and Raiffa. Unlike the probabilistic case, our
axiomatic framework leads to interval-valued utilities, and therefore, to a
partial (incomplete) preference order on the set of all belief function
lotteries. If the belief function reference lotteries we use are Bayesian
belief functions, then our representation theorem coincides with Jaffray's
representation theorem for his linear utility theory for belief functions. We
illustrate our representation theorem using some examples discussed in the
literature, and we propose a simple model for assessing utilities based on an
interval-valued pessimism index representing a decision-maker's attitude to
ambiguity and indeterminacy. Finally, we compare our decision theory with those
proposed by Jaffray, Smets, Dubois et al., Giang and Shenoy, and Shafer.
","An Interval-Valued Utility Theory for Decision Making with
  Dempster-Shafer Belief Functions",Thierry Denoeux and Prakash P. Shenoy,2020,Artificial Intelligence,
"  Counterfactual Thinking is a human cognitive ability studied in a wide
variety of domains. It captures the process of reasoning about a past event
that did not occur, namely what would have happened had this event occurred,
or, otherwise, to reason about an event that did occur but what would ensue had
it not. Given the wide cognitive empowerment of counterfactual reasoning in the
human individual, the question arises of how the presence of individuals with
this capability may improve cooperation in populations of self-regarding
individuals. Here we propose a mathematical model, grounded on Evolutionary
Game Theory, to examine the population dynamics emerging from the interplay
between counterfactual thinking and social learning (i.e., individuals that
learn from the actions and success of others) whenever the individuals in the
population face a collective dilemma. Our results suggest that counterfactual
reasoning fosters coordination in collective action problems occurring in large
populations, and has a limited impact on cooperation dilemmas in which
coordination is not required. Moreover, we show that a small prevalence of
individuals resorting to counterfactual thinking is enough to nudge an entire
population towards highly cooperative standards.
",Counterfactual thinking in cooperation dynamics,Luis Moniz Pereira and Francisco C. Santos,2019,Artificial Intelligence,
"  Recent work on fairness in machine learning has primarily emphasized how to
define, quantify, and encourage ""fair"" outcomes. Less attention has been paid,
however, to the ethical foundations which underlie such efforts. Among the
ethical perspectives that should be taken into consideration is
consequentialism, the position that, roughly speaking, outcomes are all that
matter. Although consequentialism is not free from difficulties, and although
it does not necessarily provide a tractable way of choosing actions (because of
the combined problems of uncertainty, subjectivity, and aggregation), it
nevertheless provides a powerful foundation from which to critique the existing
literature on machine learning fairness. Moreover, it brings to the fore some
of the tradeoffs involved, including the problem of who counts, the pros and
cons of using a policy, and the relative value of the distant future. In this
paper we provide a consequentialist critique of common definitions of fairness
within machine learning, as well as a machine learning perspective on
consequentialism. We conclude with a broader discussion of the issues of
learning and randomization, which have important implications for the ethics of
automated decision making systems.
",On Consequentialism and Fairness,Dallas Card and Noah A. Smith,2020,Artificial Intelligence,
"  The wide adoption of machine learning in the critical domains such as medical
diagnosis, law, education had propelled the need for interpretable techniques
due to the need for end users to understand the reasoning behind decisions due
to learning systems. The computational intractability of interpretable learning
led practitioners to design heuristic techniques, which fail to provide sound
handles to tradeoff accuracy and interpretability.
  Motivated by the success of MaxSAT solvers over the past decade, recently
MaxSAT-based approach, called MLIC, was proposed that seeks to reduce the
problem of learning interpretable rules expressed in Conjunctive Normal Form
(CNF) to a MaxSAT query. While MLIC was shown to achieve accuracy similar to
that of other state of the art black-box classifiers while generating small
interpretable CNF formulas, the runtime performance of MLIC is significantly
lagging and renders approach unusable in practice. In this context, authors
raised the question: Is it possible to achieve the best of both worlds, i.e., a
sound framework for interpretable learning that can take advantage of MaxSAT
solvers while scaling to real-world instances?
  In this paper, we take a step towards answering the above question in
affirmation. We propose IMLI: an incremental approach to MaxSAT based framework
that achieves scalable runtime performance via partition-based training
methodology. Extensive experiments on benchmarks arising from UCI repository
demonstrate that IMLI achieves up to three orders of magnitude runtime
improvement without loss of accuracy and interpretability.
","IMLI: An Incremental Framework for MaxSAT-Based Learning of
  Interpretable Classification Rules",Bishwamittra Ghosh and Kuldeep S. Meel,2019,Artificial Intelligence,
"  This work presents an architecture that generates curiosity-driven
goal-directed exploration behaviours for an image sensor of a microfarming
robot. A combination of deep neural networks for offline unsupervised learning
of low-dimensional features from images, and of online learning of shallow
neural networks representing the inverse and forward kinematics of the system
have been used. The artificial curiosity system assigns interest values to a
set of pre-defined goals, and drives the exploration towards those that are
expected to maximise the learning progress. We propose the integration of an
episodic memory in intrinsic motivation systems to face catastrophic forgetting
issues, typically experienced when performing online updates of artificial
neural networks. Our results show that adopting an episodic memory system not
only prevents the computational models from quickly forgetting knowledge that
has been previously acquired, but also provides new avenues for modulating the
balance between plasticity and stability of the models.
","Intrinsic Motivation and Episodic Memories for Robot Exploration of
  High-Dimensional Sensory Spaces","Guido Schillaci, Antonio Pico Villalpando, Verena Vanessa Hafner,
  Peter Hanappe, David Colliaux, Timoth\'ee Wintz",2020,Artificial Intelligence,
"  Autonomous systems are often required to operate in partially observable
environments. They must reliably execute a specified objective even with
incomplete information about the state of the environment. We propose a
methodology to synthesize policies that satisfy a linear temporal logic formula
in a partially observable Markov decision process (POMDP). By formulating a
planning problem, we show how to use point-based value iteration methods to
efficiently approximate the maximum probability of satisfying a desired logical
formula and compute the associated belief state policy. We demonstrate that our
method scales to large POMDP domains and provides strong bounds on the
performance of the resulting policy.
","Point-Based Methods for Model Checking in Partially Observable Markov
  Decision Processes","Maxime Bouton, Jana Tumova, and Mykel J. Kochenderfer",2020,Artificial Intelligence,
"  In this paper, we present a visual emulator of the emotions seen in
characters in stories. This system is based on a simplified view of the
cognitive structure of emotions proposed by Ortony, Clore and Collins (OCC
Model). The goal of this paper is to provide a visual platform that allows us
to observe changes in the characters' different emotions, and the intricate
interrelationships between: 1) each character's emotions, 2) their affective
relationships and actions, 3) The events that take place in the development of
a plot, and 4) the objects of desire that make up the emotional map of any
story. This tool was tested on stories with a contrasting variety of emotional
and affective environments: Othello, Twilight, and Harry Potter, behaving
sensibly and in keeping with the atmosphere in which the characters were
immersed.
",Visual Simplified Characters' Emotion Emulator Implementing OCC Model,"Ana Lilia Laureano-Cruces, Laura Hern\'andez-Dom\'inguez, Martha
  Mora-Torres, Juan-Manuel Torres-Moreno, Jaime Enrique Cabrera-L\'opez",2011,Artificial Intelligence,
"  Explaining sophisticated machine-learning based systems is an important issue
at the foundations of AI. Recent efforts have shown various methods for
providing explanations. These approaches can be broadly divided into two
schools: those that provide a local and human interpreatable approximation of a
machine learning algorithm, and logical approaches that exactly characterise
one aspect of the decision. In this paper we focus upon the second school of
exact explanations with a rigorous logical foundation. There is an
epistemological problem with these exact methods. While they can furnish
complete explanations, such explanations may be too complex for humans to
understand or even to write down in human readable form. Interpretability
requires epistemically accessible explanations, explanations humans can grasp.
Yet what is a sufficiently complete epistemically accessible explanation still
needs clarification. We do this here in terms of counterfactuals, following
[Wachter et al., 2017]. With counterfactual explanations, many of the
assumptions needed to provide a complete explanation are left implicit. To do
so, counterfactual explanations exploit the properties of a particular data
point or sample, and as such are also local as well as partial explanations. We
explore how to move from local partial explanations to what we call complete
local explanations and then to global ones. But to preserve accessibility we
argue for the need for partiality. This partiality makes it possible to hide
explicit biases present in the algorithm that may be injurious or unfair.We
investigate how easy it is to uncover these biases in providing complete and
fair explanations by exploiting the structure of the set of counterfactuals
providing a complete local explanation.
",Adequate and fair explanations,"Nicholas Asher, Soumya Paul, Chris Russell",2021,Artificial Intelligence,
"  Automated decision making based on big data and machine learning (ML)
algorithms can result in discriminatory decisions against certain protected
groups defined upon personal data like gender, race, sexual orientation etc.
Such algorithms designed to discover patterns in big data might not only pick
up any encoded societal biases in the training data, but even worse, they might
reinforce such biases resulting in more severe discrimination. The majority of
thus far proposed fairness-aware machine learning approaches focus solely on
the pre-, in- or post-processing steps of the machine learning process, that
is, input data, learning algorithms or derived models, respectively. However,
the fairness problem cannot be isolated to a single step of the ML process.
Rather, discrimination is often a result of complex interactions between big
data and algorithms, and therefore, a more holistic approach is required. The
proposed FAE (Fairness-Aware Ensemble) framework combines fairness-related
interventions at both pre- and postprocessing steps of the data analysis
process. In the preprocessing step, we tackle the problems of
under-representation of the protected group (group imbalance) and of
class-imbalance by generating balanced training samples. In the post-processing
step, we tackle the problem of class overlapping by shifting the decision
boundary in the direction of fairness.
",FAE: A Fairness-Aware Ensemble Framework,"Vasileios Iosifidis, Besnik Fetahu, Eirini Ntoutsi",2019,Artificial Intelligence,
"  Task-allocation is an important problem in multi-agent systems. It becomes
more challenging when the team-members are humans with imperfect knowledge
about their teammates' costs and the overall performance metric. While
distributed task-allocation methods let the team-members engage in iterative
dialog to reach a consensus, the process can take a considerable amount of time
and communication. On the other hand, a centralized method that simply outputs
an allocation may result in discontented human team-members who, due to their
imperfect knowledge and limited computation capabilities, perceive the
allocation to be unfair. To address these challenges, we propose a centralized
Artificial Intelligence Task Allocation (AITA) that simulates a negotiation and
produces a negotiation-aware task allocation that is fair. If a team-member is
unhappy with the proposed allocation, we allow them to question the proposed
allocation using a counterfactual. By using parts of the simulated negotiation,
we are able to provide contrastive explanations that providing minimum
information about other's costs to refute their foil. With human studies, we
show that (1) the allocation proposed using our method does indeed appear fair
to the majority, and (2) when a counterfactual is raised, explanations
generated are easy to comprehend and convincing. Finally, we empirically study
the effect of different kinds of incompleteness on the explanation-length and
find that underestimation of a teammate's costs often increases it.
","`Why didn't you allocate this task to them?' Negotiation-Aware Task
  Allocation and Contrastive Explanation Generation","Zahra Zahedi, Sailik Sengupta, Subbarao Kambhampati",2020,Artificial Intelligence,
"  Monte-Carlo Tree Search (MCTS) is one of the most-widely used methods for
planning, and has powered many recent advances in artificial intelligence. In
MCTS, one typically performs computations (i.e., simulations) to collect
statistics about the possible future consequences of actions, and then chooses
accordingly. Many popular MCTS methods such as UCT and its variants decide
which computations to perform by trading-off exploration and exploitation. In
this work, we take a more direct approach, and explicitly quantify the value of
a computation based on its expected impact on the quality of the action
eventually chosen. Our approach goes beyond the ""myopic"" limitations of
existing computation-value-based methods in two senses: (I) we are able to
account for the impact of non-immediate (ie, future) computations (II) on
non-immediate actions. We show that policies that greedily optimize computation
values are optimal under certain assumptions and obtain results that are
competitive with the state-of-the-art.
",Static and Dynamic Values of Computation in MCTS,Eren Sezener and Peter Dayan,2020,Artificial Intelligence,
"  Developmental machine learning studies how artificial agents can model the
way children learn open-ended repertoires of skills. Such agents need to create
and represent goals, select which ones to pursue and learn to achieve them.
Recent approaches have considered goal spaces that were either fixed and
hand-defined or learned using generative models of states. This limited agents
to sample goals within the distribution of known effects. We argue that the
ability to imagine out-of-distribution goals is key to enable creative
discoveries and open-ended learning. Children do so by leveraging the
compositionality of language as a tool to imagine descriptions of outcomes they
never experienced before, targeting them as goals during play. We introduce
IMAGINE, an intrinsically motivated deep reinforcement learning architecture
that models this ability. Such imaginative agents, like children, benefit from
the guidance of a social peer who provides language descriptions. To take
advantage of goal imagination, agents must be able to leverage these
descriptions to interpret their imagined out-of-distribution goals. This
generalization is made possible by modularity: a decomposition between learned
goal-achievement reward function and policy relying on deep sets, gated
attention and object-centered representations. We introduce the Playground
environment and study how this form of goal imagination improves generalization
and exploration over agents lacking this capacity. In addition, we identify the
properties of goal imagination that enable these results and study the impacts
of modularity and social interactions.
","Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven
  Exploration","C\'edric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux,
  Cl\'ement Moulin-Frier, Peter Ford Dominey, Pierre-Yves Oudeyer",2020,Artificial Intelligence,
"  In this paper I argue that the search for explainable models and
interpretable decisions in AI must be reformulated in terms of the broader
project of offering a pragmatic and naturalistic account of understanding in
AI. Intuitively, the purpose of providing an explanation of a model or a
decision is to make it understandable to its stakeholders. But without a
previous grasp of what it means to say that an agent understands a model or a
decision, the explanatory strategies will lack a well-defined goal. Aside from
providing a clearer objective for XAI, focusing on understanding also allows us
to relax the factivity condition on explanation, which is impossible to fulfill
in many machine learning models, and to focus instead on the pragmatic
conditions that determine the best fit between a model and the methods and
devices deployed to understand it. After an examination of the different types
of understanding discussed in the philosophical and psychological literature, I
conclude that interpretative or approximation models not only provide the best
way to achieve the objectual understanding of a machine learning model, but are
also a necessary condition to achieve post-hoc interpretability. This
conclusion is partly based on the shortcomings of the purely functionalist
approach to post-hoc interpretability that seems to be predominant in most
recent literature.
",The Pragmatic Turn in Explainable Artificial Intelligence (XAI),Andr\'es P\'aez,2019,Artificial Intelligence,
"  Knowledge Graphs (KGs) are graph-structured knowledge bases storing factual
information about real-world entities. Understanding the uniqueness of each
entity is crucial to the analyzing, sharing, and reusing of KGs. Traditional
profiling technologies encompass a vast array of methods to find distinctive
features in various applications, which can help to differentiate entities in
the process of human understanding of KGs. In this work, we present a novel
profiling approach to identify distinctive entity features. The distinctiveness
of features is carefully measured by a HAS model, which is a scalable
representation learning model to produce a multi-pattern entity embedding. We
fully evaluate the quality of entity profiles generated from real KGs. The
results show that our approach facilitates human understanding of entities in
KGs.
",Entity Profiling in Knowledge Graphs,"Xiang Zhang, Qingqing Yang, Jinru Ding and Ziyue Wang",2020,Artificial Intelligence,
"  In this paper we discuss how systems with Artificial Intelligence (AI) can
undergo safety assessment. This is relevant, if AI is used in safety related
applications. Taking a deeper look into AI models, we show, that many models of
artificial intelligence, in particular machine learning, are statistical
models. Safety assessment would then have t o concentrate on the model that is
used in AI, besides the normal assessment procedure. Part of the budget of
dangerous random failures for the relevant safety integrity level needs to be
used for the probabilistic faulty behavior of the AI system. We demonstrate our
thoughts with a simple example and propose a research challenge that may be
decisive for the use of AI in safety related systems.
",On Safety Assessment of Artificial Intelligence,"Jens Braband and Hendrik Sch\""abe",2020,Artificial Intelligence,
"  Explainability and interpretability of AI models is an essential factor
affecting the safety of AI. While various explainable AI (XAI) approaches aim
at mitigating the lack of transparency in deep networks, the evidence of the
effectiveness of these approaches in improving usability, trust, and
understanding of AI systems are still missing. We evaluate multimodal
explanations in the setting of a Visual Question Answering (VQA) task, by
asking users to predict the response accuracy of a VQA agent with and without
explanations. We use between-subjects and within-subjects experiments to probe
explanation effectiveness in terms of improving user prediction accuracy,
confidence, and reliance, among other factors. The results indicate that the
explanations help improve human prediction accuracy, especially in trials when
the VQA system's answer is inaccurate. Furthermore, we introduce active
attention, a novel method for evaluating causal attentional effects through
intervention by editing attention maps. User explanation ratings are strongly
correlated with human prediction accuracy and suggest the efficacy of these
explanations in human-machine AI collaboration tasks.
","A Study on Multimodal and Interactive Explanations for Visual Question
  Answering","Kamran Alipour, Jurgen P. Schulze, Yi Yao, Avi Ziskind, Giedrius
  Burachas",2020,Artificial Intelligence,
"  Given the large variety of existing logical formalisms it is of utmost
importance to select the most adequate one for a specific purpose, e.g. for
representing the knowledge relevant for a particular application or for using
the formalism as a modeling tool for problem solving. Awareness of the nature
of a logical formalism, in other words, of its fundamental intrinsic
properties, is indispensable and provides the basis of an informed choice. In
this treatise we consider the existence characterization logics as well as
properties like existence and uniqueness, expressibility, replaceability and
verifiability in the realm of abstract argumentation
","On the Existence of Characterization Logics and Fundamental Properties
  of Argumentation Semantics",Ringo Baumann,2019,Artificial Intelligence,
"  ML models are increasingly deployed in settings with real world interactions
such as vehicles, but unfortunately, these models can fail in systematic ways.
To prevent errors, ML engineering teams monitor and continuously improve these
models. We propose a new abstraction, model assertions, that adapts the
classical use of program assertions as a way to monitor and improve ML models.
Model assertions are arbitrary functions over a model's input and output that
indicate when errors may be occurring, e.g., a function that triggers if an
object rapidly changes its class in a video. We propose methods of using model
assertions at all stages of ML system deployment, including runtime monitoring,
validating labels, and continuously improving ML models. For runtime
monitoring, we show that model assertions can find high confidence errors,
where a model returns the wrong output with high confidence, which
uncertainty-based monitoring techniques would not detect. For training, we
propose two methods of using model assertions. First, we propose a bandit-based
active learning algorithm that can sample from data flagged by assertions and
show that it can reduce labeling costs by up to 40% over traditional
uncertainty-based methods. Second, we propose an API for generating
""consistency assertions"" (e.g., the class change example) and weak labels for
inputs where the consistency assertions fail, and show that these weak labels
can improve relative model quality by up to 46%. We evaluate model assertions
on four real-world tasks with video, LIDAR, and ECG data.
",Model Assertions for Monitoring and Improving ML Models,"Daniel Kang, Deepti Raghavan, Peter Bailis, Matei Zaharia",2020,Artificial Intelligence,
"  We present a novel technique called Dynamic Experience Replay (DER) that
allows Reinforcement Learning (RL) algorithms to use experience replay samples
not only from human demonstrations but also successful transitions generated by
RL agents during training and therefore improve training efficiency. It can be
combined with an arbitrary off-policy RL algorithm, such as DDPG or DQN, and
their distributed versions. We build upon Ape-X DDPG and demonstrate our
approach on robotic tight-fitting joint assembly tasks, based on force/torque
and Cartesian pose observations. In particular, we run experiments on two
different tasks: peg-in-hole and lap-joint. In each case, we compare different
replay buffer structures and how DER affects them. Our ablation studies show
that Dynamic Experience Replay is a crucial ingredient that either largely
shortens the training time in these challenging environments or solves the
tasks that the vanilla Ape-X DDPG cannot solve. We also show that our policies
learned purely in simulation can be deployed successfully on the real robot.
The video presenting our experiments is available at
https://sites.google.com/site/dynamicexperiencereplay
",Dynamic Experience Replay,Jieliang Luo and Hui Li,2020,Artificial Intelligence,
"  We propose the Interactive Constrained MAP-Elites, a quality-diversity
solution for game content generation, implemented as a new feature of the
Evolutionary Dungeon Designer: a mixed-initiative co-creativity tool for
designing dungeons. The feature uses the MAP-Elites algorithm, an illumination
algorithm that segregates the population among several cells depending on their
scores with respect to different behavioral dimensions. Users can flexibly and
dynamically alternate between these dimensions anytime, thus guiding the
evolutionary process in an intuitive way, and then incorporate suggestions
produced by the algorithm in their room designs. At the same time, any
modifications performed by the human user will feed back into MAP-Elites,
closing a circular workflow of constant mutual inspiration. This paper presents
the algorithm followed by an in-depth analysis of its behaviour, with the aims
of evaluating the expressive range of all possible dimension combinations in
several scenarios, as well as discussing their influence in the fitness
landscape and in the overall performance of the mixed-initiative procedural
content generation.
","Interactive Constrained MAP-Elites: Analysis and Evaluation of the
  Expressiveness of the Feature Dimensions","Alberto Alvarez, Steve Dahlskog, Jose Font and Julian Togelius",2020,Artificial Intelligence,
"  We describe nearly fifteen years of General Game Playing experimental
research history in the context of reproducibility and fairness of comparisons
between various GGP agents and systems designed to play games described by
different formalisms. We think our survey may provide an interesting
perspective of how chaotic methods were allowed when nothing better was
possible. Finally, from our experience-based view, we would like to propose a
few recommendations of how such specific heterogeneous branch of research
should be handled appropriately in the future. The goal of this note is to
point out common difficulties and problems in the experimental research in the
area. We hope that our recommendations will help in avoiding them in future
works and allow more fair and reproducible comparisons.
",Experimental Studies in General Game Playing: An Experience Report,"Jakub Kowalski, Marek Szyku{\l}a",2020,Artificial Intelligence,
"  This work focuses on a comparison between the performances of two well-known
Swarm algorithms: Cuckoo Search (CS) and Firefly Algorithm (FA), in estimating
the parameters of Software Reliability Growth Models. This study is further
reinforced using Particle Swarm Optimization (PSO) and Ant Colony Optimization
(ACO). All algorithms are evaluated according to real software failure data,
the tests are performed and the obtained results are compared to show the
performance of each of the used algorithms. Furthermore, CS and FA are also
compared with each other on bases of execution time and iteration number.
Experimental results show that CS is more efficient in estimating the
parameters of SRGMs, and it has outperformed FA in addition to PSO and ACO for
the selected Data sets and employed models.
","A Comparative Study on Parameter Estimation in Software Reliability
  Modeling using Swarm Intelligence","Najla Akram AL-Saati, Marrwa Abd-AlKareem Alabajee",2016,Artificial Intelligence,
"  Continual learning studies agents that learn from streams of tasks without
forgetting previous ones while adapting to new ones. Two recent
continual-learning scenarios have opened new avenues of research. In
meta-continual learning, the model is pre-trained to minimize catastrophic
forgetting of previous tasks. In continual-meta learning, the aim is to train
agents for faster remembering of previous tasks through adaptation. In their
original formulations, both methods have limitations. We stand on their
shoulders to propose a more general scenario, OSAKA, where an agent must
quickly solve new (out-of-distribution) tasks, while also requiring fast
remembering. We show that current continual learning, meta-learning,
meta-continual learning, and continual-meta learning techniques fail in this
new scenario. We propose Continual-MAML, an online extension of the popular
MAML algorithm as a strong baseline for this scenario. We empirically show that
Continual-MAML is better suited to the new scenario than the aforementioned
methodologies, as well as standard continual learning and meta-learning
approaches.
","Online Fast Adaptation and Knowledge Accumulation: a New Approach to
  Continual Learning","Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin,
  Min Lin, Lucas Caccia, Issam Laradji, Irina Rish, Alexandre Lacoste, David
  Vazquez, Laurent Charlin",2020,Artificial Intelligence,
"  Smart home environments equipped with distributed sensor networks are capable
of helping people by providing services related to health, emergency detection
or daily routine management. A backbone to these systems relies often on the
system's ability to track and detect activities performed by the users in their
home. Despite the continuous progress in the area of activity recognition in
smart homes, many systems make a strong underlying assumption that the number
of occupants in the home at any given moment of time is always known.
Estimating the number of persons in a Smart Home at each time step remains a
challenge nowadays. Indeed, unlike most (crowd) counting solution which are
based on computer vision techniques, the sensors considered in a Smart Home are
often very simple and do not offer individually a good overview of the
situation. The data gathered needs therefore to be fused in order to infer
useful information. This paper aims at addressing this challenge and presents a
probabilistic approach able to estimate the number of persons in the
environment at each time step. This approach works in two steps: first, an
estimate of the number of persons present in the environment is done using a
Constraint Satisfaction Problem solver, based on the topology of the sensor
network and the sensor activation pattern at this time point. Then, a Hidden
Markov Model refines this estimate by considering the uncertainty related to
the sensors. Using both simulated and real data, our method has been tested and
validated on two smart homes of different sizes and configuration and
demonstrates the ability to accurately estimate the number of inhabitants.
","Online Guest Detection in a Smart Home using Pervasive Sensors and
  Probabilistic Reasoning","Jennifer Renoux, Uwe K\""ockemann, Amy Loutfi",2018,Artificial Intelligence,
"  Autonomous driving is of great interest to industry and academia alike. The
use of machine learning approaches for autonomous driving has long been
studied, but mostly in the context of perception. In this paper we take a
deeper look on the so called end-to-end approaches for autonomous driving,
where the entire driving pipeline is replaced with a single neural network. We
review the learning methods, input and output modalities, network architectures
and evaluation schemes in end-to-end driving literature. Interpretability and
safety are discussed separately, as they remain challenging for this approach.
Beyond providing a comprehensive overview of existing methods, we conclude the
review with an architecture that combines the most promising elements of the
end-to-end autonomous driving systems.
",A Survey of End-to-End Driving: Architectures and Training Methods,"Ardi Tampuu, Maksym Semikin, Naveed Muhammad, Dmytro Fishman and
  Tambet Matiisen",2020,Artificial Intelligence,
"  In this paper, we solve the problem of predicting the next locations of the
moving objects with a historical dataset of trajectories. We present a Next
Location Predictor with Markov Modeling (NLPMM) which has the following
advantages: (1) it considers both individual and collective movement patterns
in making prediction, (2) it is effective even when the trajectory data is
sparse, (3) it considers the time factor and builds models that are suited to
different time periods. We have conducted extensive experiments in a real
dataset, and the results demonstrate the superiority of NLPMM over existing
methods.
",NLPMM: a Next Location Predictor with Markov Modeling,"Meng Chen, Yang Liu, Xiaohui Yu",2014,Artificial Intelligence,
"  Decentralized online planning can be an attractive paradigm for cooperative
multi-agent systems, due to improved scalability and robustness. A key
difficulty of such approach lies in making accurate predictions about the
decisions of other agents. In this paper, we present a trainable online
decentralized planning algorithm based on decentralized Monte Carlo Tree
Search, combined with models of teammates learned from previous episodic runs.
By only allowing one agent to adapt its models at a time, under the assumption
of ideal policy approximation, successive iterations of our method are
guaranteed to improve joint policies, and eventually lead to convergence to a
Nash equilibrium. We test the efficiency of the algorithm by performing
experiments in several scenarios of the spatial task allocation environment
introduced in [Claes et al., 2015]. We show that deep learning and
convolutional neural networks can be employed to produce accurate policy
approximators which exploit the spatial features of the problem, and that the
proposed algorithm improves over the baseline planning performance for
particularly challenging domain configurations.
",Decentralized MCTS via Learned Teammate Models,"Aleksander Czechowski, Frans A. Oliehoek",2020,Artificial Intelligence,
"  We present a system that utilizes machine learning for tactic proof search in
the Coq Proof Assistant. In a similar vein as the TacticToe project for HOL4,
our system predicts appropriate tactics and finds proofs in the form of tactic
scripts. To do this, it learns from previous tactic scripts and how they are
applied to proof states. The performance of the system is evaluated on the Coq
Standard Library. Currently, our predictor can identify the correct tactic to
be applied to a proof state 23.4% of the time. Our proof searcher can fully
automatically prove 39.3% of the lemmas. When combined with the CoqHammer
system, the two systems together prove 56.7% of the library's lemmas.
",Tactic Learning and Proving for the Coq Proof Assistant,"Lasse Blaauwbroek, Josef Urban, and Herman Geuvers",2020,Artificial Intelligence,
"  In process mining, process models are extracted from event logs using process
discovery algorithms and are commonly assessed using multiple quality
dimensions. While the metrics that measure the relationship of an extracted
process model to its event log are well-studied, quantifying the level by which
a process model can describe the unobserved behavior of its underlying system
falls short in the literature. In this paper, a novel deep learning-based
methodology called Adversarial System Variant Approximation (AVATAR) is
proposed to overcome this issue. Sequence Generative Adversarial Networks are
trained on the variants contained in an event log with the intention to
approximate the underlying variant distribution of the system behavior.
Unobserved realistic variants are sampled either directly from the Sequence
Generative Adversarial Network or by leveraging the Metropolis-Hastings
algorithm. The degree by which a process model relates to its underlying
unknown system behavior is then quantified based on the realistic observed and
estimated unobserved variants using established process model quality metrics.
Significant performance improvements in revealing realistic unobserved variants
are demonstrated in a controlled experiment on 15 ground truth systems.
Additionally, the proposed methodology is experimentally tested and evaluated
to quantify the generalization of 60 discovered process models with respect to
their systems.
","Adversarial System Variant Approximation to Quantify Process Model
  Generalization",Julian Theis and Houshang Darabi,2020,Artificial Intelligence,
"  The question of whether artificial beings or machines could become self-aware
or consciousness has been a philosophical question for centuries. The main
problem is that self-awareness cannot be observed from an outside perspective
and the distinction of whether something is really self-aware or merely a
clever program that pretends to do so cannot be answered without access to
accurate knowledge about the mechanism's inner workings. We review the current
state-of-the-art regarding these developments and investigate common machine
learning approaches with respect to their potential ability to become
self-aware. We realise that many important algorithmic steps towards machines
with a core consciousness have already been devised. For human-level
intelligence, however, many additional techniques have to be discovered.
",Will we ever have Conscious Machines?,"Patrick Krauss, Andreas Maier",2020,Artificial Intelligence,
"  Two indicators are classically used to evaluate the quality of rule-based
classification systems: predictive accuracy, i.e. the system's ability to
successfully reproduce learning data and coverage, i.e. the proportion of
possible cases for which the logical rules constituting the system apply. In
this work, we claim that these two indicators may be insufficient, and
additional measures of quality may need to be developed. We theoretically show
that classification systems presenting ""good"" predictive accuracy and coverage
can, nonetheless, be trivially improved and illustrate this proposition with
examples.
",On Evaluating the Quality of Rule-Based Classification Systems,Nassim Dehouche,2017,Artificial Intelligence,
"  Reinforcement Learning (RL) methods have emerged as a popular choice for
training an efficient and effective dialogue policy. However, these methods
suffer from sparse and unstable reward signals returned by a user simulator
only when a dialogue finishes. Besides, the reward signal is manually designed
by human experts, which requires domain knowledge. Recently, a number of
adversarial learning methods have been proposed to learn the reward function
together with the dialogue policy. However, to alternatively update the
dialogue policy and the reward model on the fly, we are limited to
policy-gradient-based algorithms, such as REINFORCE and PPO. Moreover, the
alternating training of a dialogue agent and the reward model can easily get
stuck in local optima or result in mode collapse. To overcome the listed
issues, we propose to decompose the adversarial training into two steps. First,
we train the discriminator with an auxiliary dialogue generator and then
incorporate a derived reward model into a common RL method to guide the
dialogue policy learning. This approach is applicable to both on-policy and
off-policy RL methods. Based on our extensive experimentation, we can conclude
the proposed method: (1) achieves a remarkable task success rate using both
on-policy and off-policy RL methods; and (2) has the potential to transfer
knowledge from existing domains to a new domain.
",Guided Dialog Policy Learning without Adversarial Learning in the Loop,"Ziming Li, Sungjin Lee, Baolin Peng, Jinchao Li, Julia Kiseleva,
  Maarten de Rijke, Shahin Shayandeh, Jianfeng Gao",2020,Artificial Intelligence,
"  Purpose: In this study, the recently emerged advances in Fuzzy Cognitive Maps
(FCM) are investigated and employed, for achieving the automatic and
non-invasive diagnosis of Coronary Artery Disease (CAD). Methods: A
Computer-Aided Diagnostic model for the acceptable and non-invasive prediction
of CAD using the State Space Advanced FCM (AFCM) approach is proposed. Also, a
rule-based mechanism is incorporated, to further increase the knowledge of the
system and the interpretability of the decision mechanism. The proposed method
is tested utilizing a CAD dataset from the Laboratory of Nuclear Medicine of
the University of Patras. More specifically, two architectures of AFCMs are
designed, and different parameter testing is performed. Furthermore, the
proposed AFCMs, which are based on the new equations proposed recently, are
compared with the traditional FCM approach. Results: The experiments highlight
the effectiveness of the AFCM approach and the new equations over the
traditional approach, which obtained an accuracy of 78.21%, achieving an
increase of seven percent (+7%) on the classification task, and obtaining
85.47% accuracy. Conclusions: It is demonstrated that the AFCM approach in
developing Fuzzy Cognitive Maps outperforms the conventional approach, while it
constitutes a reliable method for the diagnosis of Coronary Artery Disease.
Conclusions and future research related to recent pandemic of coronavirus are
provided.
","State Space Advanced Fuzzy Cognitive Map approach for automatic and non
  Invasive diagnosis of Coronary Artery Disease","Ioannis D. Apostolopoulos, Peter P. Groumpos, Dimitris I.
  Apostolopoulos",2021,Artificial Intelligence,
"  In reinforcement learning (RL), dealing with non-stationarity is a
challenging issue. However, some domains such as traffic optimization are
inherently non-stationary. Causes for and effects of this are manifold. In
particular, when dealing with traffic signal controls, addressing
non-stationarity is key since traffic conditions change over time and as a
function of traffic control decisions taken in other parts of a network. In
this paper we analyze the effects that different sources of non-stationarity
have in a network of traffic signals, in which each signal is modeled as a
learning agent. More precisely, we study both the effects of changing the
\textit{context} in which an agent learns (e.g., a change in flow rates
experienced by it), as well as the effects of reducing agent observability of
the true environment state. Partial observability may cause distinct states (in
which distinct actions are optimal) to be seen as the same by the traffic
signal agents. This, in turn, may lead to sub-optimal performance. We show that
the lack of suitable sensors to provide a representative observation of the
real state seems to affect the performance more drastically than the changes to
the underlying traffic patterns.
","Quantifying the Impact of Non-Stationarity in Reinforcement
  Learning-Based Traffic Signal Control","Lucas N. Alegre, Ana L. C. Bazzan, Bruno C. da Silva",2021,Artificial Intelligence,
"  In this paper, we introduce a novel methodology to efficiently construct a
corpus for question answering over structured data. For this, we introduce an
intermediate representation that is based on the logical query plan in a
database called Operation Trees (OT). This representation allows us to invert
the annotation process without losing flexibility in the types of queries that
we generate. Furthermore, it allows for fine-grained alignment of query tokens
to OT operations. In our method, we randomly generate OTs from a context-free
grammar. Afterwards, annotators have to write the appropriate natural language
question that is represented by the OT. Finally, the annotators assign the
tokens to the OT operations. We apply the method to create a new corpus OTTA
(Operation Trees and Token Assignment), a large semantic parsing corpus for
evaluating natural language interfaces to databases. We compare OTTA to Spider
and LC-QuaD 2.0 and show that our methodology more than triples the annotation
speed while maintaining the complexity of the queries. Finally, we train a
state-of-the-art semantic parsing model on our data and show that our corpus is
a challenging dataset and that the token alignment can be leveraged to increase
the performance significantly.
","A Methodology for Creating Question Answering Corpora Using Inverse Data
  Annotation","Jan Deriu, Katsiaryna Mlynchyk, Philippe Schl\""apfer, Alvaro Rodrigo,
  Dirk von Gr\""unigen, Nicolas Kaiser, Kurt Stockinger, Eneko Agirre, and Mark
  Cieliebak",2020,Artificial Intelligence,
"  Recent progress in deep learning is essentially based on a ""big data for
small tasks"" paradigm, under which massive amounts of data are used to train a
classifier for a single narrow task. In this paper, we call for a shift that
flips this paradigm upside down. Specifically, we propose a ""small data for big
tasks"" paradigm, wherein a single artificial intelligence (AI) system is
challenged to develop ""common sense"", enabling it to solve a wide range of
tasks with little training data. We illustrate the potential power of this new
paradigm by reviewing models of common sense that synthesize recent
breakthroughs in both machine and human vision. We identify functionality,
physics, intent, causality, and utility (FPICU) as the five core domains of
cognitive AI with humanlike common sense. When taken as a unified concept,
FPICU is concerned with the questions of ""why"" and ""how"", beyond the dominant
""what"" and ""where"" framework for understanding vision. They are invisible in
terms of pixels but nevertheless drive the creation, maintenance, and
development of visual scenes. We therefore coin them the ""dark matter"" of
vision. Just as our universe cannot be understood by merely studying observable
matter, we argue that vision cannot be understood without studying FPICU. We
demonstrate the power of this perspective to develop cognitive AI systems with
humanlike common sense by showing how to observe and apply FPICU with little
training data to solve a wide range of challenging tasks, including tool use,
planning, utility inference, and social learning. In summary, we argue that the
next generation of AI must embrace ""dark"" humanlike common sense for solving
novel tasks.
","Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike
  Common Sense","Yixin Zhu, Tao Gao, Lifeng Fan, Siyuan Huang, Mark Edmonds, Hangxin
  Liu, Feng Gao, Chi Zhang, Siyuan Qi, Ying Nian Wu, Joshua B. Tenenbaum,
  Song-Chun Zhu",2020,Artificial Intelligence,
"  Recently, AlphaZero has achieved landmark results in deep reinforcement
learning, by providing a single self-play architecture that learned three
different games at super human level. AlphaZero is a large and complicated
system with many parameters, and success requires much compute power and
fine-tuning. Reproducing results in other games is a challenge, and many
researchers are looking for ways to improve results while reducing
computational demands. AlphaZero's design is purely based on self-play and
makes no use of labeled expert data ordomain specific enhancements; it is
designed to learn from scratch. We propose a novel approach to deal with this
cold-start problem by employing simple search enhancements at the beginning
phase of self-play training, namely Rollout, Rapid Action Value Estimate (RAVE)
and dynamically weighted combinations of these with the neural network, and
Rolling Horizon Evolutionary Algorithms (RHEA). Our experiments indicate that
most of these enhancements improve the performance of their baseline player in
three different (small) board games, with especially RAVE based variants
playing strongly.
",Warm-Start AlphaZero Self-Play Search Enhancements,"Hui Wang, Mike Preuss, Aske Plaat",2020,Artificial Intelligence,
"  Black-box Artificial Intelligence (AI) methods, e.g. deep neural networks,
have been widely utilized to build predictive models that can extract complex
relationships in a dataset and make predictions for new unseen data records.
However, it is difficult to trust decisions made by such methods since their
inner working and decision logic is hidden from the user. Explainable
Artificial Intelligence (XAI) refers to systems that try to explain how a
black-box AI model produces its outcomes. Post-hoc XAI methods approximate the
behavior of a black-box by extracting relationships between feature values and
the predictions. Perturbation-based and decision set methods are among commonly
used post-hoc XAI systems. The former explanators rely on random perturbations
of data records to build local or global linear models that explain individual
predictions or the whole model. The latter explanators use those feature values
that appear more frequently to construct a set of decision rules that produces
the same outcomes as the target black-box. However, these two classes of XAI
methods have some limitations. Random perturbations do not take into account
the distribution of feature values in different subspaces, leading to
misleading approximations. Decision sets only pay attention to frequent feature
values and miss many important correlations between features and class labels
that appear less frequently but accurately represent decision boundaries of the
model. In this paper, we address the above challenges by proposing an
explanation method named Confident Itemsets Explanation (CIE). We introduce
confident itemsets, a set of feature values that are highly correlated to a
specific class label. CIE utilizes confident itemsets to discretize the whole
decision space of a model to smaller subspaces.
",Post-hoc explanation of black-box classifiers using confident itemsets,"Milad Moradi, Matthias Samwald",2021,Artificial Intelligence,
"  This paper analyses the application of artificial intelligence techniques to
various areas of archaeology and more specifically: a) The use of software
tools as a creative stimulus for the organization of exhibitions; the use of
humanoid robots and holographic displays as guides that interact and involve
museum visitors; b) The analysis of methods for the classification of fragments
found in archaeological excavations and for the reconstruction of ceramics,
with the recomposition of the parts of text missing from historical documents
and epigraphs; c) The cataloguing and study of human remains to understand the
social and historical context of belonging with the demonstration of the
effectiveness of the AI techniques used; d) The detection of particularly
difficult terrestrial archaeological sites with the analysis of the
architectures of the Artificial Neural Networks most suitable for solving the
problems presented by the site; the design of a study for the exploration of
marine archaeological sites, located at depths that cannot be reached by man,
through the construction of a freely explorable 3D version.
",The computerization of archaeology: survey on AI techniques,Lorenzo Mantovan and Loris Nanni,2020,Artificial Intelligence,
"  Artificial behavioral agents are often evaluated based on their consistent
behaviors and performance to take sequential actions in an environment to
maximize some notion of cumulative reward. However, human decision making in
real life usually involves different strategies and behavioral trajectories
that lead to the same empirical outcome. Motivated by clinical literature of a
wide range of neurological and psychiatric disorders, we propose here a more
general and flexible parametric framework for sequential decision making that
involves a two-stream reward processing mechanism. We demonstrated that this
framework is flexible and unified enough to incorporate a family of problems
spanning multi-armed bandits (MAB), contextual bandits (CB) and reinforcement
learning (RL), which decompose the sequential decision making process in
different levels. Inspired by the known reward processing abnormalities of many
mental disorders, our clinically-inspired agents demonstrated interesting
behavioral trajectories and comparable performance on simulated tasks with
particular reward distributions, a real-world dataset capturing human
decision-making in gambling tasks, and the PacMan game across different reward
stationarities in a lifelong learning setting.
","Unified Models of Human Behavioral Agents in Bandits, Contextual Bandits
  and RL","Baihan Lin, Guillermo Cecchi, Djallel Bouneffouf, Jenna Reinen, Irina
  Rish",2021,Artificial Intelligence,
"  Information gathering in a partially observable environment can be formulated
as a reinforcement learning (RL), problem where the reward depends on the
agent's uncertainty. For example, the reward can be the negative entropy of the
agent's belief over an unknown (or hidden) variable. Typically, the rewards of
an RL agent are defined as a function of the state-action pairs and not as a
function of the belief of the agent; this hinders the direct application of
deep RL methods for such tasks. This paper tackles the challenge of using
belief-based rewards for a deep RL agent, by offering a simple insight that
maximizing any convex function of the belief of the agent can be approximated
by instead maximizing a prediction reward: a reward based on prediction
accuracy. In particular, we derive the exact error between negative entropy and
the expected prediction reward. This insight provides theoretical motivation
for several fields using prediction rewards---namely visual attention, question
answering systems, and intrinsic motivation---and highlights their connection
to the usually distinct fields of active perception, active sensing, and sensor
placement. Based on this insight we present deep anticipatory networks (DANs),
which enables an agent to take actions to reduce its uncertainty without
performing explicit belief inference. We present two applications of DANs:
building a sensor selection system for tracking people in a shopping mall and
learning discrete models of attention on fashion MNIST and MNIST digit
classification.
","Maximizing Information Gain in Partially Observable Environments via
  Prediction Reward","Yash Satsangi, Sungsu Lim, Shimon Whiteson, Frans Oliehoek, Martha
  White",2020,Artificial Intelligence,
"  We explore state-of-the-art neural models for question answering on
electronic medical records and improve their ability to generalize better on
previously unseen (paraphrased) questions at test time. We enable this by
learning to predict logical forms as an auxiliary task along with the main task
of answer span detection. The predicted logical forms also serve as a rationale
for the answer. Further, we also incorporate medical entity information in
these models via the ERNIE architecture. We train our models on the large-scale
emrQA dataset and observe that our multi-task entity-enriched models generalize
to paraphrased questions ~5% better than the baseline BERT model.
",Entity-Enriched Neural Models for Clinical Question Answering,"Bhanu Pratap Singh Rawat, Wei-Hung Weng, So Yeon Min, Preethi
  Raghavan, Peter Szolovits",2020,Artificial Intelligence,
"  The travelling thief problem (TTP) is a multi-component optimisation problem
involving two interdependent NP-hard components: the travelling salesman
problem (TSP) and the knapsack problem (KP). Recent state-of-the-art TTP
solvers modify the underlying TSP and KP solutions in an iterative and
interleaved fashion. The TSP solution (cyclic tour) is typically changed in a
deterministic way, while changes to the KP solution typically involve a random
search, effectively resulting in a quasi-meandering exploration of the TTP
solution space. Once a plateau is reached, the iterative search of the TTP
solution space is restarted by using a new initial TSP tour. We propose to make
the search more efficient through an adaptive surrogate model (based on a
customised form of Support Vector Regression) that learns the characteristics
of initial TSP tours that lead to good TTP solutions. The model is used to
filter out non-promising initial TSP tours, in effect reducing the amount of
time spent to find a good TTP solution. Experiments on a broad range of
benchmark TTP instances indicate that the proposed approach filters out a
considerable number of non-promising initial tours, at the cost of omitting
only a small number of the best TTP solutions.
",Surrogate Assisted Optimisation for Travelling Thief Problems,"Majid Namazi, Conrad Sanderson, M.A. Hakim Newton, Abdul Sattar",2020,Artificial Intelligence,
"  Maneuvering in dense traffic is a challenging task for autonomous vehicles
because it requires reasoning about the stochastic behaviors of many other
participants. In addition, the agent must achieve the maneuver within a limited
time and distance. In this work, we propose a combination of reinforcement
learning and game theory to learn merging behaviors. We design a training
curriculum for a reinforcement learning agent using the concept of level-$k$
behavior. This approach exposes the agent to a broad variety of behaviors
during training, which promotes learning policies that are robust to model
discrepancies. We show that our approach learns more efficient policies than
traditional training methods.
","Reinforcement Learning with Iterative Reasoning for Merging in Dense
  Traffic","Maxime Bouton, Alireza Nakhaei, David Isele, Kikuo Fujimura, and Mykel
  J. Kochenderfer",2020,Artificial Intelligence,
"  Recently, a groundswell of research has identified the use of counterfactual
explanations as a potentially significant solution to the Explainable AI (XAI)
problem. It is argued that (a) technically, these counterfactual cases can be
generated by permuting problem-features until a class change is found, (b)
psychologically, they are much more causally informative than factual
explanations, (c) legally, they are GDPR-compliant. However, there are issues
around the finding of good counterfactuals using current techniques (e.g.
sparsity and plausibility). We show that many commonly-used datasets appear to
have few good counterfactuals for explanation purposes. So, we propose a new
case based approach for generating counterfactuals using novel ideas about the
counterfactual potential and explanatory coverage of a case-base. The new
technique reuses patterns of good counterfactuals, present in a case-base, to
generate analogous counterfactuals that can explain new problems and their
solutions. Several experiments show how this technique can improve the
counterfactual potential and explanatory coverage of case-bases that were
previously found wanting.
","Good Counterfactuals and Where to Find Them: A Case-Based Technique for
  Generating Counterfactuals for Explainable AI (XAI)","Mark T. Keane, Barry Smyth",2020,Artificial Intelligence,
"  Recommendation system or also known as a recommender system is a tool to help
the user in providing a suggestion of a specific dilemma. Thus, recently, the
interest in developing a recommendation system in many fields has increased.
Fuzzy Logic system (FLSs) is one of the approaches that can be used to model
the recommendation systems as it can deal with uncertainty and imprecise
information. However, one of the fundamental issues in FLS is the problem of
the curse of dimensionality. That is, the number of rules in FLSs is increasing
exponentially with the number of input variables. One effective way to overcome
this problem is by using Hierarchical Fuzzy System (HFSs). This paper aims to
explore the use of HFSs for Recommendation system. Specifically, we are
interested in exploring and comparing the HFS and FLS for the Career path
recommendation system (CPRS) based on four key criteria, namely topology, the
number of rules, the rules structures and interpretability. The findings
suggested that the HFS has advantages over FLS towards improving the
interpretability models, in the context of a recommendation system example.
This study contributes to providing an insight into the development of
interpretable HFSs in the Recommendation systems.
","An Exploratory Study of Hierarchical Fuzzy Systems Approach in
  Recommendation System","Tajul Rosli Razak, Iman Hazwam Abd Halim, Muhammad Nabil Fikri
  Jamaludin, Mohammad Hafiz Ismail, Shukor Sanim Mohd Fauzi",2019,Artificial Intelligence,
"  There is a growing recognition that artists use valuable ways to understand
and work with cognitive and perceptual mechanisms to convey desired experiences
and narrative in their created artworks (DiPaola et al., 2010; Zeki, 2001).
This paper documents our attempt to computationally model the creative process
of a portrait painter, who relies on understanding human traits (i.e.,
personality and emotions) to inform their art. Our system includes an empathic
conversational interaction component to capture the dominant personality
category of the user and a generative AI Portraiture system that uses this
categorization to create a personalized stylization of the user's portrait.
This paper includes the description of our systems and the real-time
interaction results obtained during the demonstration session of the NeurIPS
2019 Conference.
","Empathic AI Painter: A Computational Creativity System with Embodied
  Conversational Interaction","Ozge Nilay Yalcin, Nouf Abukhodair and Steve DiPaola",2020,Artificial Intelligence,
"  This paper provides the foundations of a unified cognitive decision-making
framework (QulBIT) which is derived from quantum theory. The main advantage of
this framework is that it can cater for paradoxical and irrational human
decision making. Although quantum approaches for cognition have demonstrated
advantages over classical probabilistic approaches and bounded rationality
models, they still lack explanatory power. To address this, we introduce a
novel explanatory analysis of the decision-maker's belief space. This is
achieved by exploiting quantum interference effects as a way of both
quantifying and explaining the decision-maker's uncertainty. We detail the main
modules of the unified framework, the explanatory analysis method, and
illustrate their application in situations violating the Sure Thing Principle.
","QuLBIT: Quantum-Like Bayesian Inference Technologies for Cognition and
  Decision","Catarina Moreira and Matheus Hammes and Rasim Serdar Kurdoglu and
  Peter Bruza",2020,Artificial Intelligence,
"  Humans possess an inherent ability to chunk sequences into their constituent
parts. In fact, this ability is thought to bootstrap language skills and
learning of image patterns which might be a key to a more animal-like type of
intelligence. Here, we propose a continual generalization of the chunking
problem (an unsupervised problem), encompassing fixed and probabilistic chunks,
discovery of temporal and causal structures and their continual variations.
Additionally, we propose an algorithm called SyncMap that can learn and adapt
to changes in the problem by creating a dynamic map which preserves the
correlation between variables. Results of SyncMap suggest that the proposed
algorithm learn near optimal solutions, despite the presence of many types of
structures and their continual variation. When compared to Word2vec, PARSER and
MRIL, SyncMap surpasses or ties with the best algorithm on $66\%$ of the
scenarios while being the second best in the remaining $34\%$. SyncMap's
model-free simple dynamics and the absence of loss functions reveal that,
perhaps surprisingly, much can be done with self-organization alone. Code
available at https://github.com/zweifel/SyncMap.
",Continual General Chunking Problem and SyncMap,Danilo Vasconcellos Vargas and Toshitake Asabuki,2021,Artificial Intelligence,
"  Decision and policy-makers in multi-criteria decision-making analysis take
into account some strategies in order to analyze outcomes and to finally make
an effective and more precise decision. Among those strategies, the
modification of the normalization process in the multiple-criteria
decision-making algorithm is still a question due to the confrontation of many
normalization tools. Normalization is the basic action in defining and solving
a MADM problem and a MADM model. Normalization is the first, also necessary,
step in solving, i.e. the application of a MADM method. It is a fact that the
selection of normalization methods has a direct effect on the results. One of
the latest normalization methods introduced is the Logarithmic Normalization
(LN) method. This new method has a distinguished advantage, reflecting in that
a sum of the normalized values of criteria always equals 1. This normalization
method had never been applied in any MADM methods before. This research study
is focused on the analysis of the classical MADM methods based on logarithmic
normalization. VIKOR and TOPSIS, as the two famous MADM methods, were selected
for this reanalysis research study. Two numerical examples were checked in both
methods, based on both the classical and the novel ways based on the LN. The
results indicate that there are differences between the two approaches.
Eventually, a sensitivity analysis is also designed to illustrate the
reliability of the final results.
","A VIKOR and TOPSIS focused reanalysis of the MADM methods based on
  logarithmic normalization","Sarfaraz Zolfani, Morteza Yazdani, Dragan Pamucar, Pascale Zarat\'e
  (IRIT-ADRIA, IRIT, UT1)",2020,Artificial Intelligence,
"  Fuzzy rule-based model is a powerful tool for imitating the human way of
thinking and solving uncertainty-related problems as it allows for
understandable and interpretable rule bases. The objective of this paper is to
study the applicability of fuzzy rule-based modelling to quantify soil
classification for engineering purposes by qualitatively considering soil index
properties. The classification system of the Highway Research Board is
considered to illustrate a fuzzy rule-based model. The soil's index properties
are fuzzified using triangular functions, and the fuzzy membership values are
calculated. Fuzzy arithmetical operators are then applied to the membership
values obtained for classification. Fuzzy decision tree classification
algorithm is used to derive fuzzy if-then rules to quantify qualitative soil
classification. The proposed system is implemented in MATLAB. The results
obtained are checked and the implementation of the proposed model is measured
against the outcomes of the laboratory tests.
","Application of Fuzzy Rule based System for Highway Research Board
  Classification of Soils","Sujatha A, L Govindaraju and N Shivakumar",2020,Artificial Intelligence,
"  This research is about the development a fuzzy decision support system for
the diagnosis of coronary artery disease based on evidence. The coronary artery
disease data sets taken from University California Irvine (UCI) are used. The
knowledge base of fuzzy decision support system is taken by using rules
extraction method based on Rough Set Theory. The rules then are selected and
fuzzified based on information from discretization of numerical attributes.
Fuzzy rules weight is proposed using the information from support of extracted
rules. UCI heart disease data sets collected from U.S., Switzerland and
Hungary, data from Ipoh Specialist Hospital Malaysia are used to verify the
proposed system. The results show that the system is able to give the
percentage of coronary artery blocking better than cardiologists and
angiography. The results of the proposed system were verified and validated by
three expert cardiologists and are considered to be more efficient and useful.
","Diagnosis of Coronary Artery Disease Using Artificial Intelligence Based
  Decision Support System","Noor Akhmad Setiawan, Paruvachi Ammasai Venkatachalam, Ahmad Fadzil M
  Hani",2009,Artificial Intelligence,
"  This paper describes an entropy equation, but one that should be used for
measuring energy and not information. In relation to the human brain therefore,
both of these quantities can be used to represent the stored information. The
human brain makes use of energy efficiency to form its structures, which is
likely to be linked to the neuron wiring. This energy efficiency can also be
used as the basis for a clustering algorithm, which is described in a different
paper. This paper is more of a discussion about global properties, where the
rules used for the clustering algorithm can also create the entropy equation E
= (mean * variance). This states that work is done through the energy released
by the 'change' in entropy. The equation is so simplistic and generic that it
can offer arguments for completely different domains, where the journey ends
with a discussion about global energy properties in physics and beyond. A
comparison with Einstein's relativity equation is made and also the audacious
suggestion that a black hole has zero-energy inside.
",An Entropy Equation for Energy,Kieran Greer,2020,Artificial Intelligence,
"  Advances in hardware technology have enabled more integration of
sophisticated software, triggering progress in the development and employment
of Unmanned Vehicles (UVs), and mitigating restraints for onboard intelligence.
As a result, UVs can now take part in more complex mission where continuous
transformation in environmental condition calls for a higher level of
situational responsiveness. This paper serves as an introduction to UVs mission
planning and management systems aiming to highlight some of the recent
developments in the field of autonomous underwater and aerial vehicles in
addition to stressing some possible future directions and discussing the
learned lessons. A comprehensive survey over autonomy assessment of UVs, and
different aspects of autonomy such as situation awareness, cognition, and
decision-making has been provided in this study. The paper separately explains
the humanoid and autonomous system's performance and highlights the role and
impact of a human in UVs operations.
","Current Advancements on Autonomous Mission Planning and Management
  Systems: an AUV and UAV perspective","Adham Atyabi, Somaiyeh MahmoudZadeh, Samia Nefti-Meziani",2018,Artificial Intelligence,
"  Tabletop roleplaying games (TTRPGs) and procedural content generators can
both be understood as systems of rules for producing content. In this paper, we
argue that TTRPG design can usefully be viewed as procedural content generator
design. We present several case studies linking key concepts from PCG research
-- including possibility spaces, expressive range analysis, and generative
pipelines -- to key concepts in TTRPG design. We then discuss the implications
of these relationships and suggest directions for future work uniting research
in TTRPGs and PCG.
",Tabletop Roleplaying Games as Procedural Content Generators,"Matthew Guzdial, Devi Acharya, Max Kreminski, Michael Cook, Mirjam
  Eladhari, Antonios Liapis and Anne Sullivan",2020,Artificial Intelligence,
"  Real-world complex systems are often modelled by sets of equations with
endogenous and exogenous variables. What can we say about the causal and
probabilistic aspects of variables that appear in these equations without
explicitly solving the equations? We make use of Simon's causal ordering
algorithm (Simon, 1953) to construct a causal ordering graph and prove that it
expresses the effects of soft and perfect interventions on the equations under
certain unique solvability assumptions. We further construct a Markov ordering
graph and prove that it encodes conditional independences in the distribution
implied by the equations with independent random exogenous variables, under a
similar unique solvability assumption. We discuss how this approach reveals and
addresses some of the limitations of existing causal modelling frameworks, such
as causal Bayesian networks and structural causal models.
","Conditional independences and causal relations implied by sets of
  equations",Tineke Blom and Mirthe M. van Diepen and Joris M. Mooij,2021,Artificial Intelligence,
"  Advances in hardware technology have facilitated more integration of
sophisticated software toward augmenting the development of Unmanned Vehicles
(UVs) and mitigating constraints for onboard intelligence. As a result, UVs can
operate in complex missions where continuous trans-formation in environmental
condition calls for a higher level of situational responsiveness and autonomous
decision making. This book is a research monograph that aims to provide a
comprehensive survey of UVs autonomy and its related properties in internal and
external situation awareness to-ward robust mission planning in severe
conditions. An advance level of intelligence is essential to minimize the
reliance on the human supervisor, which is a main concept of autonomy. A
self-controlled system needs a robust mission management strategy to push the
boundaries towards autonomous structures, and the UV should be aware of its
internal state and capabilities to assess whether current mission goal is
achievable or find an alternative solution. In this book, the AUVs will become
the major case study thread but other cases/types of vehicle will also be
considered. In-deed the research monograph, the review chapters and the new
approaches we have developed would be appropriate for use as a reference in
upper years or postgraduate degrees for its coverage of literature and
algorithms relating to Robot/Vehicle planning, tasking, routing, and trust.
","Autonomy and Unmanned Vehicles Augmented Reactive Mission-Motion
  Planning Architecture for Autonomous Vehicles","Somaiyeh MahmoudZadeh, David MW Powers, Reza Bairam Zadeh",2019,Artificial Intelligence,
"  Social network structure is one of the key determinants of human language
evolution. Previous work has shown that the network of social interactions
shapes decentralized learning in human groups, leading to the emergence of
different kinds of communicative conventions. We examined the effects of social
network organization on the properties of communication systems emerging in
decentralized, multi-agent reinforcement learning communities. We found that
the global connectivity of a social network drives the convergence of
populations on shared and symmetric communication systems, preventing the
agents from forming many local ""dialects"". Moreover, the agent's degree is
inversely related to the consistency of its use of communicative conventions.
These results show the importance of the basic properties of social network
structure on reinforcement communication learning and suggest a new
interpretation of findings on human convergence on word conventions.
","Reinforcement Communication Learning in Different Social Network
  Structures","Marina Dubova, Arseny Moskvichev, Robert Goldstone",2020,Artificial Intelligence,
"  To provide a foundation for conceptual modeling, ontologies have been
introduced to specify the entities, the existences of which are acknowledged in
the model. Ontologies are essential components as mechanisms to model a portion
of reality in software engineering. In this context, a model refers to a
description of objects and processes that populate a system. Developing such a
description constrains and directs the design, development, and use of the
corresponding system, thus avoiding such difficulties as conflicts and lack of
a common understanding. In this cross-area research between modeling and
ontology, there has been a growing interest in the development and use of
domain ontologies (e.g., Resource Description Framework, Ontology Web
Language). This paper contributes to the establishment of a broad ontological
foundation for conceptual modeling in a specific domain through proposing a
workable ontology (abbreviated as TM). A TM is a one-category ontology called a
thimac (things/machines) that is used to elaborate the design and analysis of
ontological presumptions. The focus of the study is on such notions as change,
event, and time. Several current ontological difficulties are reviewed and
remodeled in the TM. TM modeling is also contrasted with time representation in
SysML. The results demonstrate that a TM is a useful tool for addressing these
ontological problems.
",Conceptual Modeling of Time for Computational Ontologies,Sabah Al-Fedaghi,2020,Artificial Intelligence,
"  We present Tactician, a tactic learner and prover for the Coq Proof
Assistant. Tactician helps users make tactical proof decisions while they
retain control over the general proof strategy. To this end, Tactician learns
from previously written tactic scripts and gives users either suggestions about
the next tactic to be executed or altogether takes over the burden of proof
synthesis. Tactician's goal is to provide users with a seamless, interactive,
and intuitive experience together with robust and adaptive proof automation. In
this paper, we give an overview of Tactician from the user's point of view,
regarding both day-to-day usage and issues of package dependency management
while learning in the large. Finally, we give a peek into Tactician's
implementation as a Coq plugin and machine learning platform.
","The Tactician (extended version): A Seamless, Interactive Tactic Learner
  and Prover for Coq","Lasse Blaauwbroek, Josef Urban and Herman Geuvers",2020,Artificial Intelligence,
"  Ortus is a simple virtual organism that also serves as an initial framework
for investigating and developing biologically-based artificial intelligence.
Born from a goal to create complex virtual intelligence and an initial attempt
to model C. elegans, Ortus implements a number of mechanisms observed in
organic nervous systems, and attempts to fill in unknowns based upon plausible
biological implementations and psychological observations. Implemented
mechanisms include excitatory and inhibitory chemical synapses, bidirectional
gap junctions, and Hebbian learning with its Stentian extension. We present an
initial experiment that showcases Ortus' fundamental principles; specifically,
a cyclic respiratory circuit, and emotionally-driven associative learning with
respect to an input stimulus. Finally, we discuss the implications and future
directions for Ortus and similar systems.
","Ortus: an Emotion-Driven Approach to (artificial) Biological
  Intelligence","Andrew W.E. McDonald, Sean Grimes, David E. Breen",2017,Artificial Intelligence,
"  Relevant research has been highlighted in the computing community to develop
machine learning models capable of predicting the occurrence of crimes,
analyzing contexts of crimes, extracting profiles of individuals linked to
crime, and analyzing crimes over time. However, models capable of predicting
specific crimes, such as homicide, are not commonly found in the current
literature. This research presents a machine learning model to predict homicide
crimes, using a dataset that uses generic data (without study location
dependencies) based on incident report records for 34 different types of
crimes, along with time and space data from crime reports. Experimentally, data
from the city of Bel\'em - Par\'a, Brazil was used. These data were transformed
to make the problem generic, enabling the replication of this model to other
locations. In the research, analyses were performed with simple and robust
algorithms on the created dataset. With this, statistical tests were performed
with 11 different classification methods and the results are related to the
prediction's occurrence and non-occurrence of homicide crimes in the month
subsequent to the occurrence of other registered crimes, with 76% assertiveness
for both classes of the problem, using Random Forest. Results are considered as
a baseline for the proposed problem.
",Prediction of Homicides in Urban Centers: A Machine Learning Approach,"Jos\'e Ribeiro, Lair Meneses, Denis Costa, Wando Miranda, Ronnie Alves",2021,Artificial Intelligence,
"  Autonomous car racing is a major challenge in robotics. It raises fundamental
problems for classical approaches such as planning minimum-time trajectories
under uncertain dynamics and controlling the car at the limits of its handling.
Besides, the requirement of minimizing the lap time, which is a sparse
objective, and the difficulty of collecting training data from human experts
have also hindered researchers from directly applying learning-based approaches
to solve the problem. In the present work, we propose a learning-based system
for autonomous car racing by leveraging a high-fidelity physical car
simulation, a course-progress proxy reward, and deep reinforcement learning. We
deploy our system in Gran Turismo Sport, a world-leading car simulator known
for its realistic physics simulation of different race cars and tracks, which
is even used to recruit human race car drivers. Our trained policy achieves
autonomous racing performance that goes beyond what had been achieved so far by
the built-in AI, and, at the same time, outperforms the fastest driver in a
dataset of over 50,000 human players.
","Super-Human Performance in Gran Turismo Sport Using Deep Reinforcement
  Learning","Florian Fuchs, Yunlong Song, Elia Kaufmann, Davide Scaramuzza, Peter
  Duerr",2021,Artificial Intelligence,
"  Consumer electronic devices such as mobile handsets, goods tagged with RFID
labels, location and position sensors are continuously generating a vast amount
of location enriched data called geospatial data. Conventionally such
geospatial data is used for military applications. In recent times, many useful
civilian applications have been designed and deployed around such geospatial
data. For example, a recommendation system to suggest restaurants or places of
attraction to a tourist visiting a particular locality. At the same time, civic
bodies are harnessing geospatial data generated through remote sensing devices
to provide better services to citizens such as traffic monitoring, pothole
identification, and weather reporting. Typically such applications are
leveraged upon non-hierarchical machine learning techniques such as Naive-Bayes
Classifiers, Support Vector Machines, and decision trees. Recent advances in
the field of deep-learning showed that Neural Network-based techniques
outperform conventional techniques and provide effective solutions for many
geospatial data analysis tasks such as object recognition, image
classification, and scene understanding. The chapter presents a survey on the
current state of the applications of deep learning techniques for analyzing
geospatial data.
  The chapter is organized as below: (i) A brief overview of deep learning
algorithms. (ii)Geospatial Analysis: a Data Science Perspective (iii)
Deep-learning techniques for Remote Sensing data analytics tasks (iv)
Deep-learning techniques for GPS data analytics(iv) Deep-learning techniques
for RFID data analytics.
",Deep Learning Techniques for Geospatial Data Analysis,"Arvind W. Kiwelekar, Geetanjali S. Mahamunkar, Laxman D. Netak, Valmik
  B Nikam",2020,Artificial Intelligence,
"  Due to advances in machine learning and artificial intelligence (AI), a new
role is emerging for machines as intelligent assistants to radiologists in
their clinical workflows. But what systematic clinical thought processes are
these machines using? Are they similar enough to those of radiologists to be
trusted as assistants? A live demonstration of such a technology was conducted
at the 2016 Scientific Assembly and Annual Meeting of the Radiological Society
of North America (RSNA). The demonstration was presented in the form of a
question-answering system that took a radiology multiple choice question and a
medical image as inputs. The AI system then demonstrated a cognitive workflow,
involving text analysis, image analysis, and reasoning, to process the question
and generate the most probable answer. A post demonstration survey was made
available to the participants who experienced the demo and tested the question
answering system. Of the reported 54,037 meeting registrants, 2,927 visited the
demonstration booth, 1,991 experienced the demo, and 1,025 completed a
post-demonstration survey. In this paper, the methodology of the survey is
shown and a summary of its results are presented. The results of the survey
show a very high level of receptiveness to cognitive computing technology and
artificial intelligence among radiologists.
","Receptivity of an AI Cognitive Assistant by the Radiology Community: A
  Report on Data Collected at RSNA","Karina Kanjaria, Anup Pillai, Chaitanya Shivade, Marina Bendersky,
  Ashutosh Jadhav, Vandana Mukherjee, Tanveer Syeda-Mahmood",2020,Artificial Intelligence,
"  Equipping machines with comprehensive knowledge of the world's entities and
their relationships has been a long-standing goal of AI. Over the last decade,
large-scale knowledge bases, also known as knowledge graphs, have been
automatically constructed from web contents and text sources, and have become a
key asset for search engines. This machine knowledge can be harnessed to
semantically interpret textual phrases in news, social media and web tables,
and contributes to question answering, natural language processing and data
analytics. This article surveys fundamental concepts and practical methods for
creating and curating large knowledge bases. It covers models and methods for
discovering and canonicalizing entities and their semantic types and organizing
them into clean taxonomies. On top of this, the article discusses the automatic
extraction of entity-centric properties. To support the long-term life-cycle
and the quality assurance of machine knowledge, the article presents methods
for constructing open schemas and for knowledge curation. Case studies on
academic projects and industrial knowledge graphs complement the survey of
concepts and methods.
","Machine Knowledge: Creation and Curation of Comprehensive Knowledge
  Bases","Gerhard Weikum, Luna Dong, Simon Razniewski, Fabian Suchanek",2021,Artificial Intelligence,
"  Within intelligent tutoring systems, considerable research has investigated
hints, including how to generate data-driven hints, what hint content to
present, and when to provide hints for optimal learning outcomes. However, less
attention has been paid to how hints are presented. In this paper, we propose a
new hint delivery mechanism called ""Assertions"" for providing unsolicited hints
in a data-driven intelligent tutor. Assertions are partially-worked example
steps designed to appear within a student workspace, and in the same format as
student-derived steps, to show students a possible subgoal leading to the
solution. We hypothesized that Assertions can help address the well-known hint
avoidance problem. In systems that only provide hints upon request, hint
avoidance results in students not receiving hints when they are needed. Our
unsolicited Assertions do not seek to improve student help-seeking, but rather
seek to ensure students receive the help they need. We contrast Assertions with
Messages, text-based, unsolicited hints that appear after student inactivity.
Our results show that Assertions significantly increase unsolicited hint usage
compared to Messages. Further, they show a significant aptitude-treatment
interaction between Assertions and prior proficiency, with Assertions leading
students with low prior proficiency to generate shorter (more efficient)
posttest solutions faster. We also present a clustering analysis that shows
patterns of productive persistence among students with low prior knowledge when
the tutor provides unsolicited help in the form of Assertions. Overall, this
work provides encouraging evidence that hint presentation can significantly
impact how students use them and using Assertions can be an effective way to
address help avoidance.
","Avoiding Help Avoidance: Using Interface Design Changes to Promote
  Unsolicited Hint Usage in an Intelligent Tutor","Mehak Maniktala, Christa Cody, Tiffany Barnes, and Min Chi",2020,Artificial Intelligence,
"  We propose a vision for directing research and education in the ICT field.
Our Smart and Sustainable World vision targets at prosperity for the people and
the planet through better awareness and control of both human-made and natural
environment. The needs of the society, individuals, and industries are
fulfilled with intelligent systems that sense their environment, make proactive
decisions on actions advancing their goals, and perform the actions on the
environment. We emphasize artificial intelligence, feedback loops, human
acceptance and control, intelligent use of basic resources, performance
parameters, mission-oriented interdisciplinary research, and a holistic systems
view complementing the conventional analytical reductive view as a research
paradigm especially for complex problems. To serve a broad audience, we explain
these concepts and list the essential literature. We suggest planning research
and education by specifying, in a step-wise manner, scenarios, performance
criteria, system models, research problems and education content, resulting in
common goals and a coherent project portfolio as well as education curricula.
Research and education produce feedback to support evolutionary development and
encourage creativity in research. Finally, we propose concrete actions for
realizing this approach.
",Research and Education Towards Smart and Sustainable World,"Jukka Riekki and Aarne M\""ammel\""a",2021,Artificial Intelligence,
"  In this paper, we proposed a transfer learning-based English language
learning chatbot, whose output generated by GPT-2 can be explained by
corresponding ontology graph rooted by fine-tuning dataset. We design three
levels for systematically English learning, including phonetics level for
speech recognition and pronunciation correction, semantic level for specific
domain conversation, and the simulation of free-style conversation in English -
the highest level of language chatbot communication as free-style conversation
agent. For academic contribution, we implement the ontology graph to explain
the performance of free-style conversation, following the concept of XAI
(Explainable Artificial Intelligence) to visualize the connections of neural
network in bionics, and explain the output sentence from language model. From
implementation perspective, our Language Learning agent integrated the
mini-program in WeChat as front-end, and fine-tuned GPT-2 model of transfer
learning as back-end to interpret the responses by ontology graph.
","The design and implementation of Language Learning Chatbot with XAI
  using Ontology and Transfer Learning","Nuobei Shi, Qin Zeng and Raymond Lee",2020,Artificial Intelligence,
"  We addressed the problem of a lack of semantic representation for
user-centric explanations and different explanation types in our Explanation
Ontology (https://purl.org/heals/eo). Such a representation is increasingly
necessary as explainability has become an important problem in Artificial
Intelligence with the emergence of complex methods and an uptake in
high-precision and user-facing settings. In this submission, we provide
step-by-step guidance for system designers to utilize our ontology, introduced
in our resource track paper, to plan and model for explanations during the
design of their Artificial Intelligence systems. We also provide a detailed
example with our utilization of this guidance in a clinical setting.
",Explanation Ontology in Action: A Clinical Use-Case,"Shruthi Chari, Oshani Seneviratne, Daniel M. Gruen, Morgan A. Foreman,
  Amar K. Das, Deborah L. McGuinness",2020,Artificial Intelligence,
"  Explainability has been a goal for Artificial Intelligence (AI) systems since
their conception, with the need for explainability growing as more complex AI
models are increasingly used in critical, high-stakes settings such as
healthcare. Explanations have often added to an AI system in a non-principled,
post-hoc manner. With greater adoption of these systems and emphasis on
user-centric explainability, there is a need for a structured representation
that treats explainability as a primary consideration, mapping end user needs
to specific explanation types and the system's AI capabilities. We design an
explanation ontology to model both the role of explanations, accounting for the
system and user attributes in the process, and the range of different
literature-derived explanation types. We indicate how the ontology can support
user requirements for explanations in the domain of healthcare. We evaluate our
ontology with a set of competency questions geared towards a system designer
who might use our ontology to decide which explanation types to include, given
a combination of users' needs and a system's capabilities, both in system
design settings and in real-time operations. Through the use of this ontology,
system designers will be able to make informed choices on which explanations AI
systems can and should provide.
",Explanation Ontology: A Model of Explanations for User-Centered AI,"Shruthi Chari, Oshani Seneviratne, Daniel M. Gruen, Morgan A. Foreman,
  Amar K. Das, Deborah L. McGuinness",2020,Artificial Intelligence,
"  Determining when and whether to provide personalized support is a well-known
challenge called the assistance dilemma. A core problem in solving the
assistance dilemma is the need to discover when students are unproductive so
that the tutor can intervene. Such a task is particularly challenging for
open-ended domains, even those that are well-structured with defined principles
and goals. In this paper, we present a set of data-driven methods to classify,
predict, and prevent unproductive problem-solving steps in the well-structured
open-ended domain of logic. This approach leverages and extends the Hint
Factory, a set of methods that leverages prior student solution attempts to
build data-driven intelligent tutors. We present a HelpNeed classification,
that uses prior student data to determine when students are likely to be
unproductive and need help learning optimal problem-solving strategies. We
present a controlled study to determine the impact of an Adaptive pedagogical
policy that provides proactive hints at the start of each step based on the
outcomes of our HelpNeed predictor: productive vs. unproductive. Our results
show that the students in the Adaptive condition exhibited better training
behaviors, with lower help avoidance, and higher help appropriateness (a higher
chance of receiving help when it was likely to be needed), as measured using
the HelpNeed classifier, when compared to the Control. Furthermore, the results
show that the students who received Adaptive hints based on HelpNeed
predictions during training significantly outperform their Control peers on the
posttest, with the former producing shorter, more optimal solutions in less
time. We conclude with suggestions on how these HelpNeed methods could be
applied in other well-structured open-ended domains.
","Extending the Hint Factory for the assistance dilemma: A novel,
  data-driven HelpNeed Predictor for proactive problem-solving help","Mehak Maniktala, Christa Cody, Amy Isvik, Nicholas Lytle, Min Chi,
  Tiffany Barnes",2020,Artificial Intelligence,
"  The recent advances in artificial intelligence namely in machine learning and
deep learning, have boosted the performance of intelligent systems in several
ways. This gave rise to human expectations, but also created the need for a
deeper understanding of how intelligent systems think and decide. The concept
of explainability appeared, in the extent of explaining the internal system
mechanics in human terms. Recommendation systems are intelligent systems that
support human decision making, and as such, they have to be explainable in
order to increase user trust and improve the acceptance of recommendations. In
this work, we focus on a context-aware recommendation system for energy
efficiency and develop a mechanism for explainable and persuasive
recommendations, which are personalized to user preferences and habits. The
persuasive facts either emphasize on the economical saving prospects (Econ) or
on a positive ecological impact (Eco) and explanations provide the reason for
recommending an energy saving action. Based on a study conducted using a
Telegram bot, different scenarios have been validated with actual data and
human feedback. Current results show a total increase of 19\% on the
recommendation acceptance ratio when both economical and ecological persuasive
facts are employed. This revolutionary approach on recommendation systems,
demonstrates how intelligent recommendations can effectively encourage energy
saving behavior.
","The emergence of Explainability of Intelligent Systems: Delivering
  Explainable and Personalised Recommendations for Energy Efficiency","Christos Sardianos and Iraklis Varlamis and Christos Chronis and
  George Dimitrakopoulos and Abdullah Alsalemi and Yassine Himeur and Faycal
  Bensaali and Abbes Amira",2020,Artificial Intelligence,
"  Anginal symptoms can connote increased cardiac risk and a need for change in
cardiovascular management. This study evaluated the potential to extract these
symptoms from physician notes using the Bidirectional Encoder from Transformers
language model fine-tuned on a domain-specific corpus. The history of present
illness section of 459 expert annotated primary care physician notes from
consecutive patients referred for cardiac testing without known atherosclerotic
cardiovascular disease were included. Notes were annotated for positive and
negative mentions of chest pain and shortness of breath characterization. The
results demonstrate high sensitivity and specificity for the detection of chest
pain or discomfort, substernal chest pain, shortness of breath, and dyspnea on
exertion. Small sample size limited extracting factors related to provocation
and palliation of chest pain. This study provides a promising starting point
for the natural language processing of physician notes to characterize
clinically actionable anginal symptoms.
","Extracting Angina Symptoms from Clinical Notes Using Pre-Trained
  Transformer Architectures","Aaron S. Eisman, Nishant R. Shah, Carsten Eickhoff, George Zerveas,
  Elizabeth S. Chen, Wen-Chih Wu, Indra Neil Sarkar",2020,Artificial Intelligence,
"  Transfer learning is an effective technique to improve a target recommender
system with the knowledge from a source domain. Existing research focuses on
the recommendation performance of the target domain while ignores the privacy
leakage of the source domain. The transferred knowledge, however, may
unintendedly leak private information of the source domain. For example, an
attacker can accurately infer user demographics from their historical purchase
provided by a source domain data owner. This paper addresses the above
privacy-preserving issue by learning a privacy-aware neural representation by
improving target performance while protecting source privacy. The key idea is
to simulate the attacks during the training for protecting unseen users'
privacy in the future, modeled by an adversarial game, so that the transfer
learning model becomes robust to attacks. Experiments show that the proposed
PrivNet model can successfully disentangle the knowledge benefitting the
transfer from leaking the privacy.
","PrivNet: Safeguarding Private Attributes in Transfer Learning for
  Recommendation","Guangneng Hu, Qiang Yang",2020,Artificial Intelligence,
"  Game AI competitions are important to foster research and development on Game
AI and AI in general. These competitions supply different challenging problems
that can be translated into other contexts, virtual or real. They provide
frameworks and tools to facilitate the research on their core topics and
provide means for comparing and sharing results. A competition is also a way to
motivate new researchers to study these challenges. In this document, we
present the Geometry Friends Game AI Competition. Geometry Friends is a
two-player cooperative physics-based puzzle platformer computer game. The
concept of the game is simple, though its solving has proven to be difficult.
While the main and apparent focus of the game is cooperation, it also relies on
other AI-related problems such as planning, plan execution, and motion control,
all connected to situational awareness. All of these must be solved in
real-time. In this paper, we discuss the competition and the challenges it
brings, and present an overview of the current solutions.
","A Game AI Competition to foster Collaborative AI research and
  development",Ana Salta and Rui Prada and Francisco S. Melo,2020,Artificial Intelligence,
"  The Dead Sea Scrolls are tangible evidence of the Bible's ancient scribal
culture. Palaeography - the study of ancient handwriting - can provide access
to this scribal culture. However, one of the problems of traditional
palaeography is to determine writer identity when the writing style is near
uniform. This is exemplified by the Great Isaiah Scroll (1QIsaa). To this end,
we used pattern recognition and artificial intelligence techniques to innovate
the palaeography of the scrolls regarding writer identification and to pioneer
the microlevel of individual scribes to open access to the Bible's ancient
scribal culture. Although many scholars believe that 1QIsaa was written by one
scribe, we report new evidence for a breaking point in the series of columns in
this scroll. Without prior assumption of writer identity, based on point clouds
of the reduced-dimensionality feature-space, we found that columns from the
first and second halves of the manuscript ended up in two distinct zones of
such scatter plots, notably for a range of digital palaeography tools, each
addressing very different featural aspects of the script samples. In a
secondary, independent, analysis, now assuming writer difference and using yet
another independent feature method and several different types of statistical
testing, a switching point was found in the column series. A clear phase
transition is apparent around column 27. Given the statistically significant
differences between the two halves, a tertiary, post-hoc analysis was
performed. Demonstrating that two main scribes were responsible for the Great
Isaiah Scroll, this study sheds new light on the Bible's ancient scribal
culture by providing new, tangible evidence that ancient biblical texts were
not copied by a single scribe only but that multiple scribes could closely
collaborate on one particular manuscript.
","Artificial intelligence based writer identification generates new
  evidence for the unknown scribes of the Dead Sea Scrolls exemplified by the
  Great Isaiah Scroll (1QIsaa)","Mladen Popovi\'c, Maruf A. Dhali, Lambert Schomaker",2021,Artificial Intelligence,
"  The 9th International Workshop on Theorem-Proving Components for Educational
Software (ThEdu'20) was scheduled to happen on June 29 as a satellite of the
IJCAR-FSCD 2020 joint meeting, in Paris. The COVID-19 pandemic came by
surprise, though, and the main conference was virtualised. Fearing that an
online meeting would not allow our community to fully reproduce the usual
face-to-face networking opportunities of the ThEdu initiative, the Steering
Committee of ThEdu decided to cancel our workshop. Given that many of us had
already planned and worked for that moment, we decided that ThEdu'20 could
still live in the form of an EPTCS volume. The EPTCS concurred with us,
recognising this very singular situation, and accepted our proposal of
organising a special issue with papers submitted to ThEdu'20. An open call for
papers was then issued, and attracted five submissions, all of which have been
accepted by our reviewers, who produced three careful reports on each of the
contributions. The resulting revised papers are collected in the present
volume. We, the volume editors, hope that this collection of papers will help
further promoting the development of theorem-proving-based software, and that
it will collaborate to improve the mutual understanding between computer
mathematicians and stakeholders in education. With some luck, we would actually
expect that the very special circumstances set up by the worst sanitary crisis
in a century will happen to reinforce the need for the application of certified
components and of verification methods for the production of educational
software that would be available even when the traditional on-site learning
experiences turn out not to be recommendable.
","Proceedings 9th International Workshop on Theorem Proving Components for
  Educational Software","Pedro Quaresma (University of Coimbra, Portugal), Walther Neuper (JKU
  Johannes Kepler University, Linz, Austria), Jo\~ao Marcos (UFRN, Brazil)",2020,Artificial Intelligence,
"  We study the novel problem of blackbox optimization of multiple objectives
via multi-fidelity function evaluations that vary in the amount of resources
consumed and their accuracy. The overall goal is to approximate the true Pareto
set of solutions by minimizing the resources consumed for function evaluations.
For example, in power system design optimization, we need to find designs that
trade-off cost, size, efficiency, and thermal tolerance using multi-fidelity
simulators for design evaluations. In this paper, we propose a novel approach
referred as Multi-Fidelity Output Space Entropy Search for Multi-objective
Optimization (MF-OSEMO) to solve this problem. The key idea is to select the
sequence of candidate input and fidelity-vector pairs that maximize the
information gained about the true Pareto front per unit resource cost. Our
experiments on several synthetic and real-world benchmark problems show that
MF-OSEMO, with both approximations, significantly improves over the
state-of-the-art single-fidelity algorithms for multi-objective optimization.
","Multi-Fidelity Multi-Objective Bayesian Optimization: An Output Space
  Entropy Search Approach","Syrine Belakaria, Aryan Deshwal and Janardhan Rao Doppa",2020,Artificial Intelligence,
"  This paper updates the cognitive model, firstly by creating two systems and
then unifying them over the same structure. It represents information at the
semantic level only, where labelled patterns are aggregated into a
'type-set-match' form. It is described that the aggregations can be used to
match across regions with potentially different functionality and therefore
give the structure a required amount of flexibility. The theory is that if the
model stores information which can be transposed in consistent ways, then that
will result in knowledge and some level of intelligence. As part of the design,
patterns have to become distinct and that is realised by unique paths through
shared aggregated structures. An ensemble-hierarchy relation also helps to
define uniqueness through local feedback that may even be an action potential.
The earlier models are still consistent in terms of their proposed
functionality, but some of the architecture boundaries have been moved to match
them up more closely. After pattern optimisation and tree-like aggregations,
the two main models differ only in their upper, more intelligent level. One
provides a propositional logic for mutually inclusive or exclusive pattern
groups and sequences, while the other provides a behaviour script that is
constructed from node types. It can be seen that these two views are
complimentary and would allow some control over behaviours, as well as
memories, that might get selected.
",New Ideas for Brain Modelling 7,Kieran Greer,2021,Artificial Intelligence,
"  Analytic software tools and workflows are increasing in capability,
complexity, number, and scale, and the integrity of our workflows is as
important as ever. Specifically, we must be able to inspect the process of
analytic workflows to assess (1) confidence of the conclusions, (2) risks and
biases of the operations involved, (3) sensitivity of the conclusions to
sources and agents, (4) impact and pertinence of various sources and agents,
and (5) diversity of the sources that support the conclusions. We present an
approach that tracks agents' provenance with PROV-O in conjunction with agents'
appraisals and evidence links (expressed in our novel DIVE ontology). Together,
PROV-O and DIVE enable dynamic propagation of confidence and counter-factual
refutation to improve human-machine trust and analytic integrity. We
demonstrate representative software developed for user interaction with that
provenance, and discuss key needs for organizations adopting such approaches.
We demonstrate all of these assessments in a multi-agent analysis scenario,
using an interactive web-based information validation UI.
",Provenance-Based Interpretation of Multi-Agent Information Analysis,"Scott Friedman, Jeff Rye, David LaVergne, Dan Thomsen, Matthew Allen,
  Kyle Tunis",2020,Artificial Intelligence,
"  Floor space optimization is a critical revenue management problem commonly
encountered by retailers. It maximizes store revenue by optimally allocating
floor space to product categories which are assigned to their most appropriate
planograms. We formulate the problem as a connected multi-choice knapsack
problem with an additional global constraint and propose a tabu search based
meta-heuristic that exploits the multiple special neighborhood structures. We
also incorporate a mechanism to determine how to combine the multiple
neighborhood moves. A candidate list strategy based on learning from prior
search history is also employed to improve the search quality. The results of
computational testing with a set of test problems show that our tabu search
heuristic can solve all problems within a reasonable amount of time. Analyses
of individual contributions of relevant components of the algorithm were
conducted with computational experiments.
",Maximizing Store Revenues using Tabu Search for Floor Space Optimization,Jiefeng Xu and Evren Gul and Alvin Lim,2021,Artificial Intelligence,
"  This paper studies the scheduling of jobs of different families on parallel
machines with qualification constraints. Originating from semiconductor
manufacturing, this constraint imposes a time threshold between the execution
of two jobs of the same family. Otherwise, the machine becomes disqualified for
this family. The goal is to minimize both the flow time and the number of
disqualifications. Recently, an efficient constraint programming model has been
proposed. However, when priority is given to the flow time objective, the
efficiency of the model can be improved. This paper uses a polynomial-time
algorithm which minimize the flow time for a single machine relaxation where
disqualifications are not considered. Using this algorithm one can derived
filtering rules on different variables of the model. Experimental results are
presented showing the effectiveness of these rules. They improve the
competitiveness with the mixed integer linear program of the literature.
","Filtering Rules for Flow Time Minimization in a Parallel Machine
  Scheduling Problem","Margaux Nattaf (G-SCOP), Arnaud Malapert",2020,Artificial Intelligence,
"  A new fuzzy method is developed using triangular/trapezoidal fuzzy numbers
for evaluating a group's mean performance, when qualitative grades instead of
numerical scores are used for assessing its members' individual performance.
Also, a new technique is developed for solving Linear Programming problems with
fuzzy coefficients and everyday life applications are presented to illustrate
our results.
",Assessment and Linear Programming under Fuzzy Conditions,Michael Voskoglou,2020,Artificial Intelligence,
"  Planning - the ability to analyze the structure of a problem in the large and
decompose it into interrelated subproblems - is a hallmark of human
intelligence. While deep reinforcement learning (RL) has shown great promise
for solving relatively straightforward control tasks, it remains an open
problem how to best incorporate planning into existing deep RL paradigms to
handle increasingly complex environments. One prominent framework, Model-Based
RL, learns a world model and plans using step-by-step virtual rollouts. This
type of world model quickly diverges from reality when the planning horizon
increases, thus struggling at long-horizon planning. How can we learn world
models that endow agents with the ability to do temporally extended reasoning?
In this work, we propose to learn graph-structured world models composed of
sparse, multi-step transitions. We devise a novel algorithm to learn latent
landmarks that are scattered (in terms of reachability) across the goal space
as the nodes on the graph. In this same graph, the edges are the reachability
estimates distilled from Q-functions. On a variety of high-dimensional
continuous control tasks ranging from robotic manipulation to navigation, we
demonstrate that our method, named L3P, significantly outperforms prior work,
and is oftentimes the only method capable of leveraging both the robustness of
model-free RL and generalization of graph-search algorithms. We believe our
work is an important step towards scalable planning in reinforcement learning.
",World Model as a Graph: Learning Latent Landmarks for Planning,"Lunjun Zhang, Ge Yang, Bradly C. Stadie",2021,Artificial Intelligence,
"  Large software systems tune hundreds of 'constants' to optimize their runtime
performance. These values are commonly derived through intuition, lab tests, or
A/B tests. A 'one-size-fits-all' approach is often sub-optimal as the best
value depends on runtime context. In this paper, we provide an experimental
approach to replace constants with learned contextual functions for Skype - a
widely used real-time communication (RTC) application. We present Resonance, a
system based on contextual bandits (CB). We describe experiences from three
real-world experiments: applying it to the audio, video, and transport
components in Skype. We surface a unique and practical challenge of performing
machine learning (ML) inference in large software systems written using
encapsulation principles. Finally, we open-source FeatureBroker, a library to
reduce the friction in adopting ML models in such development environments
","Resonance: Replacing Software Constants with Context-Aware Models in
  Real-time Communication","Jayant Gupchup, Ashkan Aazami, Yaran Fan, Senja Filipi, Tom Finley,
  Scott Inglis, Marcus Asteborg, Luke Caroll, Rajan Chari, Markus Cozowicz,
  Vishak Gopal, Vinod Prakash, Sasikanth Bendapudi, Jack Gerrits, Eric Lau,
  Huazhou Liu, Marco Rossi, Dima Slobodianyk, Dmitri Birjukov, Matty Cooper,
  Nilesh Javar, Dmitriy Perednya, Sriram Srinivasan, John Langford, Ross
  Cutler, Johannes Gehrke",2020,Artificial Intelligence,
"  In this paper, we outline the implementation of the TFD (Totally Ordered Fast
Downward) and the PFD (Partially ordered Fast Downward) hierarchical planners
that participated in the first HTN IPC competition in 2020. These two planners
are based on forward-chaining task decomposition coupled with a compact
grounding of actions, methods, tasks and HTN problems.
",Totally and Partially Ordered Hierarchical Planners in PDDL4J Library,"Damien Pellier, Humbert Fiorino",2020,Artificial Intelligence,
"  Having a comprehensive, high-quality dataset of road sign annotation is
critical to the success of AI-based Road Sign Recognition (RSR) systems. In
practice, annotators often face difficulties in learning road sign systems of
different countries; hence, the tasks are often time-consuming and produce poor
results. We propose a novel approach using knowledge graphs and a machine
learning algorithm - variational prototyping-encoder (VPE) - to assist human
annotators in classifying road signs effectively. Annotators can query the Road
Sign Knowledge Graph using visual attributes and receive closest matching
candidates suggested by the VPE model. The VPE model uses the candidates from
the knowledge graph and a real sign image patch as inputs. We show that our
knowledge graph approach can reduce sign search space by 98.9%. Furthermore,
with VPE, our system can propose the correct single candidate for 75% of signs
in the tested datasets, eliminating the human search effort entirely in those
cases.
","Accelerating Road Sign Ground Truth Construction with Knowledge Graph
  and Machine Learning","Ji Eun Kim, Cory Henson, Kevin Huang, Tuan A. Tran, Wan-Yi Lin",2021,Artificial Intelligence,
"  The phenomenon of self-organization has been of special interest to the
neural network community for decades. In this paper, we study a variant of the
Self-Organizing Map (SOM) that models the phenomenon of self-organization of
the particles forming a string when the string is tightened from one or both
ends. The proposed variant, called the String Tightening Self-Organizing Neural
Network (STON), can be used to solve certain practical problems, such as
computation of shortest homotopic paths, smoothing paths to avoid sharp turns,
and computation of convex hull. These problems are of considerable interest in
computational geometry, robotics path planning, AI (diagrammatic reasoning),
VLSI routing, and geographical information systems. Given a set of obstacles
and a string with two fixed terminal points in a two dimensional space, the
STON model continuously tightens the given string until the unique shortest
configuration in terms of the Euclidean metric is reached. The STON minimizes
the total length of a string on convergence by dynamically creating and
selecting feature vectors in a competitive manner. Proof of correctness of this
anytime algorithm and experimental results obtained by its deployment are
presented in the paper.
","String Tightening as a Self-Organizing Phenomenon: Computation of
  Shortest Homotopic Path, Smooth Path, and Convex Hull",Bonny Banerjee,2007,Artificial Intelligence,
"  With the popularity of the Internet, traditional offline resource allocation
has evolved into a new form, called online resource allocation. It features the
online arrivals of agents in the system and the real-time decision-making
requirement upon the arrival of each online agent. Both offline and online
resource allocation have wide applications in various real-world matching
markets ranging from ridesharing to crowdsourcing. There are some emerging
applications such as rebalancing in bike sharing and trip-vehicle dispatching
in ridesharing, which involve a two-stage resource allocation process. The
process consists of an offline phase and another sequential online phase, and
both phases compete for the same set of resources. In this paper, we propose a
unified model which incorporates both offline and online resource allocation
into a single framework. Our model assumes non-uniform and known arrival
distributions for online agents in the second online phase, which can be
learned from historical data. We propose a parameterized linear programming
(LP)-based algorithm, which is shown to be at most a constant factor of $1/4$
from the optimal. Experimental results on the real dataset show that our
LP-based approaches outperform the LP-agnostic heuristics in terms of
robustness and effectiveness.
","A Unified Model for the Two-stage Offline-then-Online Resource
  Allocation","Yifan Xu, Pan Xu, Jianping Pan and Jun Tao",2020,Artificial Intelligence,
"  Several scientific studies have reported the existence of the income gap
among rideshare drivers based on demographic factors such as gender, age, race,
etc. In this paper, we study the income inequality among rideshare drivers due
to discriminative cancellations from riders, and the tradeoff between the
income inequality (called fairness objective) with the system efficiency
(called profit objective). We proposed an online bipartite-matching model where
riders are assumed to arrive sequentially following a distribution known in
advance. The highlight of our model is the concept of acceptance rate between
any pair of driver-rider types, where types are defined based on demographic
factors. Specially, we assume each rider can accept or cancel the driver
assigned to her, each occurs with a certain probability which reflects the
acceptance degree from the rider type towards the driver type. We construct a
bi-objective linear program as a valid benchmark and propose two LP-based
parameterized online algorithms. Rigorous online competitive ratio analysis is
offered to demonstrate the flexibility and efficiency of our online algorithms
in balancing the two conflicting goals, promotions of fairness and profit.
Experimental results on a real-world dataset are provided as well, which
confirm our theoretical predictions.
","Trading the System Efficiency for the Income Equality of Drivers in
  Rideshare",Yifan Xu and Pan Xu,2020,Artificial Intelligence,
"  Ranking vertices of multidimensional networks is crucial in many areas of
research, including selecting and determining the importance of decisions. Some
decisions are significantly more important than others, and their weight
categorization is also imortant. This paper defines a completely new method for
determining the weight decisions using artificial intelligence for importance
ranking of three-dimensional network vertices, improving the existing Ordered
Statistics Vertex Extraction and Tracking Algorithm (OSVETA) based on
modulation of quantized indices (QIM) and error correction codes. The technique
we propose in this paper offers significant improvements the efficiency of
determination the importance of network vertices in relation to statistical
OSVETA criteria, replacing heuristic methods with methods of precise prediction
of modern neural networks. The new artificial intelligence technique enables a
significantly better definition of the 3D meshes and a better assessment of
their topological features. The new method contributions result in a greater
precision in defining stable vertices, significantly reducing the probability
of deleting mesh vertices.
",Artificial Intelligence ordered 3D vertex importance,"Iva Vasic, Bata Vasic, and Zorica Nikolic",2020,Artificial Intelligence,
"  Previous neural solvers of math word problems (MWPs) are learned with full
supervision and fail to generate diverse solutions. In this paper, we address
this issue by introducing a \textit{weakly-supervised} paradigm for learning
MWPs. Our method only requires the annotations of the final answers and can
generate various solutions for a single problem. To boost weakly-supervised
learning, we propose a novel \textit{learning-by-fixing} (LBF) framework, which
corrects the misperceptions of the neural network via symbolic reasoning.
Specifically, for an incorrect solution tree generated by the neural network,
the \textit{fixing} mechanism propagates the error from the root node to the
leaf nodes and infers the most probable fix that can be executed to get the
desired answer. To generate more diverse solutions, \textit{tree
regularization} is applied to guide the efficient shrinkage and exploration of
the solution space, and a \textit{memory buffer} is designed to track and save
the discovered various fixes for each problem. Experimental results on the
Math23K dataset show the proposed LBF framework significantly outperforms
reinforcement learning baselines in weakly-supervised learning. Furthermore, it
achieves comparable top-1 and much better top-3/5 answer accuracies than
fully-supervised methods, demonstrating its strength in producing diverse
solutions.
",Learning by Fixing: Solving Math Word Problems with Weak Supervision,"Yining Hong, Qing Li, Daniel Ciao, Siyuan Huang, Song-Chun Zhu",2021,Artificial Intelligence,
"  The paper relies on the clinical data of a previously published study. We
identify two very questionable assumptions of said work, namely confusing
evidence of absence and absence of evidence, and neglecting the ordinal nature
of attributes' domains. We then show that using an adequate ordinal methodology
such as the dominance-based rough sets approach (DRSA) can significantly
improve the predictive accuracy of the expert system, resulting in almost
complete accuracy for a dataset of 100 instances. Beyond the performance of
DRSA in solving the diagnosis problem at hand, these results suggest the
inadequacy and triviality of the underlying dataset. We provide links to open
data from the UCI machine learning repository to allow for an easy
verification/refutation of the claims made in this paper.
",Predicting Seminal Quality with the Dominance-Based Rough Sets Approach,Nassim Dehouche,2020,Artificial Intelligence,
"  Textile manufacturing is a typical traditional industry involving high
complexity in interconnected processes with limited capacity on the application
of modern technologies. Decision-making in this domain generally takes multiple
criteria into consideration, which usually arouses more complexity. To address
this issue, the present paper proposes a decision support system that combines
the intelligent data-based random forest (RF) models and a human knowledge
based analytical hierarchical process (AHP) multi-criteria structure in
accordance to the objective and the subjective factors of the textile
manufacturing process. More importantly, the textile manufacturing process is
described as the Markov decision process (MDP) paradigm, and a deep
reinforcement learning scheme, the Deep Q-networks (DQN), is employed to
optimize it. The effectiveness of this system has been validated in a case
study of optimizing a textile ozonation process, showing that it can better
master the challenging decision-making tasks in textile manufacturing
processes.
","A Deep Reinforcement Learning Based Multi-Criteria Decision Support
  System for Textile Manufacturing Process Optimization","Zhenglei He (GEMTEX), Kim Phuc Tran (GEMTEX), Sebastien Thomassey
  (GEMTEX), Xianyi Zeng (GEMTEX), Jie Xu, Chang Haiyi",2020,Artificial Intelligence,
"  Simulations, along with other similar applications like virtual worlds and
video games, require computational models of intelligence that generate
realistic and credible behavior for the participating synthetic characters.
Cognitive architectures, which are models of the fixed structure underlying
intelligent behavior in both natural and artificial systems, provide a
conceptually valid common basis, as evidenced by the current efforts towards a
standard model of the mind, to generate human-like intelligent behavior for
these synthetic characters. Sigma is a cognitive architecture and system that
strives to combine what has been learned from four decades of independent work
on symbolic cognitive architectures, probabilistic graphical models, and more
recently neural models, under its graphical architecture hypothesis. Sigma
leverages an extended form of factor graphs towards a uniform grand unification
of not only traditional cognitive capabilities but also key non-cognitive
aspects, creating unique opportunities for the construction of new kinds of
cognitive models that possess a Theory-of-Mind and that are perceptual,
autonomous, interactive, affective, and adaptive. In this paper, we will
introduce Sigma along with its diverse capabilities and then use three distinct
proof-of-concept Sigma models to highlight combinations of these capabilities:
(1) Distributional reinforcement learning models in; (2) A pair of adaptive and
interactive agent models that demonstrate rule-based, probabilistic, and social
reasoning; and (3) A knowledge-free exploration model in which an agent
leverages only architectural appraisal variables, namely attention and
curiosity, to locate an item while building up a map in a Unity environment.
","Controlling Synthetic Characters in Simulations: A Case for Cognitive
  Architectures and Sigma","Volkan Ustun, Paul S. Rosenbloom, Seyed Sajjadi, Jeremy Nuttal",2018,Artificial Intelligence,
"  The Coronavirus (COVID-19) pandemic has led to a rapidly growing 'infodemic'
of health information online. This has motivated the need for accurate semantic
search and retrieval of reliable COVID-19 information across millions of
documents, in multiple languages. To address this challenge, this paper
proposes a novel high precision and high recall neural Multistage BiCross
encoder approach. It is a sequential three-stage ranking pipeline which uses
the Okapi BM25 retrieval algorithm and transformer-based bi-encoder and
cross-encoder to effectively rank the documents with respect to the given
query. We present experimental results from our participation in the
Multilingual Information Access (MLIA) shared task on COVID-19 multilingual
semantic search. The independently evaluated MLIA results validate our approach
and demonstrate that it outperforms other state-of-the-art approaches according
to nearly all evaluation metrics in cases of both monolingual and bilingual
runs.
","Multistage BiCross encoder for multilingual access to COVID-19 health
  information","Iknoor Singh, Carolina Scarton, Kalina Bontcheva",2021,Artificial Intelligence,
"  We present DEGARI (Dynamic Emotion Generator And ReclassIfier), an
explainable system for emotion attribution and recommendation. This system
relies on a recently introduced commonsense reasoning framework, the TCL logic,
which is based on a human-like procedure for the automatic generation of novel
concepts in a Description Logics knowledge base. Starting from an ontological
formalization of emotions based on the Plutchik model, known as ArsEmotica, the
system exploits the logic TCL to automatically generate novel commonsense
semantic representations of compound emotions (e.g. Love as derived from the
combination of Joy and Trust according to Plutchik). The generated emotions
correspond to prototypes, i.e. commonsense representations of given concepts,
and have been used to reclassify emotion-related contents in a variety of
artistic domains, ranging from art datasets to the editorial contents available
in RaiPlay, the online platform of RAI Radiotelevisione Italiana (the Italian
public broadcasting company). We show how the reported results (evaluated in
the light of the obtained reclassifications, the user ratings assigned to such
reclassifications, and their explainability) are encouraging, and pave the way
to many further research directions.
","A Commonsense Reasoning Framework for Explanatory Emotion Attribution,
  Generation and Re-classification","Antonio Lieto, Gian Luca Pozzato, Stefano Zoia, Viviana Patti, Rossana
  Damiano",2021,Artificial Intelligence,
"  Commonsense knowledge is essential for many AI applications, including those
in natural language processing, visual processing, and planning. Consequently,
many sources that include commonsense knowledge have been designed and
constructed over the past decades. Recently, the focus has been on large
text-based sources, which facilitate easier integration with neural (language)
models and application to textual tasks, typically at the expense of the
semantics of the sources and their harmonization. Efforts to consolidate
commonsense knowledge have yielded partial success, with no clear path towards
a comprehensive solution. We aim to organize these sources around a common set
of dimensions of commonsense knowledge. We survey a wide range of popular
commonsense sources with a special focus on their relations. We consolidate
these relations into 13 knowledge dimensions. This consolidation allows us to
unify the separate sources and to compute indications of their coverage,
overlap, and gaps with respect to the knowledge dimensions. Moreover, we
analyze the impact of each dimension on downstream reasoning tasks that require
commonsense knowledge, observing that the temporal and desire/goal dimensions
are very beneficial for reasoning on current downstream tasks, while
distinctness and lexical knowledge have little impact. These results reveal
preferences for some dimensions in current evaluation, and potential neglect of
others.
",Dimensions of Commonsense Knowledge,"Filip Ilievski, Alessandro Oltramari, Kaixin Ma, Bin Zhang, Deborah L.
  McGuinness, Pedro Szekely",2021,Artificial Intelligence,
"  With the powerful learning ability of deep convolutional networks, deep
clustering methods can extract the most discriminative information from
individual data and produce more satisfactory clustering results. However,
existing deep clustering methods usually ignore the relationship between the
data. Fortunately, the graph convolutional network can handle such
relationship, opening up a new research direction for deep clustering. In this
paper, we propose a cross-attention based deep clustering framework, named
Cross-Attention Fusion based Enhanced Graph Convolutional Network (CaEGCN),
which contains four main modules: the cross-attention fusion module which
innovatively concatenates the Content Auto-encoder module (CAE) relating to the
individual data and Graph Convolutional Auto-encoder module (GAE) relating to
the relationship between the data in a layer-by-layer manner, and the
self-supervised model that highlights the discriminative information for
clustering tasks. While the cross-attention fusion module fuses two kinds of
heterogeneous representation, the CAE module supplements the content
information for the GAE module, which avoids the over-smoothing problem of GCN.
In the GAE module, two novel loss functions are proposed that reconstruct the
content and relationship between the data, respectively. Finally, the
self-supervised module constrains the distributions of the middle layer
representations of CAE and GAE to be consistent. Experimental results on
different types of datasets prove the superiority and robustness of the
proposed CaEGCN.
","CaEGCN: Cross-Attention Fusion based Enhanced Graph Convolutional
  Network for Clustering","Guangyu Huo, Yong Zhang, Junbin Gao, Boyue Wang, Yongli Hu, and Baocai
  Yin",2021,Artificial Intelligence,
"  Recent advances have shown how decision trees are apt data structures for
concisely representing strategies (or controllers) satisfying various
objectives. Moreover, they also make the strategy more explainable. The recent
tool dtControl had provided pipelines with tools supporting strategy synthesis
for hybrid systems, such as SCOTS and Uppaal Stratego. We present dtControl
2.0, a new version with several fundamentally novel features. Most importantly,
the user can now provide domain knowledge to be exploited in the decision tree
learning process and can also interactively steer the process based on the
dynamically provided information. To this end, we also provide a graphical user
interface. It allows for inspection and re-computation of parts of the result,
suggesting as well as receiving advice on predicates, and visual simulation of
the decision-making process. Besides, we interface model checkers of
probabilistic systems, namely Storm and PRISM and provide dedicated support for
categorical enumeration-type state variables. Consequently, the controllers are
more explainable and smaller.
","dtControl 2.0: Explainable Strategy Representation via Decision Tree
  Learning Steered by Experts","Pranav Ashok, Mathias Jackermeier, Jan K\v{r}et\'insk\'y, Christoph
  Weinhuber, Maximilian Weininger, Mayank Yadav",2021,Artificial Intelligence,
"  For some scientific questions, empirical data are essential to develop
reliable simulation models. These data usually come from different sources with
diverse and heterogeneous formats. The design of complex data-driven models is
often shaped by the structure of the data available in research projects.
Hence, applying such models to other case studies requires either to get
similar data or to transform new data to fit the model inputs. It is the case
of agent-based models (ABMs) that use advanced data structures such as
Geographic Information Systems data. We faced this problem in the LittoSIM-GEN
project when generalizing our participatory flooding model (LittoSIM) to new
territories. From this experience, we provide a mapping approach to structure,
describe, and automatize the integration of geospatial data into ABMs.
","Mapping and Describing Geospatial Data to Generalize Complex Mapping and
  Describing Geospatial Data to Generalize Complex Models: The Case of
  LittoSIM-GEN Models","Ahmed Laatabi, Nicolas Becu (LIENSs), Nicolas Marilleau (UMMISCO),
  C\'ecilia Pignon-Mussaud (LIENSs), Marion Amalric (CITERES), X. Bertin
  (LIENSs), Brice Anselme (PRODIG), Elise Beck (PACTE)",2020,Artificial Intelligence,
"  In 2011, Hibbard suggested an intelligence measure for agents who compete in
an adversarial sequence prediction game. We argue that Hibbard's idea should
actually be considered as two separate ideas: first, that the intelligence of
such agents can be measured based on the growth rates of the runtimes of the
competitors that they defeat; and second, one specific (somewhat arbitrary)
method for measuring said growth rates. Whereas Hibbard's intelligence measure
is based on the latter growth-rate-measuring method, we survey other methods
for measuring function growth rates, and exhibit the resulting Hibbard-like
intelligence measures and taxonomies. Of particular interest, we obtain
intelligence taxonomies based on Big-O and Big-Theta notation systems, which
taxonomies are novel in that they challenge conventional notions of what an
intelligence measure should look like. We discuss how intelligence measurement
of sequence predictors can indirectly serve as intelligence measurement for
agents with Artificial General Intelligence (AGIs).
","Measuring Intelligence and Growth Rate: Variations on Hibbard's
  Intelligence Measure","Samuel Alexander, Bill Hibbard",2021,Artificial Intelligence,
"  Experimental studies are prevalent in Evolutionary Computation (EC), and
concerns about the reproducibility and replicability of such studies have
increased in recent times, reflecting similar concerns in other scientific
fields. In this article, we discuss, within the context of EC, the different
types of reproducibility and suggest a classification that refines the badge
system of the Association of Computing Machinery (ACM) adopted by ACM
Transactions on Evolutionary Learning and Optimization
(https://dlnext.acm.org/journal/telo). We identify cultural and technical
obstacles to reproducibility in the EC field. Finally, we provide guidelines
and suggest tools that may help to overcome some of these reproducibility
obstacles.
",Reproducibility in Evolutionary Computation,"Manuel L\'opez-Ib\'a\~nez (University of M\'alaga, Spain), Juergen
  Branke (University of Warwick, UK), Lu\'is Paquete (University of Coimbra,
  Portugal)",2021,Artificial Intelligence,
"  Real-world knowledge graphs are often characterized by low-frequency
relations - a challenge that has prompted an increasing interest in few-shot
link prediction methods. These methods perform link prediction for a set of new
relations, unseen during training, given only a few example facts of each
relation at test time. In this work, we perform a systematic study on a
spectrum of models derived by generalizing the current state of the art for
few-shot link prediction, with the goal of probing the limits of learning in
this few-shot setting. We find that a simple zero-shot baseline - which ignores
any relation-specific information - achieves surprisingly strong performance.
Moreover, experiments on carefully crafted synthetic datasets show that having
only a few examples of a relation fundamentally limits models from using
fine-grained structural information and only allows for exploiting the
coarse-grained positional information of entities. Together, our findings
challenge the implicit assumptions and inductive biases of prior work and
highlight new directions for research in this area.
",Exploring the Limits of Few-Shot Link Prediction in Knowledge Graphs,"Dora Jambor, Komal Teru, Joelle Pineau, William L. Hamilton",2021,Artificial Intelligence,
"  AI systems often rely on two key components: a specified goal or reward
function and an optimization algorithm to compute the optimal behavior for that
goal. This approach is intended to provide value for a principal: the user on
whose behalf the agent acts. The objectives given to these agents often refer
to a partial specification of the principal's goals. We consider the cost of
this incompleteness by analyzing a model of a principal and an agent in a
resource constrained world where the $L$ attributes of the state correspond to
different sources of utility for the principal. We assume that the reward
function given to the agent only has support on $J < L$ attributes. The
contributions of our paper are as follows: 1) we propose a novel model of an
incomplete principal-agent problem from artificial intelligence; 2) we provide
necessary and sufficient conditions under which indefinitely optimizing for any
incomplete proxy objective leads to arbitrarily low overall utility; and 3) we
show how modifying the setup to allow reward functions that reference the full
state or allowing the principal to update the proxy objective over time can
lead to higher utility solutions. The results in this paper argue that we
should view the design of reward functions as an interactive and dynamic
process and identifies a theoretical scenario where some degree of
interactivity is desirable.
",Consequences of Misaligned AI,"Simon Zhuang, Dylan Hadfield-Menell",2020,Artificial Intelligence,
"  Understanding the principles of real-world biological multi-agent behaviors
is a current challenge in various scientific and engineering fields. The rules
regarding the real-world biological multi-agent behaviors such as team sports
are often largely unknown due to their inherently higher-order interactions,
cognition, and body dynamics. Estimation of the rules from data, i.e.,
data-driven approaches such as machine learning, provides an effective way for
the analysis of such behaviors. Although most data-driven models have
non-linear structures and high prediction performances, it is sometimes hard to
interpret them. This survey focuses on data-driven analysis for quantitative
understanding of invasion team sports behaviors such as basketball and
football, and introduces two main approaches for understanding such multi-agent
behaviors: (1) extracting easily interpretable features or rules from data and
(2) generating and controlling behaviors in visually-understandable ways. The
first approach involves the visualization of learned representations and the
extraction of mathematical structures behind the behaviors. The second approach
can be used to test hypotheses by simulating and controlling future and
counterfactual behaviors. Lastly, the potential practical applications of
extracted rules, features, and generated behaviors are discussed. These
approaches can contribute to a better understanding of multi-agent behaviors in
the real world.
",Data-driven Analysis for Understanding Team Sports Behaviors,Keisuke Fujii,2021,Artificial Intelligence,
"  Accurate forecasting of medical service requirements is an important big data
problem that is crucial for resource management in critical times such as
natural disasters and pandemics. With the global spread of coronavirus disease
2019 (COVID-19), several concerns have been raised regarding the ability of
medical systems to handle sudden changes in the daily routines of healthcare
providers. One significant problem is the management of ambulance dispatch and
control during a pandemic. To help address this problem, we first analyze
ambulance dispatch data records from April 2014 to August 2020 for Nagoya City,
Japan. Significant changes were observed in the data during the pandemic,
including the state of emergency (SoE) declared across Japan. In this study, we
propose a deep learning framework based on recurrent neural networks to
estimate the number of emergency ambulance dispatches (EADs) during a SoE. The
fusion of data includes environmental factors, the localization data of mobile
phone users, and the past history of EADs, thereby providing a general
framework for knowledge discovery and better resource management. The results
indicate that the proposed blend of training data can be used efficiently in a
real-world estimation of EAD requirements during periods of high uncertainties
such as pandemics.
","Knowledge discovery from emergency ambulance dispatch during COVID-19: A
  case study of Nagoya City, Japan","Essam A. Rashed, Sachiko Kodera, Hidenobu Shirakami, Ryotetsu
  Kawaguchi, Kazuhiro Watanabe, Akimasa Hirata",2021,Artificial Intelligence,
"  A key challenge for reinforcement learning is solving long-horizon planning
problems. Recent work has leveraged programs to guide reinforcement learning in
these settings. However, these approaches impose a high manual burden on the
user since they must provide a guiding program for every new task. Partially
observed environments further complicate the programming task because the
program must implement a strategy that correctly, and ideally optimally,
handles every possible configuration of the hidden regions of the environment.
We propose a new approach, model predictive program synthesis (MPPS), that uses
program synthesis to automatically generate the guiding programs. It trains a
generative model to predict the unobserved portions of the world, and then
synthesizes a program based on samples from this model in a way that is robust
to its uncertainty. In our experiments, we show that our approach significantly
outperforms non-program-guided approaches on a set of challenging benchmarks,
including a 2D Minecraft-inspired environment where the agent must complete a
complex sequence of subtasks to achieve its goal, and achieves a similar
performance as using handcrafted programs to guide the agent. Our results
demonstrate that our approach can obtain the benefits of program-guided
reinforcement learning without requiring the user to provide a new guiding
program for every new task.
","Program Synthesis Guided Reinforcement Learning for Partially Observed
  Environments","Yichen David Yang, Jeevana Priya Inala, Osbert Bastani, Yewen Pu,
  Armando Solar-Lezama, Martin Rinard",2021,Artificial Intelligence,
"  Multiplayer Online Battle Area (MOBA) games are a recent huge success both in
the video game industry and the international eSports scene. These games
encourage team coordination and cooperation, short and long-term planning,
within a real-time combined action and strategy gameplay.
  Artificial Intelligence and Computational Intelligence in Games research
competitions offer a wide variety of challenges regarding the study and
application of AI techniques to different game genres. These events are widely
accepted by the AI/CI community as a sort of AI benchmarking that strongly
influences many other research areas in the field.
  This paper presents and describes in detail the Dota 2 Bot competition and
the Dota 2 AI framework that supports it. This challenge aims to join both,
MOBAs and AI/CI game competitions, inviting participants to submit AI
controllers for the successful MOBA \textit{Defense of the Ancients 2} (Dota 2)
to play in 1v1 matches, which aims for fostering research on AI techniques for
real-time games. The Dota 2 AI framework makes use of the actual Dota 2 game
modding capabilities to enable to connect external AI controllers to actual
Dota 2 game matches using the original Free-to-Play game.se of the actual Dota
2 game modding capabilities to enable to connect external AI controllers to
actual Dota 2 game matches using the original Free-to-Play game.
",The Dota 2 Bot Competition,Jose M. Font and Tobias Mahlmann,2018,Artificial Intelligence,
"  Deep deterministic policy gradient (DDPG)-based car-following strategy can
break through the constraints of the differential equation model due to the
ability of exploration on complex environments. However, the car-following
performance of DDPG is usually degraded by unreasonable reward function design,
insufficient training, and low sampling efficiency. In order to solve this kind
of problem, a hybrid car-following strategy based on DDPG and cooperative
adaptive cruise control (CACC) is proposed. First, the car-following process is
modeled as the Markov decision process to calculate CACC and DDPG
simultaneously at each frame. Given a current state, two actions are obtained
from CACC and DDPG, respectively. Then, an optimal action, corresponding to the
one offering a larger reward, is chosen as the output of the hybrid strategy.
Meanwhile, a rule is designed to ensure that the change rate of acceleration is
smaller than the desired value. Therefore, the proposed strategy not only
guarantees the basic performance of car-following through CACC but also makes
full use of the advantages of exploration on complex environments via DDPG.
Finally, simulation results show that the car-following performance of the
proposed strategy is improved compared with that of DDPG and CACC.
","Hybrid Car-Following Strategy based on Deep Deterministic Policy
  Gradient and Cooperative Adaptive Cruise Control","Ruidong Yan, Rui Jiang, Bin Jia, Jin Huang, and Diange Yang",2021,Artificial Intelligence,
"  Self-driving Autonomous Vehicles (SAVs) are gaining more interest each
passing day by the industry as well as the general public. Tech and automobile
companies are investing huge amounts of capital in research and development of
SAVs to make sure they have a head start in the SAV market in the future. One
of the major hurdles in the way of SAVs making it to the public roads is the
lack of confidence of public in the safety aspect of SAVs. In order to assure
safety and provide confidence to the public in the safety of SAVs, researchers
around the world have used coverage-based testing for Verification and
Validation (V&V) and safety assurance of SAVs. The objective of this paper is
to investigate the coverage criteria proposed and coverage maximizing
techniques used by researchers in the last decade up till now, to assure safety
of SAVs. We conduct a Systematic Literature Review (SLR) for this investigation
in our paper. We present a classification of existing research based on the
coverage criteria used. Several research gaps and research directions are also
provided in this SLR to enable further research in this domain. This paper
provides a body of knowledge in the domain of safety assurance of SAVs. We
believe the results of this SLR will be helpful in the progression of V&V and
safety assurance of SAVs.
","Coverage based testing for V&V and Safety Assurance of Self-driving
  Autonomous Vehicles: A Systematic Literature Review","Zaid Tahir, Rob Alexander",2020,Artificial Intelligence,
"  Vehicle trajectory prediction tasks have been commonly tackled from two
distinct perspectives: either with knowledge-driven methods or more recently
with data-driven ones. On the one hand, we can explicitly implement
domain-knowledge or physical priors such as anticipating that vehicles will
follow the middle of the roads. While this perspective leads to feasible
outputs, it has limited performance due to the difficulty to hand-craft complex
interactions in urban environments. On the other hand, recent works use
data-driven approaches which can learn complex interactions from the data
leading to superior performance. However, generalization, \textit{i.e.}, having
accurate predictions on unseen data, is an issue leading to unrealistic
outputs. In this paper, we propose to learn a ""Realistic Residual Block"" (RRB),
which effectively connects these two perspectives. Our RRB takes any
off-the-shelf knowledge-driven model and finds the required residuals to add to
the knowledge-aware trajectory. Our proposed method outputs realistic
predictions by confining the residual range and taking into account its
uncertainty. We also constrain our output with Model Predictive Control (MPC)
to satisfy kinematic constraints. Using a publicly available dataset, we show
that our method outperforms previous works in terms of accuracy and
generalization to new scenes. We will release our code and data split here:
https://github.com/vita-epfl/RRB.
",Injecting Knowledge in Data-driven Vehicle Trajectory Predictors,"Mohammadhossein Bahari, Ismail Nejjar, Alexandre Alahi",2021,Artificial Intelligence,
"  In this short paper, we outline nine classical benchmarks submitted to the
first hierarchical planning track of the International Planning competition in
2020. All of these benchmarks are based on the HDDL language. The choice of the
benchmarks was based on a questionnaire sent to the HTN community. They are the
following: Barman, Childsnack, Rover, Satellite, Blocksworld, Depots, Gripper,
and Hiking. In the rest of the paper we give a short description of these
benchmarks. All are totally ordered.
","From Classical to Hierarchical: benchmarks for the HTN Track of the
  International Planning Competition","Damien Pellier, Humbert Fiorino",2020,Artificial Intelligence,
"  High capacity end-to-end approaches for human motion (behavior) prediction
have the ability to represent subtle nuances in human behavior, but struggle
with robustness to out of distribution inputs and tail events. Planning-based
prediction, on the other hand, can reliably output decent-but-not-great
predictions: it is much more stable in the face of distribution shift (as we
verify in this work), but it has high inductive bias, missing important aspects
that drive human decisions, and ignoring cognitive biases that make human
behavior suboptimal. In this work, we analyze one family of approaches that
strive to get the best of both worlds: use the end-to-end predictor on common
cases, but do not rely on it for tail events / out-of-distribution inputs --
switch to the planning-based predictor there. We contribute an analysis of
different approaches for detecting when to make this switch, using an
autonomous driving domain. We find that promising approaches based on
ensembling or generative modeling of the training distribution might not be
reliable, but that there very simple methods which can perform surprisingly
well -- including training a classifier to pick up on tell-tale issues in
predicted trajectories.
",On complementing end-to-end human behavior predictors with planning,"Liting Sun, Xiaogang Jia, Anca D. Dragan",2021,Artificial Intelligence,
"  This paper outlines a perspective on the future of AI, discussing directions
for machines models of human-like intelligence. We explain how developmental
and evolutionary theories of human cognition should further inform artificial
intelligence. We emphasize the role of ecological niches in sculpting
intelligent behavior, and in particular that human intelligence was
fundamentally shaped to adapt to a constantly changing socio-cultural
environment. We argue that a major limit of current work in AI is that it is
missing this perspective, both theoretically and experimentally. Finally, we
discuss the promising approach of developmental artificial intelligence,
modeling infant development through multi-scale interaction between
intrinsically motivated learning, embodiment and a fastly changing
socio-cultural environment. This paper takes the form of an interview of
Pierre-Yves Oudeyer by Mandred Eppe, organized within the context of a KI -
K{\""{u}}nstliche Intelligenz special issue in developmental robotics.
","Intelligent behavior depends on the ecological niche: Scaling up AI to
  human-like intelligence in socio-cultural environments",Manfred Eppe and Pierre-Yves Oudeyer,2021,Artificial Intelligence,
"  The reporting and the analysis of current events around the globe has
expanded from professional, editor-lead journalism all the way to citizen
journalism. Nowadays, politicians and other key players enjoy direct access to
their audiences through social media, bypassing the filters of official cables
or traditional media. However, the multiple advantages of free speech and
direct communication are dimmed by the misuse of media to spread inaccurate or
misleading claims. These phenomena have led to the modern incarnation of the
fact-checker -- a professional whose main aim is to examine claims using
available evidence and to assess their veracity. As in other text forensics
tasks, the amount of information available makes the work of the fact-checker
more difficult. With this in mind, starting from the perspective of the
professional fact-checker, we survey the available intelligent technologies
that can support the human expert in the different steps of her fact-checking
endeavor. These include identifying claims worth fact-checking, detecting
relevant previously fact-checked claims, retrieving relevant evidence to
fact-check a claim, and actually verifying a claim. In each case, we pay
attention to the challenges in future work and the potential impact on
real-world fact-checking.
",Automated Fact-Checking for Assisting Human Fact-Checkers,"Preslav Nakov, David Corney, Maram Hasanain, Firoj Alam, Tamer
  Elsayed, Alberto Barr\'on-Cede\~no, Paolo Papotti, Shaden Shaar, Giovanni Da
  San Martino",2021,Artificial Intelligence,
"  Most conversational recommendation approaches are either not explainable, or
they require external user's knowledge for explaining or their explanations
cannot be applied in real time due to computational limitations. In this work,
we present a real time category based conversational recommendation approach,
which can provide concise explanations without prior user knowledge being
required. We first perform an explainable user model in the form of preferences
over the items' categories, and then use the category preferences to recommend
items. The user model is performed by applying a BERT-based neural architecture
on the conversation. Then, we translate the user model into item recommendation
scores using a Feed Forward Network. User preferences during the conversation
in our approach are represented by category vectors which are directly
interpretable. The experimental results on the real conversational
recommendation dataset ReDial demonstrate comparable performance to the
state-of-the-art, while our approach is explainable. We also show the potential
power of our framework by involving an oracle setting of category preference
prediction.
",Category Aware Explainable Conversational Recommendation,"Nikolaos Kondylidis, Jie Zou and Evangelos Kanoulas",2021,Artificial Intelligence,
"  While artificial intelligence has been applied to control players' decisions
in board games for over half a century, little attention is given to games with
no player competition. Pandemic is an exemplar collaborative board game where
all players coordinate to overcome challenges posed by events occurring during
the game's progression. This paper proposes an artificial agent which controls
all players' actions and balances chances of winning versus risk of losing in
this highly stochastic environment. The agent applies a Rolling Horizon
Evolutionary Algorithm on an abstraction of the game-state that lowers the
branching factor and simulates the game's stochasticity. Results show that the
proposed algorithm can find winning strategies more consistently in different
games of varying difficulty. The impact of a number of state evaluation metrics
is explored, balancing between optimistic strategies that favor winning and
pessimistic strategies that guard against losing.
",Collaborative Agent Gameplay in the Pandemic Board Game,Konstantinos Sfikas and Antonios Liapis,2020,Artificial Intelligence,
"  We discuss how over the last 30 to 50 years, Artificial Intelligence (AI)
systems that focused only on data have been handicapped, and how knowledge has
been critical in developing smarter, intelligent, and more effective systems.
In fact, the vast progress in AI can be viewed in terms of the three waves of
AI as identified by DARPA. During the first wave, handcrafted knowledge has
been at the center-piece, while during the second wave, the data-driven
approaches supplanted knowledge. Now we see a strong role and resurgence of
knowledge fueling major breakthroughs in the third wave of AI underpinning
future intelligent systems as they attempt human-like decision making, and seek
to become trusted assistants and companions for humans. We find a wider
availability of knowledge created from diverse sources, using manual to
automated means both by repurposing as well as by extraction. Using knowledge
with statistical learning is becoming increasingly indispensable to help make
AI systems more transparent and auditable. We will draw a parallel with the
role of knowledge and experience in human intelligence based on cognitive
science, and discuss emerging neuro-symbolic or hybrid AI systems in which
knowledge is the critical enabler for combining capabilities of the
data-intensive statistical AI systems with those of symbolic AI systems,
resulting in more capable AI systems that support more human-like intelligence.
",The Duality of Data and Knowledge Across the Three Waves of AI,Amit Sheth and Krishnaprasad Thirunarayan,2021,Artificial Intelligence,
"  We propose a new classifier based on Dempster-Shafer (DS) theory and a
convolutional neural network (CNN) architecture for set-valued classification.
In this classifier, called the evidential deep-learning classifier,
convolutional and pooling layers first extract high-dimensional features from
input data. The features are then converted into mass functions and aggregated
by Dempster's rule in a DS layer. Finally, an expected utility layer performs
set-valued classification based on mass functions. We propose an end-to-end
learning strategy for jointly updating the network parameters. Additionally, an
approach for selecting partial multi-class acts is proposed. Experiments on
image recognition, signal processing, and semantic-relationship classification
tasks demonstrate that the proposed combination of deep CNN, DS layer, and
expected utility layer makes it possible to improve classification accuracy and
to make cautious decisions by assigning confusing patterns to multi-class sets.
","An evidential classifier based on Dempster-Shafer theory and deep
  learning","Zheng Tong, Philippe Xu, Thierry Den{\oe}ux",2021,Artificial Intelligence,
"  This article outlines what we learned from the first year of the AI
Settlement Generation Competition in Minecraft, a competition about producing
AI programs that can generate interesting settlements in Minecraft for an
unseen map. This challenge seeks to focus research into adaptive and holistic
procedural content generation. Generating Minecraft towns and villages given
existing maps is a suitable task for this, as it requires the generated content
to be adaptive, functional, evocative and aesthetic at the same time. Here, we
present the results from the first iteration of the competition. We discuss the
evaluation methodology, present the different technical approaches by the
competitors, and outline the open problems.
",The AI Settlement Generation Challenge in Minecraft: First Year Report,"Christoph Salge, Michael Cerny Green, Rodrigo Canaan, Filip Skwarski,
  Rafael Fritsch, Adrian Brightmoore, Shaofang Ye, Changxing Cao and Julian
  Togelius",2020,Artificial Intelligence,
"  In the family of Intelligent Transportation Systems (ITS), Multimodal
Transport Systems (MMTS) have placed themselves as a mainstream transportation
mean of our time as a feasible integrative transportation process. The Global
Economy progressed with the help of transportation. The volume of goods and
distances covered have doubled in the last ten years, so there is a high demand
of an optimized transportation, fast but with low costs, saving resources but
also safe, with low or zero emissions. Thus, it is important to have an
overview of existing research in this field, to know what was already done and
what is to be studied next. The main objective is to explore a beneficent
selection of the existing research, methods and information in the field of
multimodal transportation research, to identify industry needs and gaps in
research and provide context for future research. The selective survey covers
multimodal transport design and optimization in terms of: cost, time, and
network topology. The multimodal transport theoretical aspects, context and
resources are also covering various aspects. The survey's selection includes
nowadays best methods and solvers for Intelligent Transportation Systems (ITS).
The gap between theory and real-world applications should be further solved in
order to optimize the global multimodal transportation system.
","Selective Survey: Most Efficient Models and Solvers for Integrative
  Multimodal Transport","Oliviu Matei, Erdei Rudolf, Camelia-M. Pintea",2021,Artificial Intelligence,
"  Along with the development of modern computing technology and social
sciences, both theoretical research and practical applications of social
computing have been continuously extended. In particular with the boom of
artificial intelligence (AI), social computing is significantly influenced by
AI. However, the conventional technologies of AI have drawbacks in dealing with
more complicated and dynamic problems. Such deficiency can be rectified by
hybrid human-artificial intelligence (H-AI) which integrates both human
intelligence and AI into one unity, forming a new enhanced intelligence. H-AI
in dealing with social problems shows the advantages that AI can not surpass.
This paper firstly introduces the concept of H-AI. AI is the intelligence in
the transition stage of H-AI, so the latest research progresses of AI in social
computing are reviewed. Secondly, it summarizes typical challenges faced by AI
in social computing, and makes it possible to introduce H-AI to solve these
challenges. Finally, the paper proposes a holistic framework of social
computing combining with H-AI, which consists of four layers: object layer,
base layer, analysis layer, and application layer. It represents H-AI has
significant advantages over AI in solving social problems.
",A Survey of Hybrid Human-Artificial Intelligence for Social Computing,"Wenxi Wang, Huansheng Ning, Feifei Shi, Sahraoui Dhelim, Weishan
  Zhang, Liming Chen",2021,Artificial Intelligence,
"  Recent machine-learning approaches to deterministic search and
domain-independent planning employ policy learning to speed up search.
Unfortunately, when attempting to solve a search problem by successively
applying a policy, no guarantees can be given on solution quality. The problem
of how to effectively use a learned policy within a bounded-suboptimal search
algorithm remains largely as an open question. In this paper, we propose
various ways in which such policies can be integrated into Focal Search,
assuming that the policy is a neural network classifier. Furthermore, we
provide mathematical foundations for some of the resulting algorithms. To
evaluate the resulting algorithms over a number of policies with varying
accuracy, we use synthetic policies which can be generated for a target
accuracy for problems where the search space can be held in memory. We evaluate
our focal search variants over three benchmark domains using our synthetic
approach, and on the 15-puzzle using a neural network learned using 1.5 million
examples. We observe that Discrepancy Focal Search, which we show expands the
node which maximizes an approximation of the probability that its corresponding
path is a prefix of an optimal path, obtains, in general, the best results in
terms of runtime and solution quality.
",Exploiting Learned Policies in Focal Search,"Pablo Araneda, Matias Greco, Jorge A. Baier",2021,Artificial Intelligence,
"  As of 2020, the international workshop on Procedural Content Generation
enters its second decade. The annual workshop, hosted by the international
conference on the Foundations of Digital Games, has collected a corpus of 95
papers published in its first 10 years. This paper provides an overview of the
workshop's activities and surveys the prevalent research topics emerging over
the years.
",10 Years of the PCG workshop: Past and Future Trends,Antonios Liapis,2020,Artificial Intelligence,
"  Objects are a centerpiece of the mathematical realm and our interaction with
and reasoning about it, just as they are of the physical one (if not more). And
humans' mathematical reasoning must ultimately be grounded in our general
intelligence. Yet in contemporary cognitive science and A.I., the physical and
mathematical domains are customarily explored separately, which allows for
baking in assumptions for what objects are for the system - and missing
potential connections.
  In this paper, I put the issue into its philosophical and cognitive context.
I then describe an abstract theoretical framework for learning object
representations, that makes room for mathematical objects on par with
non-mathematical ones. Finally, I describe a case study that builds on that
view to show how our general ability for integrating different aspects of
objects effects our conception of the natural numbers.
",The Role of General Intelligence in Mathematical Reasoning,Aviv Keren,2021,Artificial Intelligence,
"  Recently, it has been proposed that fruitful synergies may exist between Deep
Learning (DL) and Case Based Reasoning (CBR); that there are insights to be
gained by applying CBR ideas to problems in DL (what could be called DeepCBR).
In this paper, we report on a program of research that applies CBR solutions to
the problem of Explainable AI (XAI) in the DL. We describe a series of
twin-systems pairings of opaque DL models with transparent CBR models that
allow the latter to explain the former using factual, counterfactual and
semi-factual explanation strategies. This twinning shows that functional
abstractions of DL (e.g., feature weights, feature importance and decision
boundaries) can be used to drive these explanatory solutions. We also raise the
prospect that this research also applies to the problem of Data Augmentation in
DL, underscoring the fecundity of these DeepCBR ideas.
","Twin Systems for DeepCBR: A Menagerie of Deep Learning and Case-Based
  Reasoning Pairings for Explanation and Data Augmentation","Mark T Keane and Eoin M Kenny and Mohammed Temraz and Derek Greene and
  Barry Smyth",2021,Artificial Intelligence,
"  AI has the potential to revolutionize many areas of healthcare. Radiology,
dermatology, and ophthalmology are some of the areas most likely to be impacted
in the near future, and they have received significant attention from the
broader research community. But AI techniques are now also starting to be used
in in vitro fertilization (IVF), in particular for selecting which embryos to
transfer to the woman. The contribution of AI to IVF is potentially
significant, but must be done carefully and transparently, as the ethical
issues are significant, in part because this field involves creating new
people. We first give a brief introduction to IVF and review the use of AI for
embryo selection. We discuss concerns with the interpretation of the reported
results from scientific and practical perspectives. We then consider the
broader ethical issues involved. We discuss in detail the problems that result
from the use of black-box methods in this context and advocate strongly for the
use of interpretable models. Importantly, there have been no published trials
of clinical effectiveness, a problem in both the AI and IVF communities, and we
therefore argue that clinical implementation at this point would be premature.
Finally, we discuss ways for the broader AI community to become involved to
ensure scientifically sound and ethically responsible development of AI in IVF.
","Ethical Implementation of Artificial Intelligence to Select Embryos in
  In Vitro Fertilization","Michael Anis Mihdi Afnan, Cynthia Rudin, Vincent Conitzer, Julian
  Savulescu, Abhishek Mishra, Yanhe Liu, Masoud Afnan",2021,Artificial Intelligence,
"  By successfully solving the problem of forecasting, the processes in the work
of various companies are optimized and savings are achieved. In this process,
the analysis of time series data is of particular importance. Since the
creation of Facebook's Prophet, and Amazon's DeepAR+ and CNN-QR forecasting
models, algorithms have attracted a great deal of attention. The paper presents
the application and comparison of the above algorithms for sales forecasting in
distribution companies. A detailed comparison of the performance of algorithms
over real data with different lengths of sales history was made. The results
show that Prophet gives better results for items with a longer history and
frequent sales, while Amazon's algorithms show superiority for items without a
long history and items that are rarely sold.
","Comparison Analysis of Facebook's Prophet, Amazon's DeepAR+ and CNN-QR
  Algorithms for Successful Real-World Sales Forecasting","Emir Zunic, Kemal Korjenic, Sead Delalic, Zlatko Subara",2021,Artificial Intelligence,
"  We present Arianna+, a framework to design networks of ontologies for
representing knowledge enabling smart homes to perform human activity
recognition online. In the network, nodes are ontologies allowing for various
data contextualisation, while edges are general-purpose computational
procedures elaborating data. Arianna+ provides a flexible interface between the
inputs and outputs of procedures and statements, which are atomic
representations of ontological knowledge. Arianna+ schedules procedures on the
basis of events by employing logic-based reasoning, i.e., by checking the
classification of certain statements in the ontologies. Each procedure involves
input and output statements that are differently contextualised in the
ontologies based on specific prior knowledge. Arianna+ allows to design
networks that encode data within multiple contexts and, as a reference
scenario, we present a modular network based on a spatial context shared among
all activities and a temporal context specialised for each activity to be
recognised. In the paper, we argue that a network of small ontologies is more
intelligible and has a reduced computational load than a single ontology
encoding the same knowledge. Arianna+ integrates in the same architecture
heterogeneous data processing techniques, which may be better suited to
different contexts. Thus, we do not propose a new algorithmic approach to
activity recognition, instead, we focus on the architectural aspects for
accommodating logic-based and data-driven activity models in a context-oriented
way. Also, we discuss how to leverage data contextualisation and reasoning for
activity recognition, and to support an iterative development process driven by
domain experts.
",Human Activity Recognition Models in Ontology Networks,"Luca Buoncompagni, Syed Yusha Kareem and Fulvio Mastrogiovanni",2021,Artificial Intelligence,
"  Learning disentangled representations of textual data is essential for many
natural language tasks such as fair classification, style transfer and sentence
generation, among others. The existent dominant approaches in the context of
text data {either rely} on training an adversary (discriminator) that aims at
making attribute values difficult to be inferred from the latent code {or rely
on minimising variational bounds of the mutual information between latent code
and the value attribute}. {However, the available methods suffer of the
impossibility to provide a fine-grained control of the degree (or force) of
disentanglement.} {In contrast to} {adversarial methods}, which are remarkably
simple, although the adversary seems to be performing perfectly well during the
training phase, after it is completed a fair amount of information about the
undesired attribute still remains. This paper introduces a novel variational
upper bound to the mutual information between an attribute and the latent code
of an encoder. Our bound aims at controlling the approximation error via the
Renyi's divergence, leading to both better disentangled representations and in
particular, a precise control of the desirable degree of disentanglement {than
state-of-the-art methods proposed for textual data}. Furthermore, it does not
suffer from the degeneracy of other losses in multi-class scenarios. We show
the superiority of this method on fair classification and on textual style
transfer tasks. Additionally, we provide new insights illustrating various
trade-offs in style transfer when attempting to learn disentangled
representations and quality of the generated sentence.
","A Novel Estimator of Mutual Information for Learning to Disentangle
  Textual Representations",Pierre Colombo and Chloe Clavel and Pablo Piantanida,2021,Artificial Intelligence,
"  The manpower scheduling problem is a critical research field in the resource
management area. Based on the existing studies on scheduling problem solutions,
this paper transforms the manpower scheduling problem into a combinational
optimization problem under multi-constraint conditions from a new perspective.
It also uses logical paradigms to build a mathematical model for problem
solution and an improved multi-dimensional evolution algorithm for solving the
model. Moreover, the constraints discussed in this paper basically cover all
the requirements of human resource coordination in modern society and are
supported by our experiment results. In the discussion part, we compare our
model with other heuristic algorithms or linear programming methods and prove
that the model proposed in this paper makes a 25.7% increase in efficiency and
a 17% increase in accuracy at most. In addition, to the numerical solution of
the manpower scheduling problem, this paper also studies the algorithm for
scheduling task list generation and the method of displaying scheduling
results. As a result, we not only provide various modifications for the basic
algorithm to solve different condition problems but also propose a new
algorithm that increases at least 28.91% in time efficiency by comparing with
different baseline models.
",An Intelligent Model for Solving Manpower Scheduling Problems,Lingyu Zhang and Tianyu Liu and Yunhai Wang,2021,Artificial Intelligence,
"  We build on abduction-based explanations for ma-chine learning and develop a
method for computing local explanations for neural network models in natural
language processing (NLP). Our explanations comprise a subset of the words of
the in-put text that satisfies two key features: optimality w.r.t. a
user-defined cost function, such as the length of explanation, and robustness,
in that they ensure prediction invariance for any bounded perturbation in the
embedding space of the left out words. We present two solution algorithms,
respectively based on implicit hitting sets and maximum universal subsets,
introducing a number of algorithmic improvements to speed up convergence of
hard instances. We show how our method can be con-figured with different
perturbation sets in the em-bedded space and used to detect bias in predictions
by enforcing include/exclude constraints on biased terms, as well as to enhance
existing heuristic-based NLP explanation frameworks such as Anchors. We
evaluate our framework on three widely used sentiment analysis tasks and texts
of up to100words from SST, Twitter and IMDB datasets,demonstrating the
effectiveness of the derived explanations.
",On Guaranteed Optimal Robust Explanations for NLP Models,"Emanuele La Malfa, Agnieszka Zbrzezny, Rhiannon Michelmore, Nicola
  Paoletti and Marta Kwiatkowska",2021,Artificial Intelligence,
"  Privacy protection has recently been in the spotlight of attention to both
academia and industry. Society protects individual data privacy through complex
legal frameworks. The increasing number of applications of data science and
artificial intelligence has resulted in a higher demand for the ubiquitous
application of the data. The privacy protection of the broad
Data-Information-Knowledge-Wisdom (DIKW) landscape, the next generation of
information organization, has taken a secondary role. In this paper, we will
explore DIKW architecture through the applications of the popular swarm
intelligence and differential privacy. As differential privacy proved to be an
effective data privacy approach, we will look at it from a DIKW domain
perspective. Swarm Intelligence can effectively optimize and reduce the number
of items in DIKW used in differential privacy, thus accelerating both the
effectiveness and the efficiency of differential privacy for crossing multiple
modals of conceptual DIKW. The proposed approach is demonstrated through the
application of personalized data that is based on the open-sourse IRIS dataset.
This experiment demonstrates the efficiency of Swarm Intelligence in reducing
computing complexity.
","Swarm Differential Privacy for Purpose Driven
  Data-Information-Knowledge-Wisdom Architecture","Yingbo Li, Yucong Duan, Zakaria Maama, Haoyang Che, Anamaria-Beatrice
  Spulber, Stelios Fuentes",2021,Artificial Intelligence,
"  Order effects occur when judgments about a hypothesis's probability given a
sequence of information do not equal the probability of the same hypothesis
when the information is reversed. Different experiments have been performed in
the literature that supports evidence of order effects.
  We proposed a Bayesian update model for order effects where each question can
be thought of as a mini-experiment where the respondents reflect on their
beliefs. We showed that order effects appear, and they have a simple cognitive
explanation: the respondent's prior belief that two questions are correlated.
  The proposed Bayesian model allows us to make several predictions: (1) we
found certain conditions on the priors that limit the existence of order
effects; (2) we show that, for our model, the QQ equality is not necessarily
satisfied (due to symmetry assumptions); and (3) the proposed Bayesian model
has the advantage of possessing fewer parameters than its quantum counterpart.
",Order Effects in Bayesian Updates,Catarina Moreira and Jose Acacio de Barros,2021,Artificial Intelligence,
"  Artificial Intelligence techniques powered by deep neural nets have achieved
much success in several application domains, most significantly and notably in
the Computer Vision applications and Natural Language Processing tasks.
Surpassing human-level performance propelled the research in the applications
where different modalities amongst language, vision, sensory, text play an
important role in accurate predictions and identification. Several multimodal
fusion methods employing deep learning models are proposed in the literature.
Despite their outstanding performance, the complex, opaque and black-box nature
of the deep neural nets limits their social acceptance and usability. This has
given rise to the quest for model interpretability and explainability, more so
in the complex tasks involving multimodal AI methods. This paper extensively
reviews the present literature to present a comprehensive survey and commentary
on the explainability in multimodal deep neural nets, especially for the vision
and language tasks. Several topics on multimodal AI and its applications for
generic domains have been covered in this paper, including the significance,
datasets, fundamental building blocks of the methods and techniques,
challenges, applications, and future trends in this domain
",A Review on Explainability in Multimodal Deep Neural Nets,"Gargi Joshi, Rahee Walambe, Ketan Kotecha",2021,Artificial Intelligence,
"  In the last few years, AI continues demonstrating its positive impact on
society while sometimes with ethically questionable consequences. Building and
maintaining public trust in AI has been identified as the key to successful and
sustainable innovation. This chapter discusses the challenges related to
operationalizing ethical AI principles and presents an integrated view that
covers high-level ethical AI principles, the general notion of
trust/trustworthiness, and product/process support in the context of
responsible AI, which helps improve both trust and trustworthiness of AI for a
wider set of stakeholders.
",AI and Ethics -- Operationalising Responsible AI,"Liming Zhu, Xiwei Xu, Qinghua Lu, Guido Governatori, Jon Whittle",2021,Artificial Intelligence,
"  Traffic simulators act as an essential component in the operating and
planning of transportation systems. Conventional traffic simulators usually
employ a calibrated physical car-following model to describe vehicles'
behaviors and their interactions with traffic environment. However, there is no
universal physical model that can accurately predict the pattern of vehicle's
behaviors in different situations. A fixed physical model tends to be less
effective in a complicated environment given the non-stationary nature of
traffic dynamics. In this paper, we formulate traffic simulation as an inverse
reinforcement learning problem, and propose a parameter sharing adversarial
inverse reinforcement learning model for dynamics-robust simulation learning.
Our proposed model is able to imitate a vehicle's trajectories in the real
world while simultaneously recovering the reward function that reveals the
vehicle's true objective which is invariant to different dynamics. Extensive
experiments on synthetic and real-world datasets show the superior performance
of our approach compared to state-of-the-art methods and its robustness to
variant dynamics of traffic.
",Objective-aware Traffic Simulation via Inverse Reinforcement Learning,"Guanjie Zheng, Hanyang Liu, Kai Xu, Zhenhui Li",2021,Artificial Intelligence,
"  A key challenge on the path to developing agents that learn complex
human-like behavior is the need to quickly and accurately quantify
human-likeness. While human assessments of such behavior can be highly
accurate, speed and scalability are limited. We address these limitations
through a novel automated Navigation Turing Test (ANTT) that learns to predict
human judgments of human-likeness. We demonstrate the effectiveness of our
automated NTT on a navigation task in a complex 3D environment. We investigate
six classification models to shed light on the types of architectures best
suited to this task, and validate them against data collected through a human
NTT. Our best models achieve high accuracy when distinguishing true human and
agent behavior. At the same time, we show that predicting finer-grained human
assessment of agents' progress towards human-like behavior remains unsolved.
Our work takes an important step towards agents that more effectively learn
complex human-like behavior.
",Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation,"Sam Devlin, Raluca Georgescu, Ida Momennejad, Jaroslaw Rzepecki,
  Evelyn Zuniga, Gavin Costello, Guy Leroy, Ali Shaw and Katja Hofmann",2021,Artificial Intelligence,
"  Drafting, i.e., the selection of a subset of items from a larger candidate
set, is a key element of many games and related problems. It encompasses team
formation in sports or e-sports, as well as deck selection in many modern card
games. The key difficulty of drafting is that it is typically not sufficient to
simply evaluate each item in a vacuum and to select the best items. The
evaluation of an item depends on the context of the set of items that were
already selected earlier, as the value of a set is not just the sum of the
values of its members - it must include a notion of how well items go together.
  In this paper, we study drafting in the context of the card game Magic: The
Gathering. We propose the use of a contextual preference network, which learns
to compare two possible extensions of a given deck of cards. We demonstrate
that the resulting network is better able to evaluate card decks in this game
than previous attempts.
","Predicting Human Card Selection in Magic: The Gathering with Contextual
  Preference Ranking","Timo Bertram, Johannes F\""urnkranz, Martin M\""uller",2021,Artificial Intelligence,
"  This paper discusses a new variant of the Henry Gas Solubility Optimization
(HGSO) Algorithm, called Hybrid HGSO (HHGSO). Unlike its predecessor, HHGSO
allows multiple clusters serving different individual meta-heuristic algorithms
(i.e., with its own defined parameters and local best) to coexist within the
same population. Exploiting the dynamic cluster-to-algorithm mapping via
penalized and reward model with adaptive switching factor, HHGSO offers a novel
approach for meta-heuristic hybridization consisting of Jaya Algorithm, Sooty
Tern Optimization Algorithm, Butterfly Optimization Algorithm, and Owl Search
Algorithm, respectively. The acquired results from the selected two case
studies (i.e., involving team formation problem and combinatorial test suite
generation) indicate that the hybridization has notably improved the
performance of HGSO and gives superior performance against other competing
meta-heuristic and hyper-heuristic algorithms.
","Hybrid Henry Gas Solubility Optimization Algorithm with Dynamic
  Cluster-to-Algorithm Mapping for Search-based Software Engineering Problems","Kamal Z. Zamli, Md. Abdul Kader, Saiful Azad, Bestoun S. Ahmed",2021,Artificial Intelligence,
"  Motivated by the abundance of uncertain event data from multiple sources
including physical devices and sensors, this paper presents the task of
relating a stochastic process observation to a process model that can be
rendered from a dataset. In contrast to previous research that suggested to
transform a stochastically known event log into a less informative uncertain
log with upper and lower bounds on activity frequencies, we consider the
challenge of accommodating the probabilistic knowledge into conformance
checking techniques. Based on a taxonomy that captures the spectrum of
conformance checking cases under stochastic process observations, we present
three types of challenging cases. The first includes conformance checking of a
stochastically known log with respect to a given process model. The second case
extends the first to classify a stochastically known log into one of several
process models. The third case extends the two previous ones into settings in
which process models are only stochastically known. The suggested problem
captures the increasingly growing number of applications in which sensors
provide probabilistic process information.
","Uncertain Process Data with Probabilistic Knowledge: Problem
  Characterization and Challenges",Izack Cohen and Avigdor Gal,2021,Artificial Intelligence,
"  Biological agents have meaningful interactions with their environment despite
the absence of immediate reward signals. In such instances, the agent can learn
preferred modes of behaviour that lead to predictable states -- necessary for
survival. In this paper, we pursue the notion that this learnt behaviour can be
a consequence of reward-free preference learning that ensures an appropriate
trade-off between exploration and preference satisfaction. For this, we
introduce a model-based Bayesian agent equipped with a preference learning
mechanism (pepper) using conjugate priors. These conjugate priors are used to
augment the expected free energy planner for learning preferences over states
(or outcomes) across time. Importantly, our approach enables the agent to learn
preferences that encourage adaptive behaviour at test time. We illustrate this
in the OpenAI Gym FrozenLake and the 3D mini-world environments -- with and
without volatility. Given a constant environment, these agents learn confident
(i.e., precise) preferences and act to satisfy them. Conversely, in a volatile
setting, perpetual preference uncertainty maintains exploratory behaviour. Our
experiments suggest that learnable (reward-free) preferences entail a trade-off
between exploration and preference satisfaction. Pepper offers a
straightforward framework suitable for designing adaptive agents when reward
functions cannot be predefined as in real environments.
","Exploration and preference satisfaction trade-off in reward-free
  learning","Noor Sajid, Panagiotis Tigas, Alexey Zakharov, Zafeirios Fountas and
  Karl Friston",2021,Artificial Intelligence,
"  We consider a problem wherein jobs arrive at random times and assume random
values. Upon each job arrival, the decision-maker must decide immediately
whether or not to accept the job and gain the value on offer as a reward, with
the constraint that they may only accept at most $n$ jobs over some reference
time period. The decision-maker only has access to $M$ independent realisations
of the job arrival process. We propose an algorithm, Non-Parametric Sequential
Allocation (NPSA), for solving this problem. Moreover, we prove that the
expected reward returned by the NPSA algorithm converges in probability to
optimality as $M$ grows large. We demonstrate the effectiveness of the
algorithm empirically on synthetic data and on public fraud-detection datasets,
from where the motivation for this work is derived.
","Non-Parametric Stochastic Sequential Assignment With Random Arrival
  Times","Danial Dervovic, Parisa Hassanzadeh, Samuel Assefa, Prashant Reddy",2021,Artificial Intelligence,
"  Planning is hard. The use of subgoals can make planning more tractable, but
selecting these subgoals is computationally costly. What algorithms might
enable us to reap the benefits of planning using subgoals while minimizing the
computational overhead of selecting them? We propose visual scoping, a strategy
that interleaves planning and acting by alternately defining a spatial region
as the next subgoal and selecting actions to achieve it. We evaluated our
visual scoping algorithm on a variety of physical assembly problems against two
baselines: planning all subgoals in advance and planning without subgoals. We
found that visual scoping achieves comparable task performance to the subgoal
planner while requiring only a fraction of the total computational cost.
Together, these results contribute to our understanding of how humans might
make efficient use of cognitive resources to solve complex planning problems.
",Visual scoping operations for physical assembly,"Felix J Binder, Marcelo M Mattar, David Kirsh, Judith E Fan",2021,Artificial Intelligence,
"  Language is an interface to the outside world. In order for embodied agents
to use it, language must be grounded in other, sensorimotor modalities. While
there is an extended literature studying how machines can learn grounded
language, the topic of how to learn spatio-temporal linguistic concepts is
still largely uncharted. To make progress in this direction, we here introduce
a novel spatio-temporal language grounding task where the goal is to learn the
meaning of spatio-temporal descriptions of behavioral traces of an embodied
agent. This is achieved by training a truth function that predicts if a
description matches a given history of observations. The descriptions involve
time-extended predicates in past and present tense as well as spatio-temporal
references to objects in the scene. To study the role of architectural biases
in this task, we train several models including multimodal Transformer
architectures; the latter implement different attention computations between
words and objects across space and time. We test models on two classes of
generalization: 1) generalization to randomly held-out sentences; 2)
generalization to grammar primitives. We observe that maintaining object
identity in the attention computation of our Transformers is instrumental to
achieving good performance on generalization overall, and that summarizing
object traces in a single token has little influence on performance. We then
discuss how this opens new perspectives for language-guided autonomous embodied
agents. We also release our code under open-source license as well as
pretrained models and datasets to encourage the wider community to build upon
and extend our work in the future.
",Grounding Spatio-Temporal Language with Transformers,"Tristan Karch, Laetitia Teodorescu, Katja Hofmann, Cl\'ement
  Moulin-Frier and Pierre-Yves Oudeyer",2021,Artificial Intelligence,
"  One of the main challenges in model-based reinforcement learning (RL) is to
decide which aspects of the environment should be modeled. The
value-equivalence (VE) principle proposes a simple answer to this question: a
model should capture the aspects of the environment that are relevant for
value-based planning. Technically, VE distinguishes models based on a set of
policies and a set of functions: a model is said to be VE to the environment if
the Bellman operators it induces for the policies yield the correct result when
applied to the functions. As the number of policies and functions increase, the
set of VE models shrinks, eventually collapsing to a single point corresponding
to a perfect model. A fundamental question underlying the VE principle is thus
how to select the smallest sets of policies and functions that are sufficient
for planning. In this paper we take an important step towards answering this
question. We start by generalizing the concept of VE to order-$k$ counterparts
defined with respect to $k$ applications of the Bellman operator. This leads to
a family of VE classes that increase in size as $k \rightarrow \infty$. In the
limit, all functions become value functions, and we have a special
instantiation of VE which we call proper VE or simply PVE. Unlike VE, the PVE
class may contain multiple models even in the limit when all value functions
are used. Crucially, all these models are sufficient for planning, meaning that
they will yield an optimal policy despite the fact that they may ignore many
aspects of the environment. We construct a loss function for learning PVE
models and argue that popular algorithms such as MuZero can be understood as
minimizing an upper bound for this loss. We leverage this connection to propose
a modification to MuZero and show that it can lead to improved performance in
practice.
",Proper Value Equivalence,"Christopher Grimm, Andr\'e Barreto, Gregory Farquhar, David Silver,
  Satinder Singh",2021,Artificial Intelligence,
"  Path planning, the problem of efficiently discovering high-reward
trajectories, often requires optimizing a high-dimensional and multimodal
reward function. Popular approaches like CEM and CMA-ES greedily focus on
promising regions of the search space and may get trapped in local maxima. DOO
and VOOT balance exploration and exploitation, but use space partitioning
strategies independent of the reward function to be optimized. Recently, LaMCTS
empirically learns to partition the search space in a reward-sensitive manner
for black-box optimization. In this paper, we develop a novel formal regret
analysis for when and why such an adaptive region partitioning scheme works. We
also propose a new path planning method LaP3 which improves the function value
estimation within each sub-region, and uses a latent representation of the
search space. Empirically, LaP3 outperforms existing path planning methods in
2D navigation tasks, especially in the presence of difficult-to-escape local
optima, and shows benefits when plugged into the planning components of
model-based RL such as PETS. These gains transfer to highly multimodal
real-world tasks, where we outperform strong baselines in compiler phase
ordering by up to 39% on average across 9 tasks, and in molecular design by up
to 0.4 on properties on a 0-1 scale. Code is available at
https://github.com/yangkevin2/neurips2021-lap3.
",Learning Space Partitions for Path Planning,"Kevin Yang, Tianjun Zhang, Chris Cummins, Brandon Cui, Benoit Steiner,
  Linnan Wang, Joseph E. Gonzalez, Dan Klein, Yuandong Tian",2021,Artificial Intelligence,
"  In Silico Clinical Trials (ISTC), i.e., clinical experimental campaigns
carried out by means of computer simulations, hold the promise to decrease time
and cost for the safety and efficacy assessment of pharmacological treatments,
reduce the need for animal and human testing, and enable precision medicine. In
this paper we present methods and an algorithm that, by means of extensive
computer simulation--based experimental campaigns (ISTC) guided by intelligent
search, optimise a pharmacological treatment for an individual patient
(precision medicine). e show the effectiveness of our approach on a case study
involving a real pharmacological treatment, namely the downregulation phase of
a complex clinical protocol for assisted reproduction in humans.
","Optimal personalised treatment computation through in silico clinical
  trials on patient digital twins","Stefano Sinisi, Vadim Alimguzhin, Toni Mancini, Enrico Tronci,
  Federico Mari, Brigitte Leeners",2020,Artificial Intelligence,
"  In critical infrastructures like airports, much care has to be devoted in
protecting radio communication networks from external electromagnetic
interference. Protection of such mission-critical radio communication networks
is usually tackled by exploiting radiogoniometers: at least three suitably
deployed radiogoniometers, and a gateway gathering information from them,
permit to monitor and localise sources of electromagnetic emissions that are
not supposed to be present in the monitored area. Typically, radiogoniometers
are connected to the gateway through relay nodes. As a result, some degree of
fault-tolerance for the network of relay nodes is essential in order to offer a
reliable monitoring. On the other hand, deployment of relay nodes is typically
quite expensive. As a result, we have two conflicting requirements: minimise
costs while guaranteeing a given fault-tolerance. In this paper, we address the
problem of computing a deployment for relay nodes that minimises the relay node
network cost while at the same time guaranteeing proper working of the network
even when some of the relay nodes (up to a given maximum number) become faulty
(fault-tolerance). We show that, by means of a computation-intensive
pre-processing on a HPC infrastructure, the above optimisation problem can be
encoded as a 0/1 Linear Program, becoming suitable to be approached with
standard Artificial Intelligence reasoners like MILP, PB-SAT, and SMT/OMT
solvers. Our problem formulation enables us to present experimental results
comparing the performance of these three solving technologies on a real case
study of a relay node network deployment in areas of the Leonardo da Vinci
Airport in Rome, Italy.
","MILP, pseudo-boolean, and OMT solvers for optimal fault-tolerant
  placements of relay nodes in mission critical wireless networks","Quian Matteo Chen, Alberto Finzi, Toni Mancini, Igor Melatti, Enrico
  Tronci",2020,Artificial Intelligence,
"  Wikidata has been increasingly adopted by many communities for a wide variety
of applications, which demand high-quality knowledge to deliver successful
results. In this paper, we develop a framework to detect and analyze
low-quality statements in Wikidata by shedding light on the current practices
exercised by the community. We explore three indicators of data quality in
Wikidata, based on: 1) community consensus on the currently recorded knowledge,
assuming that statements that have been removed and not added back are
implicitly agreed to be of low quality; 2) statements that have been
deprecated; and 3) constraint violations in the data. We combine these
indicators to detect low-quality statements, revealing challenges with
duplicate entities, missing triples, violated type rules, and taxonomic
distinctions. Our findings complement ongoing efforts by the Wikidata community
to improve data quality, aiming to make it easier for users and editors to find
and correct mistakes.
",A Study of the Quality of Wikidata,"Kartik Shenoy and Filip Ilievski and Daniel Garijo and Daniel Schwabe
  and Pedro Szekely",2021,Artificial Intelligence,
"  This paper applies t-SNE, a visualisation technique familiar from Deep Neural
Network research to argumentation graphs by applying it to the output of graph
embeddings generated using several different methods. It shows that such a
visualisation approach can work for argumentation and show interesting
structural properties of argumentation graphs, opening up paths for further
research in the area.
",Visualising Argumentation Graphs with Graph Embeddings and t-SNE,"Lars Malmqvist, Tommy Yuan, Suresh Manandhar",2020,Artificial Intelligence,
"  In this paper, we study the problem of evaluating the addition of elements to
a set. This problem is difficult, because it can, in the general case, not be
reduced to unconditional preferences between the choices. Therefore, we model
preferences based on the context of the decision. We discuss and compare two
different Siamese network architectures for this task: a twin network that
compares the two sets resulting after the addition, and a triplet network that
models the contribution of each candidate to the existing set. We evaluate the
two settings on a real-world task; learning human card preferences for deck
building in the collectible card game Magic: The Gathering. We show that the
triplet approach achieves a better result than the twin network and that both
outperform previous results on this task.
","A Comparison of Contextual and Non-Contextual Preference Ranking for Set
  Addition Problems","Timo Bertram, Johannes F\""urnkranz, Martin M\""uller",2021,Artificial Intelligence,
"  A total of 34% of AI research and development projects fails or are
abandoned, according to a recent survey by Rackspace Technology of 1,870
companies. We propose a new strategic framework, aiSTROM, that empowers
managers to create a successful AI strategy based on a thorough literature
review. This provides a unique and integrated approach that guides managers and
lead developers through the various challenges in the implementation process.
In the aiSTROM framework, we start by identifying the top n potential projects
(typically 3-5). For each of those, seven areas of focus are thoroughly
analysed. These areas include creating a data strategy that takes into account
unique cross-departmental machine learning data requirements, security, and
legal requirements. aiSTROM then guides managers to think about how to put
together an interdisciplinary artificial intelligence (AI) implementation team
given the scarcity of AI talent. Once an AI team strategy has been established,
it needs to be positioned within the organization, either cross-departmental or
as a separate division. Other considerations include AI as a service (AIaas),
or outsourcing development. Looking at new technologies, we have to consider
challenges such as bias, legality of black-box-models, and keeping humans in
the loop. Next, like any project, we need value-based key performance
indicators (KPIs) to track and validate the progress. Depending on the
company's risk-strategy, a SWOT analysis (strengths, weaknesses, opportunities,
and threats) can help further classify the shortlisted projects. Finally, we
should make sure that our strategy includes continuous education of employees
to enable a culture of adoption. This unique and comprehensive framework offers
a valuable, literature supported, tool for managers and lead developers.
",aiSTROM -- A roadmap for developing a successful AI strategy,Dorien Herremans,2021,Artificial Intelligence,
"  Federated learning (FL) is a distributed model for deep learning that
integrates client-server architecture, edge computing, and real-time
intelligence. FL has the capability of revolutionizing machine learning (ML)
but lacks in the practicality of implementation due to technological
limitations, communication overhead, non-IID (independent and identically
distributed) data, and privacy concerns. Training a ML model over heterogeneous
non-IID data highly degrades the convergence rate and performance. The existing
traditional and clustered FL algorithms exhibit two main limitations, including
inefficient client training and static hyper-parameter utilization. To overcome
these limitations, we propose a novel hybrid algorithm, namely genetic
clustered FL (Genetic CFL), that clusters edge devices based on the training
hyper-parameters and genetically modifies the parameters cluster-wise. Then, we
introduce an algorithm that drastically increases the individual cluster
accuracy by integrating the density-based clustering and genetic
hyper-parameter optimization. The results are bench-marked using MNIST
handwritten digit dataset and the CIFAR-10 dataset. The proposed genetic CFL
shows significant improvements and works well with realistic cases of non-IID
and ambiguous data.
","Genetic CFL: Optimization of Hyper-Parameters in Clustered Federated
  Learning","Shaashwat Agrawal, Sagnik Sarkar, Mamoun Alazab, Praveen Kumar Reddy
  Maddikunta, Thippa Reddy Gadekallu and Quoc-Viet Pham",2021,Artificial Intelligence,
"  Planning for Autonomous Unmanned Ground Vehicles (AUGV) is still a challenge,
especially in difficult, off-road, critical situations. Automatic planning can
be used to reach mission objectives, to perform navigation or maneuvers. Most
of the time, the problem consists in finding a path from a source to a
destination, while satisfying some operational constraints. In a graph without
negative cycles, the computation of the single-pair shortest path from a start
node to an end node is solved in polynomial time. Additional constraints on the
solution path can however make the problem harder to solve. This becomes the
case when we need the path to pass through a few mandatory nodes without
requiring a specific order of visit. The complexity grows exponentially with
the number of mandatory nodes to visit. In this paper, we focus on shortest
path search with mandatory nodes on a given connected graph. We propose a
hybrid model that combines a constraint-based solver and a graph convolutional
neural network to improve search performance. Promising results are obtained on
realistic scenarios.
","Constrained Shortest Path Search with Graph Convolutional Neural
  Networks","Kevin Osanlou, Christophe Guettier, Andrei Bursuc, Tristan Cazenave,
  Eric Jacopin",2018,Artificial Intelligence,
"  Uncertain information is being taken into account in an increasing number of
application fields. In the meantime, abduction has been proved a powerful tool
for handling hypothetical reasoning and incomplete knowledge. Probabilistic
logical models are a suitable framework to handle uncertain information, and in
the last decade many probabilistic logical languages have been proposed, as
well as inference and learning systems for them. In the realm of Abductive
Logic Programming (ALP), a variety of proof procedures have been defined as
well. In this paper, we consider a richer logic language, coping with
probabilistic abduction with variables. In particular, we consider an ALP
program enriched with integrity constraints `a la IFF, possibly annotated with
a probability value. We first present the overall abductive language, and its
semantics according to the Distribution Semantics. We then introduce a proof
procedure, obtained by extending one previously presented, and prove its
soundness and completeness.
","Nonground Abductive Logic Programming with Probabilistic Integrity
  Constraints","Elena Bellodi, Marco Gavanelli, Riccardo Zese, Evelina Lamma, Fabrizio
  Riguzzi",2021,Artificial Intelligence,
"  Dealing with context dependent knowledge has led to different formalizations
of the notion of context. Among them is the Contextualized Knowledge Repository
(CKR) framework, which is rooted in description logics but links on the
reasoning side strongly to logic programs and Answer Set Programming (ASP) in
particular. The CKR framework caters for reasoning with defeasible axioms and
exceptions in contexts, which was extended to knowledge inheritance across
contexts in a coverage (specificity) hierarchy. However, the approach supports
only this single type of contextual relation and the reasoning procedures work
only for restricted hierarchies, due to non-trivial issues with model
preference under exceptions. In this paper, we overcome these limitations and
present a generalization of CKR hierarchies to multiple contextual relations,
along with their interpretation of defeasible axioms and preference. To support
reasoning, we use ASP with algebraic measures, which is a recent extension of
ASP with weighted formulas over semirings that allows one to associate
quantities with interpretations depending on the truth values of propositional
atoms. Notably, we show that for a relevant fragment of CKR hierarchies with
multiple contextual relations, query answering can be realized with the popular
asprin framework. The algebraic measures approach is more powerful and enables
e.g. reasoning with epistemic queries over CKRs, which opens interesting
perspectives for the use of quantitative ASP extensions in other applications.
","Reasoning on Multi-Relational Contextual Hierarchies via Answer Set
  Programming with Algebraic Measures","Loris Bozzato, Thomas Eiter, Rafael Kiesel",2021,Artificial Intelligence,
"  The traditional production paradigm of large batch production does not offer
flexibility towards satisfying the requirements of individual customers. A new
generation of smart factories is expected to support new multi-variety and
small-batch customized production modes. For that, Artificial Intelligence (AI)
is enabling higher value-added manufacturing by accelerating the integration of
manufacturing and information communication technologies, including computing,
communication, and control. The characteristics of a customized smart factory
are to include self-perception, operations optimization, dynamic
reconfiguration, and intelligent decision-making. The AI technologies will
allow manufacturing systems to perceive the environment, adapt to the external
needs, and extract the process knowledge, including business models, such as
intelligent production, networked collaboration, and extended service models.
  This paper focuses on the implementation of AI in customized manufacturing
(CM). The architecture of an AI-driven customized smart factory is presented.
Details of intelligent manufacturing devices, intelligent information
interaction, and construction of a flexible manufacturing line are showcased.
The state-of-the-art AI technologies of potential use in CM, i.e., machine
learning, multi-agent systems, Internet of Things, big data, and cloud-edge
computing are surveyed. The AI-enabled technologies in a customized smart
factory are validated with a case study of customized packaging. The
experimental results have demonstrated that the AI-assisted CM offers the
possibility of higher production flexibility and efficiency. Challenges and
solutions related to AI in CM are also discussed.
","Artificial Intelligence-Driven Customized Manufacturing Factory: Key
  Technologies, Applications, and Challenges","Jiafu Wan, Xiaomin Li, Hong-Ning Dai, Andrew Kusiak, Miguel
  Mart\'inez-Garc\'ia, Di Li",2021,Artificial Intelligence,
"  With the increasing demands of personalized learning, knowledge tracing has
become important which traces students' knowledge states based on their
historical practices. Factor analysis methods mainly use two kinds of factors
which are separately related to students and questions to model students'
knowledge states. These methods use the total number of attempts of students to
model students' learning progress and hardly highlight the impact of the most
recent relevant practices. Besides, current factor analysis methods ignore rich
information contained in questions. In this paper, we propose Multi-Factors
Aware Dual-Attentional model (MF-DAKT) which enriches question representations
and utilizes multiple factors to model students' learning progress based on a
dual-attentional mechanism. More specifically, we propose a novel
student-related factor which records the most recent attempts on relevant
concepts of students to highlight the impact of recent exercises. To enrich
questions representations, we use a pre-training method to incorporate two
kinds of question information including questions' relation and difficulty
level. We also add a regularization term about questions' difficulty level to
restrict pre-trained question representations to fine-tuning during the process
of predicting students' performance. Moreover, we apply a dual-attentional
mechanism to differentiate contributions of factors and factor interactions to
final prediction in different practice records. At last, we conduct experiments
on several real-world datasets and results show that MF-DAKT can outperform
existing knowledge tracing methods. We also conduct several studies to validate
the effects of each component of MF-DAKT.
",Multi-Factors Aware Dual-Attentional Knowledge Tracing,"Moyu Zhang (1), Xinning Zhu (1), Chunhong Zhang (1), Yang Ji (1), Feng
  Pan (1), Changchuan Yin (1) ((1) Beijing University of Posts and
  Telecommunications)",2021,Artificial Intelligence,
"  Recent systems applying Machine Learning (ML) to solve the Traveling Salesman
Problem (TSP) exhibit issues when they try to scale up to real case scenarios
with several hundred vertices. The use of Candidate Lists (CLs) has been
brought up to cope with the issues. The procedure allows to restrict the search
space during solution creation, consequently reducing the solver computational
burden. So far, ML were engaged to create CLs and values on the edges of these
CLs expressing ML preferences at solution insertion. Although promising, these
systems do not clearly restrict what the ML learns and does to create
solutions, bringing with them some generalization issues. Therefore, motivated
by exploratory and statistical studies, in this work we instead use a machine
learning model to confirm the addition in the solution just for high probable
edges. CLs of the high probable edge are employed as input, and the ML is in
charge of distinguishing cases where such edges are in the optimal solution
from those where they are not. . This strategy enables a better generalization
and creates an efficient balance between machine learning and searching
techniques. Our ML-Constructive heuristic is trained on small instances. Then,
it is able to produce solutions, without losing quality, to large problems as
well. We compare our results with classic constructive heuristics, showing good
performances for TSPLIB instances up to 1748 cities. Although our heuristic
exhibits an expensive constant time operation, we proved that the computational
complexity in worst-case scenario, for the solution construction after
training, is $O(n^2 \log n^2)$, being $n$ the number of vertices in the TSP
instance.
","A New Constructive Heuristic driven by Machine Learning for the
  Traveling Salesman Problem","Umberto Junior Mele, Luca Maria Gambardella and Roberto Montemanni",2021,Artificial Intelligence,
"  Agent-based systems have the capability to fuse information from many
distributed sources and create better plans faster. This feature makes
agent-based systems naturally suitable to address the challenges in Supply
Chain Management (SCM). Although agent-based supply chains systems have been
proposed since early 2000; industrial uptake of them has been lagging. The
reasons quoted include the immaturity of the technology, a lack of
interoperability with supply chain information systems, and a lack of trust in
Artificial Intelligence (AI). In this paper, we revisit the agent-based supply
chain and review the state of the art. We find that agent-based technology has
matured, and other supporting technologies that are penetrating supply chains;
are filling in gaps, leaving the concept applicable to a wider range of
functions. For example, the ubiquity of IoT technology helps agents ""sense"" the
state of affairs in a supply chain and opens up new possibilities for
automation. Digital ledgers help securely transfer data between third parties,
making agent-based information sharing possible, without the need to integrate
Enterprise Resource Planning (ERP) systems. Learning functionality in agents
enables agents to move beyond automation and towards autonomy. We note this
convergence effect through conceptualising an agent-based supply chain
framework, reviewing its components, and highlighting research challenges that
need to be addressed in moving forward.
","Will bots take over the supply chain? Revisiting Agent-based supply
  chain automation","Liming Xu, Stephen Mak and Alexandra Brintrup",2021,Artificial Intelligence,
"  Recently, a boxology (graphical language) with design patterns for hybrid AI
was proposed, combining symbolic and sub-symbolic learning and reasoning. In
this paper, we extend this boxology with actors and their interactions. The
main contributions of this paper are: 1) an extension of the taxonomy to
describe distributed hybrid AI systems with actors and interactions; and 2)
showing examples using a few design patterns relevant in multi-agent systems
and human-agent interaction.
",Modular Design Patterns for Hybrid Actors,"Andr\'e Meyer-Vitali, Wico Mulder, Maaike H.T. de Boer",2021,Artificial Intelligence,
"  While pre-trained language models (PLMs) are the go-to solution to tackle
many natural language processing problems, they are still very limited in their
ability to capture and to use common-sense knowledge. In fact, even if
information is available in the form of approximate (soft) logical rules, it is
not clear how to transfer it to a PLM in order to improve its performance for
deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how
to reason with soft Horn rules. We introduce a classification task where, given
facts and soft rules, the PLM should return a prediction with a probability for
a given hypothesis. We release the first dataset for this task, and we propose
a revised loss function that enables the PLM to learn how to predict precise
probabilities for the task. Our evaluation results show that the resulting
fine-tuned models achieve very high performance, even on logical rules that
were unseen at training. Moreover, we demonstrate that logical notions
expressed by the rules are transferred to the fine-tuned model, yielding
state-of-the-art results on external datasets.
",RuleBert: Teaching Soft Rules to Pre-trained Language Models,"Mohammed Saeed, Naser Ahmadi, Preslav Nakov, Paolo Papotti",2021,Artificial Intelligence,
"  On playing video games, different players usually have their own playstyles.
Recently, there have been great improvements for the video game AIs on the
playing strength. However, past researches for analyzing the behaviors of
players still used heuristic rules or the behavior features with the
game-environment support, thus being exhausted for the developers to define the
features of discriminating various playstyles. In this paper, we propose the
first metric for video game playstyles directly from the game observations and
actions, without any prior specification on the playstyle in the target game.
Our proposed method is built upon a novel scheme of learning discrete
representations that can map game observations into latent discrete states,
such that playstyles can be exhibited from these discrete states. Namely, we
measure the playstyle distance based on game observations aligned to the same
states. We demonstrate high playstyle accuracy of our metric in experiments on
some video game platforms, including TORCS, RGSK, and seven Atari games, and
for different agents including rule-based AI bots, learning-based AI bots, and
human players.
",An Unsupervised Video Game Playstyle Metric via State Discretization,"Chiu-Chou Lin, Wei-Chen Chiu and I-Chen Wu",2021,Artificial Intelligence,
"  Fact-checking on the Web has become the main mechanism through which we
detect the credibility of the news or information. Existing fact-checkers
verify the authenticity of the information (support or refute the claim) based
on secondary sources of information. However, existing approaches do not
consider the problem of model updates due to constantly increasing training
data due to user feedback. It is therefore important to conduct user studies to
correct models' inference biases and improve the model in a life-long learning
manner in the future according to the user feedback. In this paper, we present
FaxPlainAC, a tool that gathers user feedback on the output of explainable
fact-checking models. FaxPlainAC outputs both the model decision, i.e., whether
the input fact is true or not, along with the supporting/refuting evidence
considered by the model. Additionally, FaxPlainAC allows for accepting user
feedback both on the prediction and explanation. Developed in Python,
FaxPlainAC is designed as a modular and easily deployable tool. It can be
integrated with other downstream tasks and allowing for fact-checking human
annotation gathering and life-long learning.
","FaxPlainAC: A Fact-Checking Tool Based on EXPLAINable Models with HumAn
  Correction in the Loop","Zijian Zhang, Koustav Rudra, Avishek Anand",2021,Artificial Intelligence,
"  PDDL+ is an extension of PDDL2.1 which incorporates fully-featured autonomous
processes and allows for better modelling of mixed discrete-continuous domains.
Unlike PDDL2.1, PDDL+ lacks a logical semantics, relying instead on
state-transitional semantics enriched with hybrid automata semantics for the
continuous states. This complex semantics makes analysis and comparisons to
other action formalisms difficult. In this paper, we propose a natural
extension of Reiter's situation calculus theories inspired by hybrid automata.
The kinship between PDDL+ and hybrid automata allows us to develop a direct
mapping between PDDL+ and situation calculus, thereby supplying PDDL+ with a
logical semantics and the situation calculus with a modern way of representing
autonomous processes. We outline the potential benefits of the mapping by
suggesting a new approach to effective planning in PDDL+.
",A Logical Semantics for PDDL+,"Vitaliy Batusov, Mikhail Soutchanski",2019,Artificial Intelligence,
"  Wikidata is the largest general-interest knowledge base that is openly
available. It is collaboratively edited by thousands of volunteer editors and
has thus evolved considerably since its inception in 2012. In this paper, we
present Wikidated 1.0, a dataset of Wikidata's full revision history, which
encodes changes between Wikidata revisions as sets of deletions and additions
of RDF triples. To the best of our knowledge, it constitutes the first large
dataset of an evolving knowledge graph, a recently emerging research subject in
the Semantic Web community. We introduce the methodology for generating
Wikidated 1.0 from dumps of Wikidata, discuss its implementation and
limitations, and present statistical characteristics of the dataset.
","Wikidated 1.0: An Evolving Knowledge Graph Dataset of Wikidata's
  Revision History","Lukas Schmelzeisen, Corina Dima, Steffen Staab",2021,Artificial Intelligence,
"  Agent-based models have emerged as a promising paradigm for addressing ever
increasing complexity of information systems. In its initial days in the 1990s
when object-oriented modeling was at its peak, an agent was treated as a
special kind of ""object"" that had a persistent state and its own independent
thread of execution. Since then, agent-based models have diversified enormously
to even open new conceptual insights about the nature of systems in general.
This paper presents a perspective on the disparate ways in which our
understanding of agency, as well as computational models of agency have
evolved. Advances in hardware like GPUs, that brought neural networks back to
life, may also similarly infuse new life into agent-based models, as well as
pave the way for advancements in research on Artificial General Intelligence
(AGI).
",Paradigms of Computational Agency,Srinath Srinivasa and Jayati Deshmukh,2020,Artificial Intelligence,
"  Metaheuristic and self-organizing criticality (SOC) could contribute to
robust computation under perturbed environments. Implementing a logic gate in a
computing system in a critical state is one of the intriguing ways to study the
role of metaheuristics and SOCs. Here, we study the behavior of cellular
automaton, game of life (GL), in asynchronous updating and implement
probabilistic logic gates by using asynchronous GL. We find that asynchronous
GL shows a phase transition, that the density of the state of 1 decays with the
power law at the critical point, and that systems at the critical point have
the most computability in asynchronous GL. We implement AND and OR gates in
asynchronous GL with criticality, which shows good performance. Since tuning
perturbations play an essential role in operating logic gates, our study
reveals the interference between manipulation and perturbation in probabilistic
logic gates.
","Probabilistic Logic Gate in Asynchronous Game of Life with Critical
  Property","Yukio-Pegio Gunji, Yoshihiko Ohzawa and Terutaka Tanaka",2021,Artificial Intelligence,
"  We propose neural-symbolic integration for abstract concept explanation and
interactive learning. Neural-symbolic integration and explanation allow users
and domain-experts to learn about the data-driven decision making process of
large neural models. The models are queried using a symbolic logic language.
Interaction with the user then confirms or rejects a revision of the neural
model using logic-based constraints that can be distilled into the model
architecture. The approach is illustrated using the Logic Tensor Network
framework alongside Concept Activation Vectors and applied to a Convolutional
Neural Network.
","Neural-Symbolic Integration for Interactive Learning and Conceptual
  Grounding","Benedikt Wagner, Artur d'Avila Garcez",2021,Artificial Intelligence,
"  Automated Deduction in Geometry (ADG) is a forum to exchange ideas and views,
to present research results and progress, and to demonstrate software tools at
the intersection between geometry and automated deduction. Relevant topics
include (but are not limited to): polynomial algebra, invariant and
coordinate-free methods; probabilistic, synthetic, and logic approaches,
techniques for automated geometric reasoning from discrete mathematics,
combinatorics, and numerics; interactive theorem proving in geometry; symbolic
and numeric methods for geometric computation, geometric constraint solving,
automated generation/reasoning and manipulation with diagrams; design and
implementation of geometry software, automated theorem provers, special-purpose
tools, experimental studies; applications of ADG in mechanics, geometric
modelling, CAGD/CAD, computer vision, robotics and education.
  Traditionally, the ADG conference is held every two years. The previous
editions of ADG were held in Nanning in 2018, Strasbourg in 2016, Coimbra in
2014, Edinburgh in 2012, Munich in 2010, Shanghai in 2008, Pontevedra in 2006,
Gainesville in 2004, Hagenberg in 2002, Zurich in 2000, Beijing in 1998, and
Toulouse in 1996. The 13th edition of ADG was supposed to be held in 2020 in
Hagenberg, Austria, but due to the COVID-19 pandemic, it was postponed for
2021, and held online (still hosted by RISC Institute, Hagenberg, Austria),
September 15-17, 2021 (https://www.risc.jku.at/conferences/adg2021).
","Proceedings of the 13th International Conference on Automated Deduction
  in Geometry","Predrag Jani\v{c}i\'c, Zolt\'an Kov\'acs",2021,Artificial Intelligence,
"  This paper describes the evolution of our research from video analytics to a
global security system with focus on the video surveillance component. Indeed
video surveillance has evolved from a commodity security tool up to the most
efficient way of tracking perpetrators when terrorism hits our modern urban
centers. As number of cameras soars, one could expect the system to leverage
the huge amount of data carried through the video streams to provide fast
access to video evidences, actionable intelligence for monitoring real-time
events and enabling predictive capacities to assist operators in their
surveillance tasks. This research explores a hybrid platform for video
intelligence capture, automated data extraction, supervised Machine Learning
for intelligently assisted urban video surveillance; Extension to other
components of a global security system are discussed. Applying Knowledge
Management principles in this research helps with deep problem understanding
and facilitates the implementation of efficient information and experience
sharing decision support systems providing assistance to people on the field as
well as in operations centers. The originality of this work is also the
creation of ""common"" human-machine and machine to machine language and a
security ontology.
",Video Intelligence as a component of a Global Security system,"Dominique Verdejo, Eunika Mercier-Laurent (CRESTIC)",2019,Artificial Intelligence,
"  It is commonly acknowledged that the availability of the huge amount of
(training) data is one of the most important factors for many recent advances
in Artificial Intelligence (AI). However, datasets are often designed for
specific tasks in narrow AI sub areas and there is no unified way to manage and
access them. This not only creates unnecessary overheads when training or
deploying Machine Learning models but also limits the understanding of the
data, which is very important for data-centric AI. In this paper, we present
our vision about a unified framework for different datasets so that they can be
integrated and queried easily, e.g., using standard query languages. We
demonstrate this in our ongoing work to create a framework for datasets in
Computer Vision and show its advantages in different scenarios. Our
demonstration is available at https://vision.semkg.org.
",Fantastic Data and How to Query Them,"Trung-Kien Tran, Anh Le-Tuan, Manh Nguyen-Duc, Jicheng Yuan, Danh
  Le-Phuoc",2021,Artificial Intelligence,
"  Due to extensive spread of fake news on social and news media it became an
emerging research topic now a days that gained attention. In the news media and
social media the information is spread highspeed but without accuracy and hence
detection mechanism should be able to predict news fast enough to tackle the
dissemination of fake news. It has the potential for negative impacts on
individuals and society. Therefore, detecting fake news on social media is
important and also a technically challenging problem these days. We knew that
Machine learning is helpful for building Artificial intelligence systems based
on tacit knowledge because it can help us to solve complex problems due to real
word data. On the other side we knew that Knowledge engineering is helpful for
representing experts knowledge which people aware of that knowledge. Due to
this we proposed that integration of Machine learning and knowledge engineering
can be helpful in detection of fake news. In this paper we present what is fake
news, importance of fake news, overall impact of fake news on different areas,
different ways to detect fake news on social media, existing detections
algorithms that can help us to overcome the issue, similar application areas
and at the end we proposed combination of data driven and engineered knowledge
to combat fake news. We studied and compared three different modules text
classifiers, stance detection applications and fact checking existing
techniques that can help to detect fake news. Furthermore, we investigated the
impact of fake news on society. Experimental evaluation of publically available
datasets and our proposed fake news detection combination can serve better in
detection of fake news.
","Combining Machine Learning with Knowledge Engineering to detect Fake
  News in Social Networks-a survey","Sajjad Ahmed, Knut Hinkelmann, Flavio Corradini",2019,Artificial Intelligence,
"  We compare four different `game-spaces' in terms of their usefulness in
characterising multi-player tabletop games, with a particular interest in any
underlying change to a game's characteristics as the number of players changes.
In each case we take a 16-dimensional feature space, and reduce it to a
2-dimensional visualizable landscape.
  We find that a space obtained from optimization of parameters in Monte Carlo
Tree Search (MCTS) is the most directly interpretable to characterise our set
of games in terms of the relative importance of imperfect information,
adversarial opponents and reward sparsity. These results do not correlate with
a space defined using attributes of the game-tree.
  This dimensionality reduction does not show any general effect as the number
of players. We therefore consider the question using the original features to
classify the games into two sets; those for which the characteristics of the
game changes significantly as the number of players changes, and those for
which there is no such effect.
",Visualising Multiplayer Game Spaces,"James Goodman, Diego Perez-Liebana, Simon Lucas",2021,Artificial Intelligence,
"  Edward C. Tolman found reinforcement learning unsatisfactory for explaining
intelligence and proposed a clear distinction between learning and behavior.
Tolman's ideas on latent learning and cognitive maps eventually led to what is
now known as conceptual space, a geometric representation where concepts and
ideas can form points or shapes.Active navigation between ideas - reasoning -
can be expressed directly as purposive navigation in conceptual space.
Assimilating the theory of conceptual space from modern neuroscience, we
propose autonomous navigation as a valid approach for emulated cognition.
However, achieving autonomous navigation in high-dimensional Euclidean spaces
is not trivial in technology. In this work, we explore whether neoRL navigation
is up for the task; adopting Kaelbling's concerns for efficient robot
navigation, we test whether the neoRL approach is general across navigational
modalities, compositional across considerations of experience, and effective
when learning in multiple Euclidean dimensions. We find neoRL learning to be
more resemblant of biological learning than of RL in AI, and propose neoRL
navigation of conceptual space as a plausible new path toward emulated
cognition.
","Navigating Conceptual Space; A new take on Artificial General
  Intelligence",Per R. Leikanger,2021,Artificial Intelligence,
"  Wind energy has emerged as a highly promising source of renewable energy in
recent times. However, wind turbines regularly suffer from operational
inconsistencies, leading to significant costs and challenges in operations and
maintenance (O&M). Condition-based monitoring (CBM) and performance
assessment/analysis of turbines are vital aspects for ensuring efficient O&M
planning and cost minimisation. Data-driven decision making techniques have
witnessed rapid evolution in the wind industry for such O&M tasks during the
last decade, from applying signal processing methods in early 2010 to
artificial intelligence (AI) techniques, especially deep learning in 2020. In
this article, we utilise statistical computing to present a scientometric
review of the conceptual and thematic evolution of AI in the wind energy
sector, providing evidence-based insights into present strengths and
limitations of data-driven decision making in the wind industry. We provide a
perspective into the future and on current key challenges in data availability
and quality, lack of transparency in black box-natured AI models, and
prevailing issues in deploying models for real-time decision support, along
with possible strategies to overcome these problems. We hope that a systematic
analysis of the past, present and future of CBM and performance assessment can
encourage more organisations to adopt data-driven decision making techniques in
O&M towards making wind energy sources more reliable, contributing to the
global efforts of tackling climate change.
","Scientometric Review of Artificial Intelligence for Operations &
  Maintenance of Wind Turbines: The Past, Present and Future","Joyjit Chatterjee, Nina Dethlefs",2021,Artificial Intelligence,
"  This paper presents the power network reconfiguration algorithm HATSGA with a
""R"" modeling approach and evaluates its behavior in computing new
reconfiguration topologies for the power network in the Smart Grid context. The
modeling of the power distribution network with the language ""R"" is used to
represent the network and support the computation of distinct algorithm
configurations towards the evaluation of new reconfiguration topologies. The
HATSGA algorithm adopts a hybrid Tabu Search and Genetic Algorithm strategy and
can be configured in different ways to compute network reconfiguration
solutions. The evaluation of power loss with HATSGA uses the IEEE 14-Bus
topology as the power test scenario. The evaluation of reconfiguration
topologies with minimum power loss with HATSGA indicates that an efficient
solution can be reached with a feasible computational time. This suggests that
HATSGA can be potentially used for computing reconfiguration network topologies
and, beyond that, it can be used for autonomic self-healing management
approaches where a feasible computational time is required.
","On Evaluating Power Loss with HATSGA Algorithm for Power Network
  Reconfiguration in the Smart Grid","Flavio Galvao Calhau, Alysson Pezzutti and Joberto S. B. Martins",2017,Artificial Intelligence,
"  Global SLS-resolution and SLG-resolution are two representative mechanisms
for top-down evaluation of the well-founded semantics of general logic
programs. Global SLS-resolution is linear for query evaluation but suffers from
infinite loops and redundant computations. In contrast, SLG-resolution resolves
infinite loops and redundant computations by means of tabling, but it is not
linear. The principal disadvantage of a non-linear approach is that it cannot
be implemented using a simple, efficient stack-based memory structure nor can
it be easily extended to handle some strictly sequential operators such as cuts
in Prolog.
  In this paper, we present a linear tabling method, called SLT-resolution, for
top-down evaluation of the well-founded semantics. SLT-resolution is a
substantial extension of SLDNF-resolution with tabling. Its main features
include: (1) It resolves infinite loops and redundant computations while
preserving the linearity. (2) It is terminating, and sound and complete w.r.t.
the well-founded semantics for programs with the bounded-term-size property
with non-floundering queries. Its time complexity is comparable with
SLG-resolution and polynomial for function-free logic programs. (3) Because of
its linearity for query evaluation, SLT-resolution bridges the gap between the
well-founded semantics and standard Prolog implementation techniques. It can be
implemented by an extension to any existing Prolog abstract machines such as
WAM or ATOAM.
",SLT-Resolution for the Well-Founded Semantics,"Yi-Dong Shen, Li-Yan Yuan, Jia-Huai You",2002,Artificial Intelligence,
"  Infinite loops and redundant computations are long recognized open problems
in Prolog. Two ways have been explored to resolve these problems: loop checking
and tabling. Loop checking can cut infinite loops, but it cannot be both sound
and complete even for function-free logic programs. Tabling seems to be an
effective way to resolve infinite loops and redundant computations. However,
existing tabulated resolutions, such as OLDT-resolution, SLG- resolution, and
Tabulated SLS-resolution, are non-linear because they rely on the
solution-lookup mode in formulating tabling. The principal disadvantage of
non-linear resolutions is that they cannot be implemented using a simple
stack-based memory structure like that in Prolog. Moreover, some strictly
sequential operators such as cuts may not be handled as easily as in Prolog.
  In this paper, we propose a hybrid method to resolve infinite loops and
redundant computations. We combine the ideas of loop checking and tabling to
establish a linear tabulated resolution called TP-resolution. TP-resolution has
two distinctive features: (1) It makes linear tabulated derivations in the same
way as Prolog except that infinite loops are broken and redundant computations
are reduced. It handles cuts as effectively as Prolog. (2) It is sound and
complete for positive logic programs with the bounded-term-size property. The
underlying algorithm can be implemented by an extension to any existing Prolog
abstract machines such as WAM or ATOAM.
",Linear Tabulated Resolution Based on Prolog Control Strategy,"Yi-Dong Shen, Li-Yan Yuan, Jia-Huai You, Neng-Fa Zhou",2001,Artificial Intelligence,
"  A general notion of algebraic conditional plausibility measures is defined.
Probability measures, ranking functions, possibility measures, and (under the
appropriate definitions) sets of probability measures can all be viewed as
defining algebraic conditional plausibility measures. It is shown that
algebraic conditional plausibility measures can be represented using Bayesian
networks.
",Conditional Plausibility Measures and Bayesian Networks,Joseph Y. Halpern,2001,Artificial Intelligence,
"  Gene expression programming, a genotype/phenotype genetic algorithm (linear
and ramified), is presented here for the first time as a new technique for the
creation of computer programs. Gene expression programming uses character
linear chromosomes composed of genes structurally organized in a head and a
tail. The chromosomes function as a genome and are subjected to modification by
means of mutation, transposition, root transposition, gene transposition, gene
recombination, and one- and two-point recombination. The chromosomes encode
expression trees which are the object of selection. The creation of these
separate entities (genome and expression tree) with distinct functions allows
the algorithm to perform with high efficiency that greatly surpasses existing
adaptive techniques. The suite of problems chosen to illustrate the power and
versatility of gene expression programming includes symbolic regression,
sequence induction with and without constant creation, block stacking, cellular
automata rules for the density-classification problem, and two problems of
boolean concept learning: the 11-multiplexer and the GP rule problem.
","Gene Expression Programming: a New Adaptive Algorithm for Solving
  Problems",Candida Ferreira,2001,Artificial Intelligence,
"  We propose a new declarative planning language, called K, which is based on
principles and methods of logic programming. In this language, transitions
between states of knowledge can be described, rather than transitions between
completely described states of the world, which makes the language well-suited
for planning under incomplete knowledge. Furthermore, it enables the use of
default principles in the planning process by supporting negation as failure.
Nonetheless, K also supports the representation of transitions between states
of the world (i.e., states of complete knowledge) as a special case, which
shows that the language is very flexible. As we demonstrate on particular
examples, the use of knowledge states may allow for a natural and compact
problem representation. We then provide a thorough analysis of the
computational complexity of K, and consider different planning problems,
including standard planning and secure planning (also known as conformant
planning) problems. We show that these problems have different complexities
under various restrictions, ranging from NP to NEXPTIME in the propositional
case. Our results form the theoretical basis for the DLV^K system, which
implements the language K on top of the DLV logic programming system.
","A Logic Programming Approach to Knowledge-State Planning: Semantics and
  Complexity","Thomas Eiter, Wolfgang Faber, Nicola Leone, Gerald Pfeifer, Axel
  Polleres",2003,Artificial Intelligence,
"  Much work in computer science has adopted competitive analysis as a tool for
decision making under uncertainty. In this work we extend competitive analysis
to the context of multi-agent systems. Unlike classical competitive analysis
where the behavior of an agent's environment is taken to be arbitrary, we
consider the case where an agent's environment consists of other agents. These
agents will usually obey some (minimal) rationality constraints. This leads to
the definition of rational competitive analysis. We introduce the concept of
rational competitive analysis, and initiate the study of competitive analysis
for multi-agent systems. We also discuss the application of rational
competitive analysis to the context of bidding games, as well as to the
classical one-way trading problem.
",Rational Competitive Analysis,Moshe Tennenholtz,2001,Artificial Intelligence,
"  Tarski gave a general semantics for deductive reasoning: a formula a may be
deduced from a set A of formulas iff a holds in all models in which each of the
elements of A holds. A more liberal semantics has been considered: a formula a
may be deduced from a set A of formulas iff a holds in all of the ""preferred""
models in which all the elements of A hold. Shoham proposed that the notion of
""preferred"" models be defined by a partial ordering on the models of the
underlying language. A more general semantics is described in this paper, based
on a set of natural properties of choice functions. This semantics is here
shown to be equivalent to a semantics based on comparing the relative
""importance"" of sets of models, by what amounts to a qualitative probability
measure. The consequence operations defined by the equivalent semantics are
then characterized by a weakening of Tarski's properties in which the
monotonicity requirement is replaced by three weaker conditions. Classical
propositional connectives are characterized by natural introduction-elimination
rules in a nonmonotonic setting. Even in the nonmonotonic setting, one obtains
classical propositional logic, thus showing that monotonicity is not required
to justify classical propositional connectives.
",Nonmonotonic Logics and Semantics,Daniel Lehmann,2001,Artificial Intelligence,
"  We show how coupling of local optimization processes can lead to better
solutions than multi-start local optimization consisting of independent runs.
This is achieved by minimizing the average energy cost of the ensemble, subject
to synchronization constraints between the state vectors of the individual
local minimizers. From an augmented Lagrangian which incorporates the
synchronization constraints both as soft and hard constraints, a network is
derived wherein the local minimizers interact and exchange information through
the synchronization constraints. From the viewpoint of neural networks, the
array can be considered as a Lagrange programming network for continuous
optimization and as a cellular neural network (CNN). The penalty weights
associated with the soft state synchronization constraints follow from the
solution to a linear program. This expresses that the energy cost of the
ensemble should maximally decrease. In this way successful local minimizers can
implicitly impose their state to the others through a mechanism of master-slave
dynamics resulting into a cooperative search mechanism. Improved information
spreading within the ensemble is obtained by applying the concept of
small-world networks. This work suggests, in an interdisciplinary context, the
importance of information exchange and state synchronization within ensembles,
towards issues as evolution, collective behaviour, optimality and intelligence.
",Intelligence and Cooperative Search by Coupled Local Minimizers,"J.A.K. Suykens, J. Vandewalle, B. De Moor",2001,Artificial Intelligence,
"  This paper presents the DLV system, which is widely considered the
state-of-the-art implementation of disjunctive logic programming, and addresses
several aspects. As for problem solving, we provide a formal definition of its
kernel language, function-free disjunctive logic programs (also known as
disjunctive datalog), extended by weak constraints, which are a powerful tool
to express optimization problems. We then illustrate the usage of DLV as a tool
for knowledge representation and reasoning, describing a new declarative
programming methodology which allows one to encode complex problems (up to
$\Delta^P_3$-complete problems) in a declarative fashion. On the foundational
side, we provide a detailed analysis of the computational complexity of the
language of DLV, and by deriving new complexity results we chart a complete
picture of the complexity of this language and important fragments thereof.
  Furthermore, we illustrate the general architecture of the DLV system which
has been influenced by these results. As for applications, we overview
application front-ends which have been developed on top of DLV to solve
specific knowledge representation tasks, and we briefly describe the main
international projects investigating the potential of the system for industrial
exploitation. Finally, we report about thorough experimentation and
benchmarking, which has been carried out to assess the efficiency of the
system. The experimental results confirm the solidity of DLV and highlight its
potential for emerging application areas like knowledge management and
information integration.
",The DLV System for Knowledge Representation and Reasoning,"Nicola Leone, Gerald Pfeifer, Wolfgang Faber, Thomas Eiter, Georg
  Gottlob, Simona Perri, Francesco Scarcello",2006,Artificial Intelligence,
"  In this paper we present an application where we put together two methods for
clustering and classification into a force aggregation method. Both methods are
based on conflicts between elements. These methods work with different type of
elements (intelligence reports, vehicles, military units) on different
hierarchical levels using specific conflict assessment methods on each level.
We use Dempster-Shafer theory for conflict calculation between elements,
Dempster-Shafer clustering for clustering these elements, and templates for
classification. The result of these processes is a complete force aggregation
on all levels handled.
",Conflict-based Force Aggregation,"John Cantwell, Johan Schubert, Johan Walter",2001,Artificial Intelligence,
"  In this paper we develop methods for selection of templates and use these
templates to recluster an already performed Dempster-Shafer clustering taking
into account intelligence to template fit during the reclustering phase. By
this process the risk of erroneous force aggregation based on some misplace
pieces of evidence from the first clustering process is greatly reduced.
Finally, a more reliable force aggregation is performed using the result of the
second clustering. These steps are taken in order to maintain most of the
excellent computational performance of Dempster-Shafer clustering, while at the
same time improve on the clustering result by including some higher relations
among intelligence reports described by the templates. The new improved
algorithm has a computational complexity of O(n**3 log**2 n) compared to O(n**2
log**2 n) of standard Dempster-Shafer clustering using Potts spin mean field
theory.
","Reliable Force Aggregation Using a Refined Evidence Specification from
  Dempster-Shafer Clustering",Johan Schubert,2001,Artificial Intelligence,
"  In this paper we develop a method for clustering belief functions based on
attracting and conflicting metalevel evidence. Such clustering is done when the
belief functions concern multiple events, and all belief functions are mixed
up. The clustering process is used as the means for separating the belief
functions into subsets that should be handled independently. While the
conflicting metalevel evidence is generated internally from pairwise conflicts
of all belief functions, the attracting metalevel evidence is assumed given by
some external source.
","Clustering belief functions based on attracting and conflicting
  metalevel evidence",Johan Schubert,2002,Artificial Intelligence,
"  In this paper we develop a method for report level tracking based on
Dempster-Shafer clustering using Potts spin neural networks where clusters of
incoming reports are gradually fused into existing tracks, one cluster for each
track. Incoming reports are put into a cluster and continuous reclustering of
older reports is made in order to obtain maximum association fit within the
cluster and towards the track. Over time, the oldest reports of the cluster
leave the cluster for the fixed track at the same rate as new incoming reports
are put into it. Fusing reports to existing tracks in this fashion allows us to
take account of both existing tracks and the probable future of each track, as
represented by younger reports within the corresponding cluster. This gives us
a robust report-to-track association. Compared to clustering of all available
reports this approach is computationally faster and has a better
report-to-track association than simple step-by-step association.
",Robust Report Level Cluster-to-Track Fusion,Johan Schubert,2002,Artificial Intelligence,
"  In most real-world settings, due to limited time or other resources, an agent
cannot perform all potentially useful deliberation and information gathering
actions. This leads to the metareasoning problem of selecting such actions.
Decision-theoretic methods for metareasoning have been studied in AI, but there
are few theoretical results on the complexity of metareasoning.
  We derive hardness results for three settings which most real metareasoning
systems would have to encompass as special cases. In the first, the agent has
to decide how to allocate its deliberation time across anytime algorithms
running on different problem instances. We show this to be
$\mathcal{NP}$-complete. In the second, the agent has to (dynamically) allocate
its deliberation or information gathering resources across multiple actions
that it has to choose among. We show this to be $\mathcal{NP}$-hard even when
evaluating each individual action is extremely simple. In the third, the agent
has to (dynamically) choose a limited number of deliberation or information
gathering actions to disambiguate the state of the world. We show that this is
$\mathcal{NP}$-hard under a natural restriction, and $\mathcal{PSPACE}$-hard in
general.
",Definition and Complexity of Some Basic Metareasoning Problems,"Vincent Conitzer, Tuomas Sandholm",2003,Artificial Intelligence,
"  This article presents an overview of the idea that ""information compression
by multiple alignment, unification and search"" (ICMAUS) may serve as a unifying
principle in computing (including mathematics and logic) and in such aspects of
human cognition as the analysis and production of natural language, fuzzy
pattern recognition and best-match information retrieval, concept hierarchies
with inheritance of attributes, probabilistic reasoning, and unsupervised
inductive learning. The ICMAUS concepts are described together with an outline
of the SP61 software model in which the ICMAUS concepts are currently realised.
A range of examples is presented, illustrated with output from the SP61 model.
","Information Compression by Multiple Alignment, Unification and Search as
  a Unifying Principle in Computing and Cognition",J Gerard Wolff,2003,Artificial Intelligence,
"  In this paper we develop an evidential force aggregation method intended for
classification of evidential intelligence into recognized force structures. We
assume that the intelligence has already been partitioned into clusters and use
the classification method individually in each cluster. The classification is
based on a measure of fitness between template and fused intelligence that
makes it possible to handle intelligence reports with multiple nonspecific and
uncertain propositions. With this measure we can aggregate on a level-by-level
basis, starting from general intelligence to achieve a complete force structure
with recognized units on all hierarchical levels.
",Evidential Force Aggregation,Johan Schubert,2003,Artificial Intelligence,
"  The field of machine learning (ML) is concerned with the question of how to
construct algorithms that automatically improve with experience. In recent
years many successful ML applications have been developed, such as datamining
programs, information-filtering systems, etc. Although ML algorithms allow the
detection and extraction of interesting patterns of data for several kinds of
problems, most of these algorithms are based on quantitative reasoning, as they
rely on training data in order to infer so-called target functions.
  In the last years defeasible argumentation has proven to be a sound setting
to formalize common-sense qualitative reasoning. This approach can be combined
with other inference techniques, such as those provided by machine learning
theory.
  In this paper we outline different alternatives for combining defeasible
argumentation and machine learning techniques. We suggest how different aspects
of a generic argument-based framework can be integrated with other ML-based
approaches.
",Integrating Defeasible Argumentation and Machine Learning Techniques,Sergio Alejandro Gomez and Carlos Ivan Ches\~nevar,2003,Artificial Intelligence,
"  We introduce Ak, an extension of the action description language A (Gelfond
and Lifschitz, 1993) to handle actions which affect knowledge. We use sensing
actions to increase an agent's knowledge of the world and non-deterministic
actions to remove knowledge. We include complex plans involving conditionals
and loops in our query language for hypothetical reasoning. We also present a
translation of Ak domain descriptions into epistemic logic programs.
",Knowledge And The Action Description Language A,"Jorge Lobo, Gisela Mendez, Stuart R. Taylor",2001,Artificial Intelligence,
"  It is well known that, under certain conditions, it is possible to split
logic programs under stable model semantics, i.e. to divide such a program into
a number of different ""levels"", such that the models of the entire program can
be constructed by incrementally constructing models for each level. Similar
results exist for other non-monotonic formalisms, such as auto-epistemic logic
and default logic. In this work, we present a general, algebraicsplitting
theory for logics with a fixpoint semantics. Together with the framework of
approximation theory, a general fixpoint theory for arbitrary operators, this
gives us a uniform and powerful way of deriving splitting results for each
logic with a fixpoint semantics. We demonstrate the usefulness of these
results, by generalizing existing results for logic programming, auto-epistemic
logic and default logic.
","Splitting an operator: Algebraic modularity results for logics with
  fixpoint semantics","Joost Vennekens, David Gilis, Marc Denecker",2006,Artificial Intelligence,
"  The integration of different learning and adaptation techniques to overcome
individual limitations and to achieve synergetic effects through the
hybridization or fusion of these techniques has, in recent years, contributed
to a large number of new intelligent system designs. Computational intelligence
is an innovative framework for constructing intelligent hybrid architectures
involving Neural Networks (NN), Fuzzy Inference Systems (FIS), Probabilistic
Reasoning (PR) and derivative free optimization techniques such as Evolutionary
Computation (EC). Most of these hybridization approaches, however, follow an ad
hoc design methodology, justified by success in certain application domains.
Due to the lack of a common framework it often remains difficult to compare the
various hybrid systems conceptually and to evaluate their performance
comparatively. This chapter introduces the different generic architectures for
integrating intelligent systems. The designing aspects and perspectives of
different hybrid archirectures like NN-FIS, EC-FIS, EC-NN, FIS-PR and NN-FIS-EC
systems are presented. Some conclusions are also provided towards the end.
",Intelligent Systems: Architectures and Perspectives,Ajith Abraham,2002,Artificial Intelligence,
"  Neuro-fuzzy systems have attracted growing interest of researchers in various
scientific and engineering areas due to the increasing need of intelligent
systems. This paper evaluates the use of two popular soft computing techniques
and conventional statistical approach based on Box--Jenkins autoregressive
integrated moving average (ARIMA) model to predict electricity demand in the
State of Victoria, Australia. The soft computing methods considered are an
evolving fuzzy neural network (EFuNN) and an artificial neural network (ANN)
trained using scaled conjugate gradient algorithm (CGA) and backpropagation
(BP) algorithm. The forecast accuracy is compared with the forecasts used by
Victorian Power Exchange (VPX) and the actual energy demand. To evaluate, we
considered load demand patterns for 10 consecutive months taken every 30 min
for training the different prediction models. Test results show that the
neuro-fuzzy system performed better than neural networks, ARIMA model and the
VPX forecasts.
",A Neuro-Fuzzy Approach for Modelling Electricity Demand in Victoria,Ajith Abraham and Baikunth Nath,2001,Artificial Intelligence,
"  Fusion of Artificial Neural Networks (ANN) and Fuzzy Inference Systems (FIS)
have attracted the growing interest of researchers in various scientific and
engineering areas due to the growing need of adaptive intelligent systems to
solve the real world problems. ANN learns from scratch by adjusting the
interconnections between layers. FIS is a popular computing framework based on
the concept of fuzzy set theory, fuzzy if-then rules, and fuzzy reasoning. The
advantages of a combination of ANN and FIS are obvious. There are several
approaches to integrate ANN and FIS and very often it depends on the
application. We broadly classify the integration of ANN and FIS into three
categories namely concurrent model, cooperative model and fully fused model.
This paper starts with a discussion of the features of each model and
generalize the advantages and deficiencies of each model. We further focus the
review on the different types of fused neuro-fuzzy systems and citing the
advantages and disadvantages of each model.
",Neuro Fuzzy Systems: Sate-of-the-Art Modeling Techniques,Ajith Abraham,2001,Artificial Intelligence,
"  Long-term rainfall prediction is a challenging task especially in the modern
world where we are facing the major environmental problem of global warming. In
general, climate and rainfall are highly non-linear phenomena in nature
exhibiting what is known as the butterfly effect. While some regions of the
world are noticing a systematic decrease in annual rainfall, others notice
increases in flooding and severe storms. The global nature of this phenomenon
is very complicated and requires sophisticated computer modeling and simulation
to predict accurately. In this paper, we report a performance analysis for
Multivariate Adaptive Regression Splines (MARS)and artificial neural networks
for one month ahead prediction of rainfall. To evaluate the prediction
efficiency, we made use of 87 years of rainfall data in Kerala state, the
southern part of the Indian peninsula situated at latitude -longitude pairs
(8o29'N - 76o57' E). We used an artificial neural network trained using the
scaled conjugate gradient algorithm. The neural network and MARS were trained
with 40 years of rainfall data. For performance evaluation, network predicted
outputs were compared with the actual rainfall data. Simulation results reveal
that MARS is a good forecasting tool and performed better than the considered
neural network.
",Is Neural Network a Reliable Forecaster on Earth? A MARS Query!,Ajith Abraham & Dan Steinberg,2001,Artificial Intelligence,
"  Sorting by reversals is an important problem in inferring the evolutionary
relationship between two genomes. The problem of sorting unsigned permutation
has been proven to be NP-hard. The best guaranteed error bounded is the 3/2-
approximation algorithm. However, the problem of sorting signed permutation can
be solved easily. Fast algorithms have been developed both for finding the
sorting sequence and finding the reversal distance of signed permutation. In
this paper, we present a way to view the problem of sorting unsigned
permutation as signed permutation. And the problem can then be seen as
searching an optimal signed permutation in all n2 corresponding signed
permutations. We use genetic algorithm to conduct the search. Our experimental
result shows that the proposed method outperform the 3/2-approximation
algorithm.
",Estimating Genome Reversal Distance by Genetic Algorithm,Andy AuYeung and Ajith Abraham,2003,Artificial Intelligence,
"  Past few years have witnessed a growing recognition of intelligent techniques
for the construction of efficient and reliable intrusion detection systems. Due
to increasing incidents of cyber attacks, building effective intrusion
detection systems (IDS) are essential for protecting information systems
security, and yet it remains an elusive goal and a great challenge. In this
paper, we report a performance analysis between Multivariate Adaptive
Regression Splines (MARS), neural networks and support vector machines. The
MARS procedure builds flexible regression models by fitting separate splines to
distinct intervals of the predictor variables. A brief comparison of different
neural network learning algorithms is also given.
",Intrusion Detection Systems Using Adaptive Regression Splines,"Srinivas Mukkamala, Andrew H. Sung, Ajith Abraham and Vitorino Ramos",2004,Artificial Intelligence,
"  The use of intelligent systems for stock market predictions has been widely
established. In this paper, we investigate how the seemingly chaotic behavior
of stock markets could be well represented using several connectionist
paradigms and soft computing techniques. To demonstrate the different
techniques, we considered Nasdaq-100 index of Nasdaq Stock MarketS and the S&P
CNX NIFTY stock index. We analyzed 7 year's Nasdaq 100 main index values and 4
year's NIFTY index values. This paper investigates the development of a
reliable and efficient technique to model the seemingly chaotic behavior of
stock markets. We considered an artificial neural network trained using
Levenberg-Marquardt algorithm, Support Vector Machine (SVM), Takagi-Sugeno
neuro-fuzzy model and a Difference Boosting Neural Network (DBNN). This paper
briefly explains how the different connectionist paradigms could be formulated
using different learning methods and then investigates whether they can provide
the required level of performance, which are sufficiently good and robust so as
to provide a reliable forecast model for stock market indices. Experiment
results reveal that all the connectionist paradigms considered could represent
the stock indices behavior very accurately.
",Modeling Chaotic Behavior of Stock Indices Using Intelligent Paradigms,"Ajith Abraham, Ninan Sajith Philip and P. Saratchandran",2003,Artificial Intelligence,
"  The purpose of this paper is to point to the usefulness of applying a linear
mathematical formulation of fuzzy multiple criteria objective decision methods
in organising business activities. In this respect fuzzy parameters of linear
programming are modelled by preference-based membership functions. This paper
begins with an introduction and some related research followed by some
fundamentals of fuzzy set theory and technical concepts of fuzzy multiple
objective decision models. Further a real case study of a manufacturing plant
and the implementation of the proposed technique is presented. Empirical
results clearly show the superiority of the fuzzy technique in optimising
individual objective functions when compared to non-fuzzy approach.
Furthermore, for the problem considered, the optimal solution helps to infer
that by incorporating fuzziness in a linear programming model either in
constraints, or both in objective functions and constraints, provides a similar
(or even better) level of satisfaction for obtained results compared to
non-fuzzy linear programming.
","Hybrid Fuzzy-Linear Programming Approach for Multi Criteria Decision
  Making Problems",Sonja Petrovic-Lazarevic and Ajith Abraham,2003,Artificial Intelligence,
"  In this paper, we present MLEANN (Meta-Learning Evolutionary Artificial
Neural Network), an automatic computational framework for the adaptive
optimization of artificial neural networks wherein the neural network
architecture, activation function, connection weights; learning algorithm and
its parameters are adapted according to the problem. We explored the
performance of MLEANN and conventionally designed artificial neural networks
for function approximation problems. To evaluate the comparative performance,
we used three different well-known chaotic time series. We also present the
state of the art popular neural network learning algorithms and some
experimentation results related to convergence speed and generalization
performance. We explored the performance of backpropagation algorithm;
conjugate gradient algorithm, quasi-Newton algorithm and Levenberg-Marquardt
algorithm for the three chaotic time series. Performances of the different
learning algorithms were evaluated when the activation functions and
architecture were changed. We further present the theoretical background,
algorithm, design strategy and further demonstrate how effective and inevitable
is the proposed MLEANN framework to design a neural network, which is smaller,
faster and with a better generalization performance.
",Meta-Learning Evolutionary Artificial Neural Networks,Ajith Abraham,2004,Artificial Intelligence,
"  Decision-making is a process of choosing among alternative courses of action
for solving complicated problems where multi-criteria objectives are involved.
The past few years have witnessed a growing recognition of Soft Computing
technologies that underlie the conception, design and utilization of
intelligent systems. Several works have been done where engineers and
scientists have applied intelligent techniques and heuristics to obtain optimal
decisions from imprecise information. In this paper, we present a concurrent
fuzzy-neural network approach combining unsupervised and supervised learning
techniques to develop the Tactical Air Combat Decision Support System (TACDSS).
Experiment results clearly demonstrate the efficiency of the proposed
technique.
",A Concurrent Fuzzy-Neural Network Approach for Decision Support Systems,"Cong Tran, Ajith Abraham and Lakhmi Jain",2003,Artificial Intelligence,
"  In a universe with a single currency, there would be no foreign exchange
market, no foreign exchange rates, and no foreign exchange. Over the past
twenty-five years, the way the market has performed those tasks has changed
enormously. The need for intelligent monitoring systems has become a necessity
to keep track of the complex forex market. The vast currency market is a
foreign concept to the average individual. However, once it is broken down into
simple terms, the average individual can begin to understand the foreign
exchange market and use it as a financial instrument for future investing. In
this paper, we attempt to compare the performance of hybrid soft computing and
hard computing techniques to predict the average monthly forex rates one month
ahead. The soft computing models considered are a neural network trained by the
scaled conjugate gradient algorithm and a neuro-fuzzy model implementing a
Takagi-Sugeno fuzzy inference system. We also considered Multivariate Adaptive
Regression Splines (MARS), Classification and Regression Trees (CART) and a
hybrid CART-MARS technique. We considered the exchange rates of Australian
dollar with respect to US dollar, Singapore dollar, New Zealand dollar,
Japanese yen and United Kingdom pounds. The models were trained using 70% of
the data and remaining was used for testing and validation purposes. It is
observed that the proposed hybrid models could predict the forex rates more
accurately than all the techniques when applied individually. Empirical results
also reveal that the hybrid hard computing approach also improved some of our
previous work using a neuro-fuzzy approach.
","Analysis of Hybrid Soft and Hard Computing Techniques for Forex
  Monitoring Systems",Ajith Abraham,2002,Artificial Intelligence,
"  The rapid e-commerce growth has made both business community and customers
face a new situation. Due to intense competition on one hand and the customer's
option to choose from several alternatives business community has realized the
necessity of intelligent marketing strategies and relationship management. Web
usage mining attempts to discover useful knowledge from the secondary data
obtained from the interactions of the users with the Web. Web usage mining has
become very critical for effective Web site management, creating adaptive Web
sites, business and support services, personalization, network traffic flow
analysis and so on. In this paper, we present the important concepts of Web
usage mining and its various practical applications. We further present a novel
approach 'intelligent-miner' (i-Miner) to optimize the concurrent architecture
of a fuzzy clustering algorithm (to discover web data clusters) and a fuzzy
inference system to analyze the Web site visitor trends. A hybrid evolutionary
fuzzy clustering algorithm is proposed in this paper to optimally segregate
similar user interests. The clustered data is then used to analyze the trends
using a Takagi-Sugeno fuzzy inference system learned using a combination of
evolutionary algorithm and neural network learning. Proposed approach is
compared with self-organizing maps (to discover patterns) and several function
approximation techniques like neural networks, linear genetic programming and
Takagi-Sugeno fuzzy inference system (to analyze the clusters). The results are
graphically illustrated and the practical significance is discussed in detail.
Empirical results clearly show that the proposed Web usage-mining framework is
efficient.
",Business Intelligence from Web Usage Mining,Ajith Abraham,2003,Artificial Intelligence,
"  Normally a decision support system is build to solve problem where
multi-criteria decisions are involved. The knowledge base is the vital part of
the decision support containing the information or data that is used in
decision-making process. This is the field where engineers and scientists have
applied several intelligent techniques and heuristics to obtain optimal
decisions from imprecise information. In this paper, we present a hybrid
neuro-genetic learning approach for the adaptation a Mamdani fuzzy inference
system for the Tactical Air Combat Decision Support System (TACDSS). Some
simulation results demonstrating the difference of the learning techniques and
are also provided.
","Adaptation of Mamdani Fuzzy Inference System Using Neuro - Genetic
  Approach for Tactical Air Combat Decision Support System","Cong Tran, Lakhmi Jain, Ajith Abraham",2002,Artificial Intelligence,
"  Several adaptation techniques have been investigated to optimize fuzzy
inference systems. Neural network learning algorithms have been used to
determine the parameters of fuzzy inference system. Such models are often
called as integrated neuro-fuzzy models. In an integrated neuro-fuzzy model
there is no guarantee that the neural network learning algorithm converges and
the tuning of fuzzy inference system will be successful. Success of
evolutionary search procedures for optimization of fuzzy inference system is
well proven and established in many application areas. In this paper, we will
explore how the optimization of fuzzy inference systems could be further
improved using a meta-heuristic approach combining neural network learning and
evolutionary computation. The proposed technique could be considered as a
methodology to integrate neural networks, fuzzy inference systems and
evolutionary search procedures. We present the theoretical frameworks and some
experimental results to demonstrate the efficiency of the proposed technique.
","EvoNF: A Framework for Optimization of Fuzzy Inference Systems Using
  Neural Network Learning and Evolutionary Computation",Ajith Abraham,2002,Artificial Intelligence,
"  Evolutionary artificial neural networks (EANNs) refer to a special class of
artificial neural networks (ANNs) in which evolution is another fundamental
form of adaptation in addition to learning. Evolutionary algorithms are used to
adapt the connection weights, network architecture and learning algorithms
according to the problem environment. Even though evolutionary algorithms are
well known as efficient global search algorithms, very often they miss the best
local solutions in the complex solution space. In this paper, we propose a
hybrid meta-heuristic learning approach combining evolutionary learning and
local search methods (using 1st and 2nd order error information) to improve the
learning and faster convergence obtained using a direct evolutionary approach.
The proposed technique is tested on three different chaotic time series and the
test results are compared with some popular neuro-fuzzy systems and a recently
developed cutting angle method of global optimization. Empirical results reveal
that the proposed technique is efficient in spite of the computational
complexity.
","Optimization of Evolutionary Neural Networks Using Hybrid Learning
  Algorithms",Ajith Abraham,2002,Artificial Intelligence,
"  The framework of algorithmic knowledge assumes that agents use algorithms to
compute the facts they explicitly know. In many cases of interest, a deductive
system, rather than a particular algorithm, captures the formal reasoning used
by the agents to compute what they explicitly know. We introduce a logic for
reasoning about both implicit and explicit knowledge with the latter defined
with respect to a deductive system formalizing a logical theory for agents. The
highly structured nature of deductive systems leads to very natural
axiomatizations of the resulting logic when interpreted over any fixed
deductive system. The decision problem for the logic, in the presence of a
single agent, is NP-complete in general, no harder than propositional logic. It
remains NP-complete when we fix a deductive system that is decidable in
nondeterministic polynomial time. These results extend in a straightforward way
to multiple agents.
",Deductive Algorithmic Knowledge,Riccardo Pucella,2006,Artificial Intelligence,
"  The academic literature suggests that the extent of exporting by
multinational corporation subsidiaries (MCS) depends on their product
manufactured, resources, tax protection, customers and markets, involvement
strategy, financial independence and suppliers' relationship with a
multinational corporation (MNC). The aim of this paper is to model the complex
export pattern behaviour using a Takagi-Sugeno fuzzy inference system in order
to determine the actual volume of MCS export output (sales exported). The
proposed fuzzy inference system is optimised by using neural network learning
and evolutionary computation. Empirical results clearly show that the proposed
approach could model the export behaviour reasonable well compared to a direct
neural network approach.
",Export Behaviour Modeling Using EvoNF Approach,"Ron Edwards, Ajith Abraham and Sonja Petrovic-Lazarevic",2003,Artificial Intelligence,
"  The costs of fatalities and injuries due to traffic accident have a great
impact on society. This paper presents our research to model the severity of
injury resulting from traffic accidents using artificial neural networks and
decision trees. We have applied them to an actual data set obtained from the
National Automotive Sampling System (NASS) General Estimates System (GES).
Experiment results reveal that in all the cases the decision tree outperforms
the neural network. Our research analysis also shows that the three most
important factors in fatal injury are: driver's seat belt usage, light
condition of the roadway, and driver's alcohol usage.
",Traffic Accident Analysis Using Decision Trees and Neural Networks,"Miao M. Chong, Ajith Abraham, Marcin Paprzycki",2004,Artificial Intelligence,
"  This paper presents a comparative study of six soft computing models namely
multilayer perceptron networks, Elman recurrent neural network, radial basis
function network, Hopfield model, fuzzy inference system and hybrid fuzzy
neural network for the hourly electricity demand forecast of Czech Republic.
The soft computing models were trained and tested using the actual hourly load
data for seven years. A comparison of the proposed techniques is presented for
predicting 2 day ahead demands for electricity. Simulation results indicate
that hybrid fuzzy neural network and radial basis function networks are the
best candidates for the analysis and forecasting of electricity demand.
","Short Term Load Forecasting Models in Czech Republic Using Soft
  Computing Paradigms",Muhammad Riaz Khan and Ajith Abraham,2003,Artificial Intelligence,
"  Defeasible logic is a rule-based nonmonotonic logic, with both strict and
defeasible rules, and a priority relation on rules. We show that inference in
the propositional form of the logic can be performed in linear time. This
contrasts markedly with most other propositional nonmonotonic logics, in which
inference is intractable.
",Propositional Defeasible Logic has Linear Complexity,Michael J. Maher,2001,Artificial Intelligence,
"  We introduce a logic for reasoning about evidence that essentially views
evidence as a function from prior beliefs (before making an observation) to
posterior beliefs (after making the observation). We provide a sound and
complete axiomatization for the logic, and consider the complexity of the
decision problem. Although the reasoning in the logic is mainly propositional,
we allow variables representing numbers and quantification over them. This
expressive power seems necessary to capture important properties of evidence.
",A Logic for Reasoning about Evidence,"Joseph Y. Halpern, Riccardo Pucella",2006,Artificial Intelligence,
"  In this paper one proposes a simple algorithm of combining the fusion rules,
those rules which first use the conjunctive rule and then the transfer of
conflicting mass to the non-empty sets, in such a way that they gain the
property of associativity and fulfill the Markovian requirement for dynamic
fusion. Also, a new rule, SDL-improved, is presented.
","An Algorithm for Quasi-Associative and Quasi-Markovian Rules of
  Combination in Information Fusion","Florentin Smarandache, Jean Dezert",2011,Artificial Intelligence,
"  This paper presents in detail the generalized pignistic transformation (GPT)
succinctly developed in the Dezert-Smarandache Theory (DSmT) framework as a
tool for decision process. The GPT allows to provide a subjective probability
measure from any generalized basic belief assignment given by any corpus of
evidence. We mainly focus our presentation on the 3D case and provide the
complete result obtained by the GPT and its validation drawn from the
probability theory.
",The Generalized Pignistic Transformation,"Jean Dezert, Florentin Smarandache, Milan Daniel",2004,Artificial Intelligence,
"  The rapid e-commerce growth has made both business community and customers
face a new situation. Due to intense competition on one hand and the customer's
option to choose from several alternatives business community has realized the
necessity of intelligent marketing strategies and relationship management. Web
usage mining attempts to discover useful knowledge from the secondary data
obtained from the interactions of the users with the Web. Web usage mining has
become very critical for effective Web site management, creating adaptive Web
sites, business and support services, personalization, network traffic flow
analysis and so on. The study of ant colonies behavior and their
self-organizing capabilities is of interest to knowledge retrieval/management
and decision support systems sciences, because it provides models of
distributed adaptive organization, which are useful to solve difficult
optimization, classification, and distributed control problems, among others.
In this paper, we propose an ant clustering algorithm to discover Web usage
patterns (data clusters) and a linear genetic programming approach to analyze
the visitor trends. Empirical results clearly shows that ant colony clustering
performs well when compared to a self-organizing map (for clustering Web usage
patterns) even though the performance accuracy is not that efficient when
comparared to evolutionary-fuzzy clustering (i-miner) approach. KEYWORDS: Web
Usage Mining, Swarm Intelligence, Ant Systems, Stigmergy, Data-Mining, Linear
Genetic Programming.
","Web Usage Mining Using Artificial Ant Colony Clustering and Genetic
  Programming","Ajith Abraham, Vitorino Ramos",2003,Artificial Intelligence,
"  While being it extremely important, many Exploratory Data Analysis (EDA)
systems have the inhability to perform classification and visualization in a
continuous basis or to self-organize new data-items into the older ones
(evenmore into new labels if necessary), which can be crucial in KDD -
Knowledge Discovery, Retrieval and Data Mining Systems (interactive and online
forms of Web Applications are just one example). This disadvantge is also
present in more recent approaches using Self-Organizing Maps. On the present
work, and exploiting past sucesses in recently proposed Stigmergic Ant Systems
a robust online classifier is presented, which produces class decisions on a
continuous stream data, allowing for continuous mappings. Results show that
increasingly better results are achieved, as demonstraded by other authors in
different areas. KEYWORDS: Swarm Intelligence, Ant Systems, Stigmergy,
Data-Mining, Exploratory Data Analysis, Image Retrieval, Continuous
Classification.
",Swarms on Continuous Data,"Vitorino Ramos, Ajith Abraham",2003,Artificial Intelligence,
"  Social insect societies and more specifically ant colonies, are distributed
systems that, in spite of the simplicity of their individuals, present a highly
structured social organization. As a result of this organization, ant colonies
can accomplish complex tasks that in some cases exceed the individual
capabilities of a single ant. The study of ant colonies behavior and of their
self-organizing capabilities is of interest to knowledge retrieval/management
and decision support systems sciences, because it provides models of
distributed adaptive organization which are useful to solve difficult
optimization, classification, and distributed control problems, among others.
In the present work we overview some models derived from the observation of
real ants, emphasizing the role played by stigmergy as distributed
communication paradigm, and we present a novel strategy to tackle unsupervised
clustering as well as data retrieval problems. The present ant clustering
system (ACLUSTER) avoids not only short-term memory based strategies, as well
as the use of several artificial ant types (using different speeds), present in
some recent approaches. Moreover and according to our knowledge, this is also
the first application of ant systems into textual document clustering.
KEYWORDS: Swarm Intelligence, Ant Systems, Unsupervised Clustering, Data
Retrieval, Data Mining, Distributed Computing, Document Maps, Textual Document
Clustering.
","Self-Organized Stigmergic Document Maps: Environment as a Mechanism for
  Context Learning","Vitorino Ramos, Juan J. Merelo",2002,Artificial Intelligence,
"  Automatic marbles classification based on their visual appearance is an
important industrial issue. However, there is no definitive solution to the
problem mainly due to the presence of randomly distributed high number of
different colours and its subjective evaluation by the human expert. In this
paper we present a study of segmentation techniques, we evaluate they overall
performance using a training set and standard quality measures and finally we
apply different clustering techniques to automatically classify the marbles.
KEYWORDS: Segmentation, Clustering, Quadtrees, Learning Vector Quantization
(LVQ), Simulated Annealing (SA).
",Clustering Techniques for Marbles Classification,"J.R. Caldas-Pinto, Pedro Pina, Vitorino Ramos, Mario Ramalho",2002,Artificial Intelligence,
"  Imagine a ""machine"" where there is no pre-commitment to any particular
representational scheme: the desired behaviour is distributed and roughly
specified simultaneously among many parts, but there is minimal specification
of the mechanism required to generate that behaviour, i.e. the global behaviour
evolves from the many relations of multiple simple behaviours. A machine that
lives to and from/with Synergy. An artificial super-organism that avoids
specific constraints and emerges within multiple low-level implicit
bio-inspired mechanisms. KEYWORDS: Complex Science, ArtSBots Project, Swarm
Intelligence, Stigmergy, UnManned Art, Symbiotic Art, Swarm Paintings, Robot
Paintings, Non-Human Art, Painting Emergence and Cooperation, Art and
Complexity, ArtBots: The Robot Talent Show.
","On the Implicit and on the Artificial - Morphogenesis and Emergent
  Aesthetics in Autonomous Collective Systems",Vitorino Ramos,2002,Artificial Intelligence,
"  Synergy (from the Greek word synergos), broadly defined, refers to combined
or co-operative effects produced by two or more elements (parts or
individuals). The definition is often associated with the holistic conviction
quote that ""the whole is greater than the sum of its parts"" (Aristotle, in
Metaphysics), or the whole cannot exceed the sum of the energies invested in
each of its parts (e.g. first law of thermodynamics) even if it is more
accurate to say that the functional effects produced by wholes are different
from what the parts can produce alone. Synergy is a ubiquitous phenomena in
nature and human societies alike. One well know example is provided by the
emergence of self-organization in social insects, via direct or indirect
interactions. The latter types are more subtle and defined as stigmergy to
explain task coordination and regulation in the context of nest reconstruction
in termites. An example, could be provided by two individuals, who interact
indirectly when one of them modifies the environment and the other responds to
the new environment at a later time. In other words, stigmergy could be defined
as a particular case of environmental or spatial synergy. The system is purely
holistic, and their properties are intrinsically emergent and autocatalytic. On
the present work we present a ""machine"" where there is no precommitment to any
particular representational scheme: the desired behaviour is distributed and
roughly specified simultaneously among many parts, but there is minimal
specification of the mechanism required to generate that behaviour, i.e. the
global behaviour evolves from the many relations of multiple simple behaviours.
","The MC2 Project [Machines of Collective Conscience]: A possible walk, up
  to Life-like Complexity and Behaviour, from bottom, basic and simple
  bio-inspired heuristics - a walk, up into the morphogenesis of information",Vitorino Ramos,2001,Artificial Intelligence,
"  Neoteny, also spelled Paedomorphosis, can be defined in biological terms as
the retention by an organism of juvenile or even larval traits into later life.
In some species, all morphological development is retarded; the organism is
juvenilized but sexually mature. Such shifts of reproductive capability would
appear to have adaptive significance to organisms that exhibit it. In terms of
evolutionary theory, the process of paedomorphosis suggests that larval stages
and developmental phases of existing organisms may give rise, under certain
circumstances, to wholly new organisms. Although the present work does not
pretend to model or simulate the biological details of such a concept in any
way, these ideas were incorporated by a rather simple abstract computational
strategy, in order to allow (if possible) for faster convergence into simple
non-memetic Genetic Algorithms, i.e. without using local improvement procedures
(e.g. via Baldwin or Lamarckian learning). As a case-study, the Genetic
Algorithm was used for colour image segmentation purposes by using K-mean
unsupervised clustering methods, namely for guiding the evolutionary algorithm
in his search for finding the optimal or sub-optimal data partition. Average
results suggest that the use of neotonic strategies by employing juvenile
genotypes into the later generations and the use of linear-dynamic mutation
rates instead of constant, can increase fitness values by 58% comparing to
classical Genetic Algorithms, independently from the starting population
characteristics on the search space. KEYWORDS: Genetic Algorithms, Artificial
Neoteny, Dynamic Mutation Rates, Faster Convergence, Colour Image Segmentation,
Classification, Clustering.
","The Biological Concept of Neoteny in Evolutionary Colour Image
  Segmentation - Simple Experiments in Simple Non-Memetic Genetic Algorithms",Vitorino Ramos,2001,Artificial Intelligence,
"  This paper presents an approach to enhance search engines with information
about word senses available in WordNet. The approach exploits information about
the conceptual relations within the lexical-semantic net. In the wrapper for
search engines presented, WordNet information is used to specify user's request
or to classify the results of a publicly available web search engine, like
google, yahoo, etc.
",Clever Search: A WordNet Based Wrapper for Internet Search Engines,Peter M. Kruse and Andre Naujoks and Dietmar Roesner and Manuela Kunze,2005,Artificial Intelligence,
"  The aim of the project presented in this paper is to design a system for an
NLG architecture, which supports the documentation process of eBusiness models.
A major task is to enrich the formal description of an eBusiness model with
additional information needed in an NLG task.
",Transforming Business Rules Into Natural Language Text,Manuela Kunze and Dietmar Roesner,2005,Artificial Intelligence,
"  Lexical semantic resources, like WordNet, are often used in real applications
of natural language document processing. For example, we integrated GermaNet in
our document suite XDOC of processing of German forensic autopsy protocols. In
addition to the hypernymy and synonymy relation, we want to adapt GermaNet's
verb frames for our analysis. In this paper we outline an approach for the
domain related enrichment of GermaNet verb frames by corpus based syntactic and
co-occurred data analyses of real documents.
",Corpus based Enrichment of GermaNet Verb Frames,Manuela Kunze and Dietmar Roesner,2004,Artificial Intelligence,
"  Real applications of natural language document processing are very often
confronted with domain specific lexical gaps during the analysis of documents
of a new domain. This paper describes an approach for the derivation of domain
specific concepts for the extension of an existing ontology. As resources we
need an initial ontology and a partially processed corpus of a domain. We
exploit the specific characteristic of the sublanguage in the corpus. Our
approach is based on syntactical structures (noun phrases) and compound
analyses to extract information required for the extension of GermaNet's
lexical resources.
",Context Related Derivation of Word Senses,Manuela Kunze and Dietmar Roesner,2004,Artificial Intelligence,
"  We suggest to employ techniques from Natural Language Processing (NLP) and
Knowledge Representation (KR) to transform existing documents into documents
amenable for the Semantic Web. Semantic Web documents have at least part of
their semantics and pragmatics marked up explicitly in both a machine
processable as well as human readable manner. XML and its related standards
(XSLT, RDF, Topic Maps etc.) are the unifying platform for the tools and
methodologies developed for different application scenarios.
",Transforming and Enriching Documents for the Semantic Web,Dietmar Roesner and Manuela Kunze and Sylke Kroetzsch,2004,Artificial Intelligence,
"  We describe a polynomial network technique developed for learning to classify
clinical electroencephalograms (EEGs) presented by noisy features. Using an
evolutionary strategy implemented within Group Method of Data Handling, we
learn classification models which are comprehensively described by sets of
short-term polynomials. The polynomial models were learnt to classify the EEGs
recorded from Alzheimer and healthy patients and recognize the EEG artifacts.
Comparing the performances of our technique and some machine learning methods
we conclude that our technique can learn well-suited polynomial models which
experts can find easy-to-understand.
","Learning Polynomial Networks for Classification of Clinical
  Electroencephalograms",Vitaly Schetinin and Joachim Schult,2005,Artificial Intelligence,
"  Multiple Classifier Systems (MCSs) allow evaluation of the uncertainty of
classification outcomes that is of crucial importance for safety critical
applications. The uncertainty of classification is determined by a trade-off
between the amount of data available for training, the classifier diversity and
the required performance. The interpretability of MCSs can also give useful
information for experts responsible for making reliable classifications. For
this reason Decision Trees (DTs) seem to be attractive classification models
for experts. The required diversity of MCSs exploiting such classification
models can be achieved by using two techniques, the Bayesian model averaging
and the randomised DT ensemble. Both techniques have revealed promising results
when applied to real-world problems. In this paper we experimentally compare
the classification uncertainty of the Bayesian model averaging with a
restarting strategy and the randomised DT ensemble on a synthetic dataset and
some domain problems commonly used in the machine learning community. To make
the Bayesian DT averaging feasible, we use a Markov Chain Monte Carlo
technique. The classification uncertainty is evaluated within an Uncertainty
Envelope technique dealing with the class posterior distribution and a given
confidence probability. Exploring a full posterior distribution, this technique
produces realistic estimates which can be easily interpreted in statistical
terms. In our experiments we found out that the Bayesian DTs are superior to
the randomised DT ensembles within the Uncertainty Envelope technique.
","Comparison of the Bayesian and Randomised Decision Tree Ensembles within
  an Uncertainty Envelope Technique","Vitaly Schetinin, Jonathan E. Fieldsend, Derek Partridge, Wojtek J.
  Krzanowski, Richard M. Everson, Trevor C. Bailey, and Adolfo Hernandez",2005,Artificial Intelligence,
"  This paper presents an artificial evolutionbased method for stereo image
analysis and its application to real-time obstacle detection and avoidance for
a mobile robot. It uses the Parisian approach, which consists here in splitting
the representation of the robot's environment into a large number of simple
primitives, the ""flies"", which are evolved following a biologically inspired
scheme and give a fast, low-cost solution to the obstacle detection problem in
mobile robotics.
",Applying Evolutionary Optimisation to Robot Obstacle Avoidance,"Olivier Pauplin (INRIA Rocquencourt), Jean Louchet (INRIA
  Rocquencourt), Evelyne Lutton (INRIA Rocquencourt), Michel Parent (INRIA
  Rocquencourt)",2004,Artificial Intelligence,
"  A fuzzy controller is usually designed by formulating the knowledge of a
human expert into a set of linguistic variables and fuzzy rules. Among the most
successful methods to automate the fuzzy controllers development process are
evolutionary algorithms. In this work, we propose the Recurrent Fuzzy Voronoi
(RFV) model, a representation for recurrent fuzzy systems. It is an extension
of the FV model proposed by Kavka and Schoenauer that extends the application
domain to include temporal problems. The FV model is a representation for fuzzy
controllers based on Voronoi diagrams that can represent fuzzy systems with
synergistic rules, fulfilling the $\epsilon$-completeness property and
providing a simple way to introduce a priory knowledge. In the proposed
representation, the temporal relations are embedded by including internal units
that provide feedback by connecting outputs to inputs. These internal units act
as memory elements. In the RFV model, the semantic of the internal units can be
specified together with the a priori rules. The geometric interpretation of the
rules allows the use of geometric variational operators during the evolution.
The representation and the algorithms are validated in two problems in the area
of system identification and evolutionary robotics.
",Evolution of Voronoi based Fuzzy Recurrent Controllers,"Carlos Kavka (INRIA Futurs, UNSL-DI), Patricia Roggero (UNSL-DI), Marc
  Schoenauer (INRIA Futurs)",2005,Artificial Intelligence,
"  An original approach, termed Divide-and-Evolve is proposed to hybridize
Evolutionary Algorithms (EAs) with Operational Research (OR) methods in the
domain of Temporal Planning Problems (TPPs). Whereas standard Memetic
Algorithms use local search methods to improve the evolutionary solutions, and
thus fail when the local method stops working on the complete problem, the
Divide-and-Evolve approach splits the problem at hand into several, hopefully
easier, sub-problems, and can thus solve globally problems that are intractable
when directly fed into deterministic OR algorithms. But the most prominent
advantage of the Divide-and-Evolve approach is that it immediately opens up an
avenue for multi-objective optimization, even though the OR method that is used
is single-objective. Proof of concept approach on the standard
(single-objective) Zeno transportation benchmark is given, and a small original
multi-objective benchmark is proposed in the same Zeno framework to assess the
multi-objective capabilities of the proposed methodology, a breakthrough in
Temporal Planning.
","Divide-and-Evolve: a New Memetic Scheme for Domain-Independent Temporal
  Planning","Marc Schoenauer (INRIA Futurs), Pierre Sav\'eant (TRT), Vincent Vidal
  (CRIL)",2006,Artificial Intelligence,
"  Constraint Programming (CP) has proved an effective paradigm to model and
solve difficult combinatorial satisfaction and optimisation problems from
disparate domains. Many such problems arising from the commercial world are
permeated by data uncertainty. Existing CP approaches that accommodate
uncertainty are less suited to uncertainty arising due to incomplete and
erroneous data, because they do not build reliable models and solutions
guaranteed to address the user's genuine problem as she perceives it. Other
fields such as reliable computation offer combinations of models and associated
methods to handle these types of uncertain data, but lack an expressive
framework characterising the resolution methodology independently of the model.
  We present a unifying framework that extends the CP formalism in both model
and solutions, to tackle ill-defined combinatorial problems with incomplete or
erroneous data. The certainty closure framework brings together modelling and
solving methodologies from different fields into the CP paradigm to provide
reliable and efficient approches for uncertain constraint problems. We
demonstrate the applicability of the framework on a case study in network
diagnosis. We define resolution forms that give generic templates, and their
associated operational semantics, to derive practical solution methods for
reliable solutions.
","Certainty Closure: Reliable Constraint Reasoning with Incomplete or
  Erroneous Data",Neil Yorke-Smith and Carmen Gervet,2009,Artificial Intelligence,
"  Program analysis and verification require decision procedures to reason on
theories of data structures. Many problems can be reduced to the satisfiability
of sets of ground literals in theory T. If a sound and complete inference
system for first-order logic is guaranteed to terminate on T-satisfiability
problems, any theorem-proving strategy with that system and a fair search plan
is a T-satisfiability procedure. We prove termination of a rewrite-based
first-order engine on the theories of records, integer offsets, integer offsets
modulo and lists. We give a modularity theorem stating sufficient conditions
for termination on a combinations of theories, given termination on each. The
above theories, as well as others, satisfy these conditions. We introduce
several sets of benchmarks on these theories and their combinations, including
both parametric synthetic benchmarks to test scalability, and real-world
problems to test performances on huge sets of literals. We compare the
rewrite-based theorem prover E with the validity checkers CVC and CVC Lite.
Contrary to the folklore that a general-purpose prover cannot compete with
reasoners with built-in theories, the experiments are overall favorable to the
theorem prover, showing that not only the rewriting approach is elegant and
conceptually simple, but has important practical implications.
",New results on rewrite-based satisfiability procedures,"Alessandro Armando, Maria Paola Bonacina, Silvio Ranise, Stephan
  Schulz",2009,Artificial Intelligence,
"  Fuzzy automata, whose input alphabet is a set of numbers or symbols, are a
formal model of computing with values. Motivated by Zadeh's paradigm of
computing with words rather than numbers, Ying proposed a kind of fuzzy
automata, whose input alphabet consists of all fuzzy subsets of a set of
symbols, as a formal model of computing with all words. In this paper, we
introduce a somewhat general formal model of computing with (some special)
words. The new features of the model are that the input alphabet only comprises
some (not necessarily all) fuzzy subsets of a set of symbols and the fuzzy
transition function can be specified arbitrarily. By employing the methodology
of fuzzy control, we establish a retraction principle from computing with words
to computing with values for handling crisp inputs and a generalized extension
principle from computing with words to computing with all words for handling
fuzzy inputs. These principles show that computing with values and computing
with all words can be respectively implemented by computing with words. Some
algebraic properties of retractions and generalized extensions are addressed as
well.
",Retraction and Generalized Extension of Computing with Words,"Yongzhi Cao, Mingsheng Ying, and Guoqing Chen",2007,Artificial Intelligence,
"  In this paper we consider and analyze the behavior of two combinational rules
for temporal (sequential) attribute data fusion for target type estimation. Our
comparative analysis is based on Dempster's fusion rule proposed in
Dempster-Shafer Theory (DST) and on the Proportional Conflict Redistribution
rule no. 5 (PCR5) recently proposed in Dezert-Smarandache Theory (DSmT). We
show through very simple scenario and Monte-Carlo simulation, how PCR5 allows a
very efficient Target Type Tracking and reduces drastically the latency delay
for correct Target Type decision with respect to Demspter's rule. For cases
presenting some short Target Type switches, Demspter's rule is proved to be
unable to detect the switches and thus to track correctly the Target Type
changes. The approach proposed here is totally new, efficient and promising to
be incorporated in real-time Generalized Data Association - Multi Target
Tracking systems (GDA-MTT) and provides an important result on the behavior of
PCR5 with respect to Dempster's rule. The MatLab source code is provided in
","Target Type Tracking with PCR5 and Dempster's rules: A Comparative
  Analysis","Jean Dezert, Albena Tchamova, Florentin Smarandache, Pavlina
  Konstantinova",2006,Artificial Intelligence,
"  This paper introduces the notion of qualitative belief assignment to model
beliefs of human experts expressed in natural language (with linguistic
labels). We show how qualitative beliefs can be efficiently combined using an
extension of Dezert-Smarandache Theory (DSmT) of plausible and paradoxical
quantitative reasoning to qualitative reasoning. We propose a new arithmetic on
linguistic labels which allows a direct extension of classical DSm fusion rule
or DSm Hybrid rules. An approximate qualitative PCR5 rule is also proposed
jointly with a Qualitative Average Operator. We also show how crisp or interval
mappings can be used to deal indirectly with linguistic labels. A very simple
example is provided to illustrate our qualitative fusion rules.
",Fusion of qualitative beliefs using DSmT,"Florentin Smarandache, Jean Dezert",2006,Artificial Intelligence,
"  Event-driven reactive functionalities are an urgent need in nowadays
distributed service-oriented applications and (Semantic) Web-based
environments. An important problem to be addressed is how to correctly and
efficiently capture and process the event-based behavioral, reactive logic
represented as ECA rules in combination with other conditional decision logic
which is represented as derivation rules. In this paper we elaborate on a
homogeneous integration approach which combines derivation rules, reaction
rules (ECA rules) and other rule types such as integrity constraint into the
general framework of logic programming. The developed ECA-LP language provides
expressive features such as ID-based updates with support for external and
self-updates of the intensional and extensional knowledge, transac-tions
including integrity testing and an event algebra to define and process complex
events and actions based on a novel interval-based Event Calculus variant.
","ECA-LP / ECA-RuleML: A Homogeneous Event-Condition-Action Logic
  Programming Language",Adrian Paschke,2006,Artificial Intelligence,
"  In this paper we elaborate on a specific application in the context of hybrid
description logic programs (hybrid DLPs), namely description logic Semantic Web
type systems (DL-types) which are used for term typing of LP rules based on a
polymorphic, order-sorted, hybrid DL-typed unification as procedural semantics
of hybrid DLPs. Using Semantic Web ontologies as type systems facilitates
interchange of domain-independent rules over domain boundaries via dynamically
typing and mapping of explicitly defined type ontologies.
","A Typed Hybrid Description Logic Programming Language with Polymorphic
  Order-Sorted DL-Typed Unification for Semantic Web Type Systems",Adrian Paschke,2006,Artificial Intelligence,
"  This paper presents a model and implementation techniques for speeding up
constraint propagation. Three fundamental approaches to improving constraint
propagation based on propagators as implementations of constraints are
explored: keeping track of which propagators are at fixpoint, choosing which
propagator to apply next, and how to combine several propagators for the same
constraint. We show how idempotence reasoning and events help track fixpoints
more accurately. We improve these methods by using them dynamically (taking
into account current domains to improve accuracy). We define priority-based
approaches to choosing a next propagator and show that dynamic priorities can
improve propagation. We illustrate that the use of multiple propagators for the
same constraint can be advantageous with priorities, and introduce staged
propagators that combine the effects of multiple propagators with priorities
for greater efficiency.
",Efficient constraint propagation engines,Christian Schulte and Peter J. Stuckey,2008,Artificial Intelligence,
"  Description Logics (DLs) are appropriate, widely used, logics for managing
structured knowledge. They allow reasoning about individuals and concepts, i.e.
set of individuals with common properties. Typically, DLs are limited to
dealing with crisp, well defined concepts. That is, concepts for which the
problem whether an individual is an instance of it is yes/no question. More
often than not, the concepts encountered in the real world do not have a
precisely defined criteria of membership: we may say that an individual is an
instance of a concept only to a certain degree, depending on the individual's
properties. The DLs that deal with such fuzzy concepts are called fuzzy DLs. In
order to deal with fuzzy, incomplete, indeterminate and inconsistent concepts,
we need to extend the fuzzy DLs, combining the neutrosophic logic with a
classical DL. In particular, concepts become neutrosophic (here neutrosophic
means fuzzy, incomplete, indeterminate, and inconsistent), thus reasoning about
neutrosophic concepts is supported. We'll define its syntax, its semantics, and
describe its properties.
",A Neutrosophic Description Logic,"Haibin Wang, Andre Rogatko, Florentin Smarandache, Rajshekhar
  Sunderraman",2006,Artificial Intelligence,
"  We propose a method for improving approximate inference methods that corrects
for the influence of loops in the graphical model. The method is applicable to
arbitrary factor graphs, provided that the size of the Markov blankets is not
too large. It is an alternative implementation of an idea introduced recently
by Montanari and Rizzo (2005). In its simplest form, which amounts to the
assumption that no loops are present, the method reduces to the minimal Cluster
Variation Method approximation (which uses maximal factors as outer clusters).
On the other hand, using estimates of the effect of loops (obtained by some
approximate inference algorithm) and applying the Loop Correcting (LC) method
usually gives significantly better results than applying the approximate
inference algorithm directly without loop corrections. Indeed, we often observe
that the loop corrected error is approximately the square of the error of the
approximate inference method used to estimate the effect of loops. We compare
different variants of the Loop Correcting method with other approximate
inference methods on a variety of graphical models, including ""real world""
networks, and conclude that the LC approach generally obtains the most accurate
results.
",Loop corrections for approximate inference,Joris Mooij and Bert Kappen,2007,Artificial Intelligence,
"  Recently, M. Chertkov and V.Y. Chernyak derived an exact expression for the
partition sum (normalization constant) corresponding to a graphical model,
which is an expansion around the Belief Propagation solution. By adding
correction terms to the BP free energy, one for each ""generalized loop"" in the
factor graph, the exact partition sum is obtained. However, the usually
enormous number of generalized loops generally prohibits summation over all
correction terms. In this article we introduce Truncated Loop Series BP
(TLSBP), a particular way of truncating the loop series of M. Chertkov and V.Y.
Chernyak by considering generalized loops as compositions of simple loops. We
analyze the performance of TLSBP in different scenarios, including the Ising
model, regular random graphs and on Promedas, a large probabilistic medical
diagnostic system. We show that TLSBP often improves upon the accuracy of the
BP solution, at the expense of increased computation time. We also show that
the performance of TLSBP strongly depends on the degree of interaction between
the variables. For weak interactions, truncating the series leads to
significant improvements, whereas for strong interactions it can be
ineffective, even if a high number of terms is considered.
",Truncating the loop series expansion for Belief Propagation,"Vicenc Gomez, J. M. Mooij, H. J. Kappen",2007,Artificial Intelligence,
"  This short paper introduces two new fusion rules for combining quantitative
basic belief assignments. These rules although very simple have not been
proposed in literature so far and could serve as useful alternatives because of
their low computation cost with respect to the recent advanced Proportional
Conflict Redistribution rules developed in the DSmT framework.
",Uniform and Partially Uniform Redistribution Rules,"Florentin Smarandache, Jean Dezert",2011,Artificial Intelligence,
"  For academics and practitioners concerned with computers, business and
mathematics, one central issue is supporting decision makers. In this paper, we
propose a generalization of Decision Matrix Method (DMM), using Neutrosophic
logic. It emerges as an alternative to the existing logics and it represents a
mathematical model of uncertainty and indeterminacy. This paper proposes the
Neutrosophic Decision Matrix Method as a more realistic tool for decision
making. In addition, a de-neutrosophication process is included.
","Redesigning Decision Matrix Method with an indeterminacy-based inference
  process","Jose L. Salmeron, Florentin Smarandache",2006,Artificial Intelligence,
A well known N P-hard problem called the Generalized Traveling Salesman Problem (GTSP) is considered. In GTSP the nodes of a complete undirected graph are partitioned into clusters. The objective is to find a minimum cost tour passing through exactly one node from each cluster. An exact exponential time algorithm and an effective meta-heuristic algorithm for the problem are presented. The meta-heuristic proposed is a modified Ant Colony System (ACS) algorithm called Reinforcing Ant Colony System (RACS) which introduces new correction rules in the ACS algorithm. Computational results are reported for many standard test problems. The proposed algorithm is competitive with the other already proposed heuristics for the GTSP in both solution quality and computational time.,The Generalized Traveling Salesman Problem solved with Ant Algorithms,"Camelia-M. Pintea, Petrica C. Pop, Camelia Chira",2017,Artificial Intelligence,1310.2350
"This paper describes a new method for classifying a dataset that partitions elements into their categories. It has relations with neural networks but a slightly different structure, requiring only a single pass through the classifier to generate the weight sets. A grid-like structure is required as part of a novel idea of converting a 1-D row of real values into a 2-D structure of value bands. Each cell in any band then stores a distinct set of weights, to represent its own importance and its relation to each output category. During classification, all of the output weight lists can be retrieved and summed to produce a probability for what the correct output category is. The bands possibly work like hidden layers of neurons, but they are variable specific, making the process orthogonal. The construction process can be a single update process without iterations, making it potentially much faster. It can also be compared with k-NN and may be practical for partial or competitive updating.",A Single-Pass Classifier for Categorical Data,Kieran Greer,2017,Artificial Intelligence,1503.02521
"Every day, billions of mobile network events (i.e. CDRs) are generated by cellular phone operator companies. Latent in this data are inspiring insights about human actions and behaviors, the discovery of which is important because context-aware applications and services hold the key to user-driven, intelligent services, which can enhance our everyday lives such as social and economic development, urban planning, and health prevention. The major challenge in this area is that interpreting such a big stream of data requires a deep understanding of mobile network events' context through available background knowledge. This article addresses the issues in context awareness given heterogeneous and uncertain data of mobile network events missing reliable information on the context of this activity. The contribution of this research is a model from a combination of logical and statistical reasoning standpoints for enabling human activity inference in qualitative terms from open geographical data that aimed at improving the quality of human behaviors recognition tasks from CDRs. We use open geographical data, Openstreetmap (OSM), as a proxy for predicting the content of human activity in the area. The user study performed in Trento shows that predicted human activities (top level) match the survey data with around 93% overall accuracy. The extensive validation for predicting a more specific economic type of human activity performed in Barcelona, by employing credit card transaction data. The analysis identifies that appropriately normalized data on points of interest (POI) is a good proxy for predicting human economical activities, with 84% accuracy on average. So the model is proven to be efficient for predicting the context of human activity, when its total level could be efficiently observed from cell phone data records, missing contextual information however.",Semantic Enrichment of Mobile Phone Data Records Using Background Knowledge,Zolzaya Dashdorj and Stanislav Sobolevsky and Luciano Serafini and Fabrizio Antonelli and Carlo Ratti,2018,Artificial Intelligence,1504.05895
"This paper describes a new method for reducing the error in a classifier. It uses an error correction update that includes the very simple rule of either adding or subtracting the error adjustment, based on whether the variable value is currently larger or smaller than the desired value. While a traditional neuron would sum the inputs together and then apply a function to the total, this new method can change the function decision for each input value. This gives added flexibility to the convergence procedure, where through a series of transpositions, variables that are far away can continue towards the desired value, whereas variables that are originally much closer can oscillate from one side to the other. Tests show that the method can successfully classify some benchmark datasets. It can also work in a batch mode, with reduced training times and can be used as part of a neural network architecture. Some comparisons with an earlier wave shape paper are also made.",A New Oscillating-Error Technique for Classifiers,Kieran Greer,2017,Artificial Intelligence,1505.05312
"We establish an equivalence between two seemingly different theories: one is the traditional axiomatisation of incomplete preferences on horse lotteries based on the mixture independence axiom; the other is the theory of desirable gambles developed in the context of imprecise probability. The equivalence allows us to revisit incomplete preferences from the viewpoint of desirability and through the derived notion of coherent lower previsions. On this basis, we obtain new results and insights: in particular, we show that the theory of incomplete preferences can be developed assuming only the existence of a worst act---no best act is needed---, and that a weakened Archimedean axiom suffices too; this axiom allows us also to address some controversy about the regularity assumption (that probabilities should be positive---they need not), which enables us also to deal with uncountable possibility spaces; we show that it is always possible to extend in a minimal way a preference relation to one with a worst act, and yet the resulting relation is never Archimedean, except in a trivial case; we show that the traditional notion of state independence coincides with the notion called strong independence in imprecise probability---this leads us to give much a weaker definition of state independence than the traditional one; we rework and uniform the notions of complete preferences, beliefs, values; we argue that Archimedeanity does not capture all the problems that can be modelled with sets of expected utilities and we provide a new notion that does precisely that. Perhaps most importantly, we argue throughout that desirability is a powerful and natural setting to model, and work with, incomplete preferences, even in case of non-Archimedean problems. This leads us to suggest that desirability, rather than preference, should be the primitive notion at the basis of decision-theoretic axiomatisations.",Desirability and the birth of incomplete preferences,Marco Zaffalon and Enrique Miranda,2017,Artificial Intelligence,1506.00529
"Detection of non-technical losses (NTL) which include electricity theft, faulty meters or billing errors has attracted increasing attention from researchers in electrical engineering and computer science. NTLs cause significant harm to the economy, as in some countries they may range up to 40% of the total electricity distributed. The predominant research direction is employing artificial intelligence to predict whether a customer causes NTL. This paper first provides an overview of how NTLs are defined and their impact on economies, which include loss of revenue and profit of electricity providers and decrease of the stability and reliability of electrical power grids. It then surveys the state-of-the-art research efforts in a up-to-date and comprehensive review of algorithms, features and data sets used. It finally identifies the key scientific and engineering challenges in NTL detection and suggests how they could be addressed in the future.",The Challenge of Non-Technical Loss Detection using Artificial Intelligence: A Survey,"Patrick Glauner, Jorge Augusto Meira, Petko Valtchev, Radu State, Franck Bettinger",2017,Artificial Intelligence,1606.00626
"A Concept Tree is a structure for storing knowledge where the trees are stored in a database called a Concept Base. It sits between the highly distributed neural architectures and the distributed information systems, with the intention of bringing brain-like and computer systems closer together. Concept Trees can grow from the semi-structured sources when consistent sequences of concepts are presented. Each tree ideally represents a single cohesive concept and the trees can link with each other for navigation and semantic purposes. The trees are therefore also a type of semantic network and would benefit from having a consistent level of context for each node. A consistent build process is managed through a 'counting rule' and some other rules that can normalise the database structure. This restricted structure can then be complimented and enriched by the more dynamic context. It is also suggested to use the linking structure of the licas system [15] as a basis for the context links, where the mathematical model is extended further to define this. A number of tests have demonstrated the soundness of the architecture. Building the trees from text documents shows that the tree structure could be inherent in natural language. Then, two types of query language are described. Both of these can perform consistent query processes to return knowledge to the user and even enhance the query with new knowledge. This is supported even further with direct comparisons to a cognitive model, also being developed by the author.",Adding Context to Concept Trees,Kieran Greer,2019,Artificial Intelligence,1606.05597
"Sequential data modeling and analysis have become indispensable tools for analyzing sequential data, such as time-series data, because larger amounts of sensed event data have become available. These methods capture the sequential structure of data of interest, such as input-output relations and correlation among datasets. However, because most studies in this area are specialized or limited to their respective applications, rigorous requirement analysis of such models has not been undertaken from a general perspective. Therefore, we particularly examine the structure of sequential data, and extract the necessity of `state duration' and `state interval' of events for efficient and rich representation of sequential data. Specifically addressing the hidden semi-Markov model (HSMM) that represents such state duration inside a model, we attempt to add representational capability of a state interval of events onto HSMM. To this end, we propose two extended models: an interval state hidden semi-Markov model (IS-HSMM) to express the length of a state interval with a special state node designated as ""interval state node""; and an interval length probability hidden semi-Markov model (ILP-HSMM) which represents the length of the state interval with a new probabilistic parameter ""interval length probability."" Exhaustive simulations have revealed superior performance of the proposed models in comparison with HSMM. These proposed models are the first reported extensions of HMM to support state interval representation as well as state duration representation.",State Duration and Interval Modeling in Hidden Semi-Markov Model for Sequential Data Analysis,Hiromi Narimatsu and Hiroyuki Kasai,2017,Artificial Intelligence,1608.06954
"This paper describes an approach to the methodology of answer set programming (ASP) that can facilitate the design of encodings that are easy to understand and provably correct. Under this approach, after appending a rule or a small group of rules to the emerging program we include a comment that states what has been ""achieved"" so far. This strategy allows us to set out our understanding of the design of the program by describing the roles of small parts of the program in a mathematically precise way.",Achievements in Answer Set Programming,Vladimir Lifschitz,2017,Artificial Intelligence,1608.08144
"Graph aggregation is the process of computing a single output graph that constitutes a good compromise between several input graphs, each provided by a different source. One needs to perform graph aggregation in a wide variety of situations, e.g., when applying a voting rule (graphs as preference orders), when consolidating conflicting views regarding the relationships between arguments in a debate (graphs as abstract argumentation frameworks), or when computing a consensus between several alternative clusterings of a given dataset (graphs as equivalence relations). In this paper, we introduce a formal framework for graph aggregation grounded in social choice theory. Our focus is on understanding which properties shared by the individual input graphs will transfer to the output graph returned by a given aggregation rule. We consider both common properties of graphs, such as transitivity and reflexivity, and arbitrary properties expressible in certain fragments of modal logic. Our results establish several connections between the types of properties preserved under aggregation and the choice-theoretic axioms satisfied by the rules used. The most important of these results is a powerful impossibility theorem that generalises Arrow's seminal result for the aggregation of preference orders to a large collection of different types of graphs.",Graph Aggregation,Ulle Endriss and Umberto Grandi,2017,Artificial Intelligence,1609.03765
"Pairwise comparisons between alternatives are a well-known method for measuring preferences of a decision-maker. Since these often do not exhibit consistency, a number of inconsistency indices has been introduced in order to measure the deviation from this ideal case. We axiomatically characterize the inconsistency ranking induced by the Koczkodaj inconsistency index: six independent properties are presented such that they determine a unique linear order on the set of all pairwise comparison matrices.",Characterization of an inconsistency ranking for pairwise comparison matrices,L\'aszl\'o Csat\'o,2018,Artificial Intelligence,1610.07388
"This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach's strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.",DeepBach: a Steerable Model for Bach Chorales Generation,"Ga\""etan Hadjeres and Fran\c{c}ois Pachet and Frank Nielsen",2017,Artificial Intelligence,1612.01010
"Crowdsourcing, a major economic issue, is the fact that the firm outsources internal task to the crowd. It is a form of digital subcontracting for the general public. The evaluation of the participants work quality is a major issue in crowdsourcing. Indeed, contributions must be controlled to ensure the effectiveness and relevance of the campaign. We are particularly interested in small, fast and not automatable tasks. Several methods have been proposed to solve this problem, but they are applicable when the ""golden truth"" is not always known. This work has the particularity to propose a method for calculating the degree of expertise in the presence of gold data in crowdsourcing. This method is based on the belief function theory and proposes a structuring of data using graphs. The proposed approach will be assessed and applied to the data.",Une mesure d'expertise pour le crowdsourcing,"Hosna Ouni (IRISA, DRUID), Arnaud Martin (IRISA, UR1, DRUID), Laetitia Gros, Mouloud Kharoune (IRISA, DRUID), Zoltan Miklos (IRISA, DRUID)",2017,Artificial Intelligence,1701.04645
"The 7th Symposium on Educational Advances in Artificial Intelligence (EAAI'17, co-chaired by Sven Koenig and Eric Eaton) launched the EAAI New and Future AI Educator Program to support the training of early-career university faculty, secondary school faculty, and future educators (PhD candidates or postdocs who intend a career in academia). As part of the program, awardees were asked to address one of the following ""blue sky"" questions: * How could/should Artificial Intelligence (AI) courses incorporate ethics into the curriculum? * How could we teach AI topics at an early undergraduate or a secondary school level? * AI has the potential for broad impact to numerous disciplines. How could we make AI education more interdisciplinary, specifically to benefit non-engineering fields? This paper is a collection of their responses, intended to help motivate discussion around these issues in AI education.",Blue Sky Ideas in Artificial Intelligence Education from the EAAI 2017 New and Future AI Educator Program,"Eric Eaton, Sven Koenig, Claudia Schulz, Francesco Maurelli, John Lee, Joshua Eckroth, Mark Crowley, Richard G. Freedman, Rogelio E. Cardona-Rivera, Tiago Machado, Tom Williams",2018,Artificial Intelligence,1702.00137
"Autonomous agents must often detect affordances: the set of behaviors enabled by a situation. Affordance detection is particularly helpful in domains with large action spaces, allowing the agent to prune its search space by avoiding futile behaviors. This paper presents a method for affordance extraction via word embeddings trained on a Wikipedia corpus. The resulting word vectors are treated as a common knowledge database which can be queried using linear algebra. We apply this method to a reinforcement learning agent in a text-only environment and show that affordance-based action selection improves performance most of the time. Our method increases the computational complexity of each learning step but significantly reduces the total number of steps needed. In addition, the agent's action selections begin to resemble those a human would choose.",What can you do with a rock? Affordance extraction via word embeddings,Nancy Fulda and Daniel Ricks and Ben Murdoch and David Wingate,2017,Artificial Intelligence,1703.03429
"Programming by Example (PBE) targets at automatically inferring a computer program for accomplishing a certain task from sample input and output. In this paper, we propose a deep neural networks (DNN) based PBE model called Neural Programming by Example (NPBE), which can learn from input-output strings and induce programs that solve the string manipulation problems. Our NPBE model has four neural network based components: a string encoder, an input-output analyzer, a program generator, and a symbol selector. We demonstrate the effectiveness of NPBE by training it end-to-end to solve some common string manipulation problems in spreadsheet systems. The results show that our model can induce string manipulation programs effectively. Our work is one step towards teaching DNN to generate computer programs.",Neural Programming by Example,"Chengxun Shu, Hongyu Zhang",2017,Artificial Intelligence,1703.04990
"Since Alan Turing envisioned Artificial Intelligence (AI) [1], a major driving force behind technical progress has been competition with human cognition. Historical milestones have been frequently associated with computers matching or outperforming humans in difficult cognitive tasks (e.g. face recognition [2], personality classification [3], driving cars [4], or playing video games [5]), or defeating humans in strategic zero-sum encounters (e.g. Chess [6], Checkers [7], Jeopardy! [8], Poker [9], or Go [10]). In contrast, less attention has been given to developing autonomous machines that establish mutually cooperative relationships with people who may not share the machine's preferences. A main challenge has been that human cooperation does not require sheer computational power, but rather relies on intuition [11], cultural norms [12], emotions and signals [13, 14, 15, 16], and pre-evolved dispositions toward cooperation [17], common-sense mechanisms that are difficult to encode in machines for arbitrary contexts. Here, we combine a state-of-the-art machine-learning algorithm with novel mechanisms for generating and acting on signals to produce a new learning algorithm that cooperates with people and other machines at levels that rival human cooperation in a variety of two-player repeated stochastic games. This is the first general-purpose algorithm that is capable, given a description of a previously unseen game environment, of learning to cooperate with people within short timescales in scenarios previously unanticipated by algorithm designers. This is achieved without complex opponent modeling or higher-order theories of mind, thus showing that flexible, fast, and general human-machine cooperation is computationally achievable using a non-trivial, but ultimately simple, set of algorithmic mechanisms.",Cooperating with Machines,"Jacob W. Crandall, Mayada Oudah, Tennom, Fatimah Ishowo-Oloko, Sherief Abdallah, Jean-Fran\c{c}ois Bonnefon, Manuel Cebrian, Azim Shariff, Michael A. Goodrich, and Iyad Rahwan",2018,Artificial Intelligence,1703.06207
"Catastrophic forgetting is of special importance in reinforcement learning, as the data distribution is generally non-stationary over time. We study and compare several pseudorehearsal approaches for Q-learning with function approximation in a pole balancing task. We have found that pseudorehearsal seems to assist learning even in such very simple problems, given proper initialization of the rehearsal parameters.",Pseudorehearsal in value function approximation,"Vladimir Marochko, Leonard Johard, Manuel Mazzara",2017,Artificial Intelligence,1703.07075
"Content marketing is todays one of the most remarkable approaches in the context of marketing processes of companies. Value of this kind of marketing has improved in time, thanks to the latest developments regarding to computer and communication technologies. Nowadays, especially social media based platforms have a great importance on enabling companies to design multimedia oriented, interactive content. But on the other hand, there is still something more to do for improved content marketing approaches. In this context, objective of this study is to focus on intelligent content marketing, which can be done by using artificial intelligence. Artificial Intelligence is todays one of the most remarkable research fields and it can be used easily as multidisciplinary. So, this study has aimed to discuss about its potential on improving content marketing. In detail, the study has enabled readers to improve their awareness about the intersection point of content marketing and artificial intelligence. Furthermore, the authors have introduced some example models of intelligent content marketing, which can be achieved by using current Web technologies and artificial intelligence techniques.",Improving content marketing processes with the approaches by artificial intelligence,"Utku Kose, Selcuk Sert",2017,Artificial Intelligence,1704.02114
"Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.",Stochastic Neural Networks for Hierarchical Reinforcement Learning,"Carlos Florensa, Yan Duan, Pieter Abbeel",2017,Artificial Intelligence,1704.03012
"The AGM model is the most remarkable framework for modeling belief revision. However, it is not perfect in all aspects. Paraconsistent belief revision, multi-agent belief revision and non-prioritized belief revision are three different extensions to AGM to address three important criticisms applied to it. In this article, we propose a framework based on AGM that takes a position in each of these categories. Also, we discuss some features of our framework and study the satisfiability of AGM postulates in this new context.",Source-Sensitive Belief Change,Shahab Ebrahimi,2017,Artificial Intelligence,1704.03396
"Monte Carlo Tree Search techniques have generally dominated General Video Game Playing, but recent research has started looking at Evolutionary Algorithms and their potential at matching Tree Search level of play or even outperforming these methods. Online or Rolling Horizon Evolution is one of the options available to evolve sequences of actions for planning in General Video Game Playing, but no research has been done up to date that explores the capabilities of the vanilla version of this algorithm in multiple games. This study aims to critically analyse the different configurations regarding population size and individual length in a set of 20 games from the General Video Game AI corpus. Distinctions are made between deterministic and stochastic games, and the implications of using superior time budgets are studied. Results show that there is scope for the use of these techniques, which in some configurations outperform Monte Carlo Tree Search, and also suggest that further research in these methods could boost their performance.",Analysis of Vanilla Rolling Horizon Evolution Parameters in General Video Game Playing,Raluca D. Gaina and Jialin Liu and Simon M. Lucas and Diego Perez-Liebana,2017,Artificial Intelligence,1704.07075
"The notion of events has occupied a central role in modeling and has an influence in computer science and philosophy. Recent developments in diagrammatic modeling have made it possible to examine conceptual representation of events. This paper explores some aspects of the notion of events that are produced by applying a new diagrammatic methodology with a focus on the interaction of events with such concepts as time and space, objects. The proposed description applies to abstract machines where events form the dynamic phases of a system. The results of this nontechnical research can be utilized in many fields where the notion of an event is typically used in interdisciplinary application.",Modeling Events as Machines,Sabah Al-Fedaghi,2017,Artificial Intelligence,1704.08588
"We present a rational analysis of curiosity, proposing that people's curiosity is driven by seeking stimuli that maximize their ability to make appropriate responses in the future. This perspective offers a way to unify previous theories of curiosity into a single framework. Experimental results confirm our model's predictions, showing how the relationship between curiosity and confidence can change significantly depending on the nature of the environment. Please refer to https://psyarxiv.com/wg5m6/ for a more updated version of this manuscript with a more detailed modeling section with extensive experiments.",A rational analysis of curiosity,Rachit Dubey and Thomas L. Griffiths,2017,Artificial Intelligence,1705.04351
"This paper proposes a design of hierarchical fuzzy inference tree (HFIT). An HFIT produces an optimum treelike structure, i.e., a natural hierarchical structure that accommodates simplicity by combining several low-dimensional fuzzy inference systems (FISs). Such a natural hierarchical structure provides a high degree of approximation accuracy. The construction of HFIT takes place in two phases. Firstly, a nondominated sorting based multiobjective genetic programming (MOGP) is applied to obtain a simple tree structure (a low complexity model) with a high accuracy. Secondly, the differential evolution algorithm is applied to optimize the obtained tree's parameters. In the derived tree, each node acquires a different input's combination, where the evolutionary process governs the input's combination. Hence, HFIT nodes are heterogeneous in nature, which leads to a high diversity among the rules generated by the HFIT. Additionally, the HFIT provides an automatic feature selection because it uses MOGP for the tree's structural optimization that accepts inputs only relevant to the knowledge contained in data. The HFIT was studied in the context of both type-1 and type-2 FISs, and its performance was evaluated through six application problems. Moreover, the proposed multiobjective HFIT was compared both theoretically and empirically with recently proposed FISs methods from the literature, such as McIT2FIS, TSCIT2FNN, SIT2FNN, RIT2FNS-WB, eT2FIS, MRIT2NFS, IT2FNN-SVR, etc. From the obtained results, it was found that the HFIT provided less complex and highly accurate models compared to the models produced by the most of other methods. Hence, the proposed HFIT is an efficient and competitive alternative to the other FISs for function approximation and feature selection.",Multiobjective Programming for Type-2 Hierarchical Fuzzy Inference Trees,"Varun Kumar Ojha, Vaclav Snasel, Ajith Abraham",2017,Artificial Intelligence,1705.05769
"In this work, we present a methodology that enables an agent to make efficient use of its exploratory actions by autonomously identifying possible objectives in its environment and learning them in parallel. The identification of objectives is achieved using an online and unsupervised adaptive clustering algorithm. The identified objectives are learned (at least partially) in parallel using Q-learning. Using a simulated agent and environment, it is shown that the converged or partially converged value function weights resulting from off-policy learning can be used to accumulate knowledge about multiple objectives without any additional exploration. We claim that the proposed approach could be useful in scenarios where the objectives are initially unknown or in real world scenarios where exploration is typically a time and energy intensive process. The implications and possible extensions of this work are also briefly discussed.",Identification and Off-Policy Learning of Multiple Objectives Using Adaptive Clustering,"Thommen George Karimpanal, Erik Wilhelm",2017,Artificial Intelligence,1705.06342
"The human reasoning process is seldom a one-way process from an input leading to an output. Instead, it often involves a systematic deduction by ruling out other possible outcomes as a self-checking mechanism. In this paper, we describe the design of a hybrid neural network for logical learning that is similar to the human reasoning through the introduction of an auxiliary input, namely the indicators, that act as the hints to suggest logical outcomes. We generate these indicators by digging into the hidden information buried underneath the original training data for direct or indirect suggestions. We used the MNIST data to demonstrate the design and use of these indicators in a convolutional neural network. We trained a series of such hybrid neural networks with variations of the indicators. Our results show that these hybrid neural networks are very robust in generating logical outcomes with inherently higher prediction accuracy than the direct use of the original input and output in apparent models. Such improved predictability with reassured logical confidence is obtained through the exhaustion of all possible indicators to rule out all illogical outcomes, which is not available in the apparent models. Our logical learning process can effectively cope with the unknown unknowns using a full exploitation of all existing knowledge available for learning. The design and implementation of the hints, namely the indicators, become an essential part of artificial intelligence for logical learning. We also introduce an ongoing application setup for this hybrid neural network in an autonomous grasping robot, namely as_DeepClaw, aiming at learning an optimized grasping pose through logical learning.",Logical Learning Through a Hybrid Neural Network with Auxiliary Inputs,Fang Wan and Chaoyang Song,2018,Artificial Intelligence,1705.08200
"In this paper we explore the theoretical boundaries of planning in a setting where no model of the agent's actions is given. Instead of an action model, a set of successfully executed plans are given and the task is to generate a plan that is safe, i.e., guaranteed to achieve the goal without failing. To this end, we show how to learn a conservative model of the world in which actions are guaranteed to be applicable. This conservative model is then given to an off-the-shelf classical planner, resulting in a plan that is guaranteed to achieve the goal. However, this reduction from a model-free planning to a model-based planning is not complete: in some cases a plan will not be found even when such exists. We analyze the relation between the number of observed plans and the likelihood that our conservative approach will indeed fail to solve a solvable problem. Our analysis show that the number of trajectories needed scales gracefully.","Efficient, Safe, and Probably Approximately Complete Learning of Action Models",Roni Stern and Brendan Juba,2017,Artificial Intelligence,1705.08961
"Humans are expert in the amount of sensory data they deal with each moment. Human brain not only analyses these data but also starts synthesizing new information from the existing data. The current age Big-data systems are needed not just to analyze data but also to come up new interpretation. We believe that the pivotal ability in human brain which enables us to do this is what is known as ""intuition"". Here, we present an intuition based architecture for big data analysis and synthesis.",ICABiDAS: Intuition Centred Architecture for Big Data Analysis and Synthesis,Amit Kumar Mishra,2018,Artificial Intelligence,1706.00638
"This paper introduces a cognitive architecture for a humanoid robot to engage in a proactive, mixed-initiative exploration and manipulation of its environment, where the initiative can originate from both the human and the robot. The framework, based on a biologically-grounded theory of the brain and mind, integrates a reactive interaction engine, a number of state-of-the-art perceptual and motor learning algorithms, as well as planning abilities and an autobiographical memory. The architecture as a whole drives the robot behavior to solve the symbol grounding problem, acquire language capabilities, execute goal-oriented behavior, and express a verbal narrative of its own experience in the world. We validate our approach in human-robot interaction experiments with the iCub humanoid robot, showing that the proposed cognitive architecture can be applied in real time within a realistic scenario and that it can be used with naive users.",DAC-h3: A Proactive Robot Cognitive Architecture to Acquire and Express Knowledge About the World and the Self,"Cl\'ement Moulin-Frier, Tobias Fischer, Maxime Petit, Gr\'egoire Pointeau, Jordi-Ysard Puigbo, Ugo Pattacini, Sock Ching Low, Daniel Camilleri, Phuong Nguyen, Matej Hoffmann, Hyung Jin Chang, Martina Zambelli, Anne-Laure Mealier, Andreas Damianou, Giorgio Metta, Tony J. Prescott, Yiannis Demiris, Peter Ford Dominey, Paul F. M. J. Verschure",2018,Artificial Intelligence,1706.03661
"Inductive Logic Programming (ILP) combines rule-based and statistical artificial intelligence methods, by learning a hypothesis comprising a set of rules given background knowledge and constraints for the search space. We focus on extending the XHAIL algorithm for ILP which is based on Answer Set Programming and we evaluate our extensions using the Natural Language Processing application of sentence chunking. With respect to processing natural language, ILP can cater for the constant change in how we use language on a daily basis. At the same time, ILP does not require huge amounts of training examples such as other statistical methods and produces interpretable results, that means a set of rules, which can be analysed and tweaked if necessary. As contributions we extend XHAIL with (i) a pruning mechanism within the hypothesis generalisation algorithm which enables learning from larger datasets, (ii) a better usage of modern solver technology using recently developed optimisation methods, and (iii) a time budget that permits the usage of suboptimal results. We evaluate these improvements on the task of sentence chunking using three datasets from a recent SemEval competition. Results show that our improvements allow for learning on bigger datasets with results that are of similar quality to state-of-the-art systems on the same task. Moreover, we compare the hypotheses obtained on datasets to gain insights on the structure of each dataset.",Improving Scalability of Inductive Logic Programming via Pruning and Best-Effort Optimisation,"Mishal Kazmi and Peter Sch\""uller and Y\""ucel Sayg{\i}n",2017,Artificial Intelligence,1706.05171
"Answer Set Programming (ASP) is a well-established declarative paradigm. One of the successes of ASP is the availability of efficient systems. State-of-the-art systems are based on the ground+solve approach. In some applications this approach is infeasible because the grounding of one or few constraints is expensive. In this paper, we systematically compare alternative strategies to avoid the instantiation of problematic constraints, that are based on custom extensions of the solver. Results on real and synthetic benchmarks highlight some strengths and weaknesses of the different strategies. (Under consideration for acceptance in TPLP, ICLP 2017 Special Issue.)","Constraints, Lazy Constraints, or Propagators in ASP Solving: An Empirical Analysis","Bernardo Cuteri, Carmine Dodaro, Francesco Ricca, Peter Sch\""uller",2017,Artificial Intelligence,1707.04027
"The paper investigates navigability with imperfect information. It shows that the properties of navigability with perfect recall are exactly those captured by Armstrong's axioms from the database theory. If the assumption of perfect recall is omitted, then Armstrong's transitivity axiom is not valid, but it can be replaced by two new weaker principles. The main technical results are soundness and completeness theorems for the logical systems describing properties of navigability with and without perfect recall.",Armstrong's Axioms and Navigation Strategies,Kaya Deuser and Pavel Naumov,2018,Artificial Intelligence,1707.04106
"As intelligent systems gain autonomy and capability, it becomes vital to ensure that their objectives match those of their human users; this is known as the value-alignment problem. In robotics, value alignment is key to the design of collaborative robots that can integrate into human workflows, successfully inferring and adapting to their users' objectives as they go. We argue that a meaningful solution to value alignment must combine multi-agent decision theory with rich mathematical models of human cognition, enabling robots to tap into people's natural collaborative capabilities. We present a solution to the cooperative inverse reinforcement learning (CIRL) dynamic game based on well-established cognitive models of decision making and theory of mind. The solution captures a key reciprocity relation: the human will not plan her actions in isolation, but rather reason pedagogically about how the robot might learn from them; the robot, in turn, can anticipate this and interpret the human's actions pragmatically. To our knowledge, this work constitutes the first formal analysis of value alignment grounded in empirically validated cognitive models.",Pragmatic-Pedagogic Value Alignment,"Jaime F. Fisac, Monica A. Gates, Jessica B. Hamrick, Chang Liu, Dylan Hadfield-Menell, Malayandi Palaniappan, Dhruv Malik, S. Shankar Sastry, Thomas L. Griffiths, and Anca D. Dragan",2017,Artificial Intelligence,1707.06354
"Sequential pattern mining algorithms are widely used to explore care pathways database, but they generate a deluge of patterns, mostly redundant or useless. Clinicians need tools to express complex mining queries in order to generate less but more significant patterns. These algorithms are not versatile enough to answer complex clinician queries. This article proposes to apply a declarative pattern mining approach based on Answer Set Programming paradigm. It is exemplified by a pharmaco-epidemiological study investigating the possible association between hospitalization for seizure and antiepileptic drug switch from a french medico-administrative database.",Declarative Sequential Pattern Mining of Care Pathways,"Thomas Guyet (1), Andr\'e Happe, Yann Dauxais (2) ((1) LACODAM, (2) UR1)",2017,Artificial Intelligence,1707.08342
"Nowadays, there are many approaches designed for the task of detecting communities in social networks. Among them, some methods only consider the topological graph structure, while others take use of both the graph structure and the node attributes. In real-world networks, there are many uncertain and noisy attributes in the graph. In this paper, we will present how we detect communities in graphs with uncertain attributes in the first step. The numerical, probabilistic as well as evidential attributes are generated according to the graph structure. In the second step, some noise will be added to the attributes. We perform experiments on graphs with different types of attributes and compare the detection results in terms of the Normalized Mutual Information (NMI) values. The experimental results show that the clustering with evidential attributes gives better results comparing to those with probabilistic and numerical attributes. This illustrates the advantages of evidential attributes.",The Advantage of Evidential Attributes in Social Networks,"Salma Ben Dhaou (LARODEC, DRUID), Kuang Zhou (NPU), Mouloud Kharoune (DRUID), Arnaud Martin (DRUID), Boutheina Ben Yaghlane (LARODEC)",2017,Artificial Intelligence,1707.08418
"Traffic speed is a key indicator for the efficiency of an urban transportation system. Accurate modeling of the spatiotemporally varying traffic speed thus plays a crucial role in urban planning and development. This paper addresses the problem of efficient fine-grained traffic speed prediction using big traffic data obtained from static sensors. Gaussian processes (GPs) have been previously used to model various traffic phenomena, including flow and speed. However, GPs do not scale with big traffic data due to their cubic time complexity. In this work, we address their efficiency issues by proposing local GPs to learn from and make predictions for correlated subsets of data. The main idea is to quickly group speed variables in both spatial and temporal dimensions into a finite number of clusters, so that future and unobserved traffic speed queries can be heuristically mapped to one of such clusters. A local GP corresponding to that cluster can then be trained on the fly to make predictions in real-time. We call this method localization. We use non-negative matrix factorization for localization and propose simple heuristics for cluster mapping. We additionally leverage on the expressiveness of GP kernel functions to model road network topology and incorporate side information. Extensive experiments using real-world traffic data collected in the two U.S. cities of Pittsburgh and Washington, D.C., show that our proposed local GPs significantly improve both runtime performances and prediction accuracies compared to the baseline global and local GPs.",Local Gaussian Processes for Efficient Fine-Grained Traffic Speed Prediction,"Truc Viet Le, Richard J. Oentaryo, Siyuan Liu, Hoong Chuin Lau",2017,Artificial Intelligence,1708.08079
"Traditionally psychometric tests were used for profiling incoming workers. These methods use DISC profiling method to classify people into distinct personality types, which are further used to predict if a person may be a possible fit to the organizational culture. This concept is taken further by introducing a novel technique to predict if a particular pair of an incoming worker and the manager being assigned are compatible at a psychological scale. This is done using multilayer perceptron neural network which can be adaptively trained to showcase the true nature of the compatibility index. The proposed prototype model is used to quantify the relevant attributes, use them to train the prediction engine, and to define the data pipeline required for it.",An Automated Compatibility Prediction Engine using DISC Theory Based Classification and Neural Networks,"Chandrasekaran Anirudh Bhardwaj, Megha Mishra and Sweetlin Hemalatha",2017,Artificial Intelligence,1709.00539
"We present theoretical analysis and a suite of tests and procedures for addressing a broad class of redundant and misleading association rules we call \emph{specious rules}. Specious dependencies, also known as \emph{spurious}, \emph{apparent}, or \emph{illusory associations}, refer to a well-known phenomenon where marginal dependencies are merely products of interactions with other variables and disappear when conditioned on those variables. The most extreme example is Yule-Simpson's paradox where two variables present positive dependence in the marginal contingency table but negative in all partial tables defined by different levels of a confounding factor. It is accepted wisdom that in data of any nontrivial dimensionality it is infeasible to control for all of the exponentially many possible confounds of this nature. In this paper, we consider the problem of specious dependencies in the context of statistical association rule mining. We define specious rules and show they offer a unifying framework which covers many types of previously proposed redundant or misleading association rules. After theoretical analysis, we introduce practical algorithms for detecting and pruning out specious association rules efficiently under many key goodness measures, including mutual information and exact hypergeometric probabilities. We demonstrate that the procedure greatly reduces the number of associations discovered, providing an elegant and effective solution to the problem of association mining discovering large numbers of misleading and redundant rules.",Specious rules: an efficient and effective unifying method for removing misleading and uninformative patterns in association rule mining,"Wilhelmiina H\""am\""al\""ainen and Geoffrey I. Webb",2017,Artificial Intelligence,1709.03915
"The incorporation of macro-actions (temporally extended actions) into multi-agent decision problems has the potential to address the curse of dimensionality associated with such decision problems. Since macro-actions last for stochastic durations, multiple agents executing decentralized policies in cooperative environments must act asynchronously. We present an algorithm that modifies generalized advantage estimation for temporally extended actions, allowing a state-of-the-art policy optimization algorithm to optimize policies in Dec-POMDPs in which agents act asynchronously. We show that our algorithm is capable of learning optimal policies in two cooperative domains, one involving real-time bus holding control and one involving wildfire fighting with unmanned aircraft. Our algorithm works by framing problems as ""event-driven decision processes,"" which are scenarios in which the sequence and timing of actions and events are random and governed by an underlying stochastic process. In addition to optimizing policies with continuous state and action spaces, our algorithm also facilitates the use of event-driven simulators, which do not require time to be discretized into time-steps. We demonstrate the benefit of using event-driven simulation in the context of multiple agents taking asynchronous actions. We show that fixed time-step simulation risks obfuscating the sequence in which closely separated events occur, adversely affecting the policies learned. In addition, we show that arbitrarily shrinking the time-step scales poorly with the number of agents.",Deep Reinforcement Learning for Event-Driven Multi-Agent Decision Processes,"Kunal Menda, Yi-Chun Chen, Justin Grana, James W. Bono, Brendan D. Tracey, Mykel J. Kochenderfer, and David Wolpert",2019,Artificial Intelligence,1709.06656
"We present PRM-RL, a hierarchical method for long-range navigation task completion that combines sampling based path planning with reinforcement learning (RL). The RL agents learn short-range, point-to-point navigation policies that capture robot dynamics and task constraints without knowledge of the large-scale topology. Next, the sampling-based planners provide roadmaps which connect robot configurations that can be successfully navigated by the RL agent. The same RL agents are used to control the robot under the direction of the planning, enabling long-range navigation. We use the Probabilistic Roadmaps (PRMs) for the sampling-based planner. The RL agents are constructed using feature-based and deep neural net policies in continuous state and action spaces. We evaluate PRM-RL, both in simulation and on-robot, on two navigation tasks with non-trivial robot dynamics: end-to-end differential drive indoor navigation in office environments, and aerial cargo delivery in urban environments with load displacement constraints. Our results show improvement in task completion over both RL agents on their own and traditional sampling-based planners. In the indoor navigation task, PRM-RL successfully completes up to 215 m long trajectories under noisy sensor conditions, and the aerial cargo delivery completes flights over 1000 m without violating the task constraints in an environment 63 million times larger than used in training.",PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-based Planning,"Aleksandra Faust, Oscar Ramirez, Marek Fiser, Kenneth Oslund, Anthony Francis, James Davidson, and Lydia Tapia",2018,Artificial Intelligence,1710.03937
"Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction. We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest. However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict. We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards). We call this consequentialist conditional cooperation. We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games. We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.",Consequentialist conditional cooperation in social dilemmas with imperfect information,"Alexander Peysakhovich, Adam Lerer",2018,Artificial Intelligence,1710.06975
"Little by little, newspapers are revealing the bright future that Artificial Intelligence (AI) is building. Intelligent machines will help everywhere. However, this bright future has a dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers will need financial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of AI is sustained by the biggest knowledge theft of the recent years. Learning AI machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, these workers are digging their own graves. In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI) as a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward aware and unaware knowledge producers with a different scheme: decisions of AI systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions. As modern Robin Hoods, HIT-AI researchers should fight for a fairer Artificial Intelligence that gives back what it steals.",Human-in-the-loop Artificial Intelligence,Fabio Massimo Zanzotto,2019,Artificial Intelligence,1710.08191
"The Advent of the Internet-of-Things (IoT) paradigm has brought opportunities to solve many real-world problems. Energy management, for example, has attracted huge interest from academia, industries, governments and regulatory bodies. It involves collecting energy usage data, analyzing it, and optimizing the energy consumption by applying control strategies. However, in industrial environments, performing such optimization is not trivial. The changes in business rules, process control, and customer requirements make it much more challenging. In this paper, a Semantic Rules Engine (SRE) for industrial gateways is presented that allows implementing dynamic and flexible rule-based control strategies. It is simple, expressive, and allows managing rules on-the-fly without causing any service interruption. Additionally, it can handle semantic queries and provide results by inferring additional knowledge from previously defined concepts in ontologies. SRE has been validated and tested on different hardware platforms and in commercial products. Performance evaluations are also presented to validate its conformance to the customer requirements.",SRE: Semantic Rules Engine For the Industrial Internet-Of-Things Gateways,"Charbel El Kaed, Imran Khan, Andre Van Den Berg, Hicham Hossayni and Christophe Saint-Marcel",2017,Artificial Intelligence,1710.09627
"The semantic web has received many contributions of researchers as ontologies which, in this context, i.e. within RDF linked data, are formalized conceptualizations that might use different protocols, such as RDFS, OWL DL and OWL FULL. In this article, we describe new expressive techniques which were found necessary after elaborating dozens of OWL ontologies for the scientific academy, the State and the civil society. They consist in: 1) stating possible uses a property might have without incurring into axioms or restrictions; 2) assigning a level of priority for an element (class, property, triple); 3) correct depiction in diagrams of relations between classes, between individuals which are imperative, and between individuals which are optional; 4) a convenient association between OWL classes and SKOS concepts. We propose specific rules to accomplish these enhancements and exemplify both its use and the difficulties that arise because these techniques are currently not established as standards to the ontology designer.",Enhancements of linked data expressiveness for ontologies,Renato Fabbri,2017,Artificial Intelligence,1710.09952
"There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.",Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR,"Sandra Wachter, Brent Mittelstadt, Chris Russell",2018,Artificial Intelligence,1711.00399
"In this paper we analyse the benefits of incorporating interval-valued fuzzy sets into the Bousi-Prolog system. A syntax, declarative semantics and im- plementation for this extension is presented and formalised. We show, by using potential applications, that fuzzy logic programming frameworks enhanced with them can correctly work together with lexical resources and ontologies in order to improve their capabilities for knowledge representation and reasoning.","On the incorporation of interval-valued fuzzy sets into the Bousi-Prolog system: declarative semantics, implementation and applications","Clemente Rubio-Manzano, Martin Pereira-Fari\~na",2018,Artificial Intelligence,1711.03147
"Cooperative multi-agent planning (MAP) is a relatively recent research field that combines technologies, algorithms and techniques developed by the Artificial Intelligence Planning and Multi-Agent Systems communities. While planning has been generally treated as a single-agent task, MAP generalizes this concept by considering multiple intelligent agents that work cooperatively to develop a course of action that satisfies the goals of the group. This paper reviews the most relevant approaches to MAP, putting the focus on the solvers that took part in the 2015 Competition of Distributed and Multi-Agent Planning, and classifies them according to their key features and relative performance.",Cooperative Multi-Agent Planning: A Survey,"Alejandro Torre\~no, Eva Onaindia, Anton\'in Komenda, Michal \v{S}tolba",2017,Artificial Intelligence,1711.09057
"Interval Pairwise Comparison Matrices have been widely used to account for uncertain statements concerning the preferences of decision makers. Several approaches have been proposed in the literature, such as multiplicative and fuzzy interval matrices. In this paper, we propose a general unified approach to Interval Pairwise Comparison Matrices, based on Abelian linearly ordered groups. In this framework, we generalize some consistency conditions provided for multiplicative and/or fuzzy interval pairwise comparison matrices and provide inclusion relations between them. Then, we provide a concept of distance between intervals that, together with a notion of mean defined over real continuous Abelian linearly ordered groups, allows us to provide a consistency index and an indeterminacy index. In this way, by means of suitable isomorphisms between Abelian linearly ordered groups, we will be able to compare the inconsistency and the indeterminacy of different kinds of Interval Pairwise Comparison Matrices, e.g. multiplicative, additive, and fuzzy, on a unique Cartesian coordinate system.",A general unified framework for interval pairwise comparison matrices,Bice Cavallo and Matteo Brunelli,2018,Artificial Intelligence,1711.09441
"Artificial Intelligence is a central topic in the computer science curriculum. From the year 2011 a project-based learning methodology based on computer games has been designed and implemented into the intelligence artificial course at the University of the Bio-Bio. The project aims to develop software-controlled agents (bots) which are programmed by using heuristic algorithms seen during the course. This methodology allows us to obtain good learning results, however several challenges have been founded during its implementation. In this paper we show how linguistic descriptions of data can help to provide students and teachers with technical and personalized feedback about the learned algorithms. Algorithm behavior profile and a new Turing test for computer games bots based on linguistic modelling of complex phenomena are also proposed in order to deal with such challenges. In order to show and explore the possibilities of this new technology, a web platform has been designed and implemented by one of authors and its incorporation in the process of assessment allows us to improve the teaching learning process.","How linguistic descriptions of data can help to the teaching-learning process in higher education, case of study: artificial intelligence","Clemente Rubio-Manzano, Tomas Lermanda Senoceain",2019,Artificial Intelligence,1711.09744
"Recently, model-free reinforcement learning algorithms have been shown to solve challenging problems by learning from extensive interaction with the environment. A significant issue with transferring this success to the robotics domain is that interaction with the real world is costly, but training on limited experience is prone to overfitting. We present a method for learning to navigate, to a fixed goal and in a known environment, on a mobile robot. The robot leverages an interactive world model built from a single traversal of the environment, a pre-trained visual feature encoder, and stochastic environmental augmentation, to demonstrate successful zero-shot transfer under real-world environmental variations without fine-tuning.",One-Shot Reinforcement Learning for Robot Navigation with Interactive Replay,"Jake Bruce, Niko Suenderhauf, Piotr Mirowski, Raia Hadsell, Michael Milford",2017,Artificial Intelligence,1711.10137
"Web-based human trafficking activity has increased in recent years but it remains sparsely dispersed among escort advertisements and difficult to identify due to its often-latent nature. The use of intelligent systems to detect trafficking can thus have a direct impact on investigative resource allocation and decision-making, and, more broadly, help curb a widespread social problem. Trafficking detection involves assigning a normalized score to a set of escort advertisements crawled from the Web -- a higher score indicates a greater risk of trafficking-related (involuntary) activities. In this paper, we define and study the problem of trafficking detection and present a trafficking detection pipeline architecture developed over three years of research within the DARPA Memex program. Drawing on multi-institutional data, systems, and experiences collected during this time, we also conduct post hoc bias analyses and present a bias mitigation plan. Our findings show that, while automatic trafficking detection is an important application of AI for social good, it also provides cautionary lessons for deploying predictive machine learning algorithms without appropriate de-biasing. This ultimately led to integration of an interpretable solution into a search system that contains over 100 million advertisements and is used by over 200 law enforcement agencies to investigate leads.",Always Lurking: Understanding and Mitigating Bias in Online Human Trafficking Detection,"Kyle Hundman, Thamme Gowda, Mayank Kejriwal, and Benedikt Boecking",2018,Artificial Intelligence,1712.00846
"Literature involving preferences of artificial agents or human beings often assume their preferences can be represented using a complete transitive binary relation. Much has been written however on different models of preferences. We review some of the reasons that have been put forward to justify more complex modeling, and review some of the techniques that have been proposed to obtain models of such preferences.",Reasons and Means to Model Preferences as Incomplete,"Olivier Cailloux (LAMSADE), S\'ebastien Destercke (Labex MS2T)",2017,Artificial Intelligence,1801.01657
"Artificial intelligence (AI) is an extensive scientific discipline which enables computer systems to solve problems by emulating complex biological processes such as learning, reasoning and self-correction. This paper presents a comprehensive review of the application of AI techniques for improving performance of optical communication systems and networks. The use of AI-based techniques is first studied in applications related to optical transmission, ranging from the characterization and operation of network components to performance monitoring, mitigation of nonlinearities, and quality of transmission estimation. Then, applications related to optical network control and management are also reviewed, including topics like optical network planning and operation in both transport and access networks. Finally, the paper also presents a summary of opportunities and challenges in optical networking where AI is expected to play a key role in the near future.",Artificial Intelligence (AI) Methods in Optical Networks: A Comprehensive Survey,"Javier Mata, Ignacio de Miguel, Ram\'o n J. Dur\'a n, Noem\'i Merayo, Sandeep Kumar Singh, Admela Jukan, Mohit Chamania",2018,Artificial Intelligence,1801.01704
"We propose a deep learning model - Probabilistic Prognostic Estimates of Survival in Metastatic Cancer Patients (PPES-Met) for estimating short-term life expectancy (3 months) of the patients by analyzing free-text clinical notes in the electronic medical record, while maintaining the temporal visit sequence. In a single framework, we integrated semantic data mapping and neural embedding technique to produce a text processing method that extracts relevant information from heterogeneous types of clinical notes in an unsupervised manner, and we designed a recurrent neural network to model the temporal dependency of the patient visits. The model was trained on a large dataset (10,293 patients) and validated on a separated dataset (1818 patients). Our method achieved an area under the ROC curve (AUC) of 0.89. To provide explain-ability, we developed an interactive graphical tool that may improve physician understanding of the basis for the model's predictions. The high accuracy and explain-ability of the PPES-Met model may enable our model to be used as a decision support tool to personalize metastatic cancer treatment and provide valuable assistance to the physicians.",Abstract: Probabilistic Prognostic Estimates of Survival in Metastatic Cancer Patients,"Imon Banerjee, Michael Francis Gensheimer, Douglas J. Wood, Solomon Henry, Daniel Chang, Daniel L. Rubin",2018,Artificial Intelligence,1801.03058
"Pairwise comparison matrices often exhibit inconsistency, therefore many indices have been suggested to measure their deviation from a consistent matrix. A set of axioms has been proposed recently that is required to be satisfied by any reasonable inconsistency index. This set seems to be not exhaustive as illustrated by an example, hence it is expanded by adding two new properties. All axioms are considered on the set of triads, pairwise comparison matrices with three alternatives, which is the simplest case of inconsistency. We choose the logically independent properties and prove that they characterize, that is, uniquely determine the inconsistency ranking induced by most inconsistency indices that coincide on this restricted domain. Since triads play a prominent role in a number of inconsistency indices, our results can also contribute to the measurement of inconsistency for pairwise comparison matrices with more than three alternatives.",Axiomatizations of inconsistency indices for triads,L\'aszl\'o Csat\'o,2019,Artificial Intelligence,1801.03355
"Humans use signs, e.g., sentences in a spoken language, for communication and thought. Hence, symbol systems like language are crucial for our communication with other agents and adaptation to our real-world environment. The symbol systems we use in our human society adaptively and dynamically change over time. In the context of artificial intelligence (AI) and cognitive systems, the symbol grounding problem has been regarded as one of the central problems related to {\it symbols}. However, the symbol grounding problem was originally posed to connect symbolic AI and sensorimotor information and did not consider many interdisciplinary phenomena in human communication and dynamic symbol systems in our society, which semiotics considered. In this paper, we focus on the symbol emergence problem, addressing not only cognitive dynamics but also the dynamics of symbol systems in society, rather than the symbol grounding problem. We first introduce the notion of a symbol in semiotics from the humanities, to leave the very narrow idea of symbols in symbolic AI. Furthermore, over the years, it became more and more clear that symbol emergence has to be regarded as a multifaceted problem. Therefore, secondly, we review the history of the symbol emergence problem in different fields, including both biological and artificial systems, showing their mutual relations. We summarize the discussion and provide an integrative viewpoint and comprehensive overview of symbol emergence in cognitive systems. Additionally, we describe the challenges facing the creation of cognitive systems that can be part of symbol emergence systems.",Symbol Emergence in Cognitive Developmental Systems: a Survey,"Tadahiro Taniguchi, Emre Ugur, Matej Hoffmann, Lorenzo Jamone, Takayuki Nagai, Benjamin Rosman, Toshihiko Matsuka, Naoto Iwahashi, Erhan Oztop, Justus Piater, Florentin W\""org\""otter",2019,Artificial Intelligence,1801.08829
"Semantic Web Rule Language (SWRL) combines OWL (Web Ontology Language) ontologies with Horn Logic rules of the Rule Markup Language (RuleML) family. Being supported by ontology editors, rule engines and ontology reasoners, it has become a very popular choice for developing rule-based applications on top of ontologies. However, SWRL is probably not go-ing to become a WWW Consortium standard, prohibiting industrial acceptance. On the other hand, SPIN (SPARQL Inferencing Notation) has become a de-facto industry standard to rep-resent SPARQL rules and constraints on Semantic Web models, building on the widespread acceptance of SPARQL (SPARQL Protocol and RDF Query Language). In this paper, we ar-gue that the life of existing SWRL rule-based ontology applications can be prolonged by con-verting them to SPIN. To this end, we have developed the SWRL2SPIN tool in Prolog that transforms SWRL rules into SPIN rules, considering the object-orientation of SPIN, i.e. linking rules to the appropriate ontology classes and optimizing them, as derived by analysing the rule conditions.",SWRL2SPIN: A tool for transforming SWRL rule bases in OWL ontologies to object-oriented SPIN rules,Nick Bassiliades,2020,Artificial Intelligence,1801.09061
"We consider a team of reinforcement learning agents that concurrently learn to operate in a common environment. We identify three properties - adaptivity, commitment, and diversity - which are necessary for efficient coordinated exploration and demonstrate that straightforward extensions to single-agent optimistic and posterior sampling approaches fail to satisfy them. As an alternative, we propose seed sampling, which extends posterior sampling in a manner that meets these requirements. Simulation results investigate how per-agent regret decreases as the number of agents grows, establishing substantial advantages of seed sampling over alternative exploration schemes.",Coordinated Exploration in Concurrent Reinforcement Learning,"Maria Dimakopoulou, Benjamin Van Roy",2018,Artificial Intelligence,1802.01282
"The Semantic Web aims at representing knowledge about the real world at web scale - things, their attributes and relationships among them can be represented as nodes and edges in an inter-linked semantic graph. In the presence of noisy data, as is typical of data on the Semantic Web, a software Agent needs to be able to robustly infer one or more associated actionable classes for the individuals in order to act automatically on it. We model this problem as a multi-label classification task where we want to robustly identify types of the individuals in a semantic graph such as DBpedia, which we use as an exemplary dataset on the Semantic Web. Our approach first extracts multiple features for the individuals using random walks and then performs multi-label classification using fully-connected Neural Networks. Through systematic exploration and experimentation, we identify the effect of hyper-parameters of the feature extraction and the fully-connected Neural Network structure on the classification performance. Our final results show that our method performs better than state-of-the-art inferencing systems like SDtype and SLCN, from which we can conclude that random-walk-based feature extraction of individuals and their multi-label classification using Deep Neural Networks is a promising alternative to these systems for type classification of individuals on the Semantic Web. The main contribution of our work is to introduce a novel approach that allows us to use Deep Neural Networks to identify types of individuals in a noisy semantic graph by extracting features using random walks",Classification of Things in DBpedia using Deep Neural Networks,Rahul Parundekar,2018,Artificial Intelligence,1802.02528
"We propose to use a supervised machine learning technique to track the location of a mobile agent in real time. Hidden Markov Models are used to build artificial intelligence that estimates the unknown position of a mobile target moving in a defined environment. This narrow artificial intelligence performs two distinct tasks. First, it provides real-time estimation of the mobile agent's position using the forward algorithm. Second, it uses the Baum-Welch algorithm as a statistical learning tool to gain knowledge of the mobile target. Finally, an experimental environment is proposed, namely a video game that we use to test our artificial intelligence. We present statistical and graphical results to illustrate the efficiency of our method.",Narrow Artificial Intelligence with Machine Learning for Real-Time Estimation of a Mobile Agents Location Using Hidden Markov Models,C\'edric Beaulac and Fabrice Larribe,2017,Artificial Intelligence,1802.03417
"The problem of detecting bots, automated social media accounts governed by software but disguising as human users, has strong implications. For example, bots have been used to sway political elections by distorting online discourse, to manipulate the stock market, or to push anti-vaccine conspiracy theories that caused health epidemics. Most techniques proposed to date detect bots at the account level, by processing large amount of social media posts, and leveraging information from network structure, temporal dynamics, sentiment analysis, etc. In this paper, we propose a deep neural network based on contextual long short-term memory (LSTM) architecture that exploits both content and metadata to detect bots at the tweet level: contextual features are extracted from user metadata and fed as auxiliary input to LSTM deep nets processing the tweet text. Another contribution that we make is proposing a technique based on synthetic minority oversampling to generate a large labeled dataset, suitable for deep nets training, from a minimal amount of labeled data (roughly 3,000 examples of sophisticated Twitter bots). We demonstrate that, from just one single tweet, our architecture can achieve high classification accuracy (AUC > 96%) in separating bots from humans. We apply the same architecture to account-level bot detection, achieving nearly perfect classification accuracy (AUC > 99%). Our system outperforms previous state of the art while leveraging a small and interpretable set of features yet requiring minimal training data.",Deep Neural Networks for Bot Detection,"Sneha Kudugunta, Emilio Ferrara",2018,Artificial Intelligence,1802.04289
"We address the problem of inferring the causal direction between two variables by comparing the least-squares errors of the predictions in both possible directions. Under the assumption of an independence between the function relating cause and effect, the conditional noise distribution, and the distribution of the cause, we show that the errors are smaller in causal direction if both variables are equally scaled and the causal relation is close to deterministic. Based on this, we provide an easily applicable algorithm that only requires a regression in both possible causal directions and a comparison of the errors. The performance of the algorithm is compared with various related causal inference methods in different artificial and real-world data sets.",Analysis of cause-effect inference by comparing regression errors,"Patrick Bl\""obaum, Dominik Janzing, Takashi Washio, Shohei Shimizu, Bernhard Sch\""olkopf",2019,Artificial Intelligence,1802.06698
"This paper proposes a class of well-conditioned neural networks in which a unit amount of change in the inputs causes at most a unit amount of change in the outputs or any of the internal layers. We develop the known methodology of controlling Lipschitz constants to realize its full potential in maximizing robustness, with a new regularization scheme for linear layers, new ways to adapt nonlinearities and a new loss function. With MNIST and CIFAR-10 classifiers, we demonstrate a number of advantages. Without needing any adversarial training, the proposed classifiers exceed the state of the art in robustness against white-box L2-bounded adversarial attacks. They generalize better than ordinary networks from noisy data with partially random labels. Their outputs are quantitatively meaningful and indicate levels of confidence and generalization, among other desirable properties.",L2-Nonexpansive Neural Networks,"Haifeng Qian, Mark N. Wegman",2019,Artificial Intelligence,1802.07896
"Reasoning systems with too simple a model of the world and human intent are unable to consider potential negative side effects of their actions and modify their plans to avoid them (e.g., avoiding potential errors). However, hand-encoding the enormous and subtle body of facts that constitutes common sense into a knowledge base has proved too difficult despite decades of work. Distributed semantic vector spaces learned from large text corpora, on the other hand, can learn representations that capture shades of meaning of common-sense concepts and perform analogical and associational reasoning in ways that knowledge bases are too rigid to perform, by encoding concepts and the relations between them as geometric structures. These have, however, the disadvantage of being unreliable, poorly understood, and biased in their view of the world by the source material. This chapter will discuss how these approaches may be combined in a way that combines the best properties of each for understanding the world and human intentions in a richer way.",Semantic Vector Spaces for Broadening Consideration of Consequences,Douglas Summers Stay,2017,Artificial Intelligence,1802.08554
"What makes humans so good at solving seemingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play. Videos and the game manipulations are available at https://rach0012.github.io/humanRL_website/",Investigating Human Priors for Playing Video Games,"Rachit Dubey, Pulkit Agrawal, Deepak Pathak, Thomas L. Griffiths, and Alexei A. Efros",2018,Artificial Intelligence,1802.10217
"The tasks that an agent will need to solve often are not known during training. However, if the agent knows which properties of the environment are important then, after learning how its actions affect those properties, it may be able to use this knowledge to solve complex tasks without training specifically for them. Towards this end, we consider a setup in which an environment is augmented with a set of user defined attributes that parameterize the features of interest. We propose a method that learns a policy for transitioning between ""nearby"" sets of attributes, and maintains a graph of possible transitions. Given a task at test time that can be expressed in terms of a target set of attributes, and a current state, our model infers the attributes of the current state and searches over paths through attribute space to get a high level plan, and then uses its low level policy to execute the plan. We show in 3D block stacking, grid-world games, and StarCraft that our model is able to generalize to longer, more complex tasks at test time by composing simpler learned policies.",Composable Planning with Attributes,"Amy Zhang, Adam Lerer, Sainbayar Sukhbaatar, Rob Fergus, Arthur Szlam",2018,Artificial Intelligence,1803.00512
"The 6th International Workshop on Theorem proving components for Educational software (ThEdu'17) was held in Gothenburg, Sweden, on 6 Aug 2017. It was associated to the conference CADE26. Topics of interest include: methods of automated deduction applied to checking students' input; methods of automated deduction applied to prove post-conditions for particular problem solutions; combinations of deduction and computation enabling systems to propose next steps; automated provers specific for dynamic geometry systems; proof and proving in mathematics education. ThEdu'17 was a vibrant workshop, with one invited talk and eight contributions. It triggered the post-proceedings at hand.",Proceedings 6th International Workshop on Theorem proving components for Educational software,"Pedro Quaresma (University of Coimbra), Walther Neuper (IICM at Graz University of Technology)",2018,Artificial Intelligence,1803.00722
"This paper describes a process for combining patterns and features, to guide a search process and make predictions. It is based on the functionality that a human brain might have, which is a highly distributed network of simple neuronal components that can apply some level of matching and cross-referencing over retrieved patterns. The process uses memory in a dynamic way and it is directed through the pattern matching. The paper firstly describes the mechanisms for neuronal search, memory and prediction. The paper then presents a formal language for defining cognitive processes, that is, pattern-based sequences and transitions. The language can define an outer framework for concept sets that are linked to perform the cognitive act. The language also has a mathematical basis, allowing for the rule construction to be consistent. Now, both static memory and dynamic process hierarchies can be built as tree structures. The new information can also be used to further integrate the cognitive model and the ensemble-hierarchy structure becomes an essential part. A theory about linking can suggest that nodes in different regions link together when generally they represent the same thing.",New Ideas for Brain Modelling 5,Kieran Greer,2021,Artificial Intelligence,1803.01690
"Highly automated driving requires precise models of traffic participants. Many state of the art models are currently based on machine learning techniques. Among others, the required amount of labeled data is one major challenge. An autonomous learning process addressing this problem is proposed. The initial models are iteratively refined in three steps: (1) detection and context identification, (2) novelty detection and active learning and (3) online model adaption.",Highly Automated Learning for Improved Active Safety of Vulnerable Road Users,"Maarten Bieshaar and G\""unther Reitberger and Viktor Kre{\ss} and Stefan Zernetsch and Konrad Doll and Erich Fuchs and Bernhard Sick",2017,Artificial Intelligence,1803.03479
"This article explores the ideas that went into George Boole's development of an algebra for logical inference in his book The Laws of Thought. We explore in particular his wife Mary Boole's claim that he was deeply influenced by Indian logic and argue that his work was more than a framework for processing propositions. By exploring parallels between his work and Indian logic, we are able to explain several peculiarities of this work.",On the Algebra in Boole's Laws of Thought,Subhash Kak,2018,Artificial Intelligence,1803.04994
"The authors present an overview of a hierarchical framework for coordinating task- and motion-level operations in multirobot systems. Their framework is based on the idea of using simple temporal networks to simultaneously reason about precedence/causal constraints required for task-level coordination and simple temporal constraints required to take some kinematic constraints of robots into account. In the plan-generation phase, the framework provides a computationally scalable method for generating plans that achieve high-level tasks for groups of robots and take some of their kinematic constraints into account. In the plan-execution phase, the framework provides a method for absorbing an imperfect plan execution to avoid time-consuming re-planning in many cases. The authors use the multirobot path-planning problem as a case study to present the key ideas behind their framework for the long-term autonomy of multirobot systems.",Overview: A Hierarchical Framework for Plan Generation and Execution in Multi-Robot Systems,"Hang Ma, Wolfgang H\""onig, Liron Cohen, Tansel Uras, Hong Xu, T. K. Satish Kumar, Nora Ayanian, Sven Koenig",2017,Artificial Intelligence,1804.00038
"Navigating through unstructured environments is a basic capability of intelligent creatures, and thus is of fundamental interest in the study and development of artificial intelligence. Long-range navigation is a complex cognitive task that relies on developing an internal representation of space, grounded by recognisable landmarks and robust visual processing, that can simultaneously support continuous self-localisation (""I am here"") and a representation of the goal (""I am going there""). Building upon recent research that applies deep reinforcement learning to maze navigation problems, we present an end-to-end deep reinforcement learning approach that can be applied on a city scale. Recognising that successful navigation relies on integration of general policies with locale-specific knowledge, we propose a dual pathway architecture that allows locale-specific features to be encapsulated, while still enabling transfer to multiple cities. We present an interactive navigation environment that uses Google StreetView for its photographic content and worldwide coverage, and demonstrate that our learning method allows agents to learn to navigate multiple cities and to traverse to target destinations that may be kilometres away. The project webpage http://streetlearn.cc contains a video summarising our research and showing the trained agent in diverse city environments and on the transfer task, the form to request the StreetLearn dataset and links to further resources. The StreetLearn environment code is available at https://github.com/deepmind/streetlearn",Learning to Navigate in Cities Without a Map,"Piotr Mirowski, Matthew Koichi Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, Koray Kavukcuoglu, Andrew Zisserman, Raia Hadsell",2018,Artificial Intelligence,1804.00168
Fuzzy relation equations (FRE)are associated with the composition of binary fuzzy relations. In the present work FRE are used as a tool for studying the process of learning a new subject matter by a student class. A classroom application and other csuitable examples connected to the student learning of the derivative are also presented illustrating our results and useful conclusions are obtained.,A Study of Student Learning Skills Using Fuzzy Relation Equations,Michael Gr. Voskoglou,2018,Artificial Intelligence,1804.00421
"The theory of grey systems plays an important role in science,engineering and in the everyday life in general for handling approximate data. In the present paper grey numbers are used as a tool for assessing with linguistic expressions the mean performance of a group of objects participating in a certain activity. Two applications to student and football player assessment are also presented illustrating our results.",Application of Grey Numbers to Assessment Processes,"Michael Gr. Voskoglou, Yiannis Theodorou",2017,Artificial Intelligence,1804.00423
"We implement a automated tactical prover TacticToe on top of the HOL4 interactive theorem prover. TacticToe learns from human proofs which mathematical technique is suitable in each proof situation. This knowledge is then used in a Monte Carlo tree search algorithm to explore promising tactic-level proof paths. On a single CPU, with a time limit of 60 seconds, TacticToe proves 66.4 percent of the 7164 theorems in HOL4's standard library, whereas E prover with auto-schedule solves 34.5 percent. The success rate rises to 69.0 percent by combining the results of TacticToe and E prover.",TacticToe: Learning to Prove with Tactics,"Thibault Gauthier, Cezary Kaliszyk, Josef Urban, Ramana Kumar, Michael Norrish",2021,Artificial Intelligence,1804.00596
"In the past several years, we have taken advantage of a number of opportunities to advance the intersection of next generation high-performance computing AI and big data technologies through partnerships in precision medicine. Today we are in the throes of piecing together what is likely the most unique convergence of medical data and computer technologies. But more deeply, we observe that the traditional paradigm of computer simulation and prediction needs fundamental revision. This is the time for a number of reasons. We will review what the drivers are, why now, how this has been approached over the past several years, and where we are heading.",Precision Medicine as an Accelerator for Next Generation Cognitive Supercomputing,"Edmon Begoli, Jim Brase, Bambi DeLaRosa, Penelope Jones, Dimitri Kusnezov, Jason Paragas, Rick Stevens, Fred Streitz, Georgia Tourassi",2018,Artificial Intelligence,1804.11002
"The sine-cosine algorithm (SCA) is a new population-based meta-heuristic algorithm. In addition to exploiting sine and cosine functions to perform local and global searches (hence the name sine-cosine), the SCA introduces several random and adaptive parameters to facilitate the search process. Although it shows promising results, the search process of the SCA is vulnerable to local minima/maxima due to the adoption of a fixed switch probability and the bounded magnitude of the sine and cosine functions (from -1 to 1). In this paper, we propose a new hybrid Q-learning sine-cosine- based strategy, called the Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the switching probability. Instead, we rely on the Q-learning algorithm (based on the penalty and reward mechanism) to dynamically identify the best operation during runtime. Additionally, we integrate two new operations (L\'evy flight motion and crossover) into the QLSCA to facilitate jumping out of local minima/maxima and enhance the solution diversity. To assess its performance, we adopt the QLSCA for the combinatorial test suite minimization problem. Experimental results reveal that the QLSCA is statistically superior with regard to test suite size reduction compared to recent state-of-the-art strategies, including the original SCA, the particle swarm test generator (PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search strategy (CS) at the 95% confidence level. However, concerning the comparison with discrete particle swarm optimization (DPSO), there is no significant difference in performance at the 95% confidence level. On a positive note, the QLSCA statistically outperforms the DPSO in certain configurations at the 90% confidence level.",A Hybrid Q-Learning Sine-Cosine-based Strategy for Addressing the Combinatorial Test Suite Minimization Problem,Kamal Z. Zamli and Fakhrud Din and Bestoun S. Ahmed and Miroslav Bures,2018,Artificial Intelligence,1805.00873
"In many applications that involve processing high-dimensional data, it is important to identify a small set of entities that account for a significant fraction of detections. Rather than formalize this as a clustering problem, in which all detections must be grouped into hard or soft categories, we formalize it as an instance of the frequent items or heavy hitters problem, which finds groups of tightly clustered objects that have a high density in the feature space. We show that the heavy hitters formulation generates solutions that are more accurate and effective than the clustering formulation. In addition, we present a novel online algorithm for heavy hitters, called HAC, which addresses problems in continuous space, and demonstrate its effectiveness on real video and household domains.",Finding Frequent Entities in Continuous Data,"Ferran Alet, Rohan Chitnis, Leslie P. Kaelbling, Tomas Lozano-Perez",2018,Artificial Intelligence,1805.02874
"End-to-end trained neural networks (NNs) are a compelling approach to autonomous vehicle control because of their ability to learn complex tasks without manual engineering of rule-based decisions. However, challenging road conditions, ambiguous navigation situations, and safety considerations require reliable uncertainty estimation for the eventual adoption of full-scale autonomous vehicles. Bayesian deep learning approaches provide a way to estimate uncertainty by approximating the posterior distribution of weights given a set of training data. Dropout training in deep NNs approximates Bayesian inference in a deep Gaussian process and can thus be used to estimate model uncertainty. In this paper, we propose a Bayesian NN for end-to-end control that estimates uncertainty by exploiting feature map correlation during training. This approach achieves improved model fits, as well as tighter uncertainty estimates, than traditional element-wise dropout. We evaluate our algorithms on a challenging dataset collected over many different road types, times of day, and weather conditions, and demonstrate how uncertainties can be used in conjunction with a human controller in a parallel autonomous setting.",Spatial Uncertainty Sampling for End-to-End Control,"Alexander Amini, Ava Soleimany, Sertac Karaman, Daniela Rus",2018,Artificial Intelligence,1805.04829
"Structural Causal Models (SCMs) provide a popular causal modeling framework. In this work, we show that SCMs are not flexible enough to give a complete causal representation of dynamical systems at equilibrium. Instead, we propose a generalization of the notion of an SCM, that we call Causal Constraints Model (CCM), and prove that CCMs do capture the causal semantics of such systems. We show how CCMs can be constructed from differential equations and initial conditions and we illustrate our ideas further on a simple but ubiquitous (bio)chemical reaction. Our framework also allows to model functional laws, such as the ideal gas law, in a sensible and intuitive way.",Beyond Structural Causal Models: Causal Constraints Models,"Tineke Blom, Stephan Bongers, Joris M. Mooij",2019,Artificial Intelligence,1805.06539
"{Radio Frequency Identification technology has gained popularity for cheap and easy deployment. In the realm of manufacturing shopfloor, it can be used to track the location of manufacturing objects to achieve better efficiency. The underlying challenge of localization lies in the non-stationary characteristics of manufacturing shopfloor which calls for an adaptive life-long learning strategy in order to arrive at accurate localization results. This paper presents an evolving model based on a novel evolving intelligent system, namely evolving Type-2 Quantum Fuzzy Neural Network (eT2QFNN), which features an interval type-2 quantum fuzzy set with uncertain jump positions. The quantum fuzzy set possesses a graded membership degree which enables better identification of overlaps between classes. The eT2QFNN works fully in the evolving mode where all parameters including the number of rules are automatically adjusted and generated on the fly. The parameter adjustment scenario relies on decoupled extended Kalman filter method. Our numerical study shows that eT2QFNN is able to deliver comparable accuracy compared to state-of-the-art algorithms.",An Online RFID Localization in the Manufacturing Shopfloor,"Andri Ashfahani, Mahardhika Pratama, Edwin Lughofer, Qing Cai, and Huang Sheng",2019,Artificial Intelligence,1805.07715
"The Sensor, Observation, Sample, and Actuator (SOSA) ontology provides a formal but lightweight general-purpose specification for modeling the interaction between the entities involved in the acts of observation, actuation, and sampling. SOSA is the result of rethinking the W3C-XG Semantic Sensor Network (SSN) ontology based on changes in scope and target audience, technical developments, and lessons learned over the past years. SOSA also acts as a replacement of SSN's Stimulus Sensor Observation (SSO) core. It has been developed by the first joint working group of the Open Geospatial Consortium (OGC) and the World Wide Web Consortium (W3C) on \emph{Spatial Data on the Web}. In this work, we motivate the need for SOSA, provide an overview of the main classes and properties, and briefly discuss its integration with the new release of the SSN ontology as well as various other alignments to specifications such as OGC's Observations and Measurements (O\&M), Dolce-Ultralite (DUL), and other prominent ontologies. We will also touch upon common modeling problems and application areas related to publishing and searching observation, sampling, and actuation data on the Web. The SOSA ontology and standard can be accessed at \url{https://www.w3.org/TR/vocab-ssn/}.","SOSA: A Lightweight Ontology for Sensors, Observations, Samples, and Actuators","Krzysztof Janowicz, Armin Haller, Simon J D Cox, Danh Le Phuoc, Maxime Lefrancois",2018,Artificial Intelligence,1805.09979
"We consider the problem of how to improve automatic target recognition by fusing the naive sensor-level classification decisions with ""intuition,"" or context, in a mathematically principled way. This is a general approach that is compatible with many definitions of context, but for specificity, we consider context as co-occurrence in imagery. In particular, we consider images that contain multiple objects identified at various confidence levels. We learn the patterns of co-occurrence in each context, then use these patterns as hyper-parameters for a Hierarchical Bayesian Model. The result is that low-confidence sensor classification decisions can be dramatically improved by fusing those readings with context. We further use hyperpriors to address the case where multiple contexts may be appropriate. We also consider the Bayesian Network, an alternative to the Hierarchical Bayesian Model, which is computationally more efficient but assumes that context and sensor readings are uncorrelated.",Context Exploitation using Hierarchical Bayesian Models,"Christopher A. George, Pranab Banerjee, Kendra E. Moore",2018,Artificial Intelligence,1805.12183
"During the 60s and 70s, AI researchers explored intuitions about intelligence by writing programs that displayed intelligent behavior. Many good ideas came out from this work but programs written by hand were not robust or general. After the 80s, research increasingly shifted to the development of learners capable of inferring behavior and functions from experience and data, and solvers capable of tackling well-defined but intractable models like SAT, classical planning, Bayesian networks, and POMDPs. The learning approach has achieved considerable success but results in black boxes that do not have the flexibility, transparency, and generality of their model-based counterparts. Model-based approaches, on the other hand, require models and scalable algorithms. Model-free learners and model-based solvers have close parallels with Systems 1 and 2 in current theories of the human mind: the first, a fast, opaque, and inflexible intuitive mind; the second, a slow, transparent, and flexible analytical mind. In this paper, I review developments in AI and draw on these theories to discuss the gap between model-free learners and model-based solvers, a gap that needs to be bridged in order to have intelligent systems that are robust and general.","Model-free, Model-based, and General Intelligence",Hector Geffner,2018,Artificial Intelligence,1806.02308
"Machine learning practitioners are often ambivalent about the ethical aspects of their products. We believe anything that gets us from that current state to one in which our systems are achieving some degree of fairness is an improvement that should be welcomed. This is true even when that progress does not get us 100% of the way to the goal of ""complete"" fairness or perfectly align with our personal belief on which measure of fairness is used. Some measure of fairness being built would still put us in a better position than the status quo. Impediments to getting fairness and ethical concerns applied in real applications, whether they are abstruse philosophical debates or technical overhead such as the introduction of ever more hyper-parameters, should be avoided. In this paper we further elaborate on our argument for this viewpoint and its importance.",What About Applied Fairness?,"Jared Sylvester, Edward Raff",2018,Artificial Intelligence,1806.05250
"In many real-world problems, there is the possibility to configure, to a limited extent, some environmental parameters to improve the performance of a learning agent. In this paper, we propose a novel framework, Configurable Markov Decision Processes (Conf-MDPs), to model this new type of interaction with the environment. Furthermore, we provide a new learning algorithm, Safe Policy-Model Iteration (SPMI), to jointly and adaptively optimize the policy and the environment configuration. After having introduced our approach and derived some theoretical results, we present the experimental evaluation in two explicative problems to show the benefits of the environment configurability on the performance of the learned policy.",Configurable Markov Decision Processes,"Alberto Maria Metelli, Mirco Mutti and Marcello Restelli",2018,Artificial Intelligence,1806.05415
"The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult. Bayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs) extend POMDPs to allow the model to be learned during execution. BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between exploitation and exploration. Unfortunately, BA-POMDPs are currently impractical to solve for any non-trivial domain. In this paper, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle problems that previous solution methods have been unable to solve. Additionally, we introduce several techniques that exploit the BA-POMDP structure to improve the efficiency of BA-POMCP along with proof of their convergence.",Learning in POMDPs with Monte Carlo Tree Search,"Sammie Katt, Frans A. Oliehoek, Christopher Amato",2017,Artificial Intelligence,1806.05631
"The well-known Late Acceptance Hill Climbing (LAHC) search aims to overcome the main downside of traditional Hill Climbing (HC) search, which is often quickly trapped in a local optimum due to strictly accepting only non-worsening moves within each iteration. In contrast, LAHC also accepts worsening moves, by keeping a circular array of fitness values of previously visited solutions and comparing the fitness values of candidate solutions against the least recent element in the array. While this straightforward strategy has proven effective, there are nevertheless situations where LAHC can unfortunately behave in a similar manner to HC. For example, when a new local optimum is found, often the same fitness value is stored many times in the array. To address this shortcoming, we propose new acceptance and replacement strategies to take into account worsening, improving, and sideways movement scenarios with the aim to improve the diversity of values in the array. Compared to LAHC, the proposed Diversified Late Acceptance Search approach is shown to lead to better quality solutions that are obtained with a lower number of iterations on benchmark Travelling Salesman Problems and Quadratic Assignment Problems.",Diversified Late Acceptance Search,"Majid Namazi, Conrad Sanderson, M.A. Hakim Newton, M.M.A. Polash, Abdul Sattar",2018,Artificial Intelligence,1806.09328
"Fully observable non-deterministic (FOND) planning is becoming increasingly important as an approach for computing proper policies in probabilistic planning, extended temporal plans in LTL planning, and general plans in generalized planning. In this work, we introduce a SAT encoding for FOND planning that is compact and can produce compact strong cyclic policies. Simple variations of the encodings are also introduced for strong planning and for what we call, dual FOND planning, where some non-deterministic actions are assumed to be fair (e.g., probabilistic) and others unfair (e.g., adversarial). The resulting FOND planners are compared empirically with existing planners over existing and new benchmarks. The notion of ""probabilistic interesting problems"" is also revisited to yield a more comprehensive picture of the strengths and limitations of current FOND planners and the proposed SAT approach.",Compact Policies for Fully-Observable Non-Deterministic Planning as SAT,Tomas Geffner and Hector Geffner,2018,Artificial Intelligence,1806.09455
"Combinatorial preference aggregation has many applications in AI. Given the exponential nature of these preferences, compact representations are needed and ($m$)CP-nets are among the most studied ones. Sequential and global voting are two ways to aggregate preferences over CP-nets. In the former, preferences are aggregated feature-by-feature. Hence, when preferences have specific feature dependencies, sequential voting may exhibit voting paradoxes, i.e., it might select sub-optimal outcomes. To avoid paradoxes in sequential voting, one has often assumed the $\mathcal{O}$-legality restriction, which imposes a shared topological order among all the CP-nets. On the contrary, in global voting, CP-nets are considered as a whole during preference aggregation. For this reason, global voting is immune from paradoxes, and there is no need to impose restrictions over the CP-nets' topological structure. Sequential voting over $\mathcal{O}$-legal CP-nets has extensively been investigated. On the other hand, global voting over non-$\mathcal{O}$-legal CP-nets has not carefully been analyzed, despite it was stated in the literature that a theoretical comparison between global and sequential voting was promising and a precise complexity analysis for global voting has been asked for multiple times. In quite few works, very partial results on the complexity of global voting over CP-nets have been given. We start to fill this gap by carrying out a thorough complexity analysis of Pareto and majority global voting over not necessarily $\mathcal{O}$-legal acyclic binary polynomially connected (m)CP-nets. We settle these problems in the polynomial hierarchy, and some of them in PTIME or LOGSPACE, whereas EXPTIME was the previously known upper bound for most of them. We show various tight lower bounds and matching upper bounds for problems that up to date did not have any explicit non-obvious lower bound.",Complexity Results for Preference Aggregation over (m)CP-nets: Pareto and Majority Voting,"Thomas Lukasiewicz, Enrico Malizia",2019,Artificial Intelligence,1806.10018
"Human posture recognition provides a dynamic field that has produced many methods. Using fuzzy subsets based data fusion methods to aggregate the results given by different types of recognition processes is a convenient way to improve recognition methods. Nevertheless, choosing a defuzzification method to imple-ment the decision is a crucial point of this approach. The goal of this paper is to present an approach where the choice of the defuzzification method is driven by the constraints of the final data user, which are expressed as limitations on indica-tors like confidence or accuracy. A practical experimentation illustrating this ap-proach is presented: from a depth camera sensor, human posture is interpreted and the defuzzification method is selected in accordance with the constraints of the final information consumer. The paper illustrates the interest of the approach in a context of postures based human robot communication.",Decision method choice in a human posture recognition context,"St\'ephane Perrin (LISTIC), Eric Benoit (LISTIC), Didier Coquin (LISTIC)",2018,Artificial Intelligence,1807.04170
"This paper introduces a fully automatic method for generating video game tutorials. The AtDELFI system (AuTomatically DEsigning Legible, Full Instructions for games) was created to investigate procedural generation of instructions that teach players how to play video games. We present a representation of game rules and mechanics using a graph system as well as a tutorial generation method that uses said graph representation. We demonstrate the concept by testing it on games within the General Video Game Artificial Intelligence (GVG-AI) framework; the paper discusses tutorials generated for eight different games. Our findings suggest that a graph representation scheme works well for simple arcade style games such as Space Invaders and Pacman, but it appears that tutorials for more complex games might require higher-level understanding of the game than just single mechanics.","AtDelfi: Automatically Designing Legible, Full Instructions For Games","Michael Cerny Green, Ahmed Khalifa, Gabriella A.B. Barros, Tiago Machado, Andy Nealen and Julian Togelius",2018,Artificial Intelligence,1807.04375
"We introduce a new generative model for human planning under the Bayesian Inverse Reinforcement Learning (BIRL) framework which takes into account the fact that humans often plan using hierarchical strategies. We describe the Bayesian Inverse Hierarchical RL (BIHRL) algorithm for inferring the values of hierarchical planners, and use an illustrative toy model to show that BIHRL retains accuracy where standard BIRL fails. Furthermore, BIHRL is able to accurately predict the goals of `Wikispeedia' game players, with inclusion of hierarchical structure in the model resulting in a large boost in accuracy. We show that BIHRL is able to significantly outperform BIRL even when we only have a weak prior on the hierarchical structure of the plans available to the agent, and discuss the significant challenges that remain for scaling up this framework to more realistic settings.",Exploring Hierarchy-Aware Inverse Reinforcement Learning,"Chris Cundy, Daniel Filan",2018,Artificial Intelligence,1807.05037
"It is the focus of this work to extend and study the previously proposed quantum-like Bayesian networks to deal with decision-making scenarios by incorporating the notion of maximum expected utility in influence diagrams. The general idea is to take advantage of the quantum interference terms produced in the quantum-like Bayesian Network to influence the probabilities used to compute the expected utility of some action. This way, we are not proposing a new type of expected utility hypothesis. On the contrary, we are keeping it under its classical definition. We are only incorporating it as an extension of a probabilistic graphical model in a compact graphical representation called an influence diagram in which the utility function depends on the probabilistic influences of the quantum-like Bayesian network. Our findings suggest that the proposed quantum-like influence digram can indeed take advantage of the quantum interference effects of quantum-like Bayesian Networks to maximise the utility of a cooperative behaviour in detriment of a fully rational defect behaviour under the prisoner's dilemma game.",Introducing Quantum-Like Influence Diagrams for Violations of the Sure Thing Principle,Catarina Moreira and Andreas Wichert,2018,Artificial Intelligence,1807.06142
Recommendation systems are an integral part of Artificial Intelligence (AI) and have become increasingly important in the growing age of commercialization in AI. Deep learning (DL) techniques for recommendation systems (RS) provide powerful latent-feature models for effective recommendation but suffer from the major drawback of being non-interpretable. In this paper we describe a framework for explainable temporal recommendations in a DL model. We consider an LSTM based Recurrent Neural Network (RNN) architecture for recommendation and a neighbourhood-based scheme for generating explanations in the model. We demonstrate the effectiveness of our approach through experiments on the Netflix dataset by jointly optimizing for both prediction accuracy and explainability.,Explanations for Temporal Recommendations,"Homanga Bharadhwaj, Shruti Joshi",2018,Artificial Intelligence,1807.06161
"Monte Carlo tree search (MCTS) is a popular choice for solving sequential anytime problems. However, it depends on a numeric feedback signal, which can be difficult to define. Real-time MCTS is a variant which may only rarely encounter states with an explicit, extrinsic reward. To deal with such cases, the experimenter has to supply an additional numeric feedback signal in the form of a heuristic, which intrinsically guides the agent. Recent work has shown evidence that in different areas the underlying structure is ordinal and not numerical. Hence erroneous and biased heuristics are inevitable, especially in such domains. In this paper, we propose a MCTS variant which only depends on qualitative feedback, and therefore opens up new applications for MCTS. We also find indications that translating absolute into ordinal feedback may be beneficial. Using a puzzle domain, we show that our preference-based MCTS variant, wich only receives qualitative feedback, is able to reach a performance level comparable to a regular MCTS baseline, which obtains quantitative feedback.",Preference-Based Monte Carlo Tree Search,"Tobias Joppen, Christian Wirth, and Johannes F\""urnkranz",2018,Artificial Intelligence,1807.06286
"We present the design of a competitive artificial intelligence for Scopone, a popular Italian card game. We compare rule-based players using the most established strategies (one for beginners and two for advanced players) against players using Monte Carlo Tree Search (MCTS) and Information Set Monte Carlo Tree Search (ISMCTS) with different reward functions and simulation strategies. MCTS requires complete information about the game state and thus implements a cheating player while ISMCTS can deal with incomplete information and thus implements a fair player. Our results show that, as expected, the cheating MCTS outperforms all the other strategies; ISMCTS is stronger than all the rule-based players implementing well-known and most advanced strategies and it also turns out to be a challenging opponent for human players.",Traditional Wisdom and Monte Carlo Tree Search Face-to-Face in the Card Game Scopone,Stefano Di Palma and Pier Luca Lanzi,2018,Artificial Intelligence,1807.06813
"Many distributed machine learning frameworks have recently been built to speed up the large-scale data learning process. However, most distributed machine learning used in these frameworks still uses an offline algorithm model which cannot cope with the data stream problems. In fact, large-scale data are mostly generated by the non-stationary data stream where its pattern evolves over time. To address this problem, we propose a novel Evolving Large-scale Data Stream Analytics framework based on a Scalable Parsimonious Network based on Fuzzy Inference System (Scalable PANFIS), where the PANFIS evolving algorithm is distributed over the worker nodes in the cloud to learn large-scale data stream. Scalable PANFIS framework incorporates the active learning (AL) strategy and two model fusion methods. The AL accelerates the distributed learning process to generate an initial evolving large-scale data stream model (initial model), whereas the two model fusion methods aggregate an initial model to generate the final model. The final model represents the update of current large-scale data knowledge which can be used to infer future data. Extensive experiments on this framework are validated by measuring the accuracy and running time of four combinations of Scalable PANFIS and other Spark-based built in algorithms. The results indicate that Scalable PANFIS with AL improves the training time to be almost two times faster than Scalable PANFIS without AL. The results also show both rule merging and the voting mechanisms yield similar accuracy in general among Scalable PANFIS algorithms and they are generally better than Spark-based algorithms. In terms of running time, the Scalable PANFIS training time outperforms all Spark-based algorithms when classifying numerous benchmark datasets.",Evolving Large-Scale Data Stream Analytics based on Scalable PANFIS,"Mahardhika Pratama, Choiru Za'in, Eric Pardede",2018,Artificial Intelligence,1807.06996
"Unmanned aerial vehicles (UAVs), also known as drones, have emerged as a promising mode of fast, energy-efficient, and cost-effective package delivery. A considerable number of works have studied different aspects of drone package delivery service by a supplier, one of which is delivery planning. However, existing works addressing the planning issues consider a simple case of perfect delivery without service interruption, e.g., due to accident which is common and realistic. Therefore, this paper introduces the joint ground and aerial delivery service optimization and planning (GADOP) framework. The framework explicitly incorporates uncertainty of drone package delivery, i.e., takeoff and breakdown conditions. The GADOP framework aims to minimize the total delivery cost given practical constraints, e.g., traveling distance limit. Specifically, we formulate the GADOP framework as a three-stage stochastic integer programming model. To deal with the high complexity issue of the problem, a decomposition method is adopted. Then, the performance of the GADOP framework is evaluated by using two data sets including Solomon benchmark suite and the real data from one of the Singapore logistics companies. The performance evaluation clearly shows that the GADOP framework can achieve significantly lower total payment than that of the baseline methods which do not take uncertainty into account.",Joint Ground and Aerial Package Delivery Services: A Stochastic Optimization Approach,"Suttinee Sawadsitang, Dusit Niyato, Puay-Siew Tan, and Ping Wang",2018,Artificial Intelligence,1808.04617
"Approaches to decision-making under uncertainty in the belief function framework are reviewed. Most methods are shown to blend criteria for decision under ignorance with the maximum expected utility principle of Bayesian decision theory. A distinction is made between methods that construct a complete preference relation among acts, and those that allow incomparability of some acts due to lack of information. Methods developed in the imprecise probability framework are applicable in the Dempster-Shafer context and are also reviewed. Shafer's constructive decision theory, which substitutes the notion of goal for that of utility, is described and contrasted with other approaches. The paper ends by pointing out the need to carry out deeper investigation of fundamental issues related to decision-making with belief functions and to assess the descriptive, normative and prescriptive values of the different approaches.",Decision-Making with Belief Functions: a Review,Thierry Denoeux,2019,Artificial Intelligence,1808.05322
"With the increasing need of personalised decision making, such as personalised medicine and online recommendations, a growing attention has been paid to the discovery of the context and heterogeneity of causal relationships. Most existing methods, however, assume a known cause (e.g. a new drug) and focus on identifying from data the contexts of heterogeneous effects of the cause (e.g. patient groups with different responses to the new drug). There is no approach to efficiently detecting directly from observational data context specific causal relationships, i.e. discovering the causes and their contexts simultaneously. In this paper, by taking the advantages of highly efficient decision tree induction and the well established causal inference framework, we propose the Tree based Context Causal rule discovery (TCC) method, for efficient exploration of context specific causal relationships from data. Experiments with both synthetic and real world data sets show that TCC can effectively discover context specific causal rules from the data.",Discovering Context Specific Causal Relationships,"Saisai Ma, Jiuyong Li, Lin Liu, Thuc Duy Le",2019,Artificial Intelligence,1808.06316
"Artificial intelligence (AI) is the core technology of technological revolution and industrial transformation. As one of the new intelligent needs in the AI 2.0 era, financial intelligence has elicited much attention from the academia and industry. In our current dynamic capital market, financial intelligence demonstrates a fast and accurate machine learning capability to handle complex data and has gradually acquired the potential to become a ""financial brain"". In this work, we survey existing studies on financial intelligence. First, we describe the concept of financial intelligence and elaborate on its position in the financial technology field. Second, we introduce the development of financial intelligence and review state-of-the-art techniques in wealth management, risk management, financial security, financial consulting, and blockchain. Finally, we propose a research framework called FinBrain and summarize four open issues, namely, explainable financial agents and causality, perception and prediction under uncertainty, risk-sensitive and robust decision making, and multi-agent game and mechanism design. We believe that these research directions can lay the foundation for the development of AI 2.0 in the finance field.",FinBrain: When Finance Meets AI 2.0,"Xiaolin Zheng, Mengying Zhu, Qibing Li, Chaochao Chen, Yanchao Tan",2018,Artificial Intelligence,1808.08497
"Developing visual perception models for active agents and sensorimotor control are cumbersome to be done in the physical world, as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we are concerned with the problem of developing real-world perception for active agents, propose Gibson Virtual Environment for this purpose, and showcase sample perceptual tasks learned therein. Gibson is based on virtualizing real spaces, rather than using artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism, ""Goggles"", enabling deploying the trained models in real-world without needing further domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.",Gibson Env: Real-World Perception for Embodied Agents,"Fei Xia, Amir Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik, Silvio Savarese",2018,Artificial Intelligence,1808.10654
"Using a game engine, we have developed a virtual environment which models important aspects of critical incident scenarios. We focused on modelling phenomena relating to the identification and gathering of key forensic evidence, in order to develop and test a system which can handle chemical, biological, radiological/nuclear or explosive (CBRNe) events autonomously. This allows us to build and validate AI-based technologies, which can be trained and tested in our custom virtual environment before being deployed in real-world scenarios. We have used our virtual scenario to rapidly prototype a system which can use simulated Remote Aerial Vehicles (RAVs) to gather images from the environment for the purpose of mapping. Our environment provides us with an effective medium through which we can develop and test various AI methodologies for critical incident scene assessment, in a safe and controlled manner",Using a Game Engine to Simulate Critical Incidents and Data Collection by Autonomous Drones,"David L. Smyth, Frank G. Glavin, Michael G. Madden",2018,Artificial Intelligence,1808.10784
"This paper introduces an information-theoretic method for selecting a subset of problems which gives the most information about a group of problem-solving algorithms. This method was tested on the games in the General Video Game AI (GVGAI) framework, allowing us to identify a smaller set of games that still gives a large amount of information about the abilities of different game-playing agents. This approach can be used to make agent testing more efficient. We can achieve almost as good discriminatory accuracy when testing on only a handful of games as when testing on more than a hundred games, something which is often computationally infeasible. Furthermore, this method can be extended to study the dimensions of the effective variance in game design between these games, allowing us to identify which games differentiate between agents in the most complementary ways.",A Continuous Information Gain Measure to Find the Most Discriminatory Problems for AI Benchmarking,"Matthew Stephenson, Damien Anderson, Ahmed Khalifa, John Levine, Jochen Renz, Julian Togelius, Christoph Salge",2020,Artificial Intelligence,1809.02904
"As an exquisite and concise literary form, poetry is a gem of human culture. Automatic poetry generation is an essential step towards computer creativity. In recent years, several neural models have been designed for this task. However, among lines of a whole poem, the coherence in meaning and topics still remains a big challenge. In this paper, inspired by the theoretical concept in cognitive psychology, we propose a novel Working Memory model for poetry generation. Different from previous methods, our model explicitly maintains topics and informative limited history in a neural memory. During the generation process, our model reads the most relevant parts from memory slots to generate the current line. After each line is generated, it writes the most salient parts of the previous line into memory slots. By dynamic manipulation of the memory, our model keeps a coherent information flow and learns to express each topic flexibly and naturally. We experiment on three different genres of Chinese poetry: quatrain, iambic and chinoiserie lyric. Both automatic and human evaluation results show that our model outperforms current state-of-the-art methods.",Chinese Poetry Generation with a Working Memory Model,"Xiaoyuan Yi, Maosong Sun, Ruoyu Li, Zonghan Yang",2018,Artificial Intelligence,1809.04306
"When modeling real world domains we have to deal with information that is incomplete or that comes from sources with different trust levels. This motivates the need for managing uncertainty in the Semantic Web. To this purpose, we introduced a probabilistic semantics, named DISPONTE, in order to combine description logics with probability theory. The probability of a query can be then computed from the set of its explanations by building a Binary Decision Diagram (BDD). The set of explanations can be found using the tableau algorithm, which has to handle non-determinism. Prolog, with its efficient handling of non-determinism, is suitable for implementing the tableau algorithm. TRILL and TRILLP are systems offering a Prolog implementation of the tableau algorithm. TRILLP builds a pinpointing formula, that compactly represents the set of explanations and can be directly translated into a BDD. Both reasoners were shown to outperform state-of-the-art DL reasoners. In this paper, we present an improvement of TRILLP, named TORNADO, in which the BDD is directly built during the construction of the tableau, further speeding up the overall inference process. An experimental comparison shows the effectiveness of TORNADO. All systems can be tried online in the TRILL on SWISH web application at http://trill.ml.unife.it/.",Probabilistic DL Reasoning with Pinpointing Formulas: A Prolog-based Approach,"Riccardo Zese, Giuseppe Cota, Evelina Lamma, Elena Bellodi, Fabrizio Riguzzi",2019,Artificial Intelligence,1809.06180
"Autonomous robotics and artificial intelligence techniques can be used to support human personnel in the event of critical incidents. These incidents can pose great danger to human life. Some examples of such assistance include: multi-robot surveying of the scene; collection of sensor data and scene imagery, real-time risk assessment and analysis; object identification and anomaly detection; and retrieval of relevant supporting documentation such as standard operating procedures (SOPs). These incidents, although often rare, can involve chemical, biological, radiological/nuclear or explosive (CBRNE) substances and can be of high consequence. Real-world training and deployment of these systems can be costly and sometimes not feasible. For this reason, we have developed a realistic 3D model of a CBRNE scenario to act as a testbed for an initial set of assisting AI tools that we have developed.","A Virtual Testbed for Critical Incident Investigation with Autonomous Remote Aerial Vehicle Surveying, Artificial Intelligence, and Decision Support","David L. Smyth, Sai Abinesh, Nazli B. Karimi, Brett Drury, Ihsan Ullah, Frank G. Glavin, Michael G. Madden",2018,Artificial Intelligence,1809.06244
"The growing influence and decision-making capacities of Autonomous systems and Artificial Intelligence in our lives force us to consider the values embedded in these systems. But how ethics should be implemented into these systems? In this study, the solution is seen on philosophical conceptualization as a framework to form practical implementation model for ethics of AI. To take the first steps on conceptualization main concepts used on the field needs to be identified. A keyword based Systematic Mapping Study (SMS) on the keywords used in AI and ethics was conducted to help in identifying, defying and comparing main concepts used in current AI ethics discourse. Out of 1062 papers retrieved SMS discovered 37 re-occurring keywords in 83 academic papers. We suggest that the focus on finding keywords is the first step in guiding and providing direction for future research in the AI ethics field.",The Key Concepts of Ethics of Artificial Intelligence - A Keyword based Systematic Mapping Study,Ville Vakkuri and Pekka Abrahamsson,2018,Artificial Intelligence,1809.07027
"The Winograd Schema (WS) challenge, proposed as an al-ternative to the Turing Test, has become the new standard for evaluating progress in natural language understanding (NLU). In this paper we will not however be concerned with how this challenge might be addressed. Instead, our aim here is threefold: (i) we will first formally 'situate' the WS challenge in the data-information-knowledge continuum, suggesting where in that continuum a good WS resides; (ii) we will show that a WS is just special case of a more general phenomenon in language understanding, namely the missing text phenomenon (henceforth, MTP) - in particular, we will argue that what we usually call thinking in the process of language understanding involves discovering a significant amount of 'missing text' - text that is not explicitly stated, but is often implicitly assumed as shared background knowledge; and (iii) we conclude by a brief discussion on why MTP is inconsistent with the data-driven and machine learning approach to language understanding.",On the Winograd Schema: Situating Language Understanding in the Data-Information-Knowledge Continuum,Walid S. Saba,2019,Artificial Intelligence,1810.00324
"This paper introduces DATA Agent, a system which creates murder mystery adventures from open data. In the game, the player takes on the role of a detective tasked with finding the culprit of a murder. All characters, places, and items in DATA Agent games are generated using open data as source content. The paper discusses the general game design and user interface of DATA Agent, and provides details on the generative algorithms which transform linked data into different game objects. Findings from a user study with 30 participants playing through two games of DATA Agent show that the game is easy and fun to play, and that the mysteries it generates are straightforward to solve.",DATA Agent,"Michael Cerny Green, Gabriella A.B. Barros, Antonios Liapis, Julian Togelius",2018,Artificial Intelligence,1810.02251
"Perceiving the surrounding environment in terms of objects is useful for any general purpose intelligent agent. In this paper, we investigate a fundamental mechanism making object perception possible, namely the identification of spatio-temporally invariant structures in the sensorimotor experience of an agent. We take inspiration from the Sensorimotor Contingencies Theory to define a computational model of this mechanism through a sensorimotor, unsupervised and predictive approach. Our model is based on processing the unsupervised interaction of an artificial agent with its environment. We show how spatio-temporally invariant structures in the environment induce regularities in the sensorimotor experience of an agent, and how this agent, while building a predictive model of its sensorimotor experience, can capture them as densely connected subgraphs in a graph of sensory states connected by motor commands. Our approach is focused on elementary mechanisms, and is illustrated with a set of simple experiments in which an agent interacts with an environment. We show how the agent can build an internal model of moving but spatio-temporally invariant structures by performing a Spectral Clustering of the graph modeling its overall sensorimotor experiences. We systematically examine properties of the model, shedding light more globally on the specificities of the paradigm with respect to methods based on the supervised processing of collections of static images.",Identification of Invariant Sensorimotor Structures as a Prerequisite for the Discovery of Objects,"Nicolas Le Hir, Olivier Sigaud, Alban Laflaqui\`ere",2018,Artificial Intelligence,1810.05057
"Most of agents that learn policy for tasks with reinforcement learning (RL) lack the ability to communicate with people, which makes human-agent collaboration challenging. We believe that, in order for RL agents to comprehend utterances from human colleagues, RL agents must infer the mental states that people attribute to them because people sometimes infer an interlocutor's mental states and communicate on the basis of this mental inference. This paper proposes PublicSelf model, which is a model of a person who infers how the person's own behavior appears to their colleagues. We implemented the PublicSelf model for an RL agent in a simulated environment and examined the inference of the model by comparing it with people's judgment. The results showed that the agent's intention that people attributed to the agent's movement was correctly inferred by the model in scenes where people could find certain intentionality from the agent's behavior.",Bayesian Inference of Self-intention Attributed by Observer,"Yosuke Fukuchi, Masahiko Osawa, Hiroshi Yamakawa, Tatsuji Takahashi, Michita Imai",2018,Artificial Intelligence,1810.05564
"This paper presents a technology for simple and computationally efficient improvements of a generic Artificial Intelligence (AI) system, including Multilayer and Deep Learning neural networks. The improvements are, in essence, small network ensembles constructed on top of the existing AI architectures. Theoretical foundations of the technology are based on Stochastic Separation Theorems and the ideas of the concentration of measure. We show that, subject to mild technical assumptions on statistical properties of internal signals in the original AI system, the technology enables instantaneous and computationally efficient removal of spurious and systematic errors with probability close to one on the datasets which are exponentially large in dimension. The method is illustrated with numerical examples and a case study of ten digits recognition from American Sign Language.",Fast Construction of Correcting Ensembles for Legacy Artificial Intelligence Systems: Algorithms and a Case Study,"Ivan Y. Tyukin, Alexander N. Gorban, Stephen Green, Danil Prokhorov",2019,Artificial Intelligence,1810.05593
"In open-ended environments, autonomous learning agents must set their own goals and build their own curriculum through an intrinsically motivated exploration. They may consider a large diversity of goals, aiming to discover what is controllable in their environments, and what is not. Because some goals might prove easy and some impossible, agents must actively select which goal to practice at any moment, to maximize their overall mastery on the set of learnable goals. This paper proposes CURIOUS, an algorithm that leverages 1) a modular Universal Value Function Approximator with hindsight learning to achieve a diversity of goals of different kinds within a unique policy and 2) an automated curriculum learning mechanism that biases the attention of the agent towards goals maximizing the absolute learning progress. Agents focus sequentially on goals of increasing complexity, and focus back on goals that are being forgotten. Experiments conducted in a new modular-goal robotic environment show the resulting developmental self-organization of a learning curriculum, and demonstrate properties of robustness to distracting goals, forgetting and changes in body properties.",CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning,"C\'edric Colas, Pierre Fournier, Olivier Sigaud, Mohamed Chetouani, Pierre-Yves Oudeyer",2019,Artificial Intelligence,1810.06284
"Traditional data quality control methods are based on users experience or previously established business rules, and this limits performance in addition to being a very time consuming process with lower than desirable accuracy. Utilizing deep learning, we can leverage computing resources and advanced techniques to overcome these challenges and provide greater value to users. In this paper, we, the authors, first review relevant works and discuss machine learning techniques, tools, and statistical quality models. Second, we offer a creative data quality framework based on deep learning and statistical model algorithm for identifying data quality. Third, we use data involving salary levels from an open dataset published by the state of Arkansas to demonstrate how to identify outlier data and how to improve data quality via deep learning. Finally, we discuss future work.",Improving Data Quality through Deep Learning and Statistical Models,"Wei Dai, Kenji Yoshigoe, William Parsley",2018,Artificial Intelligence,1810.07132
"The increasing presence of robots in industries has not gone unnoticed. Large industrial players have incorporated them into their production lines, but smaller companies hesitate due to high initial costs and the lack of programming expertise. In this work we introduce a framework that combines two disciplines, Programming by Demonstration and Automated Planning, to allow users without any programming knowledge to program a robot. The user teaches the robot atomic actions together with their semantic meaning and represents them in terms of preconditions and effects. Using these atomic actions the robot can generate action sequences autonomously to reach any goal given by the user. We evaluated the usability of our framework in terms of user experiments with a Baxter Research Robot and showed that it is well-adapted to users without any programming experience.",A Framework for Robot Programming in Cobotic Environments: First user experiments,Ying Siu Liang and Damien Pellier and Humbert Fiorino and Sylvie Pesty,2017,Artificial Intelligence,1810.08492
"In cooperation, the workers must know how co-workers behave. However, an agent's policy, which is embedded in a statistical machine learning model, is hard to understand, and requires much time and knowledge to comprehend. Therefore, it is difficult for people to predict the behavior of machine learning robots, which makes Human Robot Cooperation challenging. In this paper, we propose Instruction-based Behavior Explanation (IBE), a method to explain an autonomous agent's future behavior. In IBE, an agent can autonomously acquire the expressions to explain its own behavior by reusing the instructions given by a human expert to accelerate the learning of the agent's policy. IBE also enables a developmental agent, whose policy may change during the cooperation, to explain its own behavior with sufficient time granularity.",Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents,"Yosuke Fukuchi, Masahiko Osawa, Hiroshi Yamakawa, Michita Imai",2017,Artificial Intelligence,1810.08811
"Counterfactual Regret Minimization (CFR) is the leading framework for solving large imperfect-information games. It converges to an equilibrium by iteratively traversing the game tree. In order to deal with extremely large games, abstraction is typically applied before running CFR. The abstracted game is solved with tabular CFR, and its solution is mapped back to the full game. This process can be problematic because aspects of abstraction are often manual and domain specific, abstraction algorithms may miss important strategic nuances of the game, and there is a chicken-and-egg problem because determining a good abstraction requires knowledge of the equilibrium of the game. This paper introduces Deep Counterfactual Regret Minimization, a form of CFR that obviates the need for abstraction by instead using deep neural networks to approximate the behavior of CFR in the full game. We show that Deep CFR is principled and achieves strong performance in large poker games. This is the first non-tabular variant of CFR to be successful in large games.",Deep Counterfactual Regret Minimization,"Noam Brown, Adam Lerer, Sam Gross, Tuomas Sandholm",2019,Artificial Intelligence,1811.00164
"This article deals with the problem of the uncertainty in rule-based systems (RBS), but from the perspective of quantum computing (QC). In this work we first remember the characteristics of Quantum Rule-Based Systems (QRBS), a concept defined in a previous article by one of the authors of this paper, and we introduce the problem of quantum uncertainty. We assume that the subjective uncertainty that affects the facts of classical RBSs can be treated as a direct consequence of the probabilistic nature of quantum mechanics (QM), and we also assume that the uncertainty associated with a given hypothesis is a consequence of the propagation of the imprecision through the inferential circuits of RBSs. This article does not intend to contribute anything new to the QM field: it is a work of artificial intelligence (AI) that uses QC techniques to solve the problem of uncertainty in RBSs. Bearing the above arguments in mind a quantum model is proposed. This model has been applied to a problem already defined by one of the authors of this work in a previous publication and which is briefly described in this article. Then the model is generalized, and it is thoroughly evaluated. The results obtained show that QC is a valid, effective and efficient method to deal with the inherent uncertainty of RBSs",Uncertainty in Quantum Rule-Based Systems,"Vicente Moret-Bonillo, Isaac Fern\'andez-Varela, Diego Alvarez-Estevez",2021,Artificial Intelligence,1811.02782
"Item response theory (IRT) can be applied to the analysis of the evaluation of results from AI benchmarks. The two-parameter IRT model provides two indicators (difficulty and discrimination) on the side of the item (or AI problem) while only one indicator (ability) on the side of the respondent (or AI agent). In this paper we analyse how to make this set of indicators dual, by adding a fourth indicator, generality, on the side of the respondent. Generality is meant to be dual to discrimination, and it is based on difficulty. Namely, generality is defined as a new metric that evaluates whether an agent is consistently good at easy problems and bad at difficult ones. With the addition of generality, we see that this set of four key indicators can give us more insight on the results of AI benchmarks. In particular, we explore two popular benchmarks in AI, the Arcade Learning Environment (Atari 2600 games) and the General Video Game AI competition. We provide some guidelines to estimate and interpret these indicators for other AI benchmarks and competitions.",Analysing Results from AI Benchmarks: Key Indicators and How to Obtain Them,Fernando Mart\'inez-Plumed and Jos\'e Hern\'andez-Orallo,2018,Artificial Intelligence,1811.08186
"Embedding models for deterministic Knowledge Graphs (KG) have been extensively studied, with the purpose of capturing latent semantic relations between entities and incorporating the structured knowledge into machine learning. However, there are many KGs that model uncertain knowledge, which typically model the inherent uncertainty of relations facts with a confidence score, and embedding such uncertain knowledge represents an unresolved challenge. The capturing of uncertain knowledge will benefit many knowledge-driven applications such as question answering and semantic search by providing more natural characterization of the knowledge. In this paper, we propose a novel uncertain KG embedding model UKGE, which aims to preserve both structural and uncertainty information of relation facts in the embedding space. Unlike previous models that characterize relation facts with binary classification techniques, UKGE learns embeddings according to the confidence scores of uncertain relation facts. To further enhance the precision of UKGE, we also introduce probabilistic soft logic to infer confidence scores for unseen relation facts during training. We propose and evaluate two variants of UKGE based on different learning objectives. Experiments are conducted on three real-world uncertain KGs via three tasks, i.e. confidence prediction, relation fact ranking, and relation fact classification. UKGE shows effectiveness in capturing uncertain knowledge by achieving promising results on these tasks, and consistently outperforms baselines on these tasks.",Embedding Uncertain Knowledge Graphs,"Xuelu Chen, Muhao Chen, Weijia Shi, Yizhou Sun, Carlo Zaniolo",2019,Artificial Intelligence,1811.10667
"Predicting the time to build software is a very complex task for software engineering managers. There are complex factors that can directly interfere with the productivity of the development team. Factors directly related to the complexity of the system to be developed drastically change the time necessary for the completion of the works with the software factories. This work proposes the use of a hybrid system based on artificial neural networks and fuzzy systems to assist in the construction of an expert system based on rules to support in the prediction of hours destined to the development of software according to the complexity of the elements present in the same. The set of fuzzy rules obtained by the system helps the management and control of software development by providing a base of interpretable estimates based on fuzzy rules. The model was submitted to tests on a real database, and its results were promissory in the construction of an aid mechanism in the predictability of the software construction.",Regularized Fuzzy Neural Networks to Aid Effort Forecasting in the Construction and Software Development,"Paulo Vitor de Campos Souza, Augusto Junio Guimaraes, Vanessa Souza Araujo, Thiago Silva Rezende, Vinicius Jonathan Silva Araujo",2018,Artificial Intelligence,1812.01351
"As artificial intelligence (AI) systems become increasingly ubiquitous, the topic of AI governance for ethical decision-making by AI has captured public imagination. Within the AI research community, this topic remains less familiar to many researchers. In this paper, we complement existing surveys, which largely focused on the psychological, social and legal discussions of the topic, with an analysis of recent advances in technical solutions for AI governance. By reviewing publications in leading AI conferences including AAAI, AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions. We highlight the intuitions and key techniques used in each approach, and discuss promising future research directions towards successful integration of ethical AI systems into human societies.",Building Ethics into Artificial Intelligence,"Han Yu, Zhiqi Shen, Chunyan Miao, Cyril Leung, Victor R. Lesser and Qiang Yang",2018,Artificial Intelligence,1812.02953
"The problem of allocating students to supervisors for the development of a personal project or a dissertation is a crucial activity in the higher education environment, as it enables students to get feedback on their work from an expert and improve their personal, academic, and professional abilities. In this article, we propose a multi-objective and near Pareto optimal genetic algorithm for the allocation of students to supervisors. The allocation takes into consideration the students and supervisors' preferences on research/project topics, the lower and upper supervision quotas of supervisors, as well as the workload balance amongst supervisors. We introduce novel mutation and crossover operators for the student-supervisor allocation problem. The experiments carried out show that the components of the genetic algorithm are more apt for the problem than classic components, and that the genetic algorithm is capable of producing allocations that are near Pareto optimal in a reasonable time.",A near Pareto optimal approach to student-supervisor allocation with two sided preferences and workload balance,"Victor Sanchez-Anguix, Rithin Chalumuri, Reyhan Aydogan, Vicente Julian",2018,Artificial Intelligence,1812.06474
"We propose a novel approach for trip prediction by analyzing user's trip histories. We augment users' (self-) trip histories by adding 'similar' trips from other users, which could be informative and useful for predicting future trips for a given user. This also helps to cope with noisy or sparse trip histories, where the self-history by itself does not provide a reliable prediction of future trips. We show empirical evidence that by enriching the users' trip histories with additional trips, one can improve the prediction error by 15%-40%, evaluated on multiple subsets of the Nancy2012 dataset. This real-world dataset is collected from public transportation ticket validations in the city of Nancy, France. Our prediction tool is a central component of a trip simulator system designed to analyze the functionality of public transportation in the city of Nancy.",Trip Prediction by Leveraging Trip Histories from Neighboring Users,Yuxin Chen and Morteza Haghir Chehreghani,2022,Artificial Intelligence,1812.10097
"In artificial intelligence (AI) mediated workforce management systems (e.g., crowdsourcing), long-term success depends on workers accomplishing tasks productively and resting well. This dual objective can be summarized by the concept of productive laziness. Existing scheduling approaches mostly focus on efficiency but overlook worker wellbeing through proper rest. In order to enable workforce management systems to follow the IEEE Ethically Aligned Design guidelines to prioritize worker wellbeing, we propose a distributed Computational Productive Laziness (CPL) approach in this paper. It intelligently recommends personalized work-rest schedules based on local data concerning a worker's capabilities and situational factors to incorporate opportunistic resting and achieve superlinear collective productivity without the need for explicit coordination messages. Extensive experiments based on a real-world dataset of over 5,000 workers demonstrate that CPL enables workers to spend 70% of the effort to complete 90% of the tasks on average, providing more ethically aligned scheduling than existing approaches.",Ethically Aligned Opportunistic Scheduling for Productive Laziness,"Han Yu, Chunyan Miao, Yongqing Zheng, Lizhen Cui, Simon Fauvel and Cyril Leung",2019,Artificial Intelligence,1901.00298
Predicting the behavior of surrounding vehicles is a critical problem in automated driving. We present a novel game theoretic behavior prediction model that achieves state of the art prediction accuracy by explicitly reasoning about possible future interaction between agents. We evaluate our approach on the NGSIM vehicle trajectory data set and demonstrate lower root mean square error than state-of-the-art methods.,Multi-Fidelity Recursive Behavior Prediction,"Mihir Jain, Kyle Brown, Ahmed K. Sadek",2018,Artificial Intelligence,1901.01831
"Its constant technological evolution characterizes the contemporary world, and every day the processes, once manual, become computerized. Data are stored in the cyberspace, and as a consequence, one must increase the concern with the security of this environment. Cyber-attacks are represented by a growing worldwide scale and are characterized as one of the significant challenges of the century. This article aims to propose a computational system based on intelligent hybrid models, which through fuzzy rules allows the construction of expert systems in cybernetic data attacks, focusing on the SQL Injection attack. The tests were performed with real bases of SQL Injection attacks on government computers, using fuzzy neural networks. According to the results obtained, the feasibility of constructing a system based on fuzzy rules, with the classification accuracy of cybernetic invasions within the margin of the standard deviation (compared to the state-of-the-art model in solving this type of problem) is real. The model helps countries prepare to protect their data networks and information systems, as well as create opportunities for expert systems to automate the identification of attacks in cyberspace.",Fuzzy neural networks to create an expert system for detecting attacks by SQL Injection,"Lucas Oliveira Batista, Gabriel Adriano de Silva, Vanessa Souza Ara\'ujo, Vin\'icius Jonathan Silva Ara\'ujo, Thiago Silva Rezende, Augusto Junio Guimar\~aes, Paulo Vitor de Campos Souza",2018,Artificial Intelligence,1901.02868
"In many problem settings, most notably in game playing, an agent receives a possibly delayed reward for its actions. Often, those rewards are handcrafted and not naturally given. Even simple terminal-only rewards, like winning equals 1 and losing equals -1, can not be seen as an unbiased statement, since these values are chosen arbitrarily, and the behavior of the learner may change with different encodings, such as setting the value of a loss to -0:5, which is often done in practice to encourage learning. It is hard to argue about good rewards and the performance of an agent often depends on the design of the reward signal. In particular, in domains where states by nature only have an ordinal ranking and where meaningful distance information between game state values are not available, a numerical reward signal is necessarily biased. In this paper, we take a look at Monte Carlo Tree Search (MCTS), a popular algorithm to solve MDPs, highlight a reoccurring problem concerning its use of rewards, and show that an ordinal treatment of the rewards overcomes this problem. Using the General Video Game Playing framework we show a dominance of our newly proposed ordinal MCTS algorithm over preference-based MCTS, vanilla MCTS and various other MCTS variants.",Ordinal Monte Carlo Tree Search,"Tobias Joppen and Johannes F\""urnkranz",2020,Artificial Intelligence,1901.04274
"In this paper, We Apply Reinforcement learning (RL) techniques to train a realistic biomechanical model to work with different people and on different walking environments. We benchmarking 3 RL algorithms: Deep Deterministic Policy Gradient (DDPG), Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO) in OpenSim environment, Also we apply imitation learning to a prosthetics domain to reduce the training time needed to design customized prosthetics. We use DDPG algorithm to train an original expert agent. We then propose a modification to the Dataset Aggregation (DAgger) algorithm to reuse the expert knowledge and train a new target agent to replicate that behaviour in fewer than 5 iterations, compared to the 100 iterations taken by the expert agent which means reducing training time by 95%. Our modifications to the DAgger algorithm improve the balance between exploiting the expert policy and exploring the environment. We show empirically that these improve convergence time of the target agent, particularly when there is some degree of variation between expert and naive agent.",Transfer Learning for Prosthetics Using Imitation Learning,"Montaser Mohammedalamen, Waleed D. Khamies, Benjamin Rosman",2018,Artificial Intelligence,1901.04772
"The availability of high-fidelity energy networks brings significant value to academic and commercial research. However, such releases also raise fundamental concerns related to privacy and security as they can reveal sensitive commercial information and expose system vulnerabilities. This paper investigates how to release power networks where the parameters of transmission lines and transformers are obfuscated. It does so by using the framework of Differential Privacy (DP), that provides strong privacy guarantees and has attracted significant attention in recent years. Unfortunately, simple DP mechanisms often result in AC-infeasible networks. To address these concerns, this paper presents a novel differential privacy mechanism that guarantees AC-feasibility and largely preserves the fidelity of the obfuscated network. Experimental results also show that the obfuscation significantly reduces the potential damage of an attacker exploiting the release of the dataset.",Differential Privacy for Power Grid Obfuscation,"Ferdinando Fioretto, Terrence W.K. Mak, Pascal Van Hentenryck",2020,Artificial Intelligence,1901.06949
"Collaborative filtering (CF) is the key technique for recommender systems (RSs). CF exploits user-item behavior interactions (e.g., clicks) only and hence suffers from the data sparsity issue. One research thread is to integrate auxiliary information such as product reviews and news titles, leading to hybrid filtering methods. Another thread is to transfer knowledge from other source domains such as improving the movie recommendation with the knowledge from the book domain, leading to transfer learning methods. In real-world life, no single service can satisfy a user's all information needs. Thus it motivates us to exploit both auxiliary and source information for RSs in this paper. We propose a novel neural model to smoothly enable Transfer Meeting Hybrid (TMH) methods for cross-domain recommendation with unstructured text in an end-to-end manner. TMH attentively extracts useful content from unstructured text via a memory module and selectively transfers knowledge from a source domain via a transfer network. On two real-world datasets, TMH shows better performance in terms of three ranking metrics by comparing with various baselines. We conduct thorough analyses to understand how the text content and transferred knowledge help the proposed model.",Transfer Meets Hybrid: A Synthetic Approach for Cross-Domain Collaborative Filtering with Text,"Guangneng Hu, Yu Zhang, and Qiang Yang",2019,Artificial Intelligence,1901.07199
"Learning in multi-agent scenarios is a fruitful research direction, but current approaches still show scalability problems in multiple games with general reward settings and different opponent types. The Multi-Agent Reinforcement Learning in Malm\""O (MARL\""O) competition is a new challenge that proposes research in this domain using multiple 3D games. The goal of this contest is to foster research in general agents that can learn across different games and opponent types, proposing a challenge as a milestone in the direction of Artificial General Intelligence.","The Multi-Agent Reinforcement Learning in Malm\""O (MARL\""O) Competition","Diego Perez-Liebana, Katja Hofmann, Sharada Prasanna Mohanty, Noburu Kuno, Andre Kramer, Sam Devlin, Raluca D. Gaina, Daniel Ionita",2018,Artificial Intelligence,1901.08129
"Under the project Maccoy Critical, we would like to train individuals, in virtual environments, to handle critical situations such as dilemmas. These latter refer to situations where there is no ``good'' solution. In other words, situations that lead to negative consequences whichever choice is made. Our objective is to use Knowledge Models to extract necessary properties for dilemmas to emerge. To do so, our approach consists in developing a Scenario Orchestration System that generates dilemma situations dynamically without having to write them beforehand. In this paper we present this approach and expose a proof of concept of the generation process.",A model for prohibition and obligation dilemmas generation in virtual environments,"Azzeddine Benabbou (Heudiasyc), Domitile Lourdeaux (Heudiasyc), Dominique Lenne (Heudiasyc)",2018,Artificial Intelligence,1901.09790
"Robotic mobile fulfillment systems (RMFSs) are a new type of warehousing system, which has received more attention recently, due to increasing growth in the e-commerce sector. Instead of sending pickers to the inventory area to search for and pick the ordered items, robots carry shelves (called ""pods"") including ordered items from the inventory area to picking stations. In the picking stations, human pickers put ordered items into totes; then these items are transported by a conveyor to the packing stations. This type of warehousing system relieves the human pickers and improves the picking process. In this paper, we concentrate on decisions about the assignment of pods to stations and orders to stations to fulfill picking for each incoming customer's order. In previous research for an RMFS with multiple picking stations, these decisions are made sequentially. Instead, we present a new integrated model. To improve the system performance even more, we extend our model by splitting orders. This means parts of an order are allowed to be picked at different stations. To the best of the authors' knowledge, this is the first publication on split orders in an RMFS. We analyze different performance metrics, such as pile-on, pod-station visits, robot moving distance and order turn-over time. We compare the results of our models in different instances with the sequential method in our open-source simulation framework RAWSim-O.",Efficient order picking methods in robotic mobile fulfillment systems,"Lin Xie, Nils Thieme, Ruslan Krenzler, Hanyi Li",2021,Artificial Intelligence,1902.03092
"Today's AI still faces two major challenges. One is that in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated learning framework, which includes horizontal federated learning, vertical federated learning and federated transfer learning. We provide definitions, architectures and applications for the federated learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allow knowledge to be shared without compromising user privacy.",Federated Machine Learning: Concept and Applications,"Qiang Yang, Yang Liu, Tianjian Chen, Yongxin Tong",2019,Artificial Intelligence,1902.04885
"Numerous, artificially intelligent, networked things will populate the battlefield of the future, operating in close collaboration with human warfighters, and fighting as teams in highly adversarial environments. This chapter explores the characteristics, capabilities and intelli-gence required of such a network of intelligent things and humans - Internet of Battle Things (IOBT). The IOBT will experience unique challenges that are not yet well addressed by the current generation of AI and machine learning.",Intelligent Autonomous Things on the Battlefield,"Alexander Kott, Ethan Stump",2019,Artificial Intelligence,1902.10086
"Navigating and understanding the real world remains a key challenge in machine learning and inspires a great variety of research in areas such as language grounding, planning, navigation and computer vision. We propose an instruction-following task that requires all of the above, and which combines the practicality of simulated environments with the challenges of ambiguous, noisy real world data. StreetNav is built on top of Google Street View and provides visually accurate environments representing real places. Agents are given driving instructions which they must learn to interpret in order to successfully navigate in this environment. Since humans equipped with driving instructions can readily navigate in previously unseen cities, we set a high bar and test our trained agents for similar cognitive capabilities. Although deep reinforcement learning (RL) methods are frequently evaluated only on data that closely follow the training distribution, our dataset extends to multiple cities and has a clean train/test separation. This allows for thorough testing of generalisation ability. This paper presents the StreetNav environment and tasks, models that establish strong baselines, and extensive analysis of the task and the trained agents.",Learning To Follow Directions in Street View,"Karl Moritz Hermann, Mateusz Malinowski, Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, Raia Hadsell",2020,Artificial Intelligence,1903.00401
"This paper presents a novel approach for learning STRIPS action models from examples that compiles this inductive learning task into a classical planning task. Interestingly, the compilation approach is flexible to different amounts of available input knowledge; the learning examples can range from a set of plans (with their corresponding initial and final states) to just a pair of initial and final states (no intermediate action or state is given). Moreover, the compilation accepts partially specified action models and it can be used to validate whether the observation of a plan execution follows a given STRIPS action model, even if this model is not fully specified.",Learning STRIPS Action Models with Classical Planning,"Diego Aineto, Sergio Jim\'enez and Eva Onaindia",2018,Artificial Intelligence,1903.01153
"Learning models of user behaviour is an important problem that is broadly applicable across many application domains requiring human-robot interaction. In this work we show that it is possible to learn a generative model for distinct user behavioral types, extracted from human demonstrations, by enforcing clustering of preferred task solutions within the latent space. We use this model to differentiate between user types and to find cases with overlapping solutions. Moreover, we can alter an initially guessed solution to satisfy the preferences that constitute a particular user type by backpropagating through the learned differentiable model. An advantage of structuring generative models in this way is that it allows us to extract causal relationships between symbols that might form part of the user's specification of the task, as manifested in the demonstrations. We show that the proposed method is capable of correctly distinguishing between three user types, who differ in degrees of cautiousness in their motion, while performing the task of moving objects with a kinesthetically driven robot in a tabletop environment. Our method successfully identifies the correct type, within the specified time, in 99% [97.8 - 99.8] of the cases, which outperforms an IRL baseline. We also show that our proposed method correctly changes a default trajectory to one satisfying a particular user specification even with unseen objects. The resulting trajectory is shown to be directly implementable on a PR2 humanoid robot completing the same task.",Using Causal Analysis to Learn Specifications from Task Demonstrations,"Daniel Angelov, Yordan Hristov, Subramanian Ramamoorthy",2019,Artificial Intelligence,1903.01267
"Argumentation theory is a powerful paradigm that formalizes a type of commonsense reasoning that aims to simulate the human ability to resolve a specific problem in an intelligent manner. A classical argumentation process takes into account only the properties related to the intrinsic logical soundness of an argument in order to determine its acceptability status. However, these properties are not always the only ones that matter to establish the argument's acceptability---there exist other qualities, such as strength, weight, social votes, trust degree, relevance level, and certainty degree, among others.",An Approach to Characterize Graded Entailment of Arguments through a Label-based Framework,"Maximiliano C. D. Bud\'an, Gerardo I. Simari, Ignacio Viglizzo and Guillermo R. Simari",2017,Artificial Intelligence,1903.01865
"A Timed Argumentation Framework (TAF) is a formalism where arguments are only valid for consideration in a given period of time, called availability intervals, which are defined for every individual argument. The original proposal is based on a single, abstract notion of attack between arguments that remains static and permanent in time. Thus, in general, when identifying the set of acceptable arguments, the outcome associated with a TAF will vary over time. In this work we introduce an extension of TAF adding the capability of modeling a support relation between arguments. In this sense, the resulting framework provides a suitable model for different time-dependent issues. Thus, the main contribution here is to provide an enhanced framework for modeling a positive (support) and negative (attack) interaction varying over time, which are relevant in many real-world situations. This leads to a Timed Bipolar Argumentation Framework (T-BAF), where classical argument extensions can be defined. The proposal aims at advancing in the integration of temporal argumentation in different application domain.",Bipolar in Temporal Argumentation Framework,"Maximiliano C. D. Bud\'an, Maria Laura Cobo, Diego C. Martinez and Guillermo R. Simari",2017,Artificial Intelligence,1903.01874
"In this work, we enrich a formalism for argumentation by including a formal characterization of features related to the knowledge, in order to capture proper reasoning in legal domains. We add meta-data information to the arguments in the form of labels representing quantitative and qualitative data about them. These labels are propagated through an argumentative graph according to the relations of support, conflict, and aggregation between arguments.",Dealing with Qualitative and Quantitative Features in Legal Domains,"Maximiliano C. D. Bud\'an, Mar\'ia Laura Cobo, Diego I. Mart\'inez and Antonino Rotolo",2018,Artificial Intelligence,1903.01966
"Current advances in research, development and application of artificial intelligence (AI) systems have yielded a far-reaching discourse on AI ethics. In consequence, a number of ethics guidelines have been released in recent years. These guidelines comprise normative principles and recommendations aimed to harness the ""disruptive"" potentials of new AI technologies. Designed as a comprehensive evaluation, this paper analyzes and compares these guidelines highlighting overlaps but also omissions. As a result, I give a detailed overview of the field of AI ethics. Finally, I also examine to what extent the respective ethical principles and values are implemented in the practice of research, development and application of AI systems - and how the effectiveness in the demands of AI ethics can be improved.",The Ethics of AI Ethics -- An Evaluation of Guidelines,Thilo Hagendorff,2020,Artificial Intelligence,1903.03425
"Interactive reinforcement learning has become an important apprenticeship approach to speed up convergence in classic reinforcement learning problems. In this regard, a variant of interactive reinforcement learning is policy shaping which uses a parent-like trainer to propose the next action to be performed and by doing so reduces the search space by advice. On some occasions, the trainer may be another artificial agent which in turn was trained using reinforcement learning methods to afterward becoming an advisor for other learner-agents. In this work, we analyze internal representations and characteristics of artificial agents to determine which agent may outperform others to become a better trainer-agent. Using a polymath agent, as compared to a specialist agent, an advisor leads to a larger reward and faster convergence of the reward signal and also to a more stable behavior in terms of the state visit frequency of the learner-agents. Moreover, we analyze system interaction parameters in order to determine how influential they are in the apprenticeship process, where the consistency of feedback is much more relevant when dealing with different learner obedience parameters.",Improving interactive reinforcement learning: What makes a good teacher?,"Francisco Cruz, Sven Magg, Yukie Nagai, Stefan Wermter",2018,Artificial Intelligence,1904.06879
"Many computer models such as cellular automata and artificial neural networks have been developed and successfully applied. However, in some cases, these models might be restrictive on the possible solutions or their solutions might be difficult to interpret. To overcome this problem, we outline a new approach, the so-called allagmatic method, that automatically programs and executes models with as little limitations as possible while maintaining human interpretability. Earlier we described a metamodel and its building blocks according to the philosophical concepts of structure (spatial dimension) and operation (temporal dimension). They are entity, milieu, and update function that together abstractly describe cellular automata, artificial neural networks, and possibly any kind of computer model. By automatically combining these building blocks in an evolutionary computation, interpretability might be increased by the relationship to the metamodel, and models might be translated into more interpretable models via the metamodel. We propose generic and object-oriented programming to implement the entities and their milieus as dynamic and generic arrays and the update function as a method. We show two experiments where a simple cellular automaton and an artificial neural network are automatically programmed, compiled, and executed. A target state is successfully evolved and learned in the cellular automaton and artificial neural network, respectively. We conclude that the allagmatic method can create and execute cellular automaton and artificial neural network models in an automated manner with the guidance of philosophy.",Automatic Programming of Cellular Automata and Artificial Neural Networks Guided by Philosophy,Patrik Christen and Olivier Del Fabbro,2020,Artificial Intelligence,1905.04232
"This paper surveys an approach to the XAI problem, using post-hoc explanation by example, that hinges on twinning Artificial Neural Networks (ANNs) with Case-Based Reasoning (CBR) systems, so-called ANN-CBR twins. A systematic survey of 1100+ papers was carried out to identify the fragmented literature on this topic and to trace it influence through to more recent work involving Deep Neural Networks (DNNs). The paper argues that this twin-system approach, especially using ANN-CBR twins, presents one possible coherent, generic solution to the XAI problem (and, indeed, XCBR problem). The paper concludes by road-mapping some future directions for this XAI solution involving (i) further tests of feature-weighting techniques, (iii) explorations of how explanatory cases might best be deployed (e.g., in counterfactuals, near-miss cases, a fortori cases), and (iii) the raising of the unwelcome and, much ignored, issue of human user evaluation.",How Case Based Reasoning Explained Neural Networks: An XAI Survey of Post-Hoc Explanation-by-Example in ANN-CBR Twins,Mark T Keane and Eoin M Kenny,2019,Artificial Intelligence,1905.07186
"Over the past decade, knowledge graphs became popular for capturing structured domain knowledge. Relational learning models enable the prediction of missing links inside knowledge graphs. More specifically, latent distance approaches model the relationships among entities via a distance between latent representations. Translating embedding models (e.g., TransE) are among the most popular latent distance approaches which use one distance function to learn multiple relation patterns. However, they are mostly inefficient in capturing symmetric relations since the representation vector norm for all the symmetric relations becomes equal to zero. They also lose information when learning relations with reflexive patterns since they become symmetric and transitive. We propose the Multiple Distance Embedding model (MDE) that addresses these limitations and a framework to collaboratively combine variant latent distance-based terms. Our solution is based on two principles: 1) we use a limit-based loss instead of a margin ranking loss and, 2) by learning independent embedding vectors for each of the terms we can collectively train and predict using contradicting distance terms. We further demonstrate that MDE allows modeling relations with (anti)symmetry, inversion, and composition patterns. We propose MDE as a neural network model that allows us to map non-linear relations between the embedding vectors and the expected output of the score function. Our empirical results show that MDE performs competitively to state-of-the-art embedding models on several benchmark datasets.",MDE: Multiple Distance Embeddings for Link Prediction in Knowledge Graphs,"Afshin Sadeghi, Damien Graux, Hamed Shariat Yazdi, Jens Lehmann",2020,Artificial Intelligence,1905.10702
"We propose a set of compositional design patterns to describe a large variety of systems that combine statistical techniques from machine learning with symbolic techniques from knowledge representation. As in other areas of computer science (knowledge engineering, software engineering, ontology engineering, process mining and others), such design patterns help to systematize the literature, clarify which combinations of techniques serve which purposes, and encourage re-use of software components. We have validated our set of compositional design patterns against a large body of recent literature.",A Boxology of Design Patterns for Hybrid Learning and Reasoning Systems,Frank van Harmelen and Annette ten Teije,2019,Artificial Intelligence,1905.12389
"In this paper, we present a simple and cheap ordinal bucketing algorithm that approximately generates $q$-quantiles from an incremental data stream. The bucketing is done dynamically in the sense that the amount of buckets $q$ increases with the number of seen samples. We show how this can be used in Ordinal Monte Carlo Tree Search (OMCTS) to yield better bounds on time and space complexity, especially in the presence of noisy rewards. Besides complexity analysis and quality tests of quantiles, we evaluate our method using OMCTS in the General Video Game Framework (GVGAI). Our results demonstrate its dominance over vanilla Monte Carlo Tree Search in the presence of noise, where OMCTS without bucketing has a very bad time and space complexity.",Ordinal Bucketing for Game Trees using Dynamic Quantile Approximation,"Tobias Joppen, Tilman Str\""ubig, Johannes F\""urnkranz",2019,Artificial Intelligence,1905.13449
"Kandinsky Figures and Kandinsky Patterns are mathematically describable, simple self-contained hence controllable test data sets for the development, validation and training of explainability in artificial intelligence. Whilst Kandinsky Patterns have these computationally manageable properties, they are at the same time easily distinguishable from human observers. Consequently, controlled patterns can be described by both humans and computers. We define a Kandinsky Pattern as a set of Kandinsky Figures, where for each figure an ""infallible authority"" defines that the figure belongs to the Kandinsky Pattern. With this simple principle we build training and validation data sets for automatic interpretability and context learning. In this paper we describe the basic idea and some underlying principles of Kandinsky Patterns and provide a Github repository to invite the international machine learning research community to a challenge to experiment with our Kandinsky Patterns to expand and thus make progress in the field of explainable AI and to contribute to the upcoming field of explainability and causability.",Kandinsky Patterns,Heimo Mueller and Andreas Holzinger,2021,Artificial Intelligence,1906.00657
"In this paper, we define and apply representational stability analysis (ReStA), an intuitive way of analyzing neural language models. ReStA is a variant of the popular representational similarity analysis (RSA) in cognitive neuroscience. While RSA can be used to compare representations in models, model components, and human brains, ReStA compares instances of the same model, while systematically varying single model parameter. Using ReStA, we study four recent and successful neural language models, and evaluate how sensitive their internal representations are to the amount of prior context. Using RSA, we perform a systematic study of how similar the representational spaces in the first and second (or higher) layers of these models are to each other and to patterns of activation in the human brain. Our results reveal surprisingly strong differences between language models, and give insights into where the deep linguistic processing, that integrates information over multiple sentences, is happening in these models. The combination of ReStA and RSA on models and brains allows us to start addressing the important question of what kind of linguistic processes we can hope to observe in fMRI brain imaging data. In particular, our results suggest that the data on story reading from Wehbe et al. (2014) contains a signal of shallow linguistic processing, but show no evidence on the more interesting deep linguistic processing.",Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains,"Samira Abnar, Lisa Beinborn, Rochelle Choenni, Willem Zuidema",2019,Artificial Intelligence,1906.01539
"In the last decades we have witnessed the success of applications of Artificial Intelligence to playing games. In this work we address the challenging field of games with hidden information and card games in particular. Jass is a very popular card game in Switzerland and is closely connected with Swiss culture. To the best of our knowledge, performances of Artificial Intelligence agents in the game of Jass do not outperform top players yet. Our contribution to the community is two-fold. First, we provide an overview of the current state-of-the-art of Artificial Intelligence methods for card games in general. Second, we discuss their application to the use-case of the Swiss card game Jass. This paper aims to be an entry point for both seasoned researchers and new practitioners who want to join in the Jass challenge.",Survey of Artificial Intelligence for Card Games and Its Application to the Swiss Game Jass,"Joel Niklaus, Michele Alberti, Vinaychandran Pondenkandath, Rolf Ingold, Marcus Liwicki",2019,Artificial Intelligence,1906.04439
"This article describes the application of soft computing methods for solving the problem of locating garbage accumulation points in urban scenarios. This is a relevant problem in modern smart cities, in order to reduce negative environmental and social impacts in the waste management process, and also to optimize the available budget from the city administration to install waste bins. A specific problem model is presented, which accounts for reducing the investment costs, enhance the number of citizens served by the installed bins, and the accessibility to the system. A family of single- and multi-objective heuristics based on the PageRank method and two mutiobjective evolutionary algorithms are proposed. Experimental evaluation performed on real scenarios on the cities of Montevideo (Uruguay) and Bahia Blanca (Argentina) demonstrates the effectiveness of the proposed approaches. The methods allow computing plannings with different trade-off between the problem objectives. The computed results improve over the current planning in Montevideo and provide a reasonable budget cost and quality of service for Bahia Blanca.",Soft computing methods for multiobjective location of garbage accumulation points in smart cities,"Jamal Toutouh, Diego Rossit, and Sergio Nesmachnow",2019,Artificial Intelligence,1906.10689
"Ontology-based knowledge bases (KBs) like DBpedia are very valuable resources, but their usefulness and usability is limited by various quality issues. One such issue is the use of string literals instead of semantically typed entities. In this paper we study the automated canonicalization of such literals, i.e., replacing the literal with an existing entity from the KB or with a new entity that is typed using classes from the KB. We propose a framework that combines both reasoning and machine learning in order to predict the relevant entities and types, and we evaluate this framework against state-of-the-art baselines for both semantic typing and entity matching.",Canonicalizing Knowledge Base Literals,Jiaoyan Chen and Ernesto Jimenez-Ruiz and Ian Horrocks,2019,Artificial Intelligence,1906.11180
"Making decisions in complex environments is a key challenge in artificial intelligence (AI). Situations involving multiple decision makers are particularly complex, leading to computational intractability of principled solution methods. A body of work in AI has tried to mitigate this problem by trying to distill interaction to its essence: how does the policy of one agent influence another agent? If we can find more compact representations of such influence, this can help us deal with the complexity, for instance by searching the space of influences rather than the space of policies. However, so far these notions of influence have been restricted in their applicability to special cases of interaction. In this paper we formalize influence-based abstraction (IBA), which facilitates the elimination of latent state factors without any loss in value, for a very general class of problems described as factored partially observable stochastic games (fPOSGs). On the one hand, this generalizes existing descriptions of influence, and thus can serve as the foundation for improvements in scalability and other insights in decision making in complex multiagent settings. On the other hand, since the presence of other agents can be seen as a generalization of single agent settings, our formulation of IBA also provides a sufficient statistic for decision making under abstraction for a single agent. We also give a detailed discussion of the relations to such previous works, identifying new insights and interpretations of these approaches. In these ways, this paper deepens our understanding of abstraction in a wide range of sequential decision making settings, providing the basis for new approaches and algorithms for a large class of problems.",A Sufficient Statistic for Influence in Structured Multiagent Environments,"Frans A. Oliehoek, Stefan Witwicki, Leslie P. Kaelbling",2021,Artificial Intelligence,1907.09278
"This paper deals with robust optimization applied to network flows. Two robust variants of the minimum-cost integer flow problem are considered. Thereby, uncertainty in problem formulation is limited to arc unit costs and expressed by a finite set of explicitly given scenarios. It is shown that both problem variants are NP-hard. To solve the considered variants, several heuristics based on local search or evolutionary computing are proposed. The heuristics are experimentally evaluated on appropriate problem instances.",Heuristic solutions to robust variants of the minimum-cost integer flow problem,"Marko \v{S}poljarec, Robert Manger",2020,Artificial Intelligence,1907.09468
"Humans as designers have quite versatile problem-solving strategies. Computer agents on the other hand can access large scale computational resources to solve certain design problems. Hence, if agents can learn from human behavior, a synergetic human-agent problem solving team can be created. This paper presents an approach to extract human design strategies and implicit rules, purely from historical human data, and use that for design generation. A two-step framework that learns to imitate human design strategies from observation is proposed and implemented. This framework makes use of deep learning constructs to learn to generate designs without any explicit information about objective and performance metrics. The framework is designed to interact with the problem through a visual interface as humans did when solving the problem. It is trained to imitate a set of human designers by observing their design state sequences without inducing problem-specific modelling bias or extra information about the problem. Furthermore, an end-to-end agent is developed that uses this deep learning framework as its core in conjunction with image processing to map pixel-to-design moves as a mechanism to generate designs. Finally, the designs generated by a computational team of these agents are then compared to actual human data for teams solving a truss design problem. Results demonstrates that these agents are able to create feasible and efficient truss designs without guidance, showing that this methodology allows agents to learn effective design strategies.",Learning to design from humans: Imitating human designers through deep learning,"Ayush Raina, Christopher McComb and Jonathan Cagan",2019,Artificial Intelligence,1907.11813
"This paper is the preprint of an invited commentary on Lake et al's Behavioral and Brain Sciences article titled ""Building machines that learn and think like people"". Lake et al's paper offers a timely critique on the recent accomplishments in artificial intelligence from the vantage point of human intelligence, and provides insightful suggestions about research directions for building more human-like intelligence. Since we agree with most of the points raised in that paper, we will offer a few points that are complementary.",What can the brain teach us about building artificial intelligence?,Dileep George,2017,Artificial Intelligence,1909.01561
"Over the last year, the amount of research in hierarchical planning has increased, leading to significant improvements in the performance of planners. However, the research is diverging and planners are somewhat hard to compare against each other. This is mostly caused by the fact that there is no standard set of benchmark domains, nor even a common description language for hierarchical planning problems. As a consequence, the available planners support a widely varying set of features and (almost) none of them can solve (or even parse) any problem developed for another planner. With this paper, we propose to create a new track for the IPC in which hierarchical planners will compete. This competition will result in a standardised description language, broader support for core features of that language among planners, a set of benchmark problems, a means to fairly and objectively compare HTN planners, and for new challenges for planners.",Hierarchical Planning in the IPC,"D. H\""oller, G. Behnke, P. Bercher, S. Biundo, H. Fiorino, D. Pellier, R. Alford",2019,Artificial Intelligence,1909.04405
"This paper introduces Strict Partial Order Networks (SPON), a novel neural network architecture designed to enforce asymmetry and transitive properties as soft constraints. We apply it to induce hypernymy relations by training with is-a pairs. We also present an augmented variant of SPON that can generalize type information learned for in-vocabulary terms to previously unseen ones. An extensive evaluation over eleven benchmarks across different tasks shows that SPON consistently either outperforms or attains the state of the art on all but one of these benchmarks.",Hypernym Detection Using Strict Partial Order Networks,"Sarthak Dash, Md Faisal Mahbub Chowdhury, Alfio Gliozzo, Nandana Mihindukulasooriya, Nicolas Rodolfo Fauceglia",2020,Artificial Intelligence,1909.10572
"Active inference is a first principle account of how autonomous agents operate in dynamic, non-stationary environments. This problem is also considered in reinforcement learning (RL), but limited work exists on comparing the two approaches on the same discrete-state environments. In this paper, we provide: 1) an accessible overview of the discrete-state formulation of active inference, highlighting natural behaviors in active inference that are generally engineered in RL; 2) an explicit discrete-state comparison between active inference and RL on an OpenAI gym baseline. We begin by providing a condensed overview of the active inference literature, in particular viewing the various natural behaviors of active inference agents through the lens of RL. We show that by operating in a pure belief-based setting, active inference agents can carry out epistemic exploration, and account for uncertainty about their environment in a Bayes-optimal fashion. Furthermore, we show that the reliance on an explicit reward signal in RL is removed in active inference, where reward can simply be treated as another observation; even in the total absence of rewards, agent behaviors are learned through preference learning. We make these properties explicit by showing two scenarios in which active inference agents can infer behaviors in reward-free environments compared to both Q-learning and Bayesian model-based RL agents; by placing zero prior preferences over rewards and by learning the prior preferences over the observations corresponding to reward. We conclude by noting that this formalism can be applied to more complex settings if appropriate generative models can be formulated. In short, we aim to demystify the behavior of active inference agents by presenting an accessible discrete state-space and time formulation, and demonstrate these behaviors in a OpenAI gym environment, alongside RL agents.",Active inference: demystified and compared,"Noor Sajid, Philip J. Ball, Thomas Parr, Karl J. Friston",2021,Artificial Intelligence,1909.10863
"We present ""AutoJudge"", an automated evaluation method for conversational dialogue systems. The method works by first generating dialogues based on self-talk, i.e. dialogue systems talking to itself. Then, it uses human ratings on these dialogues to train an automated judgement model. Our experiments show that AutoJudge correlates well with the human ratings and can be used to automatically evaluate dialogue systems, even in deployed systems. In a second part, we attempt to apply AutoJudge to improve existing systems. This works well for re-ranking a set of candidate utterances. However, our experiments show that AutoJudge cannot be applied as reward for reinforcement learning, although the metric can distinguish good from bad dialogues. We discuss potential reasons, but state here already that this is still an open question for further research.",Towards a Metric for Automated Conversational Dialogue System Evaluation and Improvement,"Jan Deriu, Mark Cieliebak",2019,Artificial Intelligence,1909.12066
"Dense urban traffic environments can produce situations where accurate prediction and dynamic models are insufficient for successful autonomous vehicle motion planning. We investigate how an autonomous agent can safely negotiate with other traffic participants, enabling the agent to handle potential deadlocks. Specifically we consider merges where the gap between cars is smaller than the size of the ego vehicle. We propose a game theoretic framework capable of generating and responding to interactive behaviors. Our main contribution is to show how game-tree decision making can be executed by an autonomous vehicle, including approximations and reasoning that make the tree-search computationally tractable. Additionally, to test our model we develop a stochastic rule-based traffic agent capable of generating interactive behaviors that can be used as a benchmark for simulating traffic participants in a crowded merge setting.",Interactive Decision Making for Autonomous Vehicles in Dense Traffic,David Isele,2019,Artificial Intelligence,1909.12914
"In a multi-agent setting, the optimal policy of a single agent is largely dependent on the behavior of other agents. We investigate the problem of multi-agent reinforcement learning, focusing on decentralized learning in non-stationary domains for mobile robot navigation. We identify a cause for the difficulty in training non-stationary policies: mutual adaptation to sub-optimal behaviors, and we use this to motivate a curriculum-based strategy for learning interactive policies. The curriculum has two stages. First, the agent leverages policy gradient algorithms to learn a policy that is capable of achieving multiple goals. Second, the agent learns a modifier policy to learn how to interact with other agents in a multi-agent setting. We evaluated our approach on both an autonomous driving lane-change domain and a robot navigation domain.",Interaction-Aware Multi-Agent Reinforcement Learning for Mobile Agents with Individual Goals,"Anahita Mohseni-Kabir, David Isele, and Kikuo Fujimura",2019,Artificial Intelligence,1909.12925
"Robotic navigation through crowds or herds requires the ability to both predict the future motion of nearby individuals and understand how these predictions might change in response to a robot's future action. State of the art trajectory prediction models using Recurrent Neural Networks (RNNs) do not currently account for a planned future action of a robot, and so cannot predict how an individual will move in response to a robot's planned path. We propose an approach that adapts RNNs to use a robot's next planned action as an input alongside the current position of nearby individuals. This allows the model to learn the response of individuals with regards to a robot's motion from real world observations. By linking a robot's actions to the response of those around it in training, we show that we are able to not only improve prediction accuracy in close range interactions, but also to predict the likely response of surrounding individuals to simulated actions. This allows the use of the model to simulate state transitions, without requiring any assumptions on agent interaction. We apply this model to varied datasets, including crowds of pedestrians interacting with vehicles and bicycles, and livestock interacting with a robotic vehicle.",Predicting Responses to a Robot's Future Motion using Generative Recurrent Neural Networks,Stuart Eiffert and Salah Sukkarieh,2019,Artificial Intelligence,1909.13486
"Often, when modeling human decision-making behaviors in the context of human-robot teaming, the emotion aspect of human is ignored. Nevertheless, the influence of emotion, in some cases, is not only undeniable but beneficial. This work studies the human-like characteristics brought by regret emotion in one-human-multi-robot teaming for the application of domain search. In such application, the task management load is outsourced to the robots to reduce the human's workload, freeing the human to do more important work. The regret decision model is first used by each robot for deciding whether to request human service, then is extended for optimally queuing the requests from multiple robots. For the movement of the robots in the domain search, we designed a path planning algorithm based on dynamic programming for each robot. The simulation shows that the human-like characteristics, namely, risk-seeking and risk-aversion, indeed bring some appealing effects for balancing the workload and performance in the human-multi-robot team.",Respect Your Emotion: Human-Multi-Robot Teaming based on Regret Decision Model,"Longsheng Jiang, Yue Wang",2019,Artificial Intelligence,1910.00087
"Combining neural networks with continuous logic and multicriteria decision making tools can reduce the black box nature of neural models. In this study, we show that nilpotent logical systems offer an appropriate mathematical framework for a hybridization of continuous nilpotent logic and neural models, helping to improve the interpretability and safety of machine learning. In our concept, perceptrons model soft inequalities; namely membership functions and continuous logical operators. We design the network architecture before training, using continuous logical operators and multicriteria decision tools with given weights working in the hidden layers. Designing the structure appropriately leads to a drastic reduction in the number of parameters to be learned. The theoretical basis offers a straightforward choice of activation functions (the cutting function or its differentiable approximation, the squashing function), and also suggests an explanation to the great success of the rectified linear unit (ReLU). In this study, we focus on the architecture of a hybrid model and introduce the building blocks for future application in deep neural networks. The concept is illustrated with some toy examples taken from an extended version of the tensorflow playground.",Interpretable neural networks based on continuous-valued logic and multicriteria decision operators,"Orsolya Csisz\'ar, G\'abor Csisz\'ar, J\'ozsef Dombi",2020,Artificial Intelligence,1910.02486
"Legg and Hutter, as well as subsequent authors, considered intelligent agents through the lens of interaction with reward-giving environments, attempting to assign numeric intelligence measures to such agents, with the guiding principle that a more intelligent agent should gain higher rewards from environments in some aggregate sense. In this paper, we consider a related question: rather than measure numeric intelligence of one Legg- Hutter agent, how can we compare the relative intelligence of two Legg-Hutter agents? We propose an elegant answer based on the following insight: we can view Legg-Hutter agents as candidates in an election, whose voters are environments, letting each environment vote (via its rewards) which agent (if either) is more intelligent. This leads to an abstract family of comparators simple enough that we can prove some structural theorems about them. It is an open question whether these structural theorems apply to more practical intelligence measures.",Intelligence via ultrafilters: structural properties of some intelligence comparators of deterministic Legg-Hutter agents,Samuel Allen Alexander,2019,Artificial Intelligence,1910.09721
"Using neural networks in the reinforcement learning (RL) framework has achieved notable successes. Yet, neural networks tend to forget what they learned in the past, especially when they learn online and fully incrementally, a setting in which the weights are updated after each sample is received and the sample is then discarded. Under this setting, an update can lead to overly global generalization by changing too many weights. The global generalization interferes with what was previously learned and deteriorates performance, a phenomenon known as catastrophic interference. Many previous works use mechanisms such as experience replay (ER) buffers to mitigate interference by performing minibatch updates, ensuring the data distribution is approximately independent-and-identically-distributed (i.i.d.). But using ER would become infeasible in terms of memory as problem complexity increases. Thus, it is crucial to look for more memory-efficient alternatives. Interference can be averted if we replace global updates with more local ones, so only weights responsible for the observed data sample are updated. In this work, we propose the use of dynamic self-organizing map (DSOM) with neural networks to induce such locality in the updates without ER buffers. Our method learns a DSOM to produce a mask to reweigh each hidden unit's output, modulating its degree of use. It prevents interference by replacing global updates with local ones, conditioned on the agent's state. We validate our method on standard RL benchmarks including Mountain Car and Lunar Lander, where existing methods often fail to learn without ER. Empirically, we show that our online and fully incremental method is on par with and in some cases, better than state-of-the-art in terms of final performance and learning speed. We provide visualizations and quantitative measures to show that our method indeed mitigates interference.",Overcoming Catastrophic Interference in Online Reinforcement Learning with Dynamic Self-Organizing Maps,Yat Long Lo and Sina Ghiassian,2019,Artificial Intelligence,1910.13213
"The fourth edition of the international workshop on Causation, Responsibility and Explanation took place in Prague (Czech Republic) as part of ETAPS 2019. The program consisted in 5 invited speakers and 4 regular papers, whose selection was based on a careful reviewing process and that are included in these proceedings.","Proceedings of the 4th Workshop on Formal Reasoning about Causation, Responsibility, and Explanations in Science and Technology","Georgiana Caltais (Konstanz University), Jean Krivine (CNRS)",2019,Artificial Intelligence,1910.13641
"Information retrieval (IR) systems need to constantly update their knowledge as target objects and user queries change over time. Due to the power-law nature of linguistic data, learning lexical concepts is a problem resisting standard machine learning approaches: while manual intervention is always possible, a more general and automated solution is desirable. In this work, we propose a novel end-to-end framework that models the interaction between a search engine and users as a virtuous human-in-the-loop inference. The proposed framework is the first to our knowledge combining ideas from psycholinguistics and experiment design to maximize efficiency in IR. We provide a brief overview of the main components and initial simulations in a toy world, showing how inference works end-to-end and discussing preliminary results and next steps.",Lexical Learning as an Online Optimal Experiment: Building Efficient Search Engines through Human-Machine Collaboration,"Jacopo Tagliabue, Reuben Cohn-Gordon",2019,Artificial Intelligence,1910.14164
"The Shapes Constraint Language (SHACL) has been recently introduced as a W3C recommendation to define constraints that can be validated against RDF graphs. Interactions of SHACL with other Semantic Web technologies, such as ontologies or reasoners, is a matter of ongoing research. In this paper we study the interaction of a subset of SHACL with inference rules expressed in datalog. On the one hand, SHACL constraints can be used to define a ""schema"" for graph datasets. On the other hand, inference rules can lead to the discovery of new facts that do not match the original schema. Given a set of SHACL constraints and a set of datalog rules, we present a method to detect which constraints could be violated by the application of the inference rules on some graph instance of the schema, and update the original schema, i.e, the set of SHACL constraints, in order to capture the new facts that can be inferred. We provide theoretical and experimental results of the various components of our approach.",SHACL Constraints with Inference Rules,"Paolo Pareti, George Konstantinidis, Timothy J. Norman, Murat \c{S}ensoy",2019,Artificial Intelligence,1911.00598
"The debate on AI ethics largely focuses on technical improvements and stronger regulation to prevent accidents or misuse of AI, with solutions relying on holding individual actors accountable for responsible AI development. While useful and necessary, we argue that this ""agency"" approach disregards more indirect and complex risks resulting from AI's interaction with the socio-economic and political context. This paper calls for a ""structural"" approach to assessing AI's effects in order to understand and prevent such systemic risks where no individual can be held accountable for the broader negative impacts. This is particularly relevant for AI applied to systemic issues such as climate change and food security which require political solutions and global cooperation. To properly address the wide range of AI risks and ensure 'AI for social good', agency-focused policies must be complemented by policies informed by a structural approach.",AI Ethics for Systemic Issues: A Structural Approach,"Agnes Schim van der Loeff, Iggy Bassi, Sachin Kapila, Jevgenij Gamper",2019,Artificial Intelligence,1911.03216
"In level co-creation an AI and human work together to create a video game level. One open challenge in level co-creation is how to empower human users to ensure particular qualities of the final level, such as challenge. There has been significant prior research into automated pathing and automated playtesting for video game levels, but not in how to incorporate these into tools. In this demonstration we present an improvement of the Morai Maker mixed-initiative level editor for Super Mario Bros. that includes automated pathing and challenge approximation features.",Integrating Automated Play in Level Co-Creation,"Andrew Hoyt, Matthew Guzdial, Yalini Kumar, Gillian Smith, and Mark O. Riedl",2019,Artificial Intelligence,1911.09219
"Recent superhuman results in games have largely been achieved in a variety of zero-sum settings, such as Go and Poker, in which agents need to compete against others. However, just like humans, real-world AI systems have to coordinate and communicate with other agents in cooperative partially observable environments as well. These settings commonly require participants to both interpret the actions of others and to act in a way that is informative when being interpreted. Those abilities are typically summarized as theory f mind and are seen as crucial for social interactions. In this paper we propose two different search techniques that can be applied to improve an arbitrary agreed-upon policy in a cooperative partially observable game. The first one, single-agent search, effectively converts the problem into a single agent setting by making all but one of the agents play according to the agreed-upon policy. In contrast, in multi-agent search all agents carry out the same common-knowledge search procedure whenever doing so is computationally feasible, and fall back to playing according to the agreed-upon policy otherwise. We prove that these search procedures are theoretically guaranteed to at least maintain the original performance of the agreed-upon policy (up to a bounded approximation error). In the benchmark challenge problem of Hanabi, our search technique greatly improves the performance of every agent we tested and when applied to a policy trained using RL achieves a new state-of-the-art score of 24.61 / 25 in the game, compared to a previous-best of 24.08 / 25.",Improving Policies via Search in Cooperative Partially Observable Games,"Adam Lerer, Hengyuan Hu, Jakob Foerster, Noam Brown",2020,Artificial Intelligence,1912.02318
"SUMMARY: Recently, novel machine-learning algorithms have shown potential for predicting undiscovered links in biomedical knowledge networks. However, dedicated benchmarks for measuring algorithmic progress have not yet emerged. With OpenBioLink, we introduce a large-scale, high-quality and highly challenging biomedical link prediction benchmark to transparently and reproducibly evaluate such algorithms. Furthermore, we present preliminary baseline evaluation results. AVAILABILITY AND IMPLEMENTATION: Source code, data and supplementary files are openly available at https://github.com/OpenBioLink/OpenBioLink CONTACT: matthias.samwald ((at)) meduniwien.ac.at",OpenBioLink: A benchmarking framework for large-scale biomedical link prediction,"Anna Breit, Simon Ott, Asan Agibetov, Matthias Samwald",2020,Artificial Intelligence,1912.04616
"The main goal of this paper is to describe an axiomatic utility theory for Dempster-Shafer belief function lotteries. The axiomatic framework used is analogous to von Neumann-Morgenstern's utility theory for probabilistic lotteries as described by Luce and Raiffa. Unlike the probabilistic case, our axiomatic framework leads to interval-valued utilities, and therefore, to a partial (incomplete) preference order on the set of all belief function lotteries. If the belief function reference lotteries we use are Bayesian belief functions, then our representation theorem coincides with Jaffray's representation theorem for his linear utility theory for belief functions. We illustrate our representation theorem using some examples discussed in the literature, and we propose a simple model for assessing utilities based on an interval-valued pessimism index representing a decision-maker's attitude to ambiguity and indeterminacy. Finally, we compare our decision theory with those proposed by Jaffray, Smets, Dubois et al., Giang and Shenoy, and Shafer.",An Interval-Valued Utility Theory for Decision Making with Dempster-Shafer Belief Functions,Thierry Denoeux and Prakash P. Shenoy,2020,Artificial Intelligence,1912.06594
"Counterfactual Thinking is a human cognitive ability studied in a wide variety of domains. It captures the process of reasoning about a past event that did not occur, namely what would have happened had this event occurred, or, otherwise, to reason about an event that did occur but what would ensue had it not. Given the wide cognitive empowerment of counterfactual reasoning in the human individual, the question arises of how the presence of individuals with this capability may improve cooperation in populations of self-regarding individuals. Here we propose a mathematical model, grounded on Evolutionary Game Theory, to examine the population dynamics emerging from the interplay between counterfactual thinking and social learning (i.e., individuals that learn from the actions and success of others) whenever the individuals in the population face a collective dilemma. Our results suggest that counterfactual reasoning fosters coordination in collective action problems occurring in large populations, and has a limited impact on cooperation dilemmas in which coordination is not required. Moreover, we show that a small prevalence of individuals resorting to counterfactual thinking is enough to nudge an entire population towards highly cooperative standards.",Counterfactual thinking in cooperation dynamics,Luis Moniz Pereira and Francisco C. Santos,2019,Artificial Intelligence,1912.08946
"Recent work on fairness in machine learning has primarily emphasized how to define, quantify, and encourage ""fair"" outcomes. Less attention has been paid, however, to the ethical foundations which underlie such efforts. Among the ethical perspectives that should be taken into consideration is consequentialism, the position that, roughly speaking, outcomes are all that matter. Although consequentialism is not free from difficulties, and although it does not necessarily provide a tractable way of choosing actions (because of the combined problems of uncertainty, subjectivity, and aggregation), it nevertheless provides a powerful foundation from which to critique the existing literature on machine learning fairness. Moreover, it brings to the fore some of the tradeoffs involved, including the problem of who counts, the pros and cons of using a policy, and the relative value of the distant future. In this paper we provide a consequentialist critique of common definitions of fairness within machine learning, as well as a machine learning perspective on consequentialism. We conclude with a broader discussion of the issues of learning and randomization, which have important implications for the ethics of automated decision making systems.",On Consequentialism and Fairness,Dallas Card and Noah A. Smith,2020,Artificial Intelligence,2001.00329
"The wide adoption of machine learning in the critical domains such as medical diagnosis, law, education had propelled the need for interpretable techniques due to the need for end users to understand the reasoning behind decisions due to learning systems. The computational intractability of interpretable learning led practitioners to design heuristic techniques, which fail to provide sound handles to tradeoff accuracy and interpretability. Motivated by the success of MaxSAT solvers over the past decade, recently MaxSAT-based approach, called MLIC, was proposed that seeks to reduce the problem of learning interpretable rules expressed in Conjunctive Normal Form (CNF) to a MaxSAT query. While MLIC was shown to achieve accuracy similar to that of other state of the art black-box classifiers while generating small interpretable CNF formulas, the runtime performance of MLIC is significantly lagging and renders approach unusable in practice. In this context, authors raised the question: Is it possible to achieve the best of both worlds, i.e., a sound framework for interpretable learning that can take advantage of MaxSAT solvers while scaling to real-world instances? In this paper, we take a step towards answering the above question in affirmation. We propose IMLI: an incremental approach to MaxSAT based framework that achieves scalable runtime performance via partition-based training methodology. Extensive experiments on benchmarks arising from UCI repository demonstrate that IMLI achieves up to three orders of magnitude runtime improvement without loss of accuracy and interpretability.",IMLI: An Incremental Framework for MaxSAT-Based Learning of Interpretable Classification Rules,Bishwamittra Ghosh and Kuldeep S. Meel,2019,Artificial Intelligence,2001.01891
"This work presents an architecture that generates curiosity-driven goal-directed exploration behaviours for an image sensor of a microfarming robot. A combination of deep neural networks for offline unsupervised learning of low-dimensional features from images, and of online learning of shallow neural networks representing the inverse and forward kinematics of the system have been used. The artificial curiosity system assigns interest values to a set of pre-defined goals, and drives the exploration towards those that are expected to maximise the learning progress. We propose the integration of an episodic memory in intrinsic motivation systems to face catastrophic forgetting issues, typically experienced when performing online updates of artificial neural networks. Our results show that adopting an episodic memory system not only prevents the computational models from quickly forgetting knowledge that has been previously acquired, but also provides new avenues for modulating the balance between plasticity and stability of the models.",Intrinsic Motivation and Episodic Memories for Robot Exploration of High-Dimensional Sensory Spaces,"Guido Schillaci, Antonio Pico Villalpando, Verena Vanessa Hafner, Peter Hanappe, David Colliaux, Timoth\'ee Wintz",2020,Artificial Intelligence,2001.01982
"Autonomous systems are often required to operate in partially observable environments. They must reliably execute a specified objective even with incomplete information about the state of the environment. We propose a methodology to synthesize policies that satisfy a linear temporal logic formula in a partially observable Markov decision process (POMDP). By formulating a planning problem, we show how to use point-based value iteration methods to efficiently approximate the maximum probability of satisfying a desired logical formula and compute the associated belief state policy. We demonstrate that our method scales to large POMDP domains and provides strong bounds on the performance of the resulting policy.",Point-Based Methods for Model Checking in Partially Observable Markov Decision Processes,"Maxime Bouton, Jana Tumova, and Mykel J. Kochenderfer",2020,Artificial Intelligence,2001.03809
"Explaining sophisticated machine-learning based systems is an important issue at the foundations of AI. Recent efforts have shown various methods for providing explanations. These approaches can be broadly divided into two schools: those that provide a local and human interpreatable approximation of a machine learning algorithm, and logical approaches that exactly characterise one aspect of the decision. In this paper we focus upon the second school of exact explanations with a rigorous logical foundation. There is an epistemological problem with these exact methods. While they can furnish complete explanations, such explanations may be too complex for humans to understand or even to write down in human readable form. Interpretability requires epistemically accessible explanations, explanations humans can grasp. Yet what is a sufficiently complete epistemically accessible explanation still needs clarification. We do this here in terms of counterfactuals, following [Wachter et al., 2017]. With counterfactual explanations, many of the assumptions needed to provide a complete explanation are left implicit. To do so, counterfactual explanations exploit the properties of a particular data point or sample, and as such are also local as well as partial explanations. We explore how to move from local partial explanations to what we call complete local explanations and then to global ones. But to preserve accessibility we argue for the need for partiality. This partiality makes it possible to hide explicit biases present in the algorithm that may be injurious or unfair.We investigate how easy it is to uncover these biases in providing complete and fair explanations by exploiting the structure of the set of counterfactuals providing a complete local explanation.",Adequate and fair explanations,"Nicholas Asher, Soumya Paul, Chris Russell",2021,Artificial Intelligence,2001.07578
"Automated decision making based on big data and machine learning (ML) algorithms can result in discriminatory decisions against certain protected groups defined upon personal data like gender, race, sexual orientation etc. Such algorithms designed to discover patterns in big data might not only pick up any encoded societal biases in the training data, but even worse, they might reinforce such biases resulting in more severe discrimination. The majority of thus far proposed fairness-aware machine learning approaches focus solely on the pre-, in- or post-processing steps of the machine learning process, that is, input data, learning algorithms or derived models, respectively. However, the fairness problem cannot be isolated to a single step of the ML process. Rather, discrimination is often a result of complex interactions between big data and algorithms, and therefore, a more holistic approach is required. The proposed FAE (Fairness-Aware Ensemble) framework combines fairness-related interventions at both pre- and postprocessing steps of the data analysis process. In the preprocessing step, we tackle the problems of under-representation of the protected group (group imbalance) and of class-imbalance by generating balanced training samples. In the post-processing step, we tackle the problem of class overlapping by shifting the decision boundary in the direction of fairness.",FAE: A Fairness-Aware Ensemble Framework,"Vasileios Iosifidis, Besnik Fetahu, Eirini Ntoutsi",2019,Artificial Intelligence,2002.00695
"Task-allocation is an important problem in multi-agent systems. It becomes more challenging when the team-members are humans with imperfect knowledge about their teammates' costs and the overall performance metric. While distributed task-allocation methods let the team-members engage in iterative dialog to reach a consensus, the process can take a considerable amount of time and communication. On the other hand, a centralized method that simply outputs an allocation may result in discontented human team-members who, due to their imperfect knowledge and limited computation capabilities, perceive the allocation to be unfair. To address these challenges, we propose a centralized Artificial Intelligence Task Allocation (AITA) that simulates a negotiation and produces a negotiation-aware task allocation that is fair. If a team-member is unhappy with the proposed allocation, we allow them to question the proposed allocation using a counterfactual. By using parts of the simulated negotiation, we are able to provide contrastive explanations that providing minimum information about other's costs to refute their foil. With human studies, we show that (1) the allocation proposed using our method does indeed appear fair to the majority, and (2) when a counterfactual is raised, explanations generated are easy to comprehend and convincing. Finally, we empirically study the effect of different kinds of incompleteness on the explanation-length and find that underestimation of a teammate's costs often increases it.",`Why didn't you allocate this task to them?' Negotiation-Aware Task Allocation and Contrastive Explanation Generation,"Zahra Zahedi, Sailik Sengupta, Subbarao Kambhampati",2020,Artificial Intelligence,2002.01640
"Monte-Carlo Tree Search (MCTS) is one of the most-widely used methods for planning, and has powered many recent advances in artificial intelligence. In MCTS, one typically performs computations (i.e., simulations) to collect statistics about the possible future consequences of actions, and then chooses accordingly. Many popular MCTS methods such as UCT and its variants decide which computations to perform by trading-off exploration and exploitation. In this work, we take a more direct approach, and explicitly quantify the value of a computation based on its expected impact on the quality of the action eventually chosen. Our approach goes beyond the ""myopic"" limitations of existing computation-value-based methods in two senses: (I) we are able to account for the impact of non-immediate (ie, future) computations (II) on non-immediate actions. We show that policies that greedily optimize computation values are optimal under certain assumptions and obtain results that are competitive with the state-of-the-art.",Static and Dynamic Values of Computation in MCTS,Eren Sezener and Peter Dayan,2020,Artificial Intelligence,2002.04335
"Information theory can be used to analyze the cost-benefit of visualization processes. However, the current measure of benefit contains an unbounded term that is neither easy to estimate nor intuitive to interpret. In this work, we propose to revise the existing cost-benefit measure by replacing the unbounded term with a bounded one. We examine a number of bounded measures that include the Jenson-Shannon divergence and a new divergence measure formulated as part of this work. We use visual analysis to support the multi-criteria comparison, narrowing the search down to those options with better mathematical properties. We apply those remaining options to two visualization case studies to instantiate their uses in practical scenarios, while the collected real world data further informs the selection of a bounded measure, which can be used to estimate the benefit of visualization.",A Bounded Measure for Estimating the Benefit of Visualization,"Min Chen, Mateu Sbert, Alfie Abdul-Rahman, and Deborah Silver",2022,Artificial Intelligence,2002.05282
"Developmental machine learning studies how artificial agents can model the way children learn open-ended repertoires of skills. Such agents need to create and represent goals, select which ones to pursue and learn to achieve them. Recent approaches have considered goal spaces that were either fixed and hand-defined or learned using generative models of states. This limited agents to sample goals within the distribution of known effects. We argue that the ability to imagine out-of-distribution goals is key to enable creative discoveries and open-ended learning. Children do so by leveraging the compositionality of language as a tool to imagine descriptions of outcomes they never experienced before, targeting them as goals during play. We introduce IMAGINE, an intrinsically motivated deep reinforcement learning architecture that models this ability. Such imaginative agents, like children, benefit from the guidance of a social peer who provides language descriptions. To take advantage of goal imagination, agents must be able to leverage these descriptions to interpret their imagined out-of-distribution goals. This generalization is made possible by modularity: a decomposition between learned goal-achievement reward function and policy relying on deep sets, gated attention and object-centered representations. We introduce the Playground environment and study how this form of goal imagination improves generalization and exploration over agents lacking this capacity. In addition, we identify the properties of goal imagination that enable these results and study the impacts of modularity and social interactions.",Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven Exploration,"C\'edric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Cl\'ement Moulin-Frier, Peter Ford Dominey, Pierre-Yves Oudeyer",2020,Artificial Intelligence,2002.09253
"In this paper I argue that the search for explainable models and interpretable decisions in AI must be reformulated in terms of the broader project of offering a pragmatic and naturalistic account of understanding in AI. Intuitively, the purpose of providing an explanation of a model or a decision is to make it understandable to its stakeholders. But without a previous grasp of what it means to say that an agent understands a model or a decision, the explanatory strategies will lack a well-defined goal. Aside from providing a clearer objective for XAI, focusing on understanding also allows us to relax the factivity condition on explanation, which is impossible to fulfill in many machine learning models, and to focus instead on the pragmatic conditions that determine the best fit between a model and the methods and devices deployed to understand it. After an examination of the different types of understanding discussed in the philosophical and psychological literature, I conclude that interpretative or approximation models not only provide the best way to achieve the objectual understanding of a machine learning model, but are also a necessary condition to achieve post-hoc interpretability. This conclusion is partly based on the shortcomings of the purely functionalist approach to post-hoc interpretability that seems to be predominant in most recent literature.",The Pragmatic Turn in Explainable Artificial Intelligence (XAI),Andr\'es P\'aez,2019,Artificial Intelligence,2002.09595
"Knowledge Graphs (KGs) are graph-structured knowledge bases storing factual information about real-world entities. Understanding the uniqueness of each entity is crucial to the analyzing, sharing, and reusing of KGs. Traditional profiling technologies encompass a vast array of methods to find distinctive features in various applications, which can help to differentiate entities in the process of human understanding of KGs. In this work, we present a novel profiling approach to identify distinctive entity features. The distinctiveness of features is carefully measured by a HAS model, which is a scalable representation learning model to produce a multi-pattern entity embedding. We fully evaluate the quality of entity profiles generated from real KGs. The results show that our approach facilitates human understanding of entities in KGs.",Entity Profiling in Knowledge Graphs,"Xiang Zhang, Qingqing Yang, Jinru Ding and Ziyue Wang",2020,Artificial Intelligence,2003.00172
"In this paper we discuss how systems with Artificial Intelligence (AI) can undergo safety assessment. This is relevant, if AI is used in safety related applications. Taking a deeper look into AI models, we show, that many models of artificial intelligence, in particular machine learning, are statistical models. Safety assessment would then have t o concentrate on the model that is used in AI, besides the normal assessment procedure. Part of the budget of dangerous random failures for the relevant safety integrity level needs to be used for the probabilistic faulty behavior of the AI system. We demonstrate our thoughts with a simple example and propose a research challenge that may be decisive for the use of AI in safety related systems.",On Safety Assessment of Artificial Intelligence,"Jens Braband and Hendrik Sch\""abe",2020,Artificial Intelligence,2003.00260
"Explainability and interpretability of AI models is an essential factor affecting the safety of AI. While various explainable AI (XAI) approaches aim at mitigating the lack of transparency in deep networks, the evidence of the effectiveness of these approaches in improving usability, trust, and understanding of AI systems are still missing. We evaluate multimodal explanations in the setting of a Visual Question Answering (VQA) task, by asking users to predict the response accuracy of a VQA agent with and without explanations. We use between-subjects and within-subjects experiments to probe explanation effectiveness in terms of improving user prediction accuracy, confidence, and reliance, among other factors. The results indicate that the explanations help improve human prediction accuracy, especially in trials when the VQA system's answer is inaccurate. Furthermore, we introduce active attention, a novel method for evaluating causal attentional effects through intervention by editing attention maps. User explanation ratings are strongly correlated with human prediction accuracy and suggest the efficacy of these explanations in human-machine AI collaboration tasks.",A Study on Multimodal and Interactive Explanations for Visual Question Answering,"Kamran Alipour, Jurgen P. Schulze, Yi Yao, Avi Ziskind, Giedrius Burachas",2020,Artificial Intelligence,2003.00431
"Given the large variety of existing logical formalisms it is of utmost importance to select the most adequate one for a specific purpose, e.g. for representing the knowledge relevant for a particular application or for using the formalism as a modeling tool for problem solving. Awareness of the nature of a logical formalism, in other words, of its fundamental intrinsic properties, is indispensable and provides the basis of an informed choice. In this treatise we consider the existence characterization logics as well as properties like existence and uniqueness, expressibility, replaceability and verifiability in the realm of abstract argumentation",On the Existence of Characterization Logics and Fundamental Properties of Argumentation Semantics,Ringo Baumann,2019,Artificial Intelligence,2003.00767
"ML models are increasingly deployed in settings with real world interactions such as vehicles, but unfortunately, these models can fail in systematic ways. To prevent errors, ML engineering teams monitor and continuously improve these models. We propose a new abstraction, model assertions, that adapts the classical use of program assertions as a way to monitor and improve ML models. Model assertions are arbitrary functions over a model's input and output that indicate when errors may be occurring, e.g., a function that triggers if an object rapidly changes its class in a video. We propose methods of using model assertions at all stages of ML system deployment, including runtime monitoring, validating labels, and continuously improving ML models. For runtime monitoring, we show that model assertions can find high confidence errors, where a model returns the wrong output with high confidence, which uncertainty-based monitoring techniques would not detect. For training, we propose two methods of using model assertions. First, we propose a bandit-based active learning algorithm that can sample from data flagged by assertions and show that it can reduce labeling costs by up to 40% over traditional uncertainty-based methods. Second, we propose an API for generating ""consistency assertions"" (e.g., the class change example) and weak labels for inputs where the consistency assertions fail, and show that these weak labels can improve relative model quality by up to 46%. We evaluate model assertions on four real-world tasks with video, LIDAR, and ECG data.",Model Assertions for Monitoring and Improving ML Models,"Daniel Kang, Deepti Raghavan, Peter Bailis, Matei Zaharia",2020,Artificial Intelligence,2003.01668
"We present a novel technique called Dynamic Experience Replay (DER) that allows Reinforcement Learning (RL) algorithms to use experience replay samples not only from human demonstrations but also successful transitions generated by RL agents during training and therefore improve training efficiency. It can be combined with an arbitrary off-policy RL algorithm, such as DDPG or DQN, and their distributed versions. We build upon Ape-X DDPG and demonstrate our approach on robotic tight-fitting joint assembly tasks, based on force/torque and Cartesian pose observations. In particular, we run experiments on two different tasks: peg-in-hole and lap-joint. In each case, we compare different replay buffer structures and how DER affects them. Our ablation studies show that Dynamic Experience Replay is a crucial ingredient that either largely shortens the training time in these challenging environments or solves the tasks that the vanilla Ape-X DDPG cannot solve. We also show that our policies learned purely in simulation can be deployed successfully on the real robot. The video presenting our experiments is available at https://sites.google.com/site/dynamicexperiencereplay",Dynamic Experience Replay,Jieliang Luo and Hui Li,2020,Artificial Intelligence,2003.02372
"We propose the Interactive Constrained MAP-Elites, a quality-diversity solution for game content generation, implemented as a new feature of the Evolutionary Dungeon Designer: a mixed-initiative co-creativity tool for designing dungeons. The feature uses the MAP-Elites algorithm, an illumination algorithm that segregates the population among several cells depending on their scores with respect to different behavioral dimensions. Users can flexibly and dynamically alternate between these dimensions anytime, thus guiding the evolutionary process in an intuitive way, and then incorporate suggestions produced by the algorithm in their room designs. At the same time, any modifications performed by the human user will feed back into MAP-Elites, closing a circular workflow of constant mutual inspiration. This paper presents the algorithm followed by an in-depth analysis of its behaviour, with the aims of evaluating the expressive range of all possible dimension combinations in several scenarios, as well as discussing their influence in the fitness landscape and in the overall performance of the mixed-initiative procedural content generation.",Interactive Constrained MAP-Elites: Analysis and Evaluation of the Expressiveness of the Feature Dimensions,"Alberto Alvarez, Steve Dahlskog, Jose Font and Julian Togelius",2020,Artificial Intelligence,2003.03377
"We describe nearly fifteen years of General Game Playing experimental research history in the context of reproducibility and fairness of comparisons between various GGP agents and systems designed to play games described by different formalisms. We think our survey may provide an interesting perspective of how chaotic methods were allowed when nothing better was possible. Finally, from our experience-based view, we would like to propose a few recommendations of how such specific heterogeneous branch of research should be handled appropriately in the future. The goal of this note is to point out common difficulties and problems in the experimental research in the area. We hope that our recommendations will help in avoiding them in future works and allow more fair and reproducible comparisons.",Experimental Studies in General Game Playing: An Experience Report,"Jakub Kowalski, Marek Szyku{\l}a",2020,Artificial Intelligence,2003.03410
"Continual learning studies agents that learn from streams of tasks without forgetting previous ones while adapting to new ones. Two recent continual-learning scenarios have opened new avenues of research. In meta-continual learning, the model is pre-trained to minimize catastrophic forgetting of previous tasks. In continual-meta learning, the aim is to train agents for faster remembering of previous tasks through adaptation. In their original formulations, both methods have limitations. We stand on their shoulders to propose a more general scenario, OSAKA, where an agent must quickly solve new (out-of-distribution) tasks, while also requiring fast remembering. We show that current continual learning, meta-learning, meta-continual learning, and continual-meta learning techniques fail in this new scenario. We propose Continual-MAML, an online extension of the popular MAML algorithm as a strong baseline for this scenario. We empirically show that Continual-MAML is better suited to the new scenario than the aforementioned methodologies, as well as standard continual learning and meta-learning approaches.",Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning,"Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Caccia, Issam Laradji, Irina Rish, Alexandre Lacoste, David Vazquez, Laurent Charlin",2020,Artificial Intelligence,2003.05856
"Smart home environments equipped with distributed sensor networks are capable of helping people by providing services related to health, emergency detection or daily routine management. A backbone to these systems relies often on the system's ability to track and detect activities performed by the users in their home. Despite the continuous progress in the area of activity recognition in smart homes, many systems make a strong underlying assumption that the number of occupants in the home at any given moment of time is always known. Estimating the number of persons in a Smart Home at each time step remains a challenge nowadays. Indeed, unlike most (crowd) counting solution which are based on computer vision techniques, the sensors considered in a Smart Home are often very simple and do not offer individually a good overview of the situation. The data gathered needs therefore to be fused in order to infer useful information. This paper aims at addressing this challenge and presents a probabilistic approach able to estimate the number of persons in the environment at each time step. This approach works in two steps: first, an estimate of the number of persons present in the environment is done using a Constraint Satisfaction Problem solver, based on the topology of the sensor network and the sensor activation pattern at this time point. Then, a Hidden Markov Model refines this estimate by considering the uncertainty related to the sensors. Using both simulated and real data, our method has been tested and validated on two smart homes of different sizes and configuration and demonstrates the ability to accurately estimate the number of inhabitants.",Online Guest Detection in a Smart Home using Pervasive Sensors and Probabilistic Reasoning,"Jennifer Renoux, Uwe K\""ockemann, Amy Loutfi",2018,Artificial Intelligence,2003.06347
"Autonomous driving is of great interest to industry and academia alike. The use of machine learning approaches for autonomous driving has long been studied, but mostly in the context of perception. In this paper we take a deeper look on the so called end-to-end approaches for autonomous driving, where the entire driving pipeline is replaced with a single neural network. We review the learning methods, input and output modalities, network architectures and evaluation schemes in end-to-end driving literature. Interpretability and safety are discussed separately, as they remain challenging for this approach. Beyond providing a comprehensive overview of existing methods, we conclude the review with an architecture that combines the most promising elements of the end-to-end autonomous driving systems.",A Survey of End-to-End Driving: Architectures and Training Methods,"Ardi Tampuu, Maksym Semikin, Naveed Muhammad, Dmytro Fishman and Tambet Matiisen",2020,Artificial Intelligence,2003.06404
"Decentralized online planning can be an attractive paradigm for cooperative multi-agent systems, due to improved scalability and robustness. A key difficulty of such approach lies in making accurate predictions about the decisions of other agents. In this paper, we present a trainable online decentralized planning algorithm based on decentralized Monte Carlo Tree Search, combined with models of teammates learned from previous episodic runs. By only allowing one agent to adapt its models at a time, under the assumption of ideal policy approximation, successive iterations of our method are guaranteed to improve joint policies, and eventually lead to convergence to a Nash equilibrium. We test the efficiency of the algorithm by performing experiments in several scenarios of the spatial task allocation environment introduced in [Claes et al., 2015]. We show that deep learning and convolutional neural networks can be employed to produce accurate policy approximators which exploit the spatial features of the problem, and that the proposed algorithm improves over the baseline planning performance for particularly challenging domain configurations.",Decentralized MCTS via Learned Teammate Models,"Aleksander Czechowski, Frans A. Oliehoek",2020,Artificial Intelligence,2003.08727
"We present a system that utilizes machine learning for tactic proof search in the Coq Proof Assistant. In a similar vein as the TacticToe project for HOL4, our system predicts appropriate tactics and finds proofs in the form of tactic scripts. To do this, it learns from previous tactic scripts and how they are applied to proof states. The performance of the system is evaluated on the Coq Standard Library. Currently, our predictor can identify the correct tactic to be applied to a proof state 23.4% of the time. Our proof searcher can fully automatically prove 39.3% of the lemmas. When combined with the CoqHammer system, the two systems together prove 56.7% of the library's lemmas.",Tactic Learning and Proving for the Coq Proof Assistant,"Lasse Blaauwbroek, Josef Urban, and Herman Geuvers",2020,Artificial Intelligence,2003.09140
"In process mining, process models are extracted from event logs using process discovery algorithms and are commonly assessed using multiple quality dimensions. While the metrics that measure the relationship of an extracted process model to its event log are well-studied, quantifying the level by which a process model can describe the unobserved behavior of its underlying system falls short in the literature. In this paper, a novel deep learning-based methodology called Adversarial System Variant Approximation (AVATAR) is proposed to overcome this issue. Sequence Generative Adversarial Networks are trained on the variants contained in an event log with the intention to approximate the underlying variant distribution of the system behavior. Unobserved realistic variants are sampled either directly from the Sequence Generative Adversarial Network or by leveraging the Metropolis-Hastings algorithm. The degree by which a process model relates to its underlying unknown system behavior is then quantified based on the realistic observed and estimated unobserved variants using established process model quality metrics. Significant performance improvements in revealing realistic unobserved variants are demonstrated in a controlled experiment on 15 ground truth systems. Additionally, the proposed methodology is experimentally tested and evaluated to quantify the generalization of 60 discovered process models with respect to their systems.",Adversarial System Variant Approximation to Quantify Process Model Generalization,Julian Theis and Houshang Darabi,2020,Artificial Intelligence,2003.12168
"The question of whether artificial beings or machines could become self-aware or consciousness has been a philosophical question for centuries. The main problem is that self-awareness cannot be observed from an outside perspective and the distinction of whether something is really self-aware or merely a clever program that pretends to do so cannot be answered without access to accurate knowledge about the mechanism's inner workings. We review the current state-of-the-art regarding these developments and investigate common machine learning approaches with respect to their potential ability to become self-aware. We realise that many important algorithmic steps towards machines with a core consciousness have already been devised. For human-level intelligence, however, many additional techniques have to be discovered.",Will we ever have Conscious Machines?,"Patrick Krauss, Andreas Maier",2020,Artificial Intelligence,2003.14132
"Two indicators are classically used to evaluate the quality of rule-based classification systems: predictive accuracy, i.e. the system's ability to successfully reproduce learning data and coverage, i.e. the proportion of possible cases for which the logical rules constituting the system apply. In this work, we claim that these two indicators may be insufficient, and additional measures of quality may need to be developed. We theoretically show that classification systems presenting ""good"" predictive accuracy and coverage can, nonetheless, be trivially improved and illustrate this proposition with examples.",On Evaluating the Quality of Rule-Based Classification Systems,Nassim Dehouche,2017,Artificial Intelligence,2004.02671
"Reinforcement Learning (RL) methods have emerged as a popular choice for training an efficient and effective dialogue policy. However, these methods suffer from sparse and unstable reward signals returned by a user simulator only when a dialogue finishes. Besides, the reward signal is manually designed by human experts, which requires domain knowledge. Recently, a number of adversarial learning methods have been proposed to learn the reward function together with the dialogue policy. However, to alternatively update the dialogue policy and the reward model on the fly, we are limited to policy-gradient-based algorithms, such as REINFORCE and PPO. Moreover, the alternating training of a dialogue agent and the reward model can easily get stuck in local optima or result in mode collapse. To overcome the listed issues, we propose to decompose the adversarial training into two steps. First, we train the discriminator with an auxiliary dialogue generator and then incorporate a derived reward model into a common RL method to guide the dialogue policy learning. This approach is applicable to both on-policy and off-policy RL methods. Based on our extensive experimentation, we can conclude the proposed method: (1) achieves a remarkable task success rate using both on-policy and off-policy RL methods; and (2) has the potential to transfer knowledge from existing domains to a new domain.",Guided Dialog Policy Learning without Adversarial Learning in the Loop,"Ziming Li, Sungjin Lee, Baolin Peng, Jinchao Li, Julia Kiseleva, Maarten de Rijke, Shahin Shayandeh, Jianfeng Gao",2020,Artificial Intelligence,2004.03267
"Purpose: In this study, the recently emerged advances in Fuzzy Cognitive Maps (FCM) are investigated and employed, for achieving the automatic and non-invasive diagnosis of Coronary Artery Disease (CAD). Methods: A Computer-Aided Diagnostic model for the acceptable and non-invasive prediction of CAD using the State Space Advanced FCM (AFCM) approach is proposed. Also, a rule-based mechanism is incorporated, to further increase the knowledge of the system and the interpretability of the decision mechanism. The proposed method is tested utilizing a CAD dataset from the Laboratory of Nuclear Medicine of the University of Patras. More specifically, two architectures of AFCMs are designed, and different parameter testing is performed. Furthermore, the proposed AFCMs, which are based on the new equations proposed recently, are compared with the traditional FCM approach. Results: The experiments highlight the effectiveness of the AFCM approach and the new equations over the traditional approach, which obtained an accuracy of 78.21%, achieving an increase of seven percent (+7%) on the classification task, and obtaining 85.47% accuracy. Conclusions: It is demonstrated that the AFCM approach in developing Fuzzy Cognitive Maps outperforms the conventional approach, while it constitutes a reliable method for the diagnosis of Coronary Artery Disease. Conclusions and future research related to recent pandemic of coronavirus are provided.",State Space Advanced Fuzzy Cognitive Map approach for automatic and non Invasive diagnosis of Coronary Artery Disease,"Ioannis D. Apostolopoulos, Peter P. Groumpos, Dimitris I. Apostolopoulos",2021,Artificial Intelligence,2004.03372
"In reinforcement learning (RL), dealing with non-stationarity is a challenging issue. However, some domains such as traffic optimization are inherently non-stationary. Causes for and effects of this are manifold. In particular, when dealing with traffic signal controls, addressing non-stationarity is key since traffic conditions change over time and as a function of traffic control decisions taken in other parts of a network. In this paper we analyze the effects that different sources of non-stationarity have in a network of traffic signals, in which each signal is modeled as a learning agent. More precisely, we study both the effects of changing the \textit{context} in which an agent learns (e.g., a change in flow rates experienced by it), as well as the effects of reducing agent observability of the true environment state. Partial observability may cause distinct states (in which distinct actions are optimal) to be seen as the same by the traffic signal agents. This, in turn, may lead to sub-optimal performance. We show that the lack of suitable sensors to provide a representative observation of the real state seems to affect the performance more drastically than the changes to the underlying traffic patterns.",Quantifying the Impact of Non-Stationarity in Reinforcement Learning-Based Traffic Signal Control,"Lucas N. Alegre, Ana L. C. Bazzan, Bruno C. da Silva",2021,Artificial Intelligence,2004.04778
"Reinforcement learning agents learn by encouraging behaviours which maximize their total reward, usually provided by the environment. In many environments, however, the reward is provided after a series of actions rather than each single action, leading the agent to experience ambiguity in terms of whether those actions are effective, an issue known as the credit assignment problem. In this paper, we propose two strategies inspired by behavioural psychology to enable the agent to intrinsically estimate more informative reward values for actions with no reward. The first strategy, called self-punishment (SP), discourages the agent from making mistakes that lead to undesirable terminal states. The second strategy, called the rewards backfill (RB), backpropagates the rewards between two rewarded actions. We prove that, under certain assumptions and regardless of the reinforcement learning algorithm used, these two strategies maintain the order of policies in the space of all possible policies in terms of their total reward, and, by extension, maintain the optimal policy. Hence, our proposed strategies integrate with any reinforcement learning algorithm that learns a value or action-value function through experience. We incorporated these two strategies into three popular deep reinforcement learning approaches and evaluated the results on thirty Atari games. After parameter tuning, our results indicate that the proposed strategies improve the tested methods in over 65 percent of tested games by up to over 25 times performance improvement.",Self Punishment and Reward Backfill for Deep Q-Learning,"Mohammad Reza Bonyadi, Rui Wang, Maryam Ziaei",2022,Artificial Intelligence,2004.05002
"In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. For this, we introduce an intermediate representation that is based on the logical query plan in a database called Operation Trees (OT). This representation allows us to invert the annotation process without losing flexibility in the types of queries that we generate. Furthermore, it allows for fine-grained alignment of query tokens to OT operations. In our method, we randomly generate OTs from a context-free grammar. Afterwards, annotators have to write the appropriate natural language question that is represented by the OT. Finally, the annotators assign the tokens to the OT operations. We apply the method to create a new corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases. We compare OTTA to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries. Finally, we train a state-of-the-art semantic parsing model on our data and show that our corpus is a challenging dataset and that the token alignment can be leveraged to increase the performance significantly.",A Methodology for Creating Question Answering Corpora Using Inverse Data Annotation,"Jan Deriu, Katsiaryna Mlynchyk, Philippe Schl\""apfer, Alvaro Rodrigo, Dirk von Gr\""unigen, Nicolas Kaiser, Kurt Stockinger, Eneko Agirre, and Mark Cieliebak",2020,Artificial Intelligence,2004.07633
"Recent progress in deep learning is essentially based on a ""big data for small tasks"" paradigm, under which massive amounts of data are used to train a classifier for a single narrow task. In this paper, we call for a shift that flips this paradigm upside down. Specifically, we propose a ""small data for big tasks"" paradigm, wherein a single artificial intelligence (AI) system is challenged to develop ""common sense"", enabling it to solve a wide range of tasks with little training data. We illustrate the potential power of this new paradigm by reviewing models of common sense that synthesize recent breakthroughs in both machine and human vision. We identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense. When taken as a unified concept, FPICU is concerned with the questions of ""why"" and ""how"", beyond the dominant ""what"" and ""where"" framework for understanding vision. They are invisible in terms of pixels but nevertheless drive the creation, maintenance, and development of visual scenes. We therefore coin them the ""dark matter"" of vision. Just as our universe cannot be understood by merely studying observable matter, we argue that vision cannot be understood without studying FPICU. We demonstrate the power of this perspective to develop cognitive AI systems with humanlike common sense by showing how to observe and apply FPICU with little training data to solve a wide range of challenging tasks, including tool use, planning, utility inference, and social learning. In summary, we argue that the next generation of AI must embrace ""dark"" humanlike common sense for solving novel tasks.","Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense","Yixin Zhu, Tao Gao, Lifeng Fan, Siyuan Huang, Mark Edmonds, Hangxin Liu, Feng Gao, Chi Zhang, Siyuan Qi, Ying Nian Wu, Joshua B. Tenenbaum, Song-Chun Zhu",2020,Artificial Intelligence,2004.09044
"Recently, AlphaZero has achieved landmark results in deep reinforcement learning, by providing a single self-play architecture that learned three different games at super human level. AlphaZero is a large and complicated system with many parameters, and success requires much compute power and fine-tuning. Reproducing results in other games is a challenge, and many researchers are looking for ways to improve results while reducing computational demands. AlphaZero's design is purely based on self-play and makes no use of labeled expert data ordomain specific enhancements; it is designed to learn from scratch. We propose a novel approach to deal with this cold-start problem by employing simple search enhancements at the beginning phase of self-play training, namely Rollout, Rapid Action Value Estimate (RAVE) and dynamically weighted combinations of these with the neural network, and Rolling Horizon Evolutionary Algorithms (RHEA). Our experiments indicate that most of these enhancements improve the performance of their baseline player in three different (small) board games, with especially RAVE based variants playing strongly.",Warm-Start AlphaZero Self-Play Search Enhancements,"Hui Wang, Mike Preuss, Aske Plaat",2020,Artificial Intelligence,2004.12357
"Black-box Artificial Intelligence (AI) methods, e.g. deep neural networks, have been widely utilized to build predictive models that can extract complex relationships in a dataset and make predictions for new unseen data records. However, it is difficult to trust decisions made by such methods since their inner working and decision logic is hidden from the user. Explainable Artificial Intelligence (XAI) refers to systems that try to explain how a black-box AI model produces its outcomes. Post-hoc XAI methods approximate the behavior of a black-box by extracting relationships between feature values and the predictions. Perturbation-based and decision set methods are among commonly used post-hoc XAI systems. The former explanators rely on random perturbations of data records to build local or global linear models that explain individual predictions or the whole model. The latter explanators use those feature values that appear more frequently to construct a set of decision rules that produces the same outcomes as the target black-box. However, these two classes of XAI methods have some limitations. Random perturbations do not take into account the distribution of feature values in different subspaces, leading to misleading approximations. Decision sets only pay attention to frequent feature values and miss many important correlations between features and class labels that appear less frequently but accurately represent decision boundaries of the model. In this paper, we address the above challenges by proposing an explanation method named Confident Itemsets Explanation (CIE). We introduce confident itemsets, a set of feature values that are highly correlated to a specific class label. CIE utilizes confident itemsets to discretize the whole decision space of a model to smaller subspaces.",Post-hoc explanation of black-box classifiers using confident itemsets,"Milad Moradi, Matthias Samwald",2021,Artificial Intelligence,2005.01992
"This paper analyses the application of artificial intelligence techniques to various areas of archaeology and more specifically: a) The use of software tools as a creative stimulus for the organization of exhibitions; the use of humanoid robots and holographic displays as guides that interact and involve museum visitors; b) The analysis of methods for the classification of fragments found in archaeological excavations and for the reconstruction of ceramics, with the recomposition of the parts of text missing from historical documents and epigraphs; c) The cataloguing and study of human remains to understand the social and historical context of belonging with the demonstration of the effectiveness of the AI techniques used; d) The detection of particularly difficult terrestrial archaeological sites with the analysis of the architectures of the Artificial Neural Networks most suitable for solving the problems presented by the site; the design of a study for the exploration of marine archaeological sites, located at depths that cannot be reached by man, through the construction of a freely explorable 3D version.",The computerization of archaeology: survey on AI techniques,Lorenzo Mantovan and Loris Nanni,2020,Artificial Intelligence,2005.02863
"Artificial behavioral agents are often evaluated based on their consistent behaviors and performance to take sequential actions in an environment to maximize some notion of cumulative reward. However, human decision making in real life usually involves different strategies and behavioral trajectories that lead to the same empirical outcome. Motivated by clinical literature of a wide range of neurological and psychiatric disorders, we propose here a more general and flexible parametric framework for sequential decision making that involves a two-stream reward processing mechanism. We demonstrated that this framework is flexible and unified enough to incorporate a family of problems spanning multi-armed bandits (MAB), contextual bandits (CB) and reinforcement learning (RL), which decompose the sequential decision making process in different levels. Inspired by the known reward processing abnormalities of many mental disorders, our clinically-inspired agents demonstrated interesting behavioral trajectories and comparable performance on simulated tasks with particular reward distributions, a real-world dataset capturing human decision-making in gambling tasks, and the PacMan game across different reward stationarities in a lifelong learning setting.","Unified Models of Human Behavioral Agents in Bandits, Contextual Bandits and RL","Baihan Lin, Guillermo Cecchi, Djallel Bouneffouf, Jenna Reinen, Irina Rish",2021,Artificial Intelligence,2005.04544
"Information gathering in a partially observable environment can be formulated as a reinforcement learning (RL), problem where the reward depends on the agent's uncertainty. For example, the reward can be the negative entropy of the agent's belief over an unknown (or hidden) variable. Typically, the rewards of an RL agent are defined as a function of the state-action pairs and not as a function of the belief of the agent; this hinders the direct application of deep RL methods for such tasks. This paper tackles the challenge of using belief-based rewards for a deep RL agent, by offering a simple insight that maximizing any convex function of the belief of the agent can be approximated by instead maximizing a prediction reward: a reward based on prediction accuracy. In particular, we derive the exact error between negative entropy and the expected prediction reward. This insight provides theoretical motivation for several fields using prediction rewards---namely visual attention, question answering systems, and intrinsic motivation---and highlights their connection to the usually distinct fields of active perception, active sensing, and sensor placement. Based on this insight we present deep anticipatory networks (DANs), which enables an agent to take actions to reduce its uncertainty without performing explicit belief inference. We present two applications of DANs: building a sensor selection system for tracking people in a shopping mall and learning discrete models of attention on fashion MNIST and MNIST digit classification.",Maximizing Information Gain in Partially Observable Environments via Prediction Reward,"Yash Satsangi, Sungsu Lim, Shimon Whiteson, Frans Oliehoek, Martha White",2020,Artificial Intelligence,2005.04912
"We explore state-of-the-art neural models for question answering on electronic medical records and improve their ability to generalize better on previously unseen (paraphrased) questions at test time. We enable this by learning to predict logical forms as an auxiliary task along with the main task of answer span detection. The predicted logical forms also serve as a rationale for the answer. Further, we also incorporate medical entity information in these models via the ERNIE architecture. We train our models on the large-scale emrQA dataset and observe that our multi-task entity-enriched models generalize to paraphrased questions ~5% better than the baseline BERT model.",Entity-Enriched Neural Models for Clinical Question Answering,"Bhanu Pratap Singh Rawat, Wei-Hung Weng, So Yeon Min, Preethi Raghavan, Peter Szolovits",2020,Artificial Intelligence,2005.06587
"The travelling thief problem (TTP) is a multi-component optimisation problem involving two interdependent NP-hard components: the travelling salesman problem (TSP) and the knapsack problem (KP). Recent state-of-the-art TTP solvers modify the underlying TSP and KP solutions in an iterative and interleaved fashion. The TSP solution (cyclic tour) is typically changed in a deterministic way, while changes to the KP solution typically involve a random search, effectively resulting in a quasi-meandering exploration of the TTP solution space. Once a plateau is reached, the iterative search of the TTP solution space is restarted by using a new initial TSP tour. We propose to make the search more efficient through an adaptive surrogate model (based on a customised form of Support Vector Regression) that learns the characteristics of initial TSP tours that lead to good TTP solutions. The model is used to filter out non-promising initial TSP tours, in effect reducing the amount of time spent to find a good TTP solution. Experiments on a broad range of benchmark TTP instances indicate that the proposed approach filters out a considerable number of non-promising initial tours, at the cost of omitting only a small number of the best TTP solutions.",Surrogate Assisted Optimisation for Travelling Thief Problems,"Majid Namazi, Conrad Sanderson, M.A. Hakim Newton, Abdul Sattar",2020,Artificial Intelligence,2005.06695
"Maneuvering in dense traffic is a challenging task for autonomous vehicles because it requires reasoning about the stochastic behaviors of many other participants. In addition, the agent must achieve the maneuver within a limited time and distance. In this work, we propose a combination of reinforcement learning and game theory to learn merging behaviors. We design a training curriculum for a reinforcement learning agent using the concept of level-$k$ behavior. This approach exposes the agent to a broad variety of behaviors during training, which promotes learning policies that are robust to model discrepancies. We show that our approach learns more efficient policies than traditional training methods.",Reinforcement Learning with Iterative Reasoning for Merging in Dense Traffic,"Maxime Bouton, Alireza Nakhaei, David Isele, Kikuo Fujimura, and Mykel J. Kochenderfer",2020,Artificial Intelligence,2005.11895
"Recently, a groundswell of research has identified the use of counterfactual explanations as a potentially significant solution to the Explainable AI (XAI) problem. It is argued that (a) technically, these counterfactual cases can be generated by permuting problem-features until a class change is found, (b) psychologically, they are much more causally informative than factual explanations, (c) legally, they are GDPR-compliant. However, there are issues around the finding of good counterfactuals using current techniques (e.g. sparsity and plausibility). We show that many commonly-used datasets appear to have few good counterfactuals for explanation purposes. So, we propose a new case based approach for generating counterfactuals using novel ideas about the counterfactual potential and explanatory coverage of a case-base. The new technique reuses patterns of good counterfactuals, present in a case-base, to generate analogous counterfactuals that can explain new problems and their solutions. Several experiments show how this technique can improve the counterfactual potential and explanatory coverage of case-bases that were previously found wanting.",Good Counterfactuals and Where to Find Them: A Case-Based Technique for Generating Counterfactuals for Explainable AI (XAI),"Mark T. Keane, Barry Smyth",2020,Artificial Intelligence,2005.13997
"Recommendation system or also known as a recommender system is a tool to help the user in providing a suggestion of a specific dilemma. Thus, recently, the interest in developing a recommendation system in many fields has increased. Fuzzy Logic system (FLSs) is one of the approaches that can be used to model the recommendation systems as it can deal with uncertainty and imprecise information. However, one of the fundamental issues in FLS is the problem of the curse of dimensionality. That is, the number of rules in FLSs is increasing exponentially with the number of input variables. One effective way to overcome this problem is by using Hierarchical Fuzzy System (HFSs). This paper aims to explore the use of HFSs for Recommendation system. Specifically, we are interested in exploring and comparing the HFS and FLS for the Career path recommendation system (CPRS) based on four key criteria, namely topology, the number of rules, the rules structures and interpretability. The findings suggested that the HFS has advantages over FLS towards improving the interpretability models, in the context of a recommendation system example. This study contributes to providing an insight into the development of interpretable HFSs in the Recommendation systems.",An Exploratory Study of Hierarchical Fuzzy Systems Approach in Recommendation System,"Tajul Rosli Razak, Iman Hazwam Abd Halim, Muhammad Nabil Fikri Jamaludin, Mohammad Hafiz Ismail, Shukor Sanim Mohd Fauzi",2019,Artificial Intelligence,2005.14026
"There is a growing recognition that artists use valuable ways to understand and work with cognitive and perceptual mechanisms to convey desired experiences and narrative in their created artworks (DiPaola et al., 2010; Zeki, 2001). This paper documents our attempt to computationally model the creative process of a portrait painter, who relies on understanding human traits (i.e., personality and emotions) to inform their art. Our system includes an empathic conversational interaction component to capture the dominant personality category of the user and a generative AI Portraiture system that uses this categorization to create a personalized stylization of the user's portrait. This paper includes the description of our systems and the real-time interaction results obtained during the demonstration session of the NeurIPS 2019 Conference.",Empathic AI Painter: A Computational Creativity System with Embodied Conversational Interaction,"Ozge Nilay Yalcin, Nouf Abukhodair and Steve DiPaola",2020,Artificial Intelligence,2005.14223
"This paper provides the foundations of a unified cognitive decision-making framework (QulBIT) which is derived from quantum theory. The main advantage of this framework is that it can cater for paradoxical and irrational human decision making. Although quantum approaches for cognition have demonstrated advantages over classical probabilistic approaches and bounded rationality models, they still lack explanatory power. To address this, we introduce a novel explanatory analysis of the decision-maker's belief space. This is achieved by exploiting quantum interference effects as a way of both quantifying and explaining the decision-maker's uncertainty. We detail the main modules of the unified framework, the explanatory analysis method, and illustrate their application in situations violating the Sure Thing Principle.",QuLBIT: Quantum-Like Bayesian Inference Technologies for Cognition and Decision,Catarina Moreira and Matheus Hammes and Rasim Serdar Kurdoglu and Peter Bruza,2020,Artificial Intelligence,2006.02256
"Humans possess an inherent ability to chunk sequences into their constituent parts. In fact, this ability is thought to bootstrap language skills and learning of image patterns which might be a key to a more animal-like type of intelligence. Here, we propose a continual generalization of the chunking problem (an unsupervised problem), encompassing fixed and probabilistic chunks, discovery of temporal and causal structures and their continual variations. Additionally, we propose an algorithm called SyncMap that can learn and adapt to changes in the problem by creating a dynamic map which preserves the correlation between variables. Results of SyncMap suggest that the proposed algorithm learn near optimal solutions, despite the presence of many types of structures and their continual variation. When compared to Word2vec, PARSER and MRIL, SyncMap surpasses or ties with the best algorithm on $66\%$ of the scenarios while being the second best in the remaining $34\%$. SyncMap's model-free simple dynamics and the absence of loss functions reveal that, perhaps surprisingly, much can be done with self-organization alone. Code available at https://github.com/zweifel/SyncMap.",Continual General Chunking Problem and SyncMap,Danilo Vasconcellos Vargas and Toshitake Asabuki,2021,Artificial Intelligence,2006.07853
"Decision and policy-makers in multi-criteria decision-making analysis take into account some strategies in order to analyze outcomes and to finally make an effective and more precise decision. Among those strategies, the modification of the normalization process in the multiple-criteria decision-making algorithm is still a question due to the confrontation of many normalization tools. Normalization is the basic action in defining and solving a MADM problem and a MADM model. Normalization is the first, also necessary, step in solving, i.e. the application of a MADM method. It is a fact that the selection of normalization methods has a direct effect on the results. One of the latest normalization methods introduced is the Logarithmic Normalization (LN) method. This new method has a distinguished advantage, reflecting in that a sum of the normalized values of criteria always equals 1. This normalization method had never been applied in any MADM methods before. This research study is focused on the analysis of the classical MADM methods based on logarithmic normalization. VIKOR and TOPSIS, as the two famous MADM methods, were selected for this reanalysis research study. Two numerical examples were checked in both methods, based on both the classical and the novel ways based on the LN. The results indicate that there are differences between the two approaches. Eventually, a sensitivity analysis is also designed to illustrate the reliability of the final results.",A VIKOR and TOPSIS focused reanalysis of the MADM methods based on logarithmic normalization,"Sarfaraz Zolfani, Morteza Yazdani, Dragan Pamucar, Pascale Zarat\'e (IRIT-ADRIA, IRIT, UT1)",2020,Artificial Intelligence,2006.08150
"Fuzzy rule-based model is a powerful tool for imitating the human way of thinking and solving uncertainty-related problems as it allows for understandable and interpretable rule bases. The objective of this paper is to study the applicability of fuzzy rule-based modelling to quantify soil classification for engineering purposes by qualitatively considering soil index properties. The classification system of the Highway Research Board is considered to illustrate a fuzzy rule-based model. The soil's index properties are fuzzified using triangular functions, and the fuzzy membership values are calculated. Fuzzy arithmetical operators are then applied to the membership values obtained for classification. Fuzzy decision tree classification algorithm is used to derive fuzzy if-then rules to quantify qualitative soil classification. The proposed system is implemented in MATLAB. The results obtained are checked and the implementation of the proposed model is measured against the outcomes of the laboratory tests.",Application of Fuzzy Rule based System for Highway Research Board Classification of Soils,"Sujatha A, L Govindaraju and N Shivakumar",2020,Artificial Intelligence,2006.08347
"This paper describes an entropy equation, but one that should be used for measuring energy and not information. In relation to the human brain therefore, both of these quantities can be used to represent the stored information. The human brain makes use of energy efficiency to form its structures, which is likely to be linked to the neuron wiring. This energy efficiency can also be used as the basis for a clustering algorithm, which is described in a different paper. This paper is more of a discussion about global properties, where the rules used for the clustering algorithm can also create the entropy equation E = (mean * variance). This states that work is done through the energy released by the 'change' in entropy. The equation is so simplistic and generic that it can offer arguments for completely different domains, where the journey ends with a discussion about global energy properties in physics and beyond. A comparison with Einstein's relativity equation is made and also the audacious suggestion that a black hole has zero-energy inside.",An Entropy Equation for Energy,Kieran Greer,2020,Artificial Intelligence,2007.03286
"Advances in hardware technology have enabled more integration of sophisticated software, triggering progress in the development and employment of Unmanned Vehicles (UVs), and mitigating restraints for onboard intelligence. As a result, UVs can now take part in more complex mission where continuous transformation in environmental condition calls for a higher level of situational responsiveness. This paper serves as an introduction to UVs mission planning and management systems aiming to highlight some of the recent developments in the field of autonomous underwater and aerial vehicles in addition to stressing some possible future directions and discussing the learned lessons. A comprehensive survey over autonomy assessment of UVs, and different aspects of autonomy such as situation awareness, cognition, and decision-making has been provided in this study. The paper separately explains the humanoid and autonomous system's performance and highlights the role and impact of a human in UVs operations.",Current Advancements on Autonomous Mission Planning and Management Systems: an AUV and UAV perspective,"Adham Atyabi, Somaiyeh MahmoudZadeh, Samia Nefti-Meziani",2018,Artificial Intelligence,2007.05179
"Tabletop roleplaying games (TTRPGs) and procedural content generators can both be understood as systems of rules for producing content. In this paper, we argue that TTRPG design can usefully be viewed as procedural content generator design. We present several case studies linking key concepts from PCG research -- including possibility spaces, expressive range analysis, and generative pipelines -- to key concepts in TTRPG design. We then discuss the implications of these relationships and suggest directions for future work uniting research in TTRPGs and PCG.",Tabletop Roleplaying Games as Procedural Content Generators,"Matthew Guzdial, Devi Acharya, Max Kreminski, Michael Cook, Mirjam Eladhari, Antonios Liapis and Anne Sullivan",2020,Artificial Intelligence,2007.06108
"Real-world complex systems are often modelled by sets of equations with endogenous and exogenous variables. What can we say about the causal and probabilistic aspects of variables that appear in these equations without explicitly solving the equations? We make use of Simon's causal ordering algorithm (Simon, 1953) to construct a causal ordering graph and prove that it expresses the effects of soft and perfect interventions on the equations under certain unique solvability assumptions. We further construct a Markov ordering graph and prove that it encodes conditional independences in the distribution implied by the equations with independent random exogenous variables, under a similar unique solvability assumption. We discuss how this approach reveals and addresses some of the limitations of existing causal modelling frameworks, such as causal Bayesian networks and structural causal models.",Conditional independences and causal relations implied by sets of equations,Tineke Blom and Mirthe M. van Diepen and Joris M. Mooij,2021,Artificial Intelligence,2007.07183
"Advances in hardware technology have facilitated more integration of sophisticated software toward augmenting the development of Unmanned Vehicles (UVs) and mitigating constraints for onboard intelligence. As a result, UVs can operate in complex missions where continuous trans-formation in environmental condition calls for a higher level of situational responsiveness and autonomous decision making. This book is a research monograph that aims to provide a comprehensive survey of UVs autonomy and its related properties in internal and external situation awareness to-ward robust mission planning in severe conditions. An advance level of intelligence is essential to minimize the reliance on the human supervisor, which is a main concept of autonomy. A self-controlled system needs a robust mission management strategy to push the boundaries towards autonomous structures, and the UV should be aware of its internal state and capabilities to assess whether current mission goal is achievable or find an alternative solution. In this book, the AUVs will become the major case study thread but other cases/types of vehicle will also be considered. In-deed the research monograph, the review chapters and the new approaches we have developed would be appropriate for use as a reference in upper years or postgraduate degrees for its coverage of literature and algorithms relating to Robot/Vehicle planning, tasking, routing, and trust.",Autonomy and Unmanned Vehicles Augmented Reactive Mission-Motion Planning Architecture for Autonomous Vehicles,"Somaiyeh MahmoudZadeh, David MW Powers, Reza Bairam Zadeh",2019,Artificial Intelligence,2007.09563
"Social network structure is one of the key determinants of human language evolution. Previous work has shown that the network of social interactions shapes decentralized learning in human groups, leading to the emergence of different kinds of communicative conventions. We examined the effects of social network organization on the properties of communication systems emerging in decentralized, multi-agent reinforcement learning communities. We found that the global connectivity of a social network drives the convergence of populations on shared and symmetric communication systems, preventing the agents from forming many local ""dialects"". Moreover, the agent's degree is inversely related to the consistency of its use of communicative conventions. These results show the importance of the basic properties of social network structure on reinforcement communication learning and suggest a new interpretation of findings on human convergence on word conventions.",Reinforcement Communication Learning in Different Social Network Structures,"Marina Dubova, Arseny Moskvichev, Robert Goldstone",2020,Artificial Intelligence,2007.09820
"To provide a foundation for conceptual modeling, ontologies have been introduced to specify the entities, the existences of which are acknowledged in the model. Ontologies are essential components as mechanisms to model a portion of reality in software engineering. In this context, a model refers to a description of objects and processes that populate a system. Developing such a description constrains and directs the design, development, and use of the corresponding system, thus avoiding such difficulties as conflicts and lack of a common understanding. In this cross-area research between modeling and ontology, there has been a growing interest in the development and use of domain ontologies (e.g., Resource Description Framework, Ontology Web Language). This paper contributes to the establishment of a broad ontological foundation for conceptual modeling in a specific domain through proposing a workable ontology (abbreviated as TM). A TM is a one-category ontology called a thimac (things/machines) that is used to elaborate the design and analysis of ontological presumptions. The focus of the study is on such notions as change, event, and time. Several current ontological difficulties are reviewed and remodeled in the TM. TM modeling is also contrasted with time representation in SysML. The results demonstrate that a TM is a useful tool for addressing these ontological problems.",Conceptual Modeling of Time for Computational Ontologies,Sabah Al-Fedaghi,2020,Artificial Intelligence,2007.10151
"We present Tactician, a tactic learner and prover for the Coq Proof Assistant. Tactician helps users make tactical proof decisions while they retain control over the general proof strategy. To this end, Tactician learns from previously written tactic scripts and gives users either suggestions about the next tactic to be executed or altogether takes over the burden of proof synthesis. Tactician's goal is to provide users with a seamless, interactive, and intuitive experience together with robust and adaptive proof automation. In this paper, we give an overview of Tactician from the user's point of view, regarding both day-to-day usage and issues of package dependency management while learning in the large. Finally, we give a peek into Tactician's implementation as a Coq plugin and machine learning platform.","The Tactician (extended version): A Seamless, Interactive Tactic Learner and Prover for Coq","Lasse Blaauwbroek, Josef Urban and Herman Geuvers",2020,Artificial Intelligence,2008.00120
"Ortus is a simple virtual organism that also serves as an initial framework for investigating and developing biologically-based artificial intelligence. Born from a goal to create complex virtual intelligence and an initial attempt to model C. elegans, Ortus implements a number of mechanisms observed in organic nervous systems, and attempts to fill in unknowns based upon plausible biological implementations and psychological observations. Implemented mechanisms include excitatory and inhibitory chemical synapses, bidirectional gap junctions, and Hebbian learning with its Stentian extension. We present an initial experiment that showcases Ortus' fundamental principles; specifically, a cyclic respiratory circuit, and emotionally-driven associative learning with respect to an input stimulus. Finally, we discuss the implications and future directions for Ortus and similar systems.",Ortus: an Emotion-Driven Approach to (artificial) Biological Intelligence,"Andrew W.E. McDonald, Sean Grimes, David E. Breen",2017,Artificial Intelligence,2008.04875
"Red light running at signalised intersections is a growing road safety issue worldwide, leading to the rapid development of advanced intelligent transportation technologies and countermeasures. However, existing studies have yet to summarise and present the effect of these technology based innovations in improving safety. This paper represents a comprehensive review of red light running behaviour prediction methodologies and technology-based countermeasures. Specifically, the major focus of this study is to provide a comprehensive review on two streams of literature targeting red light running and stop and go behaviour at signalised intersection (1) studies focusing on modelling and predicting the red light running and stop and go related driver behaviour and (2) studies focusing on the effectiveness of different technology based countermeasures which combat such unsafe behaviour. The study provides a systematic guide to assist researchers and stakeholders in understanding how to best identify red light running and stop and go associated driving behaviour and subsequently implement countermeasures to combat such risky behaviour and improve the associated safety.",A Review on Drivers Red Light Running Behavior Predictions and Technology Based Countermeasures,"Md Mostafizur Rahman Komol, Jack Pinnow, Mohammed Elhenawy, Shamsunnahar Yasmin, Mahmoud Masoud, Sebastien Glaser and Andry Rakotonirainy",2022,Artificial Intelligence,2008.06727
"Relevant research has been highlighted in the computing community to develop machine learning models capable of predicting the occurrence of crimes, analyzing contexts of crimes, extracting profiles of individuals linked to crime, and analyzing crimes over time. However, models capable of predicting specific crimes, such as homicide, are not commonly found in the current literature. This research presents a machine learning model to predict homicide crimes, using a dataset that uses generic data (without study location dependencies) based on incident report records for 34 different types of crimes, along with time and space data from crime reports. Experimentally, data from the city of Bel\'em - Par\'a, Brazil was used. These data were transformed to make the problem generic, enabling the replication of this model to other locations. In the research, analyses were performed with simple and robust algorithms on the created dataset. With this, statistical tests were performed with 11 different classification methods and the results are related to the prediction's occurrence and non-occurrence of homicide crimes in the month subsequent to the occurrence of other registered crimes, with 76% assertiveness for both classes of the problem, using Random Forest. Results are considered as a baseline for the proposed problem.",Prediction of Homicides in Urban Centers: A Machine Learning Approach,"Jos\'e Ribeiro, Lair Meneses, Denis Costa, Wando Miranda, Ronnie Alves",2021,Artificial Intelligence,2008.06979
"Autonomous car racing is a major challenge in robotics. It raises fundamental problems for classical approaches such as planning minimum-time trajectories under uncertain dynamics and controlling the car at the limits of its handling. Besides, the requirement of minimizing the lap time, which is a sparse objective, and the difficulty of collecting training data from human experts have also hindered researchers from directly applying learning-based approaches to solve the problem. In the present work, we propose a learning-based system for autonomous car racing by leveraging a high-fidelity physical car simulation, a course-progress proxy reward, and deep reinforcement learning. We deploy our system in Gran Turismo Sport, a world-leading car simulator known for its realistic physics simulation of different race cars and tracks, which is even used to recruit human race car drivers. Our trained policy achieves autonomous racing performance that goes beyond what had been achieved so far by the built-in AI, and, at the same time, outperforms the fastest driver in a dataset of over 50,000 human players.",Super-Human Performance in Gran Turismo Sport Using Deep Reinforcement Learning,"Florian Fuchs, Yunlong Song, Elia Kaufmann, Davide Scaramuzza, Peter Duerr",2021,Artificial Intelligence,2008.07971
"Consumer electronic devices such as mobile handsets, goods tagged with RFID labels, location and position sensors are continuously generating a vast amount of location enriched data called geospatial data. Conventionally such geospatial data is used for military applications. In recent times, many useful civilian applications have been designed and deployed around such geospatial data. For example, a recommendation system to suggest restaurants or places of attraction to a tourist visiting a particular locality. At the same time, civic bodies are harnessing geospatial data generated through remote sensing devices to provide better services to citizens such as traffic monitoring, pothole identification, and weather reporting. Typically such applications are leveraged upon non-hierarchical machine learning techniques such as Naive-Bayes Classifiers, Support Vector Machines, and decision trees. Recent advances in the field of deep-learning showed that Neural Network-based techniques outperform conventional techniques and provide effective solutions for many geospatial data analysis tasks such as object recognition, image classification, and scene understanding. The chapter presents a survey on the current state of the applications of deep learning techniques for analyzing geospatial data. The chapter is organized as below: (i) A brief overview of deep learning algorithms. (ii)Geospatial Analysis: a Data Science Perspective (iii) Deep-learning techniques for Remote Sensing data analytics tasks (iv) Deep-learning techniques for GPS data analytics(iv) Deep-learning techniques for RFID data analytics.",Deep Learning Techniques for Geospatial Data Analysis,"Arvind W. Kiwelekar, Geetanjali S. Mahamunkar, Laxman D. Netak, Valmik B Nikam",2020,Artificial Intelligence,2008.13146
"Due to advances in machine learning and artificial intelligence (AI), a new role is emerging for machines as intelligent assistants to radiologists in their clinical workflows. But what systematic clinical thought processes are these machines using? Are they similar enough to those of radiologists to be trusted as assistants? A live demonstration of such a technology was conducted at the 2016 Scientific Assembly and Annual Meeting of the Radiological Society of North America (RSNA). The demonstration was presented in the form of a question-answering system that took a radiology multiple choice question and a medical image as inputs. The AI system then demonstrated a cognitive workflow, involving text analysis, image analysis, and reasoning, to process the question and generate the most probable answer. A post demonstration survey was made available to the participants who experienced the demo and tested the question answering system. Of the reported 54,037 meeting registrants, 2,927 visited the demonstration booth, 1,991 experienced the demo, and 1,025 completed a post-demonstration survey. In this paper, the methodology of the survey is shown and a summary of its results are presented. The results of the survey show a very high level of receptiveness to cognitive computing technology and artificial intelligence among radiologists.",Receptivity of an AI Cognitive Assistant by the Radiology Community: A Report on Data Collected at RSNA,"Karina Kanjaria, Anup Pillai, Chaitanya Shivade, Marina Bendersky, Ashutosh Jadhav, Vandana Mukherjee, Tanveer Syeda-Mahmood",2020,Artificial Intelligence,2009.06082
"Equipping machines with comprehensive knowledge of the world's entities and their relationships has been a long-standing goal of AI. Over the last decade, large-scale knowledge bases, also known as knowledge graphs, have been automatically constructed from web contents and text sources, and have become a key asset for search engines. This machine knowledge can be harnessed to semantically interpret textual phrases in news, social media and web tables, and contributes to question answering, natural language processing and data analytics. This article surveys fundamental concepts and practical methods for creating and curating large knowledge bases. It covers models and methods for discovering and canonicalizing entities and their semantic types and organizing them into clean taxonomies. On top of this, the article discusses the automatic extraction of entity-centric properties. To support the long-term life-cycle and the quality assurance of machine knowledge, the article presents methods for constructing open schemas and for knowledge curation. Case studies on academic projects and industrial knowledge graphs complement the survey of concepts and methods.",Machine Knowledge: Creation and Curation of Comprehensive Knowledge Bases,"Gerhard Weikum, Luna Dong, Simon Razniewski, Fabian Suchanek",2021,Artificial Intelligence,2009.11564
"Within intelligent tutoring systems, considerable research has investigated hints, including how to generate data-driven hints, what hint content to present, and when to provide hints for optimal learning outcomes. However, less attention has been paid to how hints are presented. In this paper, we propose a new hint delivery mechanism called ""Assertions"" for providing unsolicited hints in a data-driven intelligent tutor. Assertions are partially-worked example steps designed to appear within a student workspace, and in the same format as student-derived steps, to show students a possible subgoal leading to the solution. We hypothesized that Assertions can help address the well-known hint avoidance problem. In systems that only provide hints upon request, hint avoidance results in students not receiving hints when they are needed. Our unsolicited Assertions do not seek to improve student help-seeking, but rather seek to ensure students receive the help they need. We contrast Assertions with Messages, text-based, unsolicited hints that appear after student inactivity. Our results show that Assertions significantly increase unsolicited hint usage compared to Messages. Further, they show a significant aptitude-treatment interaction between Assertions and prior proficiency, with Assertions leading students with low prior proficiency to generate shorter (more efficient) posttest solutions faster. We also present a clustering analysis that shows patterns of productive persistence among students with low prior knowledge when the tutor provides unsolicited help in the form of Assertions. Overall, this work provides encouraging evidence that hint presentation can significantly impact how students use them and using Assertions can be an effective way to address help avoidance.",Avoiding Help Avoidance: Using Interface Design Changes to Promote Unsolicited Hint Usage in an Intelligent Tutor,"Mehak Maniktala, Christa Cody, Tiffany Barnes, and Min Chi",2020,Artificial Intelligence,2009.13371
"We propose a vision for directing research and education in the ICT field. Our Smart and Sustainable World vision targets at prosperity for the people and the planet through better awareness and control of both human-made and natural environment. The needs of the society, individuals, and industries are fulfilled with intelligent systems that sense their environment, make proactive decisions on actions advancing their goals, and perform the actions on the environment. We emphasize artificial intelligence, feedback loops, human acceptance and control, intelligent use of basic resources, performance parameters, mission-oriented interdisciplinary research, and a holistic systems view complementing the conventional analytical reductive view as a research paradigm especially for complex problems. To serve a broad audience, we explain these concepts and list the essential literature. We suggest planning research and education by specifying, in a step-wise manner, scenarios, performance criteria, system models, research problems and education content, resulting in common goals and a coherent project portfolio as well as education curricula. Research and education produce feedback to support evolutionary development and encourage creativity in research. Finally, we propose concrete actions for realizing this approach.",Research and Education Towards Smart and Sustainable World,"Jukka Riekki and Aarne M\""ammel\""a",2021,Artificial Intelligence,2009.13849
"In this paper, we proposed a transfer learning-based English language learning chatbot, whose output generated by GPT-2 can be explained by corresponding ontology graph rooted by fine-tuning dataset. We design three levels for systematically English learning, including phonetics level for speech recognition and pronunciation correction, semantic level for specific domain conversation, and the simulation of free-style conversation in English - the highest level of language chatbot communication as free-style conversation agent. For academic contribution, we implement the ontology graph to explain the performance of free-style conversation, following the concept of XAI (Explainable Artificial Intelligence) to visualize the connections of neural network in bionics, and explain the output sentence from language model. From implementation perspective, our Language Learning agent integrated the mini-program in WeChat as front-end, and fine-tuned GPT-2 model of transfer learning as back-end to interpret the responses by ontology graph.",The design and implementation of Language Learning Chatbot with XAI using Ontology and Transfer Learning,"Nuobei Shi, Qin Zeng and Raymond Lee",2020,Artificial Intelligence,2009.13984
"We addressed the problem of a lack of semantic representation for user-centric explanations and different explanation types in our Explanation Ontology (https://purl.org/heals/eo). Such a representation is increasingly necessary as explainability has become an important problem in Artificial Intelligence with the emergence of complex methods and an uptake in high-precision and user-facing settings. In this submission, we provide step-by-step guidance for system designers to utilize our ontology, introduced in our resource track paper, to plan and model for explanations during the design of their Artificial Intelligence systems. We also provide a detailed example with our utilization of this guidance in a clinical setting.",Explanation Ontology in Action: A Clinical Use-Case,"Shruthi Chari, Oshani Seneviratne, Daniel M. Gruen, Morgan A. Foreman, Amar K. Das, Deborah L. McGuinness",2020,Artificial Intelligence,2010.01478
"Explainability has been a goal for Artificial Intelligence (AI) systems since their conception, with the need for explainability growing as more complex AI models are increasingly used in critical, high-stakes settings such as healthcare. Explanations have often added to an AI system in a non-principled, post-hoc manner. With greater adoption of these systems and emphasis on user-centric explainability, there is a need for a structured representation that treats explainability as a primary consideration, mapping end user needs to specific explanation types and the system's AI capabilities. We design an explanation ontology to model both the role of explanations, accounting for the system and user attributes in the process, and the range of different literature-derived explanation types. We indicate how the ontology can support user requirements for explanations in the domain of healthcare. We evaluate our ontology with a set of competency questions geared towards a system designer who might use our ontology to decide which explanation types to include, given a combination of users' needs and a system's capabilities, both in system design settings and in real-time operations. Through the use of this ontology, system designers will be able to make informed choices on which explanations AI systems can and should provide.",Explanation Ontology: A Model of Explanations for User-Centered AI,"Shruthi Chari, Oshani Seneviratne, Daniel M. Gruen, Morgan A. Foreman, Amar K. Das, Deborah L. McGuinness",2020,Artificial Intelligence,2010.01479
"Determining when and whether to provide personalized support is a well-known challenge called the assistance dilemma. A core problem in solving the assistance dilemma is the need to discover when students are unproductive so that the tutor can intervene. Such a task is particularly challenging for open-ended domains, even those that are well-structured with defined principles and goals. In this paper, we present a set of data-driven methods to classify, predict, and prevent unproductive problem-solving steps in the well-structured open-ended domain of logic. This approach leverages and extends the Hint Factory, a set of methods that leverages prior student solution attempts to build data-driven intelligent tutors. We present a HelpNeed classification, that uses prior student data to determine when students are likely to be unproductive and need help learning optimal problem-solving strategies. We present a controlled study to determine the impact of an Adaptive pedagogical policy that provides proactive hints at the start of each step based on the outcomes of our HelpNeed predictor: productive vs. unproductive. Our results show that the students in the Adaptive condition exhibited better training behaviors, with lower help avoidance, and higher help appropriateness (a higher chance of receiving help when it was likely to be needed), as measured using the HelpNeed classifier, when compared to the Control. Furthermore, the results show that the students who received Adaptive hints based on HelpNeed predictions during training significantly outperform their Control peers on the posttest, with the former producing shorter, more optimal solutions in less time. We conclude with suggestions on how these HelpNeed methods could be applied in other well-structured open-ended domains.","Extending the Hint Factory for the assistance dilemma: A novel, data-driven HelpNeed Predictor for proactive problem-solving help","Mehak Maniktala, Christa Cody, Amy Isvik, Nicholas Lytle, Min Chi, Tiffany Barnes",2020,Artificial Intelligence,2010.04124
"The recent advances in artificial intelligence namely in machine learning and deep learning, have boosted the performance of intelligent systems in several ways. This gave rise to human expectations, but also created the need for a deeper understanding of how intelligent systems think and decide. The concept of explainability appeared, in the extent of explaining the internal system mechanics in human terms. Recommendation systems are intelligent systems that support human decision making, and as such, they have to be explainable in order to increase user trust and improve the acceptance of recommendations. In this work, we focus on a context-aware recommendation system for energy efficiency and develop a mechanism for explainable and persuasive recommendations, which are personalized to user preferences and habits. The persuasive facts either emphasize on the economical saving prospects (Econ) or on a positive ecological impact (Eco) and explanations provide the reason for recommending an energy saving action. Based on a study conducted using a Telegram bot, different scenarios have been validated with actual data and human feedback. Current results show a total increase of 19\% on the recommendation acceptance ratio when both economical and ecological persuasive facts are employed. This revolutionary approach on recommendation systems, demonstrates how intelligent recommendations can effectively encourage energy saving behavior.",The emergence of Explainability of Intelligent Systems: Delivering Explainable and Personalised Recommendations for Energy Efficiency,Christos Sardianos and Iraklis Varlamis and Christos Chronis and George Dimitrakopoulos and Abdullah Alsalemi and Yassine Himeur and Faycal Bensaali and Abbes Amira,2020,Artificial Intelligence,2010.04990
"Anginal symptoms can connote increased cardiac risk and a need for change in cardiovascular management. This study evaluated the potential to extract these symptoms from physician notes using the Bidirectional Encoder from Transformers language model fine-tuned on a domain-specific corpus. The history of present illness section of 459 expert annotated primary care physician notes from consecutive patients referred for cardiac testing without known atherosclerotic cardiovascular disease were included. Notes were annotated for positive and negative mentions of chest pain and shortness of breath characterization. The results demonstrate high sensitivity and specificity for the detection of chest pain or discomfort, substernal chest pain, shortness of breath, and dyspnea on exertion. Small sample size limited extracting factors related to provocation and palliation of chest pain. This study provides a promising starting point for the natural language processing of physician notes to characterize clinically actionable anginal symptoms.",Extracting Angina Symptoms from Clinical Notes Using Pre-Trained Transformer Architectures,"Aaron S. Eisman, Nishant R. Shah, Carsten Eickhoff, George Zerveas, Elizabeth S. Chen, Wen-Chih Wu, Indra Neil Sarkar",2020,Artificial Intelligence,2010.05757
"We describe the design of a reproducing kernel suitable for attributed graphs, in which the similarity between the two graphs is defined based on the neighborhood information of the graph nodes with the aid of a product graph formulation. We represent the proposed kernel as the weighted sum of two other kernels of which one is an R-convolution kernel that processes the attribute information of the graph and the other is an optimal assignment kernel that processes label information. They are formulated in such a way that the edges processed as part of the kernel computation have the same neighborhood properties and hence the kernel proposed makes a well-defined correspondence between regions processed in graphs. These concepts are also extended to the case of the shortest paths. We identified the state-of-the-art kernels that can be mapped to such a neighborhood preserving framework. We found that the kernel value of the argument graphs in each iteration of the Weisfeiler-Lehman color refinement algorithm can be obtained recursively from the product graph formulated in our method. By incorporating the proposed kernel on support vector machines we analyzed the real-world data sets and it has shown superior performance in comparison with that of the other state-of-the-art graph kernels.",Neighborhood Preserving Kernels for Attributed Graphs,"Asif Salim, Shiju. S. S, and Sumitra. S",2022,Artificial Intelligence,2010.06261
"In classical causal inference, inferring cause-effect relations from data relies on the assumption that units are independent and identically distributed. This assumption is violated in settings where units are related through a network of dependencies. An example of such a setting is ad placement in sponsored search advertising, where the clickability of a particular ad is potentially influenced by where it is placed and where other ads are placed on the search result page. In such scenarios, confounding arises due to not only the individual ad-level covariates but also the placements and covariates of other ads in the system. In this paper, we leverage the language of causal inference in the presence of interference to model interactions among the ads. Quantification of such interactions allows us to better understand the click behavior of users, which in turn impacts the revenue of the host search engine and enhances user satisfaction. We illustrate the utility of our formalization through experiments carried out on the ad placement system of the Bing search engine.",Causal Inference in the Presence of Interference in Sponsored Search Advertising,"Razieh Nabi, Joel Pfeiffer, Murat Ali Bayir, Denis Charles, Emre K{\i}c{\i}man",2022,Artificial Intelligence,2010.07458
"Transfer learning is an effective technique to improve a target recommender system with the knowledge from a source domain. Existing research focuses on the recommendation performance of the target domain while ignores the privacy leakage of the source domain. The transferred knowledge, however, may unintendedly leak private information of the source domain. For example, an attacker can accurately infer user demographics from their historical purchase provided by a source domain data owner. This paper addresses the above privacy-preserving issue by learning a privacy-aware neural representation by improving target performance while protecting source privacy. The key idea is to simulate the attacks during the training for protecting unseen users' privacy in the future, modeled by an adversarial game, so that the transfer learning model becomes robust to attacks. Experiments show that the proposed PrivNet model can successfully disentangle the knowledge benefitting the transfer from leaking the privacy.",PrivNet: Safeguarding Private Attributes in Transfer Learning for Recommendation,"Guangneng Hu, Qiang Yang",2020,Artificial Intelligence,2010.08187
"Game AI competitions are important to foster research and development on Game AI and AI in general. These competitions supply different challenging problems that can be translated into other contexts, virtual or real. They provide frameworks and tools to facilitate the research on their core topics and provide means for comparing and sharing results. A competition is also a way to motivate new researchers to study these challenges. In this document, we present the Geometry Friends Game AI Competition. Geometry Friends is a two-player cooperative physics-based puzzle platformer computer game. The concept of the game is simple, though its solving has proven to be difficult. While the main and apparent focus of the game is cooperation, it also relies on other AI-related problems such as planning, plan execution, and motion control, all connected to situational awareness. All of these must be solved in real-time. In this paper, we discuss the competition and the challenges it brings, and present an overview of the current solutions.",A Game AI Competition to foster Collaborative AI research and development,Ana Salta and Rui Prada and Francisco S. Melo,2020,Artificial Intelligence,2010.08885
"The Dead Sea Scrolls are tangible evidence of the Bible's ancient scribal culture. Palaeography - the study of ancient handwriting - can provide access to this scribal culture. However, one of the problems of traditional palaeography is to determine writer identity when the writing style is near uniform. This is exemplified by the Great Isaiah Scroll (1QIsaa). To this end, we used pattern recognition and artificial intelligence techniques to innovate the palaeography of the scrolls regarding writer identification and to pioneer the microlevel of individual scribes to open access to the Bible's ancient scribal culture. Although many scholars believe that 1QIsaa was written by one scribe, we report new evidence for a breaking point in the series of columns in this scroll. Without prior assumption of writer identity, based on point clouds of the reduced-dimensionality feature-space, we found that columns from the first and second halves of the manuscript ended up in two distinct zones of such scatter plots, notably for a range of digital palaeography tools, each addressing very different featural aspects of the script samples. In a secondary, independent, analysis, now assuming writer difference and using yet another independent feature method and several different types of statistical testing, a switching point was found in the column series. A clear phase transition is apparent around column 27. Given the statistically significant differences between the two halves, a tertiary, post-hoc analysis was performed. Demonstrating that two main scribes were responsible for the Great Isaiah Scroll, this study sheds new light on the Bible's ancient scribal culture by providing new, tangible evidence that ancient biblical texts were not copied by a single scribe only but that multiple scribes could closely collaborate on one particular manuscript.",Artificial intelligence based writer identification generates new evidence for the unknown scribes of the Dead Sea Scrolls exemplified by the Great Isaiah Scroll (1QIsaa),"Mladen Popovi\'c, Maruf A. Dhali, Lambert Schomaker",2021,Artificial Intelligence,2010.14476
"The 9th International Workshop on Theorem-Proving Components for Educational Software (ThEdu'20) was scheduled to happen on June 29 as a satellite of the IJCAR-FSCD 2020 joint meeting, in Paris. The COVID-19 pandemic came by surprise, though, and the main conference was virtualised. Fearing that an online meeting would not allow our community to fully reproduce the usual face-to-face networking opportunities of the ThEdu initiative, the Steering Committee of ThEdu decided to cancel our workshop. Given that many of us had already planned and worked for that moment, we decided that ThEdu'20 could still live in the form of an EPTCS volume. The EPTCS concurred with us, recognising this very singular situation, and accepted our proposal of organising a special issue with papers submitted to ThEdu'20. An open call for papers was then issued, and attracted five submissions, all of which have been accepted by our reviewers, who produced three careful reports on each of the contributions. The resulting revised papers are collected in the present volume. We, the volume editors, hope that this collection of papers will help further promoting the development of theorem-proving-based software, and that it will collaborate to improve the mutual understanding between computer mathematicians and stakeholders in education. With some luck, we would actually expect that the very special circumstances set up by the worst sanitary crisis in a century will happen to reinforce the need for the application of certified components and of verification methods for the production of educational software that would be available even when the traditional on-site learning experiences turn out not to be recommendable.",Proceedings 9th International Workshop on Theorem Proving Components for Educational Software,"Pedro Quaresma (University of Coimbra, Portugal), Walther Neuper (JKU Johannes Kepler University, Linz, Austria), Jo\~ao Marcos (UFRN, Brazil)",2020,Artificial Intelligence,2010.15832
"We study the novel problem of blackbox optimization of multiple objectives via multi-fidelity function evaluations that vary in the amount of resources consumed and their accuracy. The overall goal is to approximate the true Pareto set of solutions by minimizing the resources consumed for function evaluations. For example, in power system design optimization, we need to find designs that trade-off cost, size, efficiency, and thermal tolerance using multi-fidelity simulators for design evaluations. In this paper, we propose a novel approach referred as Multi-Fidelity Output Space Entropy Search for Multi-objective Optimization (MF-OSEMO) to solve this problem. The key idea is to select the sequence of candidate input and fidelity-vector pairs that maximize the information gained about the true Pareto front per unit resource cost. Our experiments on several synthetic and real-world benchmark problems show that MF-OSEMO, with both approximations, significantly improves over the state-of-the-art single-fidelity algorithms for multi-objective optimization.",Multi-Fidelity Multi-Objective Bayesian Optimization: An Output Space Entropy Search Approach,"Syrine Belakaria, Aryan Deshwal and Janardhan Rao Doppa",2020,Artificial Intelligence,2011.01542
"This paper updates the cognitive model, firstly by creating two systems and then unifying them over the same structure. It represents information at the semantic level only, where labelled patterns are aggregated into a 'type-set-match' form. It is described that the aggregations can be used to match across regions with potentially different functionality and therefore give the structure a required amount of flexibility. The theory is that if the model stores information which can be transposed in consistent ways, then that will result in knowledge and some level of intelligence. As part of the design, patterns have to become distinct and that is realised by unique paths through shared aggregated structures. An ensemble-hierarchy relation also helps to define uniqueness through local feedback that may even be an action potential. The earlier models are still consistent in terms of their proposed functionality, but some of the architecture boundaries have been moved to match them up more closely. After pattern optimisation and tree-like aggregations, the two main models differ only in their upper, more intelligent level. One provides a propositional logic for mutually inclusive or exclusive pattern groups and sequences, while the other provides a behaviour script that is constructed from node types. It can be seen that these two views are complimentary and would allow some control over behaviours, as well as memories, that might get selected.",New Ideas for Brain Modelling 7,Kieran Greer,2021,Artificial Intelligence,2011.02223
"Analytic software tools and workflows are increasing in capability, complexity, number, and scale, and the integrity of our workflows is as important as ever. Specifically, we must be able to inspect the process of analytic workflows to assess (1) confidence of the conclusions, (2) risks and biases of the operations involved, (3) sensitivity of the conclusions to sources and agents, (4) impact and pertinence of various sources and agents, and (5) diversity of the sources that support the conclusions. We present an approach that tracks agents' provenance with PROV-O in conjunction with agents' appraisals and evidence links (expressed in our novel DIVE ontology). Together, PROV-O and DIVE enable dynamic propagation of confidence and counter-factual refutation to improve human-machine trust and analytic integrity. We demonstrate representative software developed for user interaction with that provenance, and discuss key needs for organizations adopting such approaches. We demonstrate all of these assessments in a multi-agent analysis scenario, using an interactive web-based information validation UI.",Provenance-Based Interpretation of Multi-Agent Information Analysis,"Scott Friedman, Jeff Rye, David LaVergne, Dan Thomsen, Matthew Allen, Kyle Tunis",2020,Artificial Intelligence,2011.04016
Floor space optimization is a critical revenue management problem commonly encountered by retailers. It maximizes store revenue by optimally allocating floor space to product categories which are assigned to their most appropriate planograms. We formulate the problem as a connected multi-choice knapsack problem with an additional global constraint and propose a tabu search based meta-heuristic that exploits the multiple special neighborhood structures. We also incorporate a mechanism to determine how to combine the multiple neighborhood moves. A candidate list strategy based on learning from prior search history is also employed to improve the search quality. The results of computational testing with a set of test problems show that our tabu search heuristic can solve all problems within a reasonable amount of time. Analyses of individual contributions of relevant components of the algorithm were conducted with computational experiments.,Maximizing Store Revenues using Tabu Search for Floor Space Optimization,Jiefeng Xu and Evren Gul and Alvin Lim,2021,Artificial Intelligence,2011.04422
"This paper studies the scheduling of jobs of different families on parallel machines with qualification constraints. Originating from semiconductor manufacturing, this constraint imposes a time threshold between the execution of two jobs of the same family. Otherwise, the machine becomes disqualified for this family. The goal is to minimize both the flow time and the number of disqualifications. Recently, an efficient constraint programming model has been proposed. However, when priority is given to the flow time objective, the efficiency of the model can be improved. This paper uses a polynomial-time algorithm which minimize the flow time for a single machine relaxation where disqualifications are not considered. Using this algorithm one can derived filtering rules on different variables of the model. Experimental results are presented showing the effectiveness of these rules. They improve the competitiveness with the mixed integer linear program of the literature.",Filtering Rules for Flow Time Minimization in a Parallel Machine Scheduling Problem,"Margaux Nattaf (G-SCOP), Arnaud Malapert",2020,Artificial Intelligence,2011.10307
"A new fuzzy method is developed using triangular/trapezoidal fuzzy numbers for evaluating a group's mean performance, when qualitative grades instead of numerical scores are used for assessing its members' individual performance. Also, a new technique is developed for solving Linear Programming problems with fuzzy coefficients and everyday life applications are presented to illustrate our results.",Assessment and Linear Programming under Fuzzy Conditions,Michael Voskoglou,2020,Artificial Intelligence,2011.10640
"Artificial Intelligence (AI) governance regulates the exercise of authority and control over the management of AI. It aims at leveraging AI through effective use of data and minimization of AI-related cost and risk. While topics such as AI governance and AI ethics are thoroughly discussed on a theoretical, philosophical, societal and regulatory level, there is limited work on AI governance targeted to companies and corporations. This work views AI products as systems, where key functionality is delivered by machine learning (ML) models leveraging (training) data. We derive a conceptual framework by synthesizing literature on AI and related fields such as ML. Our framework decomposes AI governance into governance of data, (ML) models and (AI) systems along four dimensions. It relates to existing IT and data governance frameworks and practices. It can be adopted by practitioners and academics alike. For practitioners the synthesis of mainly research papers, but also practitioner publications and publications of regulatory bodies provides a valuable starting point to implement AI governance, while for academics the paper highlights a number of areas of AI governance that deserve more attention.",AI Governance for Businesses,Johannes Schneider and Rene Abraham and Christian Meske and Jan vom Brocke,2022,Artificial Intelligence,2011.10672
"Planning - the ability to analyze the structure of a problem in the large and decompose it into interrelated subproblems - is a hallmark of human intelligence. While deep reinforcement learning (RL) has shown great promise for solving relatively straightforward control tasks, it remains an open problem how to best incorporate planning into existing deep RL paradigms to handle increasingly complex environments. One prominent framework, Model-Based RL, learns a world model and plans using step-by-step virtual rollouts. This type of world model quickly diverges from reality when the planning horizon increases, thus struggling at long-horizon planning. How can we learn world models that endow agents with the ability to do temporally extended reasoning? In this work, we propose to learn graph-structured world models composed of sparse, multi-step transitions. We devise a novel algorithm to learn latent landmarks that are scattered (in terms of reachability) across the goal space as the nodes on the graph. In this same graph, the edges are the reachability estimates distilled from Q-functions. On a variety of high-dimensional continuous control tasks ranging from robotic manipulation to navigation, we demonstrate that our method, named L3P, significantly outperforms prior work, and is oftentimes the only method capable of leveraging both the robustness of model-free RL and generalization of graph-search algorithms. We believe our work is an important step towards scalable planning in reinforcement learning.",World Model as a Graph: Learning Latent Landmarks for Planning,"Lunjun Zhang, Ge Yang, Bradly C. Stadie",2021,Artificial Intelligence,2011.12491
"Large software systems tune hundreds of 'constants' to optimize their runtime performance. These values are commonly derived through intuition, lab tests, or A/B tests. A 'one-size-fits-all' approach is often sub-optimal as the best value depends on runtime context. In this paper, we provide an experimental approach to replace constants with learned contextual functions for Skype - a widely used real-time communication (RTC) application. We present Resonance, a system based on contextual bandits (CB). We describe experiences from three real-world experiments: applying it to the audio, video, and transport components in Skype. We surface a unique and practical challenge of performing machine learning (ML) inference in large software systems written using encapsulation principles. Finally, we open-source FeatureBroker, a library to reduce the friction in adopting ML models in such development environments",Resonance: Replacing Software Constants with Context-Aware Models in Real-time Communication,"Jayant Gupchup, Ashkan Aazami, Yaran Fan, Senja Filipi, Tom Finley, Scott Inglis, Marcus Asteborg, Luke Caroll, Rajan Chari, Markus Cozowicz, Vishak Gopal, Vinod Prakash, Sasikanth Bendapudi, Jack Gerrits, Eric Lau, Huazhou Liu, Marco Rossi, Dima Slobodianyk, Dmitri Birjukov, Matty Cooper, Nilesh Javar, Dmitriy Perednya, Sriram Srinivasan, John Langford, Ross Cutler, Johannes Gehrke",2020,Artificial Intelligence,2011.12715
"In this paper, we outline the implementation of the TFD (Totally Ordered Fast Downward) and the PFD (Partially ordered Fast Downward) hierarchical planners that participated in the first HTN IPC competition in 2020. These two planners are based on forward-chaining task decomposition coupled with a compact grounding of actions, methods, tasks and HTN problems.",Totally and Partially Ordered Hierarchical Planners in PDDL4J Library,"Damien Pellier, Humbert Fiorino",2020,Artificial Intelligence,2011.13297
"For a long time the ability to solve abstract reasoning tasks was considered one of the hallmarks of human intelligence. Recent advances in application of deep learning (DL) methods led, as in many other domains, to surpassing human abstract reasoning performance, specifically in the most popular type of such problems - the Raven's Progressive Matrices (RPMs). While the efficacy of DL systems is indeed impressive, the way they approach the RPMs is very different from that of humans. State-of-the-art systems solving RPMs rely on massive pattern-based training and sometimes on exploiting biases in the dataset, whereas humans concentrate on identification of the rules / concepts underlying the RPM (or generally a visual reasoning task) to be solved. Motivated by this cognitive difference, this work aims at combining DL with human way of solving RPMs and getting the best of both worlds. Specifically, we cast the problem of solving RPMs into multi-label classification framework where each RPM is viewed as a multi-label data point, with labels determined by the set of abstract rules underlying the RPM. For efficient training of the system we introduce a generalisation of the Noise Contrastive Estimation algorithm to the case of multi-label samples. Furthermore, we propose a new sparse rule encoding scheme for RPMs which, besides the new training algorithm, is the key factor contributing to the state-of-the-art performance. The proposed approach is evaluated on two most popular benchmark datasets (Balanced-RAVEN and PGM) and on both of them demonstrates an advantage over the current state-of-the-art results. Contrary to applications of contrastive learning methods reported in other domains, the state-of-the-art performance reported in the paper is achieved with no need for large batch sizes or strong data augmentation.",Multi-Label Contrastive Learning for Abstract Visual Reasoning,"Miko{\l}aj Ma{\l}ki\'nski, Jacek Ma\'ndziuk",2022,Artificial Intelligence,2012.01944
"While the potential of deep learning(DL) for automating simple tasks is already well explored, recent research started investigating the use of deep learning for creative design, both for complete artifact creation and supporting humans in the creation process. In this paper, we use insights from computational creativity to conceptualize and assess current applications of generative deep learning in creative domains identified in a literature review. We highlight parallels between current systems and different models of human creativity as well as their shortcomings. While deep learning yields results of high value, such as high quality images, their novelity is typically limited due to multiple reasons such a being tied to a conceptual space defined by training data and humans. Current DL methods also do not allow for changes in the internal problem representation and they lack the capability to identify connections across highly different domains, both of which are seen as major drivers of human creativity.",Creativity of Deep Learning: Conceptualization and Assessment,Johannes Schneider and Marcus Basalla,2022,Artificial Intelligence,2012.02282
"Having a comprehensive, high-quality dataset of road sign annotation is critical to the success of AI-based Road Sign Recognition (RSR) systems. In practice, annotators often face difficulties in learning road sign systems of different countries; hence, the tasks are often time-consuming and produce poor results. We propose a novel approach using knowledge graphs and a machine learning algorithm - variational prototyping-encoder (VPE) - to assist human annotators in classifying road signs effectively. Annotators can query the Road Sign Knowledge Graph using visual attributes and receive closest matching candidates suggested by the VPE model. The VPE model uses the candidates from the knowledge graph and a real sign image patch as inputs. We show that our knowledge graph approach can reduce sign search space by 98.9%. Furthermore, with VPE, our system can propose the correct single candidate for 75% of signs in the tested datasets, eliminating the human search effort entirely in those cases.",Accelerating Road Sign Ground Truth Construction with Knowledge Graph and Machine Learning,"Ji Eun Kim, Cory Henson, Kevin Huang, Tuan A. Tran, Wan-Yi Lin",2021,Artificial Intelligence,2012.02672
"With the popularity of the Internet, traditional offline resource allocation has evolved into a new form, called online resource allocation. It features the online arrivals of agents in the system and the real-time decision-making requirement upon the arrival of each online agent. Both offline and online resource allocation have wide applications in various real-world matching markets ranging from ridesharing to crowdsourcing. There are some emerging applications such as rebalancing in bike sharing and trip-vehicle dispatching in ridesharing, which involve a two-stage resource allocation process. The process consists of an offline phase and another sequential online phase, and both phases compete for the same set of resources. In this paper, we propose a unified model which incorporates both offline and online resource allocation into a single framework. Our model assumes non-uniform and known arrival distributions for online agents in the second online phase, which can be learned from historical data. We propose a parameterized linear programming (LP)-based algorithm, which is shown to be at most a constant factor of $1/4$ from the optimal. Experimental results on the real dataset show that our LP-based approaches outperform the LP-agnostic heuristics in terms of robustness and effectiveness.",A Unified Model for the Two-stage Offline-then-Online Resource Allocation,"Yifan Xu, Pan Xu, Jianping Pan and Jun Tao",2020,Artificial Intelligence,2012.06845
"Several scientific studies have reported the existence of the income gap among rideshare drivers based on demographic factors such as gender, age, race, etc. In this paper, we study the income inequality among rideshare drivers due to discriminative cancellations from riders, and the tradeoff between the income inequality (called fairness objective) with the system efficiency (called profit objective). We proposed an online bipartite-matching model where riders are assumed to arrive sequentially following a distribution known in advance. The highlight of our model is the concept of acceptance rate between any pair of driver-rider types, where types are defined based on demographic factors. Specially, we assume each rider can accept or cancel the driver assigned to her, each occurs with a certain probability which reflects the acceptance degree from the rider type towards the driver type. We construct a bi-objective linear program as a valid benchmark and propose two LP-based parameterized online algorithms. Rigorous online competitive ratio analysis is offered to demonstrate the flexibility and efficiency of our online algorithms in balancing the two conflicting goals, promotions of fairness and profit. Experimental results on a real-world dataset are provided as well, which confirm our theoretical predictions.",Trading the System Efficiency for the Income Equality of Drivers in Rideshare,Yifan Xu and Pan Xu,2020,Artificial Intelligence,2012.06850
"Ranking vertices of multidimensional networks is crucial in many areas of research, including selecting and determining the importance of decisions. Some decisions are significantly more important than others, and their weight categorization is also imortant. This paper defines a completely new method for determining the weight decisions using artificial intelligence for importance ranking of three-dimensional network vertices, improving the existing Ordered Statistics Vertex Extraction and Tracking Algorithm (OSVETA) based on modulation of quantized indices (QIM) and error correction codes. The technique we propose in this paper offers significant improvements the efficiency of determination the importance of network vertices in relation to statistical OSVETA criteria, replacing heuristic methods with methods of precise prediction of modern neural networks. The new artificial intelligence technique enables a significantly better definition of the 3D meshes and a better assessment of their topological features. The new method contributions result in a greater precision in defining stable vertices, significantly reducing the probability of deleting mesh vertices.",Artificial Intelligence ordered 3D vertex importance,"Iva Vasic, Bata Vasic, and Zorica Nikolic",2020,Artificial Intelligence,2012.10232
"Previous neural solvers of math word problems (MWPs) are learned with full supervision and fail to generate diverse solutions. In this paper, we address this issue by introducing a \textit{weakly-supervised} paradigm for learning MWPs. Our method only requires the annotations of the final answers and can generate various solutions for a single problem. To boost weakly-supervised learning, we propose a novel \textit{learning-by-fixing} (LBF) framework, which corrects the misperceptions of the neural network via symbolic reasoning. Specifically, for an incorrect solution tree generated by the neural network, the \textit{fixing} mechanism propagates the error from the root node to the leaf nodes and infers the most probable fix that can be executed to get the desired answer. To generate more diverse solutions, \textit{tree regularization} is applied to guide the efficient shrinkage and exploration of the solution space, and a \textit{memory buffer} is designed to track and save the discovered various fixes for each problem. Experimental results on the Math23K dataset show the proposed LBF framework significantly outperforms reinforcement learning baselines in weakly-supervised learning. Furthermore, it achieves comparable top-1 and much better top-3/5 answer accuracies than fully-supervised methods, demonstrating its strength in producing diverse solutions.",Learning by Fixing: Solving Math Word Problems with Weak Supervision,"Yining Hong, Qing Li, Daniel Ciao, Siyuan Huang, Song-Chun Zhu",2021,Artificial Intelligence,2012.10582
"The paper relies on the clinical data of a previously published study. We identify two very questionable assumptions of said work, namely confusing evidence of absence and absence of evidence, and neglecting the ordinal nature of attributes' domains. We then show that using an adequate ordinal methodology such as the dominance-based rough sets approach (DRSA) can significantly improve the predictive accuracy of the expert system, resulting in almost complete accuracy for a dataset of 100 instances. Beyond the performance of DRSA in solving the diagnosis problem at hand, these results suggest the inadequacy and triviality of the underlying dataset. We provide links to open data from the UCI machine learning repository to allow for an easy verification/refutation of the claims made in this paper.",Predicting Seminal Quality with the Dominance-Based Rough Sets Approach,Nassim Dehouche,2020,Artificial Intelligence,2012.13204
"Textile manufacturing is a typical traditional industry involving high complexity in interconnected processes with limited capacity on the application of modern technologies. Decision-making in this domain generally takes multiple criteria into consideration, which usually arouses more complexity. To address this issue, the present paper proposes a decision support system that combines the intelligent data-based random forest (RF) models and a human knowledge based analytical hierarchical process (AHP) multi-criteria structure in accordance to the objective and the subjective factors of the textile manufacturing process. More importantly, the textile manufacturing process is described as the Markov decision process (MDP) paradigm, and a deep reinforcement learning scheme, the Deep Q-networks (DQN), is employed to optimize it. The effectiveness of this system has been validated in a case study of optimizing a textile ozonation process, showing that it can better master the challenging decision-making tasks in textile manufacturing processes.",A Deep Reinforcement Learning Based Multi-Criteria Decision Support System for Textile Manufacturing Process Optimization,"Zhenglei He (GEMTEX), Kim Phuc Tran (GEMTEX), Sebastien Thomassey (GEMTEX), Xianyi Zeng (GEMTEX), Jie Xu, Chang Haiyi",2020,Artificial Intelligence,2012.14794
"Simulations, along with other similar applications like virtual worlds and video games, require computational models of intelligence that generate realistic and credible behavior for the participating synthetic characters. Cognitive architectures, which are models of the fixed structure underlying intelligent behavior in both natural and artificial systems, provide a conceptually valid common basis, as evidenced by the current efforts towards a standard model of the mind, to generate human-like intelligent behavior for these synthetic characters. Sigma is a cognitive architecture and system that strives to combine what has been learned from four decades of independent work on symbolic cognitive architectures, probabilistic graphical models, and more recently neural models, under its graphical architecture hypothesis. Sigma leverages an extended form of factor graphs towards a uniform grand unification of not only traditional cognitive capabilities but also key non-cognitive aspects, creating unique opportunities for the construction of new kinds of cognitive models that possess a Theory-of-Mind and that are perceptual, autonomous, interactive, affective, and adaptive. In this paper, we will introduce Sigma along with its diverse capabilities and then use three distinct proof-of-concept Sigma models to highlight combinations of these capabilities: (1) Distributional reinforcement learning models in; (2) A pair of adaptive and interactive agent models that demonstrate rule-based, probabilistic, and social reasoning; and (3) A knowledge-free exploration model in which an agent leverages only architectural appraisal variables, namely attention and curiosity, to locate an item while building up a map in a Unity environment.",Controlling Synthetic Characters in Simulations: A Case for Cognitive Architectures and Sigma,"Volkan Ustun, Paul S. Rosenbloom, Seyed Sajjadi, Jeremy Nuttal",2018,Artificial Intelligence,2101.02231
"The Coronavirus (COVID-19) pandemic has led to a rapidly growing 'infodemic' of health information online. This has motivated the need for accurate semantic search and retrieval of reliable COVID-19 information across millions of documents, in multiple languages. To address this challenge, this paper proposes a novel high precision and high recall neural Multistage BiCross encoder approach. It is a sequential three-stage ranking pipeline which uses the Okapi BM25 retrieval algorithm and transformer-based bi-encoder and cross-encoder to effectively rank the documents with respect to the given query. We present experimental results from our participation in the Multilingual Information Access (MLIA) shared task on COVID-19 multilingual semantic search. The independently evaluated MLIA results validate our approach and demonstrate that it outperforms other state-of-the-art approaches according to nearly all evaluation metrics in cases of both monolingual and bilingual runs.",Multistage BiCross encoder for multilingual access to COVID-19 health information,"Iknoor Singh, Carolina Scarton, Kalina Bontcheva",2021,Artificial Intelligence,2101.03013
"We present DEGARI (Dynamic Emotion Generator And ReclassIfier), an explainable system for emotion attribution and recommendation. This system relies on a recently introduced commonsense reasoning framework, the TCL logic, which is based on a human-like procedure for the automatic generation of novel concepts in a Description Logics knowledge base. Starting from an ontological formalization of emotions based on the Plutchik model, known as ArsEmotica, the system exploits the logic TCL to automatically generate novel commonsense semantic representations of compound emotions (e.g. Love as derived from the combination of Joy and Trust according to Plutchik). The generated emotions correspond to prototypes, i.e. commonsense representations of given concepts, and have been used to reclassify emotion-related contents in a variety of artistic domains, ranging from art datasets to the editorial contents available in RaiPlay, the online platform of RAI Radiotelevisione Italiana (the Italian public broadcasting company). We show how the reported results (evaluated in the light of the obtained reclassifications, the user ratings assigned to such reclassifications, and their explainability) are encouraging, and pave the way to many further research directions.","A Commonsense Reasoning Framework for Explanatory Emotion Attribution, Generation and Re-classification","Antonio Lieto, Gian Luca Pozzato, Stefano Zoia, Viviana Patti, Rossana Damiano",2021,Artificial Intelligence,2101.04017
"Commonsense knowledge is essential for many AI applications, including those in natural language processing, visual processing, and planning. Consequently, many sources that include commonsense knowledge have been designed and constructed over the past decades. Recently, the focus has been on large text-based sources, which facilitate easier integration with neural (language) models and application to textual tasks, typically at the expense of the semantics of the sources and their harmonization. Efforts to consolidate commonsense knowledge have yielded partial success, with no clear path towards a comprehensive solution. We aim to organize these sources around a common set of dimensions of commonsense knowledge. We survey a wide range of popular commonsense sources with a special focus on their relations. We consolidate these relations into 13 knowledge dimensions. This consolidation allows us to unify the separate sources and to compute indications of their coverage, overlap, and gaps with respect to the knowledge dimensions. Moreover, we analyze the impact of each dimension on downstream reasoning tasks that require commonsense knowledge, observing that the temporal and desire/goal dimensions are very beneficial for reasoning on current downstream tasks, while distinctness and lexical knowledge have little impact. These results reveal preferences for some dimensions in current evaluation, and potential neglect of others.",Dimensions of Commonsense Knowledge,"Filip Ilievski, Alessandro Oltramari, Kaixin Ma, Bin Zhang, Deborah L. McGuinness, Pedro Szekely",2021,Artificial Intelligence,2101.04640
"With the powerful learning ability of deep convolutional networks, deep clustering methods can extract the most discriminative information from individual data and produce more satisfactory clustering results. However, existing deep clustering methods usually ignore the relationship between the data. Fortunately, the graph convolutional network can handle such relationship, opening up a new research direction for deep clustering. In this paper, we propose a cross-attention based deep clustering framework, named Cross-Attention Fusion based Enhanced Graph Convolutional Network (CaEGCN), which contains four main modules: the cross-attention fusion module which innovatively concatenates the Content Auto-encoder module (CAE) relating to the individual data and Graph Convolutional Auto-encoder module (GAE) relating to the relationship between the data in a layer-by-layer manner, and the self-supervised model that highlights the discriminative information for clustering tasks. While the cross-attention fusion module fuses two kinds of heterogeneous representation, the CAE module supplements the content information for the GAE module, which avoids the over-smoothing problem of GCN. In the GAE module, two novel loss functions are proposed that reconstruct the content and relationship between the data, respectively. Finally, the self-supervised module constrains the distributions of the middle layer representations of CAE and GAE to be consistent. Experimental results on different types of datasets prove the superiority and robustness of the proposed CaEGCN.",CaEGCN: Cross-Attention Fusion based Enhanced Graph Convolutional Network for Clustering,"Guangyu Huo, Yong Zhang, Junbin Gao, Boyue Wang, Yongli Hu, and Baocai Yin",2021,Artificial Intelligence,2101.06883
"Recent advances have shown how decision trees are apt data structures for concisely representing strategies (or controllers) satisfying various objectives. Moreover, they also make the strategy more explainable. The recent tool dtControl had provided pipelines with tools supporting strategy synthesis for hybrid systems, such as SCOTS and Uppaal Stratego. We present dtControl 2.0, a new version with several fundamentally novel features. Most importantly, the user can now provide domain knowledge to be exploited in the decision tree learning process and can also interactively steer the process based on the dynamically provided information. To this end, we also provide a graphical user interface. It allows for inspection and re-computation of parts of the result, suggesting as well as receiving advice on predicates, and visual simulation of the decision-making process. Besides, we interface model checkers of probabilistic systems, namely Storm and PRISM and provide dedicated support for categorical enumeration-type state variables. Consequently, the controllers are more explainable and smaller.",dtControl 2.0: Explainable Strategy Representation via Decision Tree Learning Steered by Experts,"Pranav Ashok, Mathias Jackermeier, Jan K\v{r}et\'insk\'y, Christoph Weinhuber, Maximilian Weininger, Mayank Yadav",2021,Artificial Intelligence,2101.07202
"For some scientific questions, empirical data are essential to develop reliable simulation models. These data usually come from different sources with diverse and heterogeneous formats. The design of complex data-driven models is often shaped by the structure of the data available in research projects. Hence, applying such models to other case studies requires either to get similar data or to transform new data to fit the model inputs. It is the case of agent-based models (ABMs) that use advanced data structures such as Geographic Information Systems data. We faced this problem in the LittoSIM-GEN project when generalizing our participatory flooding model (LittoSIM) to new territories. From this experience, we provide a mapping approach to structure, describe, and automatize the integration of geospatial data into ABMs.",Mapping and Describing Geospatial Data to Generalize Complex Mapping and Describing Geospatial Data to Generalize Complex Models: The Case of LittoSIM-GEN Models,"Ahmed Laatabi, Nicolas Becu (LIENSs), Nicolas Marilleau (UMMISCO), C\'ecilia Pignon-Mussaud (LIENSs), Marion Amalric (CITERES), X. Bertin (LIENSs), Brice Anselme (PRODIG), Elise Beck (PACTE)",2020,Artificial Intelligence,2101.07523
"In 2011, Hibbard suggested an intelligence measure for agents who compete in an adversarial sequence prediction game. We argue that Hibbard's idea should actually be considered as two separate ideas: first, that the intelligence of such agents can be measured based on the growth rates of the runtimes of the competitors that they defeat; and second, one specific (somewhat arbitrary) method for measuring said growth rates. Whereas Hibbard's intelligence measure is based on the latter growth-rate-measuring method, we survey other methods for measuring function growth rates, and exhibit the resulting Hibbard-like intelligence measures and taxonomies. Of particular interest, we obtain intelligence taxonomies based on Big-O and Big-Theta notation systems, which taxonomies are novel in that they challenge conventional notions of what an intelligence measure should look like. We discuss how intelligence measurement of sequence predictors can indirectly serve as intelligence measurement for agents with Artificial General Intelligence (AGIs).",Measuring Intelligence and Growth Rate: Variations on Hibbard's Intelligence Measure,"Samuel Alexander, Bill Hibbard",2021,Artificial Intelligence,2101.12047
"Experimental studies are prevalent in Evolutionary Computation (EC), and concerns about the reproducibility and replicability of such studies have increased in recent times, reflecting similar concerns in other scientific fields. In this article, we discuss, within the context of EC, the different types of reproducibility and suggest a classification that refines the badge system of the Association of Computing Machinery (ACM) adopted by ACM Transactions on Evolutionary Learning and Optimization (https://dlnext.acm.org/journal/telo). We identify cultural and technical obstacles to reproducibility in the EC field. Finally, we provide guidelines and suggest tools that may help to overcome some of these reproducibility obstacles.",Reproducibility in Evolutionary Computation,"Manuel L\'opez-Ib\'a\~nez (University of M\'alaga, Spain), Juergen Branke (University of Warwick, UK), Lu\'is Paquete (University of Coimbra, Portugal)",2021,Artificial Intelligence,2102.03380
"Real-world knowledge graphs are often characterized by low-frequency relations - a challenge that has prompted an increasing interest in few-shot link prediction methods. These methods perform link prediction for a set of new relations, unseen during training, given only a few example facts of each relation at test time. In this work, we perform a systematic study on a spectrum of models derived by generalizing the current state of the art for few-shot link prediction, with the goal of probing the limits of learning in this few-shot setting. We find that a simple zero-shot baseline - which ignores any relation-specific information - achieves surprisingly strong performance. Moreover, experiments on carefully crafted synthetic datasets show that having only a few examples of a relation fundamentally limits models from using fine-grained structural information and only allows for exploiting the coarse-grained positional information of entities. Together, our findings challenge the implicit assumptions and inductive biases of prior work and highlight new directions for research in this area.",Exploring the Limits of Few-Shot Link Prediction in Knowledge Graphs,"Dora Jambor, Komal Teru, Joelle Pineau, William L. Hamilton",2021,Artificial Intelligence,2102.03419
AI systems often rely on two key components: a specified goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial specification of the principal's goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the $L$ attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on $J < L$ attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal-agent problem from artificial intelligence; 2) we provide necessary and sufficient conditions under which indefinitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identifies a theoretical scenario where some degree of interactivity is desirable.,Consequences of Misaligned AI,"Simon Zhuang, Dylan Hadfield-Menell",2020,Artificial Intelligence,2102.03896
"Understanding the principles of real-world biological multi-agent behaviors is a current challenge in various scientific and engineering fields. The rules regarding the real-world biological multi-agent behaviors such as team sports are often largely unknown due to their inherently higher-order interactions, cognition, and body dynamics. Estimation of the rules from data, i.e., data-driven approaches such as machine learning, provides an effective way for the analysis of such behaviors. Although most data-driven models have non-linear structures and high prediction performances, it is sometimes hard to interpret them. This survey focuses on data-driven analysis for quantitative understanding of invasion team sports behaviors such as basketball and football, and introduces two main approaches for understanding such multi-agent behaviors: (1) extracting easily interpretable features or rules from data and (2) generating and controlling behaviors in visually-understandable ways. The first approach involves the visualization of learned representations and the extraction of mathematical structures behind the behaviors. The second approach can be used to test hypotheses by simulating and controlling future and counterfactual behaviors. Lastly, the potential practical applications of extracted rules, features, and generated behaviors are discussed. These approaches can contribute to a better understanding of multi-agent behaviors in the real world.",Data-driven Analysis for Understanding Team Sports Behaviors,Keisuke Fujii,2021,Artificial Intelligence,2102.07545
"Accurate forecasting of medical service requirements is an important big data problem that is crucial for resource management in critical times such as natural disasters and pandemics. With the global spread of coronavirus disease 2019 (COVID-19), several concerns have been raised regarding the ability of medical systems to handle sudden changes in the daily routines of healthcare providers. One significant problem is the management of ambulance dispatch and control during a pandemic. To help address this problem, we first analyze ambulance dispatch data records from April 2014 to August 2020 for Nagoya City, Japan. Significant changes were observed in the data during the pandemic, including the state of emergency (SoE) declared across Japan. In this study, we propose a deep learning framework based on recurrent neural networks to estimate the number of emergency ambulance dispatches (EADs) during a SoE. The fusion of data includes environmental factors, the localization data of mobile phone users, and the past history of EADs, thereby providing a general framework for knowledge discovery and better resource management. The results indicate that the proposed blend of training data can be used efficiently in a real-world estimation of EAD requirements during periods of high uncertainties such as pandemics.","Knowledge discovery from emergency ambulance dispatch during COVID-19: A case study of Nagoya City, Japan","Essam A. Rashed, Sachiko Kodera, Hidenobu Shirakami, Ryotetsu Kawaguchi, Kazuhiro Watanabe, Akimasa Hirata",2021,Artificial Intelligence,2102.08628
"Drug discovery and development is a complex and costly process. Machine learning approaches are being investigated to help improve the effectiveness and speed of multiple stages of the drug discovery pipeline. Of these, those that use Knowledge Graphs (KG) have promise in many tasks, including drug repurposing, drug toxicity prediction and target gene-disease prioritisation. In a drug discovery KG, crucial elements including genes, diseases and drugs are represented as entities, whilst relationships between them indicate an interaction. However, to construct high-quality KGs, suitable data is required. In this review, we detail publicly available sources suitable for use in constructing drug discovery focused KGs. We aim to help guide machine learning and KG practitioners who are interested in applying new techniques to the drug discovery field, but who may be unfamiliar with the relevant data sources. The datasets are selected via strict criteria, categorised according to the primary type of information contained within and are considered based upon what information could be extracted to build a KG. We then present a comparative analysis of existing public drug discovery KGs and a evaluation of selected motivating case studies from the literature. Additionally, we raise numerous and unique challenges and issues associated with the domain and its datasets, whilst also highlighting key future research directions. We hope this review will motivate KGs use in solving key and emerging questions in the drug discovery domain.",A Review of Biomedical Datasets Relating to Drug Discovery: A Knowledge Graph Perspective,Stephen Bonner and Ian P Barrett and Cheng Ye and Rowan Swiers and Ola Engkvist and Andreas Bender and Charles Tapley Hoyt and William L Hamilton,2022,Artificial Intelligence,2102.10062
"A key challenge for reinforcement learning is solving long-horizon planning problems. Recent work has leveraged programs to guide reinforcement learning in these settings. However, these approaches impose a high manual burden on the user since they must provide a guiding program for every new task. Partially observed environments further complicate the programming task because the program must implement a strategy that correctly, and ideally optimally, handles every possible configuration of the hidden regions of the environment. We propose a new approach, model predictive program synthesis (MPPS), that uses program synthesis to automatically generate the guiding programs. It trains a generative model to predict the unobserved portions of the world, and then synthesizes a program based on samples from this model in a way that is robust to its uncertainty. In our experiments, we show that our approach significantly outperforms non-program-guided approaches on a set of challenging benchmarks, including a 2D Minecraft-inspired environment where the agent must complete a complex sequence of subtasks to achieve its goal, and achieves a similar performance as using handcrafted programs to guide the agent. Our results demonstrate that our approach can obtain the benefits of program-guided reinforcement learning without requiring the user to provide a new guiding program for every new task.",Program Synthesis Guided Reinforcement Learning for Partially Observed Environments,"Yichen David Yang, Jeevana Priya Inala, Osbert Bastani, Yewen Pu, Armando Solar-Lezama, Martin Rinard",2021,Artificial Intelligence,2102.11137
"Despite many proposed algorithms to provide robustness to deep learning (DL) models, DL models remain susceptible to adversarial attacks. We hypothesize that the adversarial vulnerability of DL models stems from two factors. The first factor is data sparsity which is that in the high dimensional input data space, there exist large regions outside the support of the data distribution. The second factor is the existence of many redundant parameters in the DL models. Owing to these factors, different models are able to come up with different decision boundaries with comparably high prediction accuracy. The appearance of the decision boundaries in the space outside the support of the data distribution does not affect the prediction accuracy of the model. However, it makes an important difference in the adversarial robustness of the model. We hypothesize that the ideal decision boundary is as far as possible from the support of the data distribution. In this paper, we develop a training framework to observe if DL models are able to learn such a decision boundary spanning the space around the class distributions further from the data points themselves. Semi-supervised learning was deployed during training by leveraging unlabeled data generated in the space outside the support of the data distribution. We measured adversarial robustness of the models trained using this training framework against well-known adversarial attacks and by using robustness metrics. We found that models trained using our framework, as well as other regularization methods and adversarial training support our hypothesis of data sparsity and that models trained with these methods learn to have decision boundaries more similar to the aforementioned ideal decision boundary. The code for our training framework is available at https://github.com/MahsaPaknezhad/AdversariallyRobustTraining.",Explaining Adversarial Vulnerability with a Data Sparsity Hypothesis,"Mahsa Paknezhad, Cuong Phuc Ngo, Amadeus Aristo Winarto, Alistair Cheong, Chuen Yang Beh, Jiayang Wu, Hwee Kuan Lee",2022,Artificial Intelligence,2103.00778
"Multiplayer Online Battle Area (MOBA) games are a recent huge success both in the video game industry and the international eSports scene. These games encourage team coordination and cooperation, short and long-term planning, within a real-time combined action and strategy gameplay. Artificial Intelligence and Computational Intelligence in Games research competitions offer a wide variety of challenges regarding the study and application of AI techniques to different game genres. These events are widely accepted by the AI/CI community as a sort of AI benchmarking that strongly influences many other research areas in the field. This paper presents and describes in detail the Dota 2 Bot competition and the Dota 2 AI framework that supports it. This challenge aims to join both, MOBAs and AI/CI game competitions, inviting participants to submit AI controllers for the successful MOBA \textit{Defense of the Ancients 2} (Dota 2) to play in 1v1 matches, which aims for fostering research on AI techniques for real-time games. The Dota 2 AI framework makes use of the actual Dota 2 game modding capabilities to enable to connect external AI controllers to actual Dota 2 game matches using the original Free-to-Play game.se of the actual Dota 2 game modding capabilities to enable to connect external AI controllers to actual Dota 2 game matches using the original Free-to-Play game.",The Dota 2 Bot Competition,Jose M. Font and Tobias Mahlmann,2018,Artificial Intelligence,2103.02943
"Deep deterministic policy gradient (DDPG)-based car-following strategy can break through the constraints of the differential equation model due to the ability of exploration on complex environments. However, the car-following performance of DDPG is usually degraded by unreasonable reward function design, insufficient training, and low sampling efficiency. In order to solve this kind of problem, a hybrid car-following strategy based on DDPG and cooperative adaptive cruise control (CACC) is proposed. First, the car-following process is modeled as the Markov decision process to calculate CACC and DDPG simultaneously at each frame. Given a current state, two actions are obtained from CACC and DDPG, respectively. Then, an optimal action, corresponding to the one offering a larger reward, is chosen as the output of the hybrid strategy. Meanwhile, a rule is designed to ensure that the change rate of acceleration is smaller than the desired value. Therefore, the proposed strategy not only guarantees the basic performance of car-following through CACC but also makes full use of the advantages of exploration on complex environments via DDPG. Finally, simulation results show that the car-following performance of the proposed strategy is improved compared with that of DDPG and CACC.",Hybrid Car-Following Strategy based on Deep Deterministic Policy Gradient and Cooperative Adaptive Cruise Control,"Ruidong Yan, Rui Jiang, Bin Jia, Jin Huang, and Diange Yang",2021,Artificial Intelligence,2103.03796
"Self-driving Autonomous Vehicles (SAVs) are gaining more interest each passing day by the industry as well as the general public. Tech and automobile companies are investing huge amounts of capital in research and development of SAVs to make sure they have a head start in the SAV market in the future. One of the major hurdles in the way of SAVs making it to the public roads is the lack of confidence of public in the safety aspect of SAVs. In order to assure safety and provide confidence to the public in the safety of SAVs, researchers around the world have used coverage-based testing for Verification and Validation (V&V) and safety assurance of SAVs. The objective of this paper is to investigate the coverage criteria proposed and coverage maximizing techniques used by researchers in the last decade up till now, to assure safety of SAVs. We conduct a Systematic Literature Review (SLR) for this investigation in our paper. We present a classification of existing research based on the coverage criteria used. Several research gaps and research directions are also provided in this SLR to enable further research in this domain. This paper provides a body of knowledge in the domain of safety assurance of SAVs. We believe the results of this SLR will be helpful in the progression of V&V and safety assurance of SAVs.",Coverage based testing for V&V and Safety Assurance of Self-driving Autonomous Vehicles: A Systematic Literature Review,"Zaid Tahir, Rob Alexander",2020,Artificial Intelligence,2103.04364
"Vehicle trajectory prediction tasks have been commonly tackled from two distinct perspectives: either with knowledge-driven methods or more recently with data-driven ones. On the one hand, we can explicitly implement domain-knowledge or physical priors such as anticipating that vehicles will follow the middle of the roads. While this perspective leads to feasible outputs, it has limited performance due to the difficulty to hand-craft complex interactions in urban environments. On the other hand, recent works use data-driven approaches which can learn complex interactions from the data leading to superior performance. However, generalization, \textit{i.e.}, having accurate predictions on unseen data, is an issue leading to unrealistic outputs. In this paper, we propose to learn a ""Realistic Residual Block"" (RRB), which effectively connects these two perspectives. Our RRB takes any off-the-shelf knowledge-driven model and finds the required residuals to add to the knowledge-aware trajectory. Our proposed method outputs realistic predictions by confining the residual range and taking into account its uncertainty. We also constrain our output with Model Predictive Control (MPC) to satisfy kinematic constraints. Using a publicly available dataset, we show that our method outperforms previous works in terms of accuracy and generalization to new scenes. We will release our code and data split here: https://github.com/vita-epfl/RRB.",Injecting Knowledge in Data-driven Vehicle Trajectory Predictors,"Mohammadhossein Bahari, Ismail Nejjar, Alexandre Alahi",2021,Artificial Intelligence,2103.04854
"Monte Carlo Tree Search (MCTS) is a powerful approach to designing game-playing bots or solving sequential decision problems. The method relies on intelligent tree search that balances exploration and exploitation. MCTS performs random sampling in the form of simulations and stores statistics of actions to make more educated choices in each subsequent iteration. The method has become a state-of-the-art technique for combinatorial games, however, in more complex games (e.g. those with high branching factor or real-time ones), as well as in various practical domains (e.g. transportation, scheduling or security) an efficient MCTS application often requires its problem-dependent modification or integration with other techniques. Such domain-specific modifications and hybrid approaches are the main focus of this survey. The last major MCTS survey has been published in 2012. Contributions that appeared since its release are of particular interest for this review.",Monte Carlo Tree Search: A Review of Recent Modifications and Applications,"Maciej \'Swiechowski, Konrad Godlewski, Bartosz Sawicki, Jacek Ma\'ndziuk",2022,Artificial Intelligence,2103.04931
"In this short paper, we outline nine classical benchmarks submitted to the first hierarchical planning track of the International Planning competition in 2020. All of these benchmarks are based on the HDDL language. The choice of the benchmarks was based on a questionnaire sent to the HTN community. They are the following: Barman, Childsnack, Rover, Satellite, Blocksworld, Depots, Gripper, and Hiking. In the rest of the paper we give a short description of these benchmarks. All are totally ordered.",From Classical to Hierarchical: benchmarks for the HTN Track of the International Planning Competition,"Damien Pellier, Humbert Fiorino",2020,Artificial Intelligence,2103.05481
"High capacity end-to-end approaches for human motion (behavior) prediction have the ability to represent subtle nuances in human behavior, but struggle with robustness to out of distribution inputs and tail events. Planning-based prediction, on the other hand, can reliably output decent-but-not-great predictions: it is much more stable in the face of distribution shift (as we verify in this work), but it has high inductive bias, missing important aspects that drive human decisions, and ignoring cognitive biases that make human behavior suboptimal. In this work, we analyze one family of approaches that strive to get the best of both worlds: use the end-to-end predictor on common cases, but do not rely on it for tail events / out-of-distribution inputs -- switch to the planning-based predictor there. We contribute an analysis of different approaches for detecting when to make this switch, using an autonomous driving domain. We find that promising approaches based on ensembling or generative modeling of the training distribution might not be reliable, but that there very simple methods which can perform surprisingly well -- including training a classifier to pick up on tell-tale issues in predicted trajectories.",On complementing end-to-end human behavior predictors with planning,"Liting Sun, Xiaogang Jia, Anca D. Dragan",2021,Artificial Intelligence,2103.05661
"This paper outlines a perspective on the future of AI, discussing directions for machines models of human-like intelligence. We explain how developmental and evolutionary theories of human cognition should further inform artificial intelligence. We emphasize the role of ecological niches in sculpting intelligent behavior, and in particular that human intelligence was fundamentally shaped to adapt to a constantly changing socio-cultural environment. We argue that a major limit of current work in AI is that it is missing this perspective, both theoretically and experimentally. Finally, we discuss the promising approach of developmental artificial intelligence, modeling infant development through multi-scale interaction between intrinsically motivated learning, embodiment and a fastly changing socio-cultural environment. This paper takes the form of an interview of Pierre-Yves Oudeyer by Mandred Eppe, organized within the context of a KI - K{\""{u}}nstliche Intelligenz special issue in developmental robotics.",Intelligent behavior depends on the ecological niche: Scaling up AI to human-like intelligence in socio-cultural environments,Manfred Eppe and Pierre-Yves Oudeyer,2021,Artificial Intelligence,2103.06769
"In this paper we establish a link between fuzzy and preferential semantics for description logics and Self-Organising Maps, which have been proposed as possible candidates to explain the psychological mechanisms underlying category generalisation. In particular, we show that the input/output behavior of a Self-Organising Map after training can be described by a fuzzy description logic interpretation as well as by a preferential interpretation, based on a concept-wise multipreference semantics, which takes into account preferences with respect to different concepts and has been recently proposed for ranked and for weighted defeasible description logics. Properties of the network can be proven by model checking on the fuzzy or on the preferential interpretation. Starting from the fuzzy interpretation, we also provide a probabilistic account for this neural network model.","A conditional, a fuzzy and a probabilistic interpretation of self-organising maps","Laura Giordano, Valentina Gliozzi, Daniele Theseider Dupr\'e",2022,Artificial Intelligence,2103.06854
"The reporting and the analysis of current events around the globe has expanded from professional, editor-lead journalism all the way to citizen journalism. Nowadays, politicians and other key players enjoy direct access to their audiences through social media, bypassing the filters of official cables or traditional media. However, the multiple advantages of free speech and direct communication are dimmed by the misuse of media to spread inaccurate or misleading claims. These phenomena have led to the modern incarnation of the fact-checker -- a professional whose main aim is to examine claims using available evidence and to assess their veracity. As in other text forensics tasks, the amount of information available makes the work of the fact-checker more difficult. With this in mind, starting from the perspective of the professional fact-checker, we survey the available intelligent technologies that can support the human expert in the different steps of her fact-checking endeavor. These include identifying claims worth fact-checking, detecting relevant previously fact-checked claims, retrieving relevant evidence to fact-check a claim, and actually verifying a claim. In each case, we pay attention to the challenges in future work and the potential impact on real-world fact-checking.",Automated Fact-Checking for Assisting Human Fact-Checkers,"Preslav Nakov, David Corney, Maram Hasanain, Firoj Alam, Tamer Elsayed, Alberto Barr\'on-Cede\~no, Paolo Papotti, Shaden Shaar, Giovanni Da San Martino",2021,Artificial Intelligence,2103.07769
"We consider the problem of reaching a propositional goal condition in fully-observable non-deterministic (FOND) planning under a general class of fairness assumptions that are given explicitly. The fairness assumptions are of the form A/B and say that state trajectories that contain infinite occurrences of an action a from A in a state s and finite occurrence of actions from B, must also contain infinite occurrences of action a in s followed by each one of its possible outcomes. The infinite trajectories that violate this condition are deemed as unfair, and the solutions are policies for which all the fair trajectories reach a goal state. We show that strong and strong-cyclic FOND planning, as well as QNP planning, a planning model introduced recently for generalized planning, are all special cases of FOND planning with fairness assumptions of this form which can also be combined. FOND+ planning, as this form of planning is called, combines the syntax of FOND planning with some of the versatility of LTL for expressing fairness constraints. A new planner is implemented by reducing FOND+ planning to answer set programs, and the performance of the planner is evaluated in comparison with FOND and QNP planners, and LTL synthesis tools.",Flexible FOND Planning with Explicit Fairness Assumptions,Ivan D. Rodriguez and Blai Bonet and Sebastian Sardina and Hector Geffner,2022,Artificial Intelligence,2103.08391
"Most conversational recommendation approaches are either not explainable, or they require external user's knowledge for explaining or their explanations cannot be applied in real time due to computational limitations. In this work, we present a real time category based conversational recommendation approach, which can provide concise explanations without prior user knowledge being required. We first perform an explainable user model in the form of preferences over the items' categories, and then use the category preferences to recommend items. The user model is performed by applying a BERT-based neural architecture on the conversation. Then, we translate the user model into item recommendation scores using a Feed Forward Network. User preferences during the conversation in our approach are represented by category vectors which are directly interpretable. The experimental results on the real conversational recommendation dataset ReDial demonstrate comparable performance to the state-of-the-art, while our approach is explainable. We also show the potential power of our framework by involving an oracle setting of category preference prediction.",Category Aware Explainable Conversational Recommendation,"Nikolaos Kondylidis, Jie Zou and Evangelos Kanoulas",2021,Artificial Intelligence,2103.08733
"With the development of measurement technology, data on the movements of actual games in various sports can be obtained and used for planning and evaluating the tactics and strategy. Defense in team sports is generally difficult to be evaluated because of the lack of statistical data. Conventional evaluation methods based on predictions of scores are considered unreliable because they predict rare events throughout the game. Besides, it is difficult to evaluate various plays leading up to a score. In this study, we propose a method to evaluate team defense from a comprehensive perspective related to team performance by predicting ball recovery and being attacked, which occur more frequently than goals, using player actions and positional data of all players and the ball. Using data from 45 soccer matches, we examined the relationship between the proposed index and team performance in actual matches and throughout a season. Results show that the proposed classifiers predicted the true events (mean F1 score $>$ 0.483) better than the existing classifiers which were based on rare events or goals (mean F1 score $<$ 0.201). Also, the proposed index had a moderate correlation with the long-term outcomes of the season ($r =$ 0.397). These results suggest that the proposed index might be a more reliable indicator rather than winning or losing with the inclusion of accidental factors.",Evaluation of soccer team defense based on prediction models of ball recovery and being attacked: A pilot study,"Kosuke Toda, Masakiyo Teranishi, Keisuke Kushiro, Keisuke Fujii",2022,Artificial Intelligence,2103.09627
"While artificial intelligence has been applied to control players' decisions in board games for over half a century, little attention is given to games with no player competition. Pandemic is an exemplar collaborative board game where all players coordinate to overcome challenges posed by events occurring during the game's progression. This paper proposes an artificial agent which controls all players' actions and balances chances of winning versus risk of losing in this highly stochastic environment. The agent applies a Rolling Horizon Evolutionary Algorithm on an abstraction of the game-state that lowers the branching factor and simulates the game's stochasticity. Results show that the proposed algorithm can find winning strategies more consistently in different games of varying difficulty. The impact of a number of state evaluation metrics is explored, balancing between optimistic strategies that favor winning and pessimistic strategies that guard against losing.",Collaborative Agent Gameplay in the Pandemic Board Game,Konstantinos Sfikas and Antonios Liapis,2020,Artificial Intelligence,2103.11388
"We discuss how over the last 30 to 50 years, Artificial Intelligence (AI) systems that focused only on data have been handicapped, and how knowledge has been critical in developing smarter, intelligent, and more effective systems. In fact, the vast progress in AI can be viewed in terms of the three waves of AI as identified by DARPA. During the first wave, handcrafted knowledge has been at the center-piece, while during the second wave, the data-driven approaches supplanted knowledge. Now we see a strong role and resurgence of knowledge fueling major breakthroughs in the third wave of AI underpinning future intelligent systems as they attempt human-like decision making, and seek to become trusted assistants and companions for humans. We find a wider availability of knowledge created from diverse sources, using manual to automated means both by repurposing as well as by extraction. Using knowledge with statistical learning is becoming increasingly indispensable to help make AI systems more transparent and auditable. We will draw a parallel with the role of knowledge and experience in human intelligence based on cognitive science, and discuss emerging neuro-symbolic or hybrid AI systems in which knowledge is the critical enabler for combining capabilities of the data-intensive statistical AI systems with those of symbolic AI systems, resulting in more capable AI systems that support more human-like intelligence.",The Duality of Data and Knowledge Across the Three Waves of AI,Amit Sheth and Krishnaprasad Thirunarayan,2021,Artificial Intelligence,2103.13520
"We propose a new classifier based on Dempster-Shafer (DS) theory and a convolutional neural network (CNN) architecture for set-valued classification. In this classifier, called the evidential deep-learning classifier, convolutional and pooling layers first extract high-dimensional features from input data. The features are then converted into mass functions and aggregated by Dempster's rule in a DS layer. Finally, an expected utility layer performs set-valued classification based on mass functions. We propose an end-to-end learning strategy for jointly updating the network parameters. Additionally, an approach for selecting partial multi-class acts is proposed. Experiments on image recognition, signal processing, and semantic-relationship classification tasks demonstrate that the proposed combination of deep CNN, DS layer, and expected utility layer makes it possible to improve classification accuracy and to make cautious decisions by assigning confusing patterns to multi-class sets.",An evidential classifier based on Dempster-Shafer theory and deep learning,"Zheng Tong, Philippe Xu, Thierry Den{\oe}ux",2021,Artificial Intelligence,2103.13549
"This article outlines what we learned from the first year of the AI Settlement Generation Competition in Minecraft, a competition about producing AI programs that can generate interesting settlements in Minecraft for an unseen map. This challenge seeks to focus research into adaptive and holistic procedural content generation. Generating Minecraft towns and villages given existing maps is a suitable task for this, as it requires the generated content to be adaptive, functional, evocative and aesthetic at the same time. Here, we present the results from the first iteration of the competition. We discuss the evaluation methodology, present the different technical approaches by the competitors, and outline the open problems.",The AI Settlement Generation Challenge in Minecraft: First Year Report,"Christoph Salge, Michael Cerny Green, Rodrigo Canaan, Filip Skwarski, Rafael Fritsch, Adrian Brightmoore, Shaofang Ye, Changxing Cao and Julian Togelius",2020,Artificial Intelligence,2103.14950
"In the family of Intelligent Transportation Systems (ITS), Multimodal Transport Systems (MMTS) have placed themselves as a mainstream transportation mean of our time as a feasible integrative transportation process. The Global Economy progressed with the help of transportation. The volume of goods and distances covered have doubled in the last ten years, so there is a high demand of an optimized transportation, fast but with low costs, saving resources but also safe, with low or zero emissions. Thus, it is important to have an overview of existing research in this field, to know what was already done and what is to be studied next. The main objective is to explore a beneficent selection of the existing research, methods and information in the field of multimodal transportation research, to identify industry needs and gaps in research and provide context for future research. The selective survey covers multimodal transport design and optimization in terms of: cost, time, and network topology. The multimodal transport theoretical aspects, context and resources are also covering various aspects. The survey's selection includes nowadays best methods and solvers for Intelligent Transportation Systems (ITS). The gap between theory and real-world applications should be further solved in order to optimize the global multimodal transportation system.",Selective Survey: Most Efficient Models and Solvers for Integrative Multimodal Transport,"Oliviu Matei, Erdei Rudolf, Camelia-M. Pintea",2021,Artificial Intelligence,2103.15555
"Along with the development of modern computing technology and social sciences, both theoretical research and practical applications of social computing have been continuously extended. In particular with the boom of artificial intelligence (AI), social computing is significantly influenced by AI. However, the conventional technologies of AI have drawbacks in dealing with more complicated and dynamic problems. Such deficiency can be rectified by hybrid human-artificial intelligence (H-AI) which integrates both human intelligence and AI into one unity, forming a new enhanced intelligence. H-AI in dealing with social problems shows the advantages that AI can not surpass. This paper firstly introduces the concept of H-AI. AI is the intelligence in the transition stage of H-AI, so the latest research progresses of AI in social computing are reviewed. Secondly, it summarizes typical challenges faced by AI in social computing, and makes it possible to introduce H-AI to solve these challenges. Finally, the paper proposes a holistic framework of social computing combining with H-AI, which consists of four layers: object layer, base layer, analysis layer, and application layer. It represents H-AI has significant advantages over AI in solving social problems.",A Survey of Hybrid Human-Artificial Intelligence for Social Computing,"Wenxi Wang, Huansheng Ning, Feifei Shi, Sahraoui Dhelim, Weishan Zhang, Liming Chen",2021,Artificial Intelligence,2103.15558
"The scope of data-driven fault diagnosis models is greatly extended through deep learning (DL). However, the classical convolution and recurrent structure have their defects in computational efficiency and feature representation, while the latest Transformer architecture based on attention mechanism has not yet been applied in this field. To solve these problems, we propose a novel time-frequency Transformer (TFT) model inspired by the massive success of vanilla Transformer in sequence processing. Specially, we design a fresh tokenizer and encoder module to extract effective abstractions from the time-frequency representation (TFR) of vibration signals. On this basis, a new end-to-end fault diagnosis framework based on time-frequency Transformer is presented in this paper. Through the case studies on bearing experimental datasets, we construct the optimal Transformer structure and verify its fault diagnosis performance. The superiority of the proposed method is demonstrated in comparison with the benchmark models and other state-of-the-art methods.",A novel time-frequency Transformer based on self-attention mechanism and its application in fault diagnosis of rolling bearings,"Yifei Ding, Minping Jia, Qiuhua Miao, Yudong Cao",2022,Artificial Intelligence,2104.09079
"Recently, the research on protecting the intellectual properties (IP) of deep neural networks (DNN) has attracted serious concerns. A number of DNN copyright protection methods have been proposed. However, most of the existing watermarking methods focus on verifying the copyright of the model, which do not support the authentication and management of users' fingerprints, thus can not satisfy the requirements of commercial copyright protection. In addition, the query modification attack which was proposed recently can invalidate most of the existing backdoor-based watermarking methods. To address these challenges, in this paper, we propose a method to protect the intellectual properties of DNN models by using an additional class and steganographic images. Specifically, we use a set of watermark key samples to embed an additional class into the DNN, so that the watermarked DNN will classify the watermark key sample as the predefined additional class in the copyright verification stage. We adopt the least significant bit (LSB) image steganography to embed users' fingerprints into watermark key images. Each user will be assigned with a unique fingerprint image so that the user's identity can be authenticated later. Experimental results demonstrate that, the proposed method can protect the copyright of DNN models effectively. On Fashion-MNIST and CIFAR-10 datasets, the proposed method can obtain 100% watermark accuracy and 100% fingerprint authentication success rate. In addition, the proposed method is demonstrated to be robust to the model fine-tuning attack, model pruning attack, and the query modification attack. Compared with three existing watermarking methods (the logo-based, noise-based, and adversarial frontier stitching watermarking methods), the proposed method has better performance on watermark accuracy and robustness against the query modification attack.",Protecting the Intellectual Properties of Deep Neural Networks with an Additional Class and Steganographic Images,"Shichang Sun, Mingfu Xue, Jian Wang, Weiqiang Liu",2022,Artificial Intelligence,2104.09203
"Recent machine-learning approaches to deterministic search and domain-independent planning employ policy learning to speed up search. Unfortunately, when attempting to solve a search problem by successively applying a policy, no guarantees can be given on solution quality. The problem of how to effectively use a learned policy within a bounded-suboptimal search algorithm remains largely as an open question. In this paper, we propose various ways in which such policies can be integrated into Focal Search, assuming that the policy is a neural network classifier. Furthermore, we provide mathematical foundations for some of the resulting algorithms. To evaluate the resulting algorithms over a number of policies with varying accuracy, we use synthetic policies which can be generated for a target accuracy for problems where the search space can be held in memory. We evaluate our focal search variants over three benchmark domains using our synthetic approach, and on the 15-puzzle using a neural network learned using 1.5 million examples. We observe that Discrepancy Focal Search, which we show expands the node which maximizes an approximation of the probability that its corresponding path is a prefix of an optimal path, obtains, in general, the best results in terms of runtime and solution quality.",Exploiting Learned Policies in Focal Search,"Pablo Araneda, Matias Greco, Jorge A. Baier",2021,Artificial Intelligence,2104.10535
"As of 2020, the international workshop on Procedural Content Generation enters its second decade. The annual workshop, hosted by the international conference on the Foundations of Digital Games, has collected a corpus of 95 papers published in its first 10 years. This paper provides an overview of the workshop's activities and surveys the prevalent research topics emerging over the years.",10 Years of the PCG workshop: Past and Future Trends,Antonios Liapis,2020,Artificial Intelligence,2104.11037
"Objects are a centerpiece of the mathematical realm and our interaction with and reasoning about it, just as they are of the physical one (if not more). And humans' mathematical reasoning must ultimately be grounded in our general intelligence. Yet in contemporary cognitive science and A.I., the physical and mathematical domains are customarily explored separately, which allows for baking in assumptions for what objects are for the system - and missing potential connections. In this paper, I put the issue into its philosophical and cognitive context. I then describe an abstract theoretical framework for learning object representations, that makes room for mathematical objects on par with non-mathematical ones. Finally, I describe a case study that builds on that view to show how our general ability for integrating different aspects of objects effects our conception of the natural numbers.",The Role of General Intelligence in Mathematical Reasoning,Aviv Keren,2021,Artificial Intelligence,2104.13468
"Recently, it has been proposed that fruitful synergies may exist between Deep Learning (DL) and Case Based Reasoning (CBR); that there are insights to be gained by applying CBR ideas to problems in DL (what could be called DeepCBR). In this paper, we report on a program of research that applies CBR solutions to the problem of Explainable AI (XAI) in the DL. We describe a series of twin-systems pairings of opaque DL models with transparent CBR models that allow the latter to explain the former using factual, counterfactual and semi-factual explanation strategies. This twinning shows that functional abstractions of DL (e.g., feature weights, feature importance and decision boundaries) can be used to drive these explanatory solutions. We also raise the prospect that this research also applies to the problem of Data Augmentation in DL, underscoring the fecundity of these DeepCBR ideas.",Twin Systems for DeepCBR: A Menagerie of Deep Learning and Case-Based Reasoning Pairings for Explanation and Data Augmentation,Mark T Keane and Eoin M Kenny and Mohammed Temraz and Derek Greene and Barry Smyth,2021,Artificial Intelligence,2104.14461
"AI has the potential to revolutionize many areas of healthcare. Radiology, dermatology, and ophthalmology are some of the areas most likely to be impacted in the near future, and they have received significant attention from the broader research community. But AI techniques are now also starting to be used in in vitro fertilization (IVF), in particular for selecting which embryos to transfer to the woman. The contribution of AI to IVF is potentially significant, but must be done carefully and transparently, as the ethical issues are significant, in part because this field involves creating new people. We first give a brief introduction to IVF and review the use of AI for embryo selection. We discuss concerns with the interpretation of the reported results from scientific and practical perspectives. We then consider the broader ethical issues involved. We discuss in detail the problems that result from the use of black-box methods in this context and advocate strongly for the use of interpretable models. Importantly, there have been no published trials of clinical effectiveness, a problem in both the AI and IVF communities, and we therefore argue that clinical implementation at this point would be premature. Finally, we discuss ways for the broader AI community to become involved to ensure scientifically sound and ethically responsible development of AI in IVF.",Ethical Implementation of Artificial Intelligence to Select Embryos in In Vitro Fertilization,"Michael Anis Mihdi Afnan, Cynthia Rudin, Vincent Conitzer, Julian Savulescu, Abhishek Mishra, Yanhe Liu, Masoud Afnan",2021,Artificial Intelligence,2105.00060
"By successfully solving the problem of forecasting, the processes in the work of various companies are optimized and savings are achieved. In this process, the analysis of time series data is of particular importance. Since the creation of Facebook's Prophet, and Amazon's DeepAR+ and CNN-QR forecasting models, algorithms have attracted a great deal of attention. The paper presents the application and comparison of the above algorithms for sales forecasting in distribution companies. A detailed comparison of the performance of algorithms over real data with different lengths of sales history was made. The results show that Prophet gives better results for items with a longer history and frequent sales, while Amazon's algorithms show superiority for items without a long history and items that are rarely sold.","Comparison Analysis of Facebook's Prophet, Amazon's DeepAR+ and CNN-QR Algorithms for Successful Real-World Sales Forecasting","Emir Zunic, Kemal Korjenic, Sead Delalic, Zlatko Subara",2021,Artificial Intelligence,2105.00694
"We present Arianna+, a framework to design networks of ontologies for representing knowledge enabling smart homes to perform human activity recognition online. In the network, nodes are ontologies allowing for various data contextualisation, while edges are general-purpose computational procedures elaborating data. Arianna+ provides a flexible interface between the inputs and outputs of procedures and statements, which are atomic representations of ontological knowledge. Arianna+ schedules procedures on the basis of events by employing logic-based reasoning, i.e., by checking the classification of certain statements in the ontologies. Each procedure involves input and output statements that are differently contextualised in the ontologies based on specific prior knowledge. Arianna+ allows to design networks that encode data within multiple contexts and, as a reference scenario, we present a modular network based on a spatial context shared among all activities and a temporal context specialised for each activity to be recognised. In the paper, we argue that a network of small ontologies is more intelligible and has a reduced computational load than a single ontology encoding the same knowledge. Arianna+ integrates in the same architecture heterogeneous data processing techniques, which may be better suited to different contexts. Thus, we do not propose a new algorithmic approach to activity recognition, instead, we focus on the architectural aspects for accommodating logic-based and data-driven activity models in a context-oriented way. Also, we discuss how to leverage data contextualisation and reasoning for activity recognition, and to support an iterative development process driven by domain experts.",Human Activity Recognition Models in Ontology Networks,"Luca Buoncompagni, Syed Yusha Kareem and Fulvio Mastrogiovanni",2021,Artificial Intelligence,2105.02264
"Learning disentangled representations of textual data is essential for many natural language tasks such as fair classification, style transfer and sentence generation, among others. The existent dominant approaches in the context of text data {either rely} on training an adversary (discriminator) that aims at making attribute values difficult to be inferred from the latent code {or rely on minimising variational bounds of the mutual information between latent code and the value attribute}. {However, the available methods suffer of the impossibility to provide a fine-grained control of the degree (or force) of disentanglement.} {In contrast to} {adversarial methods}, which are remarkably simple, although the adversary seems to be performing perfectly well during the training phase, after it is completed a fair amount of information about the undesired attribute still remains. This paper introduces a novel variational upper bound to the mutual information between an attribute and the latent code of an encoder. Our bound aims at controlling the approximation error via the Renyi's divergence, leading to both better disentangled representations and in particular, a precise control of the desirable degree of disentanglement {than state-of-the-art methods proposed for textual data}. Furthermore, it does not suffer from the degeneracy of other losses in multi-class scenarios. We show the superiority of this method on fair classification and on textual style transfer tasks. Additionally, we provide new insights illustrating various trade-offs in style transfer when attempting to learn disentangled representations and quality of the generated sentence.",A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations,Pierre Colombo and Chloe Clavel and Pablo Piantanida,2021,Artificial Intelligence,2105.02685
"The manpower scheduling problem is a critical research field in the resource management area. Based on the existing studies on scheduling problem solutions, this paper transforms the manpower scheduling problem into a combinational optimization problem under multi-constraint conditions from a new perspective. It also uses logical paradigms to build a mathematical model for problem solution and an improved multi-dimensional evolution algorithm for solving the model. Moreover, the constraints discussed in this paper basically cover all the requirements of human resource coordination in modern society and are supported by our experiment results. In the discussion part, we compare our model with other heuristic algorithms or linear programming methods and prove that the model proposed in this paper makes a 25.7% increase in efficiency and a 17% increase in accuracy at most. In addition, to the numerical solution of the manpower scheduling problem, this paper also studies the algorithm for scheduling task list generation and the method of displaying scheduling results. As a result, we not only provide various modifications for the basic algorithm to solve different condition problems but also propose a new algorithm that increases at least 28.91% in time efficiency by comparing with different baseline models.",An Intelligent Model for Solving Manpower Scheduling Problems,Lingyu Zhang and Tianyu Liu and Yunhai Wang,2021,Artificial Intelligence,2105.03540
"We build on abduction-based explanations for ma-chine learning and develop a method for computing local explanations for neural network models in natural language processing (NLP). Our explanations comprise a subset of the words of the in-put text that satisfies two key features: optimality w.r.t. a user-defined cost function, such as the length of explanation, and robustness, in that they ensure prediction invariance for any bounded perturbation in the embedding space of the left out words. We present two solution algorithms, respectively based on implicit hitting sets and maximum universal subsets, introducing a number of algorithmic improvements to speed up convergence of hard instances. We show how our method can be con-figured with different perturbation sets in the em-bedded space and used to detect bias in predictions by enforcing include/exclude constraints on biased terms, as well as to enhance existing heuristic-based NLP explanation frameworks such as Anchors. We evaluate our framework on three widely used sentiment analysis tasks and texts of up to100words from SST, Twitter and IMDB datasets,demonstrating the effectiveness of the derived explanations.",On Guaranteed Optimal Robust Explanations for NLP Models,"Emanuele La Malfa, Agnieszka Zbrzezny, Rhiannon Michelmore, Nicola Paoletti and Marta Kwiatkowska",2021,Artificial Intelligence,2105.03640
"Privacy protection has recently been in the spotlight of attention to both academia and industry. Society protects individual data privacy through complex legal frameworks. The increasing number of applications of data science and artificial intelligence has resulted in a higher demand for the ubiquitous application of the data. The privacy protection of the broad Data-Information-Knowledge-Wisdom (DIKW) landscape, the next generation of information organization, has taken a secondary role. In this paper, we will explore DIKW architecture through the applications of the popular swarm intelligence and differential privacy. As differential privacy proved to be an effective data privacy approach, we will look at it from a DIKW domain perspective. Swarm Intelligence can effectively optimize and reduce the number of items in DIKW used in differential privacy, thus accelerating both the effectiveness and the efficiency of differential privacy for crossing multiple modals of conceptual DIKW. The proposed approach is demonstrated through the application of personalized data that is based on the open-sourse IRIS dataset. This experiment demonstrates the efficiency of Swarm Intelligence in reducing computing complexity.",Swarm Differential Privacy for Purpose Driven Data-Information-Knowledge-Wisdom Architecture,"Yingbo Li, Yucong Duan, Zakaria Maama, Haoyang Che, Anamaria-Beatrice Spulber, Stelios Fuentes",2021,Artificial Intelligence,2105.04045
"The recent offline reinforcement learning (RL) studies have achieved much progress to make RL usable in real-world systems by learning policies from pre-collected datasets without environment interaction. Unfortunately, existing offline RL methods still face many practical challenges in real-world system control tasks, such as computational restriction during agent training and the requirement of extra control flexibility. The model-based planning framework provides an attractive alternative. However, most model-based planning algorithms are not designed for offline settings. Simply combining the ingredients of offline RL with existing methods either provides over-restrictive planning or leads to inferior performance. We propose a new light-weighted model-based offline planning framework, namely MOPP, which tackles the dilemma between the restrictions of offline learning and high-performance planning. MOPP encourages more aggressive trajectory rollout guided by the behavior policy learned from data, and prunes out problematic trajectories to avoid potential out-of-distribution samples. Experimental results show that MOPP provides competitive performance compared with existing model-based offline planning and RL approaches.",Model-Based Offline Planning with Trajectory Pruning,"Xianyuan Zhan, Xiangyu Zhu, Haoran Xu",2022,Artificial Intelligence,2105.07351
"Order effects occur when judgments about a hypothesis's probability given a sequence of information do not equal the probability of the same hypothesis when the information is reversed. Different experiments have been performed in the literature that supports evidence of order effects. We proposed a Bayesian update model for order effects where each question can be thought of as a mini-experiment where the respondents reflect on their beliefs. We showed that order effects appear, and they have a simple cognitive explanation: the respondent's prior belief that two questions are correlated. The proposed Bayesian model allows us to make several predictions: (1) we found certain conditions on the priors that limit the existence of order effects; (2) we show that, for our model, the QQ equality is not necessarily satisfied (due to symmetry assumptions); and (3) the proposed Bayesian model has the advantage of possessing fewer parameters than its quantum counterpart.",Order Effects in Bayesian Updates,Catarina Moreira and Jose Acacio de Barros,2021,Artificial Intelligence,2105.07354
"Artificial Intelligence techniques powered by deep neural nets have achieved much success in several application domains, most significantly and notably in the Computer Vision applications and Natural Language Processing tasks. Surpassing human-level performance propelled the research in the applications where different modalities amongst language, vision, sensory, text play an important role in accurate predictions and identification. Several multimodal fusion methods employing deep learning models are proposed in the literature. Despite their outstanding performance, the complex, opaque and black-box nature of the deep neural nets limits their social acceptance and usability. This has given rise to the quest for model interpretability and explainability, more so in the complex tasks involving multimodal AI methods. This paper extensively reviews the present literature to present a comprehensive survey and commentary on the explainability in multimodal deep neural nets, especially for the vision and language tasks. Several topics on multimodal AI and its applications for generic domains have been covered in this paper, including the significance, datasets, fundamental building blocks of the methods and techniques, challenges, applications, and future trends in this domain",A Review on Explainability in Multimodal Deep Neural Nets,"Gargi Joshi, Rahee Walambe, Ketan Kotecha",2021,Artificial Intelligence,2105.07878
"In the last few years, AI continues demonstrating its positive impact on society while sometimes with ethically questionable consequences. Building and maintaining public trust in AI has been identified as the key to successful and sustainable innovation. This chapter discusses the challenges related to operationalizing ethical AI principles and presents an integrated view that covers high-level ethical AI principles, the general notion of trust/trustworthiness, and product/process support in the context of responsible AI, which helps improve both trust and trustworthiness of AI for a wider set of stakeholders.",AI and Ethics -- Operationalising Responsible AI,"Liming Zhu, Xiwei Xu, Qinghua Lu, Guido Governatori, Jon Whittle",2021,Artificial Intelligence,2105.08867
"Traffic simulators act as an essential component in the operating and planning of transportation systems. Conventional traffic simulators usually employ a calibrated physical car-following model to describe vehicles' behaviors and their interactions with traffic environment. However, there is no universal physical model that can accurately predict the pattern of vehicle's behaviors in different situations. A fixed physical model tends to be less effective in a complicated environment given the non-stationary nature of traffic dynamics. In this paper, we formulate traffic simulation as an inverse reinforcement learning problem, and propose a parameter sharing adversarial inverse reinforcement learning model for dynamics-robust simulation learning. Our proposed model is able to imitate a vehicle's trajectories in the real world while simultaneously recovering the reward function that reveals the vehicle's true objective which is invariant to different dynamics. Extensive experiments on synthetic and real-world datasets show the superior performance of our approach compared to state-of-the-art methods and its robustness to variant dynamics of traffic.",Objective-aware Traffic Simulation via Inverse Reinforcement Learning,"Guanjie Zheng, Hanyang Liu, Kai Xu, Zhenhui Li",2021,Artificial Intelligence,2105.09560
"A key challenge on the path to developing agents that learn complex human-like behavior is the need to quickly and accurately quantify human-likeness. While human assessments of such behavior can be highly accurate, speed and scalability are limited. We address these limitations through a novel automated Navigation Turing Test (ANTT) that learns to predict human judgments of human-likeness. We demonstrate the effectiveness of our automated NTT on a navigation task in a complex 3D environment. We investigate six classification models to shed light on the types of architectures best suited to this task, and validate them against data collected through a human NTT. Our best models achieve high accuracy when distinguishing true human and agent behavior. At the same time, we show that predicting finer-grained human assessment of agents' progress towards human-like behavior remains unsolved. Our work takes an important step towards agents that more effectively learn complex human-like behavior.",Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation,"Sam Devlin, Raluca Georgescu, Ida Momennejad, Jaroslaw Rzepecki, Evelyn Zuniga, Gavin Costello, Guy Leroy, Ali Shaw and Katja Hofmann",2021,Artificial Intelligence,2105.09637
"Drafting, i.e., the selection of a subset of items from a larger candidate set, is a key element of many games and related problems. It encompasses team formation in sports or e-sports, as well as deck selection in many modern card games. The key difficulty of drafting is that it is typically not sufficient to simply evaluate each item in a vacuum and to select the best items. The evaluation of an item depends on the context of the set of items that were already selected earlier, as the value of a set is not just the sum of the values of its members - it must include a notion of how well items go together. In this paper, we study drafting in the context of the card game Magic: The Gathering. We propose the use of a contextual preference network, which learns to compare two possible extensions of a given deck of cards. We demonstrate that the resulting network is better able to evaluate card decks in this game than previous attempts.",Predicting Human Card Selection in Magic: The Gathering with Contextual Preference Ranking,"Timo Bertram, Johannes F\""urnkranz, Martin M\""uller",2021,Artificial Intelligence,2105.11864
"This paper discusses a new variant of the Henry Gas Solubility Optimization (HGSO) Algorithm, called Hybrid HGSO (HHGSO). Unlike its predecessor, HHGSO allows multiple clusters serving different individual meta-heuristic algorithms (i.e., with its own defined parameters and local best) to coexist within the same population. Exploiting the dynamic cluster-to-algorithm mapping via penalized and reward model with adaptive switching factor, HHGSO offers a novel approach for meta-heuristic hybridization consisting of Jaya Algorithm, Sooty Tern Optimization Algorithm, Butterfly Optimization Algorithm, and Owl Search Algorithm, respectively. The acquired results from the selected two case studies (i.e., involving team formation problem and combinatorial test suite generation) indicate that the hybridization has notably improved the performance of HGSO and gives superior performance against other competing meta-heuristic and hyper-heuristic algorithms.",Hybrid Henry Gas Solubility Optimization Algorithm with Dynamic Cluster-to-Algorithm Mapping for Search-based Software Engineering Problems,"Kamal Z. Zamli, Md. Abdul Kader, Saiful Azad, Bestoun S. Ahmed",2021,Artificial Intelligence,2105.14923
"Motivated by the abundance of uncertain event data from multiple sources including physical devices and sensors, this paper presents the task of relating a stochastic process observation to a process model that can be rendered from a dataset. In contrast to previous research that suggested to transform a stochastically known event log into a less informative uncertain log with upper and lower bounds on activity frequencies, we consider the challenge of accommodating the probabilistic knowledge into conformance checking techniques. Based on a taxonomy that captures the spectrum of conformance checking cases under stochastic process observations, we present three types of challenging cases. The first includes conformance checking of a stochastically known log with respect to a given process model. The second case extends the first to classify a stochastically known log into one of several process models. The third case extends the two previous ones into settings in which process models are only stochastically known. The suggested problem captures the increasingly growing number of applications in which sensors provide probabilistic process information.",Uncertain Process Data with Probabilistic Knowledge: Problem Characterization and Challenges,Izack Cohen and Avigdor Gal,2021,Artificial Intelligence,2106.03324
"Biological agents have meaningful interactions with their environment despite the absence of immediate reward signals. In such instances, the agent can learn preferred modes of behaviour that lead to predictable states -- necessary for survival. In this paper, we pursue the notion that this learnt behaviour can be a consequence of reward-free preference learning that ensures an appropriate trade-off between exploration and preference satisfaction. For this, we introduce a model-based Bayesian agent equipped with a preference learning mechanism (pepper) using conjugate priors. These conjugate priors are used to augment the expected free energy planner for learning preferences over states (or outcomes) across time. Importantly, our approach enables the agent to learn preferences that encourage adaptive behaviour at test time. We illustrate this in the OpenAI Gym FrozenLake and the 3D mini-world environments -- with and without volatility. Given a constant environment, these agents learn confident (i.e., precise) preferences and act to satisfy them. Conversely, in a volatile setting, perpetual preference uncertainty maintains exploratory behaviour. Our experiments suggest that learnable (reward-free) preferences entail a trade-off between exploration and preference satisfaction. Pepper offers a straightforward framework suitable for designing adaptive agents when reward functions cannot be predefined as in real environments.",Exploration and preference satisfaction trade-off in reward-free learning,"Noor Sajid, Panagiotis Tigas, Alexey Zakharov, Zafeirios Fountas and Karl Friston",2021,Artificial Intelligence,2106.04316
"We consider a problem wherein jobs arrive at random times and assume random values. Upon each job arrival, the decision-maker must decide immediately whether or not to accept the job and gain the value on offer as a reward, with the constraint that they may only accept at most $n$ jobs over some reference time period. The decision-maker only has access to $M$ independent realisations of the job arrival process. We propose an algorithm, Non-Parametric Sequential Allocation (NPSA), for solving this problem. Moreover, we prove that the expected reward returned by the NPSA algorithm converges in probability to optimality as $M$ grows large. We demonstrate the effectiveness of the algorithm empirically on synthetic data and on public fraud-detection datasets, from where the motivation for this work is derived.",Non-Parametric Stochastic Sequential Assignment With Random Arrival Times,"Danial Dervovic, Parisa Hassanzadeh, Samuel Assefa, Prashant Reddy",2021,Artificial Intelligence,2106.04944
"Planning is hard. The use of subgoals can make planning more tractable, but selecting these subgoals is computationally costly. What algorithms might enable us to reap the benefits of planning using subgoals while minimizing the computational overhead of selecting them? We propose visual scoping, a strategy that interleaves planning and acting by alternately defining a spatial region as the next subgoal and selecting actions to achieve it. We evaluated our visual scoping algorithm on a variety of physical assembly problems against two baselines: planning all subgoals in advance and planning without subgoals. We found that visual scoping achieves comparable task performance to the subgoal planner while requiring only a fraction of the total computational cost. Together, these results contribute to our understanding of how humans might make efficient use of cognitive resources to solve complex planning problems.",Visual scoping operations for physical assembly,"Felix J Binder, Marcelo M Mattar, David Kirsh, Judith E Fan",2021,Artificial Intelligence,2106.05654
"Explainable artificial intelligence has rapidly emerged since lawmakers have started requiring interpretable models for safety-critical domains. Concept-based neural networks have arisen as explainable-by-design methods as they leverage human-understandable symbols (i.e. concepts) to predict class memberships. However, most of these approaches focus on the identification of the most relevant concepts but do not provide concise, formal explanations of how such concepts are leveraged by the classifier to make predictions. In this paper, we propose a novel end-to-end differentiable approach enabling the extraction of logic explanations from neural networks using the formalism of First-Order Logic. The method relies on an entropy-based criterion which automatically identifies the most relevant concepts. We consider four different case studies to demonstrate that: (i) this entropy-based criterion enables the distillation of concise logic explanations in safety-critical domains from clinical data to computer vision; (ii) the proposed approach outperforms state-of-the-art white-box models in terms of classification accuracy and matches black box performances.",Entropy-based Logic Explanations of Neural Networks,"Pietro Barbiero, Gabriele Ciravegna, Francesco Giannini, Pietro Li\'o, Marco Gori, Stefano Melacci",2022,Artificial Intelligence,2106.06804
"Language is an interface to the outside world. In order for embodied agents to use it, language must be grounded in other, sensorimotor modalities. While there is an extended literature studying how machines can learn grounded language, the topic of how to learn spatio-temporal linguistic concepts is still largely uncharted. To make progress in this direction, we here introduce a novel spatio-temporal language grounding task where the goal is to learn the meaning of spatio-temporal descriptions of behavioral traces of an embodied agent. This is achieved by training a truth function that predicts if a description matches a given history of observations. The descriptions involve time-extended predicates in past and present tense as well as spatio-temporal references to objects in the scene. To study the role of architectural biases in this task, we train several models including multimodal Transformer architectures; the latter implement different attention computations between words and objects across space and time. We test models on two classes of generalization: 1) generalization to randomly held-out sentences; 2) generalization to grammar primitives. We observe that maintaining object identity in the attention computation of our Transformers is instrumental to achieving good performance on generalization overall, and that summarizing object traces in a single token has little influence on performance. We then discuss how this opens new perspectives for language-guided autonomous embodied agents. We also release our code under open-source license as well as pretrained models and datasets to encourage the wider community to build upon and extend our work in the future.",Grounding Spatio-Temporal Language with Transformers,"Tristan Karch, Laetitia Teodorescu, Katja Hofmann, Cl\'ement Moulin-Frier and Pierre-Yves Oudeyer",2021,Artificial Intelligence,2106.08858
"One of the main challenges in model-based reinforcement learning (RL) is to decide which aspects of the environment should be modeled. The value-equivalence (VE) principle proposes a simple answer to this question: a model should capture the aspects of the environment that are relevant for value-based planning. Technically, VE distinguishes models based on a set of policies and a set of functions: a model is said to be VE to the environment if the Bellman operators it induces for the policies yield the correct result when applied to the functions. As the number of policies and functions increase, the set of VE models shrinks, eventually collapsing to a single point corresponding to a perfect model. A fundamental question underlying the VE principle is thus how to select the smallest sets of policies and functions that are sufficient for planning. In this paper we take an important step towards answering this question. We start by generalizing the concept of VE to order-$k$ counterparts defined with respect to $k$ applications of the Bellman operator. This leads to a family of VE classes that increase in size as $k \rightarrow \infty$. In the limit, all functions become value functions, and we have a special instantiation of VE which we call proper VE or simply PVE. Unlike VE, the PVE class may contain multiple models even in the limit when all value functions are used. Crucially, all these models are sufficient for planning, meaning that they will yield an optimal policy despite the fact that they may ignore many aspects of the environment. We construct a loss function for learning PVE models and argue that popular algorithms such as MuZero can be understood as minimizing an upper bound for this loss. We leverage this connection to propose a modification to MuZero and show that it can lead to improved performance in practice.",Proper Value Equivalence,"Christopher Grimm, Andr\'e Barreto, Gregory Farquhar, David Silver, Satinder Singh",2021,Artificial Intelligence,2106.10316
"Path planning, the problem of efficiently discovering high-reward trajectories, often requires optimizing a high-dimensional and multimodal reward function. Popular approaches like CEM and CMA-ES greedily focus on promising regions of the search space and may get trapped in local maxima. DOO and VOOT balance exploration and exploitation, but use space partitioning strategies independent of the reward function to be optimized. Recently, LaMCTS empirically learns to partition the search space in a reward-sensitive manner for black-box optimization. In this paper, we develop a novel formal regret analysis for when and why such an adaptive region partitioning scheme works. We also propose a new path planning method LaP3 which improves the function value estimation within each sub-region, and uses a latent representation of the search space. Empirically, LaP3 outperforms existing path planning methods in 2D navigation tasks, especially in the presence of difficult-to-escape local optima, and shows benefits when plugged into the planning components of model-based RL such as PETS. These gains transfer to highly multimodal real-world tasks, where we outperform strong baselines in compiler phase ordering by up to 39% on average across 9 tasks, and in molecular design by up to 0.4 on properties on a 0-1 scale. Code is available at https://github.com/yangkevin2/neurips2021-lap3.",Learning Space Partitions for Path Planning,"Kevin Yang, Tianjun Zhang, Chris Cummins, Brandon Cui, Benoit Steiner, Linnan Wang, Joseph E. Gonzalez, Dan Klein, Yuandong Tian",2021,Artificial Intelligence,2106.10544
"In Silico Clinical Trials (ISTC), i.e., clinical experimental campaigns carried out by means of computer simulations, hold the promise to decrease time and cost for the safety and efficacy assessment of pharmacological treatments, reduce the need for animal and human testing, and enable precision medicine. In this paper we present methods and an algorithm that, by means of extensive computer simulation--based experimental campaigns (ISTC) guided by intelligent search, optimise a pharmacological treatment for an individual patient (precision medicine). e show the effectiveness of our approach on a case study involving a real pharmacological treatment, namely the downregulation phase of a complex clinical protocol for assisted reproduction in humans.",Optimal personalised treatment computation through in silico clinical trials on patient digital twins,"Stefano Sinisi, Vadim Alimguzhin, Toni Mancini, Enrico Tronci, Federico Mari, Brigitte Leeners",2020,Artificial Intelligence,2106.10684
"In critical infrastructures like airports, much care has to be devoted in protecting radio communication networks from external electromagnetic interference. Protection of such mission-critical radio communication networks is usually tackled by exploiting radiogoniometers: at least three suitably deployed radiogoniometers, and a gateway gathering information from them, permit to monitor and localise sources of electromagnetic emissions that are not supposed to be present in the monitored area. Typically, radiogoniometers are connected to the gateway through relay nodes. As a result, some degree of fault-tolerance for the network of relay nodes is essential in order to offer a reliable monitoring. On the other hand, deployment of relay nodes is typically quite expensive. As a result, we have two conflicting requirements: minimise costs while guaranteeing a given fault-tolerance. In this paper, we address the problem of computing a deployment for relay nodes that minimises the relay node network cost while at the same time guaranteeing proper working of the network even when some of the relay nodes (up to a given maximum number) become faulty (fault-tolerance). We show that, by means of a computation-intensive pre-processing on a HPC infrastructure, the above optimisation problem can be encoded as a 0/1 Linear Program, becoming suitable to be approached with standard Artificial Intelligence reasoners like MILP, PB-SAT, and SMT/OMT solvers. Our problem formulation enables us to present experimental results comparing the performance of these three solving technologies on a real case study of a relay node network deployment in areas of the Leonardo da Vinci Airport in Rome, Italy.","MILP, pseudo-boolean, and OMT solvers for optimal fault-tolerant placements of relay nodes in mission critical wireless networks","Quian Matteo Chen, Alberto Finzi, Toni Mancini, Igor Melatti, Enrico Tronci",2020,Artificial Intelligence,2106.10685
"Wikidata has been increasingly adopted by many communities for a wide variety of applications, which demand high-quality knowledge to deliver successful results. In this paper, we develop a framework to detect and analyze low-quality statements in Wikidata by shedding light on the current practices exercised by the community. We explore three indicators of data quality in Wikidata, based on: 1) community consensus on the currently recorded knowledge, assuming that statements that have been removed and not added back are implicitly agreed to be of low quality; 2) statements that have been deprecated; and 3) constraint violations in the data. We combine these indicators to detect low-quality statements, revealing challenges with duplicate entities, missing triples, violated type rules, and taxonomic distinctions. Our findings complement ongoing efforts by the Wikidata community to improve data quality, aiming to make it easier for users and editors to find and correct mistakes.",A Study of the Quality of Wikidata,Kartik Shenoy and Filip Ilievski and Daniel Garijo and Daniel Schwabe and Pedro Szekely,2021,Artificial Intelligence,2107.00156
"This paper applies t-SNE, a visualisation technique familiar from Deep Neural Network research to argumentation graphs by applying it to the output of graph embeddings generated using several different methods. It shows that such a visualisation approach can work for argumentation and show interesting structural properties of argumentation graphs, opening up paths for further research in the area.",Visualising Argumentation Graphs with Graph Embeddings and t-SNE,"Lars Malmqvist, Tommy Yuan, Suresh Manandhar",2020,Artificial Intelligence,2107.00528
"In this paper, we study the problem of evaluating the addition of elements to a set. This problem is difficult, because it can, in the general case, not be reduced to unconditional preferences between the choices. Therefore, we model preferences based on the context of the decision. We discuss and compare two different Siamese network architectures for this task: a twin network that compares the two sets resulting after the addition, and a triplet network that models the contribution of each candidate to the existing set. We evaluate the two settings on a real-world task; learning human card preferences for deck building in the collectible card game Magic: The Gathering. We show that the triplet approach achieves a better result than the twin network and that both outperform previous results on this task.",A Comparison of Contextual and Non-Contextual Preference Ranking for Set Addition Problems,"Timo Bertram, Johannes F\""urnkranz, Martin M\""uller",2021,Artificial Intelligence,2107.04438
"A total of 34% of AI research and development projects fails or are abandoned, according to a recent survey by Rackspace Technology of 1,870 companies. We propose a new strategic framework, aiSTROM, that empowers managers to create a successful AI strategy based on a thorough literature review. This provides a unique and integrated approach that guides managers and lead developers through the various challenges in the implementation process. In the aiSTROM framework, we start by identifying the top n potential projects (typically 3-5). For each of those, seven areas of focus are thoroughly analysed. These areas include creating a data strategy that takes into account unique cross-departmental machine learning data requirements, security, and legal requirements. aiSTROM then guides managers to think about how to put together an interdisciplinary artificial intelligence (AI) implementation team given the scarcity of AI talent. Once an AI team strategy has been established, it needs to be positioned within the organization, either cross-departmental or as a separate division. Other considerations include AI as a service (AIaas), or outsourcing development. Looking at new technologies, we have to consider challenges such as bias, legality of black-box-models, and keeping humans in the loop. Next, like any project, we need value-based key performance indicators (KPIs) to track and validate the progress. Depending on the company's risk-strategy, a SWOT analysis (strengths, weaknesses, opportunities, and threats) can help further classify the shortlisted projects. Finally, we should make sure that our strategy includes continuous education of employees to enable a culture of adoption. This unique and comprehensive framework offers a valuable, literature supported, tool for managers and lead developers.",aiSTROM -- A roadmap for developing a successful AI strategy,Dorien Herremans,2021,Artificial Intelligence,2107.06071
"Federated learning (FL) is a distributed model for deep learning that integrates client-server architecture, edge computing, and real-time intelligence. FL has the capability of revolutionizing machine learning (ML) but lacks in the practicality of implementation due to technological limitations, communication overhead, non-IID (independent and identically distributed) data, and privacy concerns. Training a ML model over heterogeneous non-IID data highly degrades the convergence rate and performance. The existing traditional and clustered FL algorithms exhibit two main limitations, including inefficient client training and static hyper-parameter utilization. To overcome these limitations, we propose a novel hybrid algorithm, namely genetic clustered FL (Genetic CFL), that clusters edge devices based on the training hyper-parameters and genetically modifies the parameters cluster-wise. Then, we introduce an algorithm that drastically increases the individual cluster accuracy by integrating the density-based clustering and genetic hyper-parameter optimization. The results are bench-marked using MNIST handwritten digit dataset and the CIFAR-10 dataset. The proposed genetic CFL shows significant improvements and works well with realistic cases of non-IID and ambiguous data.",Genetic CFL: Optimization of Hyper-Parameters in Clustered Federated Learning,"Shaashwat Agrawal, Sagnik Sarkar, Mamoun Alazab, Praveen Kumar Reddy Maddikunta, Thippa Reddy Gadekallu and Quoc-Viet Pham",2021,Artificial Intelligence,2107.07233
"We present a storytelling robot, controlled via the ACT-R cognitive architecture, able to adopt different persuasive techniques and ethical stances while conversing about some topics concerning COVID-19. The main contribution of the paper consists in the proposal of a needs-driven model that guides and evaluates, during the dialogue, the use (if any) of persuasive techniques available in the agent procedural memory. The portfolio of persuasive techniques tested in such a model ranges from the use of storytelling, to framing techniques and rhetorical-based arguments. To the best of our knowledge, this represents the first attempt of building a persuasive agent able to integrate a mix of explicitly grounded cognitive assumptions about dialogue management, storytelling and persuasive techniques as well as ethical attitudes. The paper presents the results of an exploratory evaluation of the system on 63 participants",A Storytelling Robot managing Persuasive and Ethical Stances via ACT-R: an Exploratory Study,"Agnese Augello, Giuseppe Citt\`a, Manuel Gentile, Antonio Lieto",2022,Artificial Intelligence,2107.12845
"In this paper we present a technique for procedurally generating 3D maps using a set of premade meshes which snap together based on designer-specified visual constraints. The proposed approach avoids size and layout limitations, offering the designer control over the look and feel of the generated maps, as well as immediate feedback on a given map's navigability. A prototype implementation of the method, developed in the Unity game engine, is discussed, and a number of case studies are analyzed. These include a multiplayer game where the method was used, together with a number of illustrative examples which highlight various parameterizations and piece selection methods. The technique can be used as a designer-centric map composition method and/or as a prototyping system in 3D level design, opening the door for quality map and level creation in a fraction of the time of a fully human-based approach.",Procedural Generation of 3D Maps with Snappable Meshes,"Rafael C. e Silva, Nuno Fachada, Diogo de Andrade, N\'elio C\'odices",2022,Artificial Intelligence,2108.00056
"Planning for Autonomous Unmanned Ground Vehicles (AUGV) is still a challenge, especially in difficult, off-road, critical situations. Automatic planning can be used to reach mission objectives, to perform navigation or maneuvers. Most of the time, the problem consists in finding a path from a source to a destination, while satisfying some operational constraints. In a graph without negative cycles, the computation of the single-pair shortest path from a start node to an end node is solved in polynomial time. Additional constraints on the solution path can however make the problem harder to solve. This becomes the case when we need the path to pass through a few mandatory nodes without requiring a specific order of visit. The complexity grows exponentially with the number of mandatory nodes to visit. In this paper, we focus on shortest path search with mandatory nodes on a given connected graph. We propose a hybrid model that combines a constraint-based solver and a graph convolutional neural network to improve search performance. Promising results are obtained on realistic scenarios.",Constrained Shortest Path Search with Graph Convolutional Neural Networks,"Kevin Osanlou, Christophe Guettier, Andrei Bursuc, Tristan Cazenave, Eric Jacopin",2018,Artificial Intelligence,2108.00978
"Uncertain information is being taken into account in an increasing number of application fields. In the meantime, abduction has been proved a powerful tool for handling hypothetical reasoning and incomplete knowledge. Probabilistic logical models are a suitable framework to handle uncertain information, and in the last decade many probabilistic logical languages have been proposed, as well as inference and learning systems for them. In the realm of Abductive Logic Programming (ALP), a variety of proof procedures have been defined as well. In this paper, we consider a richer logic language, coping with probabilistic abduction with variables. In particular, we consider an ALP program enriched with integrity constraints `a la IFF, possibly annotated with a probability value. We first present the overall abductive language, and its semantics according to the Distribution Semantics. We then introduce a proof procedure, obtained by extending one previously presented, and prove its soundness and completeness.",Nonground Abductive Logic Programming with Probabilistic Integrity Constraints,"Elena Bellodi, Marco Gavanelli, Riccardo Zese, Evelina Lamma, Fabrizio Riguzzi",2021,Artificial Intelligence,2108.03033
"Dealing with context dependent knowledge has led to different formalizations of the notion of context. Among them is the Contextualized Knowledge Repository (CKR) framework, which is rooted in description logics but links on the reasoning side strongly to logic programs and Answer Set Programming (ASP) in particular. The CKR framework caters for reasoning with defeasible axioms and exceptions in contexts, which was extended to knowledge inheritance across contexts in a coverage (specificity) hierarchy. However, the approach supports only this single type of contextual relation and the reasoning procedures work only for restricted hierarchies, due to non-trivial issues with model preference under exceptions. In this paper, we overcome these limitations and present a generalization of CKR hierarchies to multiple contextual relations, along with their interpretation of defeasible axioms and preference. To support reasoning, we use ASP with algebraic measures, which is a recent extension of ASP with weighted formulas over semirings that allows one to associate quantities with interpretations depending on the truth values of propositional atoms. Notably, we show that for a relevant fragment of CKR hierarchies with multiple contextual relations, query answering can be realized with the popular asprin framework. The algebraic measures approach is more powerful and enables e.g. reasoning with epistemic queries over CKRs, which opens interesting perspectives for the use of quantitative ASP extensions in other applications.",Reasoning on Multi-Relational Contextual Hierarchies via Answer Set Programming with Algebraic Measures,"Loris Bozzato, Thomas Eiter, Rafael Kiesel",2021,Artificial Intelligence,2108.03100
"The traditional production paradigm of large batch production does not offer flexibility towards satisfying the requirements of individual customers. A new generation of smart factories is expected to support new multi-variety and small-batch customized production modes. For that, Artificial Intelligence (AI) is enabling higher value-added manufacturing by accelerating the integration of manufacturing and information communication technologies, including computing, communication, and control. The characteristics of a customized smart factory are to include self-perception, operations optimization, dynamic reconfiguration, and intelligent decision-making. The AI technologies will allow manufacturing systems to perceive the environment, adapt to the external needs, and extract the process knowledge, including business models, such as intelligent production, networked collaboration, and extended service models. This paper focuses on the implementation of AI in customized manufacturing (CM). The architecture of an AI-driven customized smart factory is presented. Details of intelligent manufacturing devices, intelligent information interaction, and construction of a flexible manufacturing line are showcased. The state-of-the-art AI technologies of potential use in CM, i.e., machine learning, multi-agent systems, Internet of Things, big data, and cloud-edge computing are surveyed. The AI-enabled technologies in a customized smart factory are validated with a case study of customized packaging. The experimental results have demonstrated that the AI-assisted CM offers the possibility of higher production flexibility and efficiency. Challenges and solutions related to AI in CM are also discussed.","Artificial Intelligence-Driven Customized Manufacturing Factory: Key Technologies, Applications, and Challenges","Jiafu Wan, Xiaomin Li, Hong-Ning Dai, Andrew Kusiak, Miguel Mart\'inez-Garc\'ia, Di Li",2021,Artificial Intelligence,2108.03383
"Several studies have reported the inability of Transformer models to generalize compositionally, a key type of generalization in many NLP tasks such as semantic parsing. In this paper we explore the design space of Transformer models showing that the inductive biases given to the model by several design decisions significantly impact compositional generalization. Through this exploration, we identified Transformer configurations that generalize compositionally significantly better than previously reported in the literature in a diverse set of compositional tasks, and that achieve state-of-the-art results in a semantic parsing compositional generalization benchmark (COGS), and a string edit operation composition benchmark (PCFG).",Making Transformers Solve Compositional Tasks,"Santiago Onta\~n\'on, Joshua Ainslie, Vaclav Cvicek and Zachary Fisher",2022,Artificial Intelligence,2108.04378
"With the increasing demands of personalized learning, knowledge tracing has become important which traces students' knowledge states based on their historical practices. Factor analysis methods mainly use two kinds of factors which are separately related to students and questions to model students' knowledge states. These methods use the total number of attempts of students to model students' learning progress and hardly highlight the impact of the most recent relevant practices. Besides, current factor analysis methods ignore rich information contained in questions. In this paper, we propose Multi-Factors Aware Dual-Attentional model (MF-DAKT) which enriches question representations and utilizes multiple factors to model students' learning progress based on a dual-attentional mechanism. More specifically, we propose a novel student-related factor which records the most recent attempts on relevant concepts of students to highlight the impact of recent exercises. To enrich questions representations, we use a pre-training method to incorporate two kinds of question information including questions' relation and difficulty level. We also add a regularization term about questions' difficulty level to restrict pre-trained question representations to fine-tuning during the process of predicting students' performance. Moreover, we apply a dual-attentional mechanism to differentiate contributions of factors and factor interactions to final prediction in different practice records. At last, we conduct experiments on several real-world datasets and results show that MF-DAKT can outperform existing knowledge tracing methods. We also conduct several studies to validate the effects of each component of MF-DAKT.",Multi-Factors Aware Dual-Attentional Knowledge Tracing,"Moyu Zhang (1), Xinning Zhu (1), Chunhong Zhang (1), Yang Ji (1), Feng Pan (1), Changchuan Yin (1) ((1) Beijing University of Posts and Telecommunications)",2021,Artificial Intelligence,2108.04741
"Recent systems applying Machine Learning (ML) to solve the Traveling Salesman Problem (TSP) exhibit issues when they try to scale up to real case scenarios with several hundred vertices. The use of Candidate Lists (CLs) has been brought up to cope with the issues. The procedure allows to restrict the search space during solution creation, consequently reducing the solver computational burden. So far, ML were engaged to create CLs and values on the edges of these CLs expressing ML preferences at solution insertion. Although promising, these systems do not clearly restrict what the ML learns and does to create solutions, bringing with them some generalization issues. Therefore, motivated by exploratory and statistical studies, in this work we instead use a machine learning model to confirm the addition in the solution just for high probable edges. CLs of the high probable edge are employed as input, and the ML is in charge of distinguishing cases where such edges are in the optimal solution from those where they are not. . This strategy enables a better generalization and creates an efficient balance between machine learning and searching techniques. Our ML-Constructive heuristic is trained on small instances. Then, it is able to produce solutions, without losing quality, to large problems as well. We compare our results with classic constructive heuristics, showing good performances for TSPLIB instances up to 1748 cities. Although our heuristic exhibits an expensive constant time operation, we proved that the computational complexity in worst-case scenario, for the solution construction after training, is $O(n^2 \log n^2)$, being $n$ the number of vertices in the TSP instance.",A New Constructive Heuristic driven by Machine Learning for the Traveling Salesman Problem,"Umberto Junior Mele, Luca Maria Gambardella and Roberto Montemanni",2021,Artificial Intelligence,2108.10224
"Agent-based systems have the capability to fuse information from many distributed sources and create better plans faster. This feature makes agent-based systems naturally suitable to address the challenges in Supply Chain Management (SCM). Although agent-based supply chains systems have been proposed since early 2000; industrial uptake of them has been lagging. The reasons quoted include the immaturity of the technology, a lack of interoperability with supply chain information systems, and a lack of trust in Artificial Intelligence (AI). In this paper, we revisit the agent-based supply chain and review the state of the art. We find that agent-based technology has matured, and other supporting technologies that are penetrating supply chains; are filling in gaps, leaving the concept applicable to a wider range of functions. For example, the ubiquity of IoT technology helps agents ""sense"" the state of affairs in a supply chain and opens up new possibilities for automation. Digital ledgers help securely transfer data between third parties, making agent-based information sharing possible, without the need to integrate Enterprise Resource Planning (ERP) systems. Learning functionality in agents enables agents to move beyond automation and towards autonomy. We note this convergence effect through conceptualising an agent-based supply chain framework, reviewing its components, and highlighting research challenges that need to be addressed in moving forward.",Will bots take over the supply chain? Revisiting Agent-based supply chain automation,"Liming Xu, Stephen Mak and Alexandra Brintrup",2021,Artificial Intelligence,2109.01703
"Recent breakthroughs in AI have shown the remarkable power of deep learning and deep reinforcement learning. These developments, however, have been tied to specific tasks, and progress in out-of-distribution generalization has been limited. While it is assumed that these limitations can be overcome by incorporating suitable inductive biases, the notion of inductive biases itself is often left vague and does not provide meaningful guidance. In the paper, I articulate a different learning approach where representations do not emerge from biases in a neural architecture but are learned over a given target language with a known semantics. The basic ideas are implicit in mainstream AI where representations have been encoded in languages ranging from fragments of first-order logic to probabilistic structural causal models. The challenge is to learn from data the representations that have traditionally been crafted by hand. Generalization is then a result of the semantics of the language. The goals of this paper are to make these ideas explicit, to place them in a broader context where the design of the target language is crucial, and to illustrate them in the context of learning to act and plan. For this, after a general discussion, I consider learning representations of actions, general policies, and subgoals (""intrinsic rewards""). In these cases, learning is formulated as a combinatorial problem but nothing prevents the use of deep learning techniques instead. Indeed, learning representations over languages with a known semantics provides an account of what is to be learned, while learning representations with neural nets provides a complementary account of how representations can be learned. The challenge and the opportunity is to bring the two together.",Target Languages (vs. Inductive Biases) for Learning to Act and Plan,Hector Geffner,2022,Artificial Intelligence,2109.07195
"We present an historical overview about the connections between the analysis of risk and the control of autonomous systems. We offer two main contributions. Our first contribution is to propose three overlapping paradigms to classify the vast body of literature: the worst-case, risk-neutral, and risk-averse paradigms. We consider an appropriate assessment for the risk of an autonomous system to depend on the application at hand. In contrast, it is typical to assess risk using an expectation, variance, or probability alone. Our second contribution is to unify the concepts of risk and autonomous systems. We achieve this by connecting approaches for quantifying and optimizing the risk that arises from a system's behaviour across academic fields. The survey is highly multidisciplinary. We include research from the communities of reinforcement learning, stochastic and robust control theory, operations research, and formal verification. We describe both model-based and model-free methods, with emphasis on the former. Lastly, we highlight fruitful areas for further research. A key direction is to blend risk-averse model-based and model-free methods to enhance the real-time adaptive capabilities of systems to improve human and environmental welfare.",Risk-averse autonomous systems: A brief history and recent developments from the perspective of optimal control,Yuheng Wang and Margaret P. Chapman,2022,Artificial Intelligence,2109.08947
"Recently, a boxology (graphical language) with design patterns for hybrid AI was proposed, combining symbolic and sub-symbolic learning and reasoning. In this paper, we extend this boxology with actors and their interactions. The main contributions of this paper are: 1) an extension of the taxonomy to describe distributed hybrid AI systems with actors and interactions; and 2) showing examples using a few design patterns relevant in multi-agent systems and human-agent interaction.",Modular Design Patterns for Hybrid Actors,"Andr\'e Meyer-Vitali, Wico Mulder, Maaike H.T. de Boer",2021,Artificial Intelligence,2109.09331
"While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer it to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first dataset for this task, and we propose a revised loss function that enables the PLM to learn how to predict precise probabilities for the task. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the fine-tuned model, yielding state-of-the-art results on external datasets.",RuleBert: Teaching Soft Rules to Pre-trained Language Models,"Mohammed Saeed, Naser Ahmadi, Preslav Nakov, Paolo Papotti",2021,Artificial Intelligence,2109.13006
"On playing video games, different players usually have their own playstyles. Recently, there have been great improvements for the video game AIs on the playing strength. However, past researches for analyzing the behaviors of players still used heuristic rules or the behavior features with the game-environment support, thus being exhausted for the developers to define the features of discriminating various playstyles. In this paper, we propose the first metric for video game playstyles directly from the game observations and actions, without any prior specification on the playstyle in the target game. Our proposed method is built upon a novel scheme of learning discrete representations that can map game observations into latent discrete states, such that playstyles can be exhibited from these discrete states. Namely, we measure the playstyle distance based on game observations aligned to the same states. We demonstrate high playstyle accuracy of our metric in experiments on some video game platforms, including TORCS, RGSK, and seven Atari games, and for different agents including rule-based AI bots, learning-based AI bots, and human players.",An Unsupervised Video Game Playstyle Metric via State Discretization,"Chiu-Chou Lin, Wei-Chen Chiu and I-Chen Wu",2021,Artificial Intelligence,2110.00950
"In Mixed Integer Linear Programming (MIP), a (strong) backdoor is a ""small"" subset of an instance's integer variables with the following property: in a branch-and-bound procedure, the instance can be solved to global optimality by branching only on the variables in the backdoor. Constructing datasets of pre-computed backdoors for widely used MIP benchmark sets or particular problem families can enable new questions around novel structural properties of a MIP, or explain why a problem that is hard in theory can be solved efficiently in practice. Existing algorithms for finding backdoors rely on sampling candidate variable subsets in various ways, an approach which has demonstrated the existence of backdoors for some instances from MIPLIB2003 and MIPLIB2010. However, these algorithms fall short of consistently succeeding at the task due to an imbalance between exploration and exploitation. We propose BaMCTS, a Monte Carlo Tree Search framework for finding backdoors to MIPs. Extensive algorithmic engineering, hybridization with traditional MIP concepts, and close integration with the CPLEX solver have enabled our method to outperform baselines on MIPLIB2017 instances, finding backdoors more frequently and more efficiently.",Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework,"Elias B. Khalil, Pashootan Vaezipoor, Bistra Dilkina",2022,Artificial Intelligence,2110.08423
"Fact-checking on the Web has become the main mechanism through which we detect the credibility of the news or information. Existing fact-checkers verify the authenticity of the information (support or refute the claim) based on secondary sources of information. However, existing approaches do not consider the problem of model updates due to constantly increasing training data due to user feedback. It is therefore important to conduct user studies to correct models' inference biases and improve the model in a life-long learning manner in the future according to the user feedback. In this paper, we present FaxPlainAC, a tool that gathers user feedback on the output of explainable fact-checking models. FaxPlainAC outputs both the model decision, i.e., whether the input fact is true or not, along with the supporting/refuting evidence considered by the model. Additionally, FaxPlainAC allows for accepting user feedback both on the prediction and explanation. Developed in Python, FaxPlainAC is designed as a modular and easily deployable tool. It can be integrated with other downstream tasks and allowing for fact-checking human annotation gathering and life-long learning.",FaxPlainAC: A Fact-Checking Tool Based on EXPLAINable Models with HumAn Correction in the Loop,"Zijian Zhang, Koustav Rudra, Avishek Anand",2021,Artificial Intelligence,2110.10144
"Gray-box graph attacks aim at disrupting the performance of the victim model by using inconspicuous attacks with limited knowledge of the victim model. The parameters of the victim model and the labels of the test nodes are invisible to the attacker. To obtain the gradient on the node attributes or graph structure, the attacker constructs an imaginary surrogate model trained under supervision. However, there is a lack of discussion on the training of surrogate models and the robustness of provided gradient information. The general node classification model loses the topology of the nodes on the graph, which is, in fact, an exploitable prior for the attacker. This paper investigates the effect of representation learning of surrogate models on the transferability of gray-box graph adversarial attacks. To reserve the topology in the surrogate embedding, we propose Surrogate Representation Learning with Isometric Mapping (SRLIM). By using Isometric mapping method, our proposed SRLIM can constrain the topological structure of nodes from the input layer to the embedding space, that is, to maintain the similarity of nodes in the propagation process. Experiments prove the effectiveness of our approach through the improvement in the performance of the adversarial attacks generated by the gradient-based attacker in untargeted poisoning gray-box setups.",Surrogate Representation Learning with Isometric Mapping for Gray-box Graph Adversarial Attacks,"Zihan Liu, Yun Luo, Zelin Zang, Stan Z. Li",2022,Artificial Intelligence,2110.10482
"We derive nearly tight and non-asymptotic convergence bounds for solutions of entropic semi-discrete optimal transport. These bounds quantify the stability of the dual solutions of the regularized problem (sometimes called Sinkhorn potentials) w.r.t. the regularization parameter, for which we ensure a better than Lipschitz dependence. Such facts may be a first step towards a mathematical justification of annealing or $\varepsilon$-scaling heuristics for the numerical resolution of regularized semi-discrete optimal transport. Our results also entail a non-asymptotic and tight expansion of the difference between the entropic and the unregularized costs.",Nearly Tight Convergence Bounds for Semi-discrete Entropic Optimal Transport,"Alex Delalande (LMO, DATASHAPE)",2022,Artificial Intelligence,2110.12678
"This two-part comprehensive survey is devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Notable models in the HDC/VSA family are Tensor Product Representations, Holographic Reduced Representations, Multiply-Add-Permute, Binary Spatter Codes, and Sparse Binary Distributed Representations but there are other models too. HDC/VSA is a highly interdisciplinary area with connections to computer science, electrical engineering, artificial intelligence, mathematics, and cognitive science. This fact makes it challenging to create a thorough overview of the area. However, due to a surge of new researchers joining the area in recent years, the necessity for a comprehensive survey of the area has become extremely important. Therefore, amongst other aspects of the area, this Part I surveys important aspects such as: known computational models of HDC/VSA and transformations of various input data types to high-dimensional distributed representations. Part II of this survey is devoted to applications, cognitive computing and architectures, as well as directions for future work. The survey is written to be useful for both newcomers and practitioners.","A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part I: Models and Data Transformations","Denis Kleyko, Dmitri A. Rachkovskij, Evgeny Osipov, Abbas Rahimi",2022,Artificial Intelligence,2111.06077
"We introduce the partially observable history process (POHP) formalism for reinforcement learning. POHP centers around the actions and observations of a single agent and abstracts away the presence of other players without reducing them to stochastic processes. Our formalism provides a streamlined interface for designing algorithms that defy categorization as exclusively single or multi-agent, and for developing theory that applies across these domains. We show how the POHP formalism unifies traditional models including the Markov decision process, the Markov game, the extensive-form game, and their partially observable extensions, without introducing burdensome technical machinery or violating the philosophical underpinnings of reinforcement learning. We illustrate the utility of our formalism by concisely exploring observable sequential rationality, examining some theoretical properties of general immediate regret minimization, and generalizing the extensive-form regret minimization (EFR) algorithm.",The Partially Observable History Process,"Dustin Morrill, Amy R. Greenwald, Michael Bowling",2022,Artificial Intelligence,2111.08102
"False alerts due to misconfigured/ compromised IDS in ICS networks can lead to severe economic and operational damage. To solve this problem, research has focused on leveraging deep learning techniques that help reduce false alerts. However, a shortcoming is that these works often require or implicitly assume the physical and cyber sensors to be trustworthy. Implicit trust of data is a major problem with using artificial intelligence or machine learning for CPS security, because during critical attack detection time they are more at risk, with greater likelihood and impact, of also being compromised. To address this shortcoming, the problem is reframed on how to make good decisions given uncertainty. Then, the decision is detection, and the uncertainty includes whether the data used for ML-based IDS is compromised. Thus, this work presents an approach for reducing false alerts in CPS power systems by dealing uncertainty without the knowledge of prior distribution of alerts. Specifically, an evidence theoretic based approach leveraging Dempster Shafer combination rules are proposed for reducing false alerts. A multi-hypothesis mass function model is designed that leverages probability scores obtained from various supervised-learning classifiers. Using this model, a location-cum-domain based fusion framework is proposed and evaluated with different combination rules, that fuse multiple evidence from inter-domain and intra-domain sensors. The approach is demonstrated in a cyber-physical power system testbed with Man-In-The-Middle attack emulation in a large-scale synthetic electric grid. For evaluating the performance, plausibility, belief, pignistic, etc. metrics as decision functions are considered. To improve the performance, a multi-objective based genetic algorithm is proposed for feature selection considering the decision metrics as the fitness function.",Inter-Domain Fusion for Enhanced Intrusion Detection in Power Systems: An Evidence Theoretic and Meta-Heuristic Approach,Abhijeet Sahu and Katherine Davis,2022,Artificial Intelligence,2111.10484
"PDDL+ is an extension of PDDL2.1 which incorporates fully-featured autonomous processes and allows for better modelling of mixed discrete-continuous domains. Unlike PDDL2.1, PDDL+ lacks a logical semantics, relying instead on state-transitional semantics enriched with hybrid automata semantics for the continuous states. This complex semantics makes analysis and comparisons to other action formalisms difficult. In this paper, we propose a natural extension of Reiter's situation calculus theories inspired by hybrid automata. The kinship between PDDL+ and hybrid automata allows us to develop a direct mapping between PDDL+ and situation calculus, thereby supplying PDDL+ with a logical semantics and the situation calculus with a modern way of representing autonomous processes. We outline the potential benefits of the mapping by suggesting a new approach to effective planning in PDDL+.",A Logical Semantics for PDDL+,"Vitaliy Batusov, Mikhail Soutchanski",2019,Artificial Intelligence,2111.11588
"Wikidata is the largest general-interest knowledge base that is openly available. It is collaboratively edited by thousands of volunteer editors and has thus evolved considerably since its inception in 2012. In this paper, we present Wikidated 1.0, a dataset of Wikidata's full revision history, which encodes changes between Wikidata revisions as sets of deletions and additions of RDF triples. To the best of our knowledge, it constitutes the first large dataset of an evolving knowledge graph, a recently emerging research subject in the Semantic Web community. We introduce the methodology for generating Wikidated 1.0 from dumps of Wikidata, discuss its implementation and limitations, and present statistical characteristics of the dataset.",Wikidated 1.0: An Evolving Knowledge Graph Dataset of Wikidata's Revision History,"Lukas Schmelzeisen, Corina Dima, Steffen Staab",2021,Artificial Intelligence,2112.05003
"Agent-based models have emerged as a promising paradigm for addressing ever increasing complexity of information systems. In its initial days in the 1990s when object-oriented modeling was at its peak, an agent was treated as a special kind of ""object"" that had a persistent state and its own independent thread of execution. Since then, agent-based models have diversified enormously to even open new conceptual insights about the nature of systems in general. This paper presents a perspective on the disparate ways in which our understanding of agency, as well as computational models of agency have evolved. Advances in hardware like GPUs, that brought neural networks back to life, may also similarly infuse new life into agent-based models, as well as pave the way for advancements in research on Artificial General Intelligence (AGI).",Paradigms of Computational Agency,Srinath Srinivasa and Jayati Deshmukh,2020,Artificial Intelligence,2112.05575
"Metaheuristic and self-organizing criticality (SOC) could contribute to robust computation under perturbed environments. Implementing a logic gate in a computing system in a critical state is one of the intriguing ways to study the role of metaheuristics and SOCs. Here, we study the behavior of cellular automaton, game of life (GL), in asynchronous updating and implement probabilistic logic gates by using asynchronous GL. We find that asynchronous GL shows a phase transition, that the density of the state of 1 decays with the power law at the critical point, and that systems at the critical point have the most computability in asynchronous GL. We implement AND and OR gates in asynchronous GL with criticality, which shows good performance. Since tuning perturbations play an essential role in operating logic gates, our study reveals the interference between manipulation and perturbation in probabilistic logic gates.",Probabilistic Logic Gate in Asynchronous Game of Life with Critical Property,"Yukio-Pegio Gunji, Yoshihiko Ohzawa and Terutaka Tanaka",2021,Artificial Intelligence,2112.07846
We propose neural-symbolic integration for abstract concept explanation and interactive learning. Neural-symbolic integration and explanation allow users and domain-experts to learn about the data-driven decision making process of large neural models. The models are queried using a symbolic logic language. Interaction with the user then confirms or rejects a revision of the neural model using logic-based constraints that can be distilled into the model architecture. The approach is illustrated using the Logic Tensor Network framework alongside Concept Activation Vectors and applied to a Convolutional Neural Network.,Neural-Symbolic Integration for Interactive Learning and Conceptual Grounding,"Benedikt Wagner, Artur d'Avila Garcez",2021,Artificial Intelligence,2112.11805
"Automated Deduction in Geometry (ADG) is a forum to exchange ideas and views, to present research results and progress, and to demonstrate software tools at the intersection between geometry and automated deduction. Relevant topics include (but are not limited to): polynomial algebra, invariant and coordinate-free methods; probabilistic, synthetic, and logic approaches, techniques for automated geometric reasoning from discrete mathematics, combinatorics, and numerics; interactive theorem proving in geometry; symbolic and numeric methods for geometric computation, geometric constraint solving, automated generation/reasoning and manipulation with diagrams; design and implementation of geometry software, automated theorem provers, special-purpose tools, experimental studies; applications of ADG in mechanics, geometric modelling, CAGD/CAD, computer vision, robotics and education. Traditionally, the ADG conference is held every two years. The previous editions of ADG were held in Nanning in 2018, Strasbourg in 2016, Coimbra in 2014, Edinburgh in 2012, Munich in 2010, Shanghai in 2008, Pontevedra in 2006, Gainesville in 2004, Hagenberg in 2002, Zurich in 2000, Beijing in 1998, and Toulouse in 1996. The 13th edition of ADG was supposed to be held in 2020 in Hagenberg, Austria, but due to the COVID-19 pandemic, it was postponed for 2021, and held online (still hosted by RISC Institute, Hagenberg, Austria), September 15-17, 2021 (https://www.risc.jku.at/conferences/adg2021).",Proceedings of the 13th International Conference on Automated Deduction in Geometry,"Predrag Jani\v{c}i\'c, Zolt\'an Kov\'acs",2021,Artificial Intelligence,2112.14770
"Online reinforcement learning (RL) algorithms are often difficult to deploy in complex human-facing applications as they may learn slowly and have poor early performance. To address this, we introduce a practical algorithm for incorporating human insight to speed learning. Our algorithm, Constraint Sampling Reinforcement Learning (CSRL), incorporates prior domain knowledge as constraints/restrictions on the RL policy. It takes in multiple potential policy constraints to maintain robustness to misspecification of individual constraints while leveraging helpful ones to learn quickly. Given a base RL learning algorithm (ex. UCRL, DQN, Rainbow) we propose an upper confidence with elimination scheme that leverages the relationship between the constraints, and their observed performance, to adaptively switch among them. We instantiate our algorithm with DQN-type algorithms and UCRL as base algorithms, and evaluate our algorithm in four environments, including three simulators based on real data: recommendations, educational activity sequencing, and HIV treatment sequencing. In all cases, CSRL learns a good policy faster than baselines.",Constraint Sampling Reinforcement Learning: Incorporating Expertise For Faster Learning,"Tong Mu, Georgios Theocharous, David Arbour, Emma Brunskill",2022,Artificial Intelligence,2112.15221
"This is Part II of the two-part comprehensive survey devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Holographic Reduced Representations is an influential HDC/VSA model that is well-known in the machine learning domain and often used to refer to the whole family. However, for the sake of consistency, we use HDC/VSA to refer to the area. Part I of this survey covered foundational aspects of the area, such as historical context leading to the development of HDC/VSA, key elements of any HDC/VSA model, known HDC/VSA models, and transforming input data of various types into high-dimensional vectors suitable for HDC/VSA. This second part surveys existing applications, the role of HDC/VSA in cognitive computing and architectures, as well as directions for future work. Most of the applications lie within the machine learning/artificial intelligence domain, however we also cover other applications to provide a thorough picture. The survey is written to be useful for both newcomers and practitioners.","A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part II: Applications, Cognitive Models, and Challenges","Denis Kleyko, Dmitri A. Rachkovskij, Evgeny Osipov, Abbas Rahimi",2022,Artificial Intelligence,2112.15424
"Humans use causality and hypothetical retrospection in their daily decision-making, planning, and understanding of life events. The human mind, while retrospecting a given situation, think about questions such as ""What was the cause of the given situation?"", ""What would be the effect of my action?"", or ""Which action led to this effect?"". It develops a causal model of the world, which learns with fewer data points, makes inferences, and contemplates counterfactual scenarios. The unseen, unknown, scenarios are known as counterfactuals. AI algorithms use a representation based on knowledge graphs (KG) to represent the concepts of time, space, and facts. A KG is a graphical data model which captures the semantic relationships between entities such as events, objects, or concepts. The existing KGs represent causal relationships extracted from texts based on linguistic patterns of noun phrases for causes and effects as in ConceptNet and WordNet. The current causality representation in KGs makes it challenging to support counterfactual reasoning. A richer representation of causality in AI systems using a KG-based approach is needed for better explainability, and support for intervention and counterfactuals reasoning, leading to improved understanding of AI systems by humans. The causality representation requires a higher representation framework to define the context, the causal information, and the causal effects. The proposed Causal Knowledge Graph (CausalKG) framework, leverages recent progress of causality and KG towards explainability. CausalKG intends to address the lack of a domain adaptable causal model and represent the complex causal relations using the hyper-relational graph representation in the KG. We show that the CausalKG's interventional and counterfactual reasoning can be used by the AI system for the domain explainability.",CausalKG: Causal Knowledge Graph Explainability using interventional and counterfactual reasoning,"Utkarshani Jaimini, Amit Sheth",2022,Artificial Intelligence,2201.03647
"This paper describes the evolution of our research from video analytics to a global security system with focus on the video surveillance component. Indeed video surveillance has evolved from a commodity security tool up to the most efficient way of tracking perpetrators when terrorism hits our modern urban centers. As number of cameras soars, one could expect the system to leverage the huge amount of data carried through the video streams to provide fast access to video evidences, actionable intelligence for monitoring real-time events and enabling predictive capacities to assist operators in their surveillance tasks. This research explores a hybrid platform for video intelligence capture, automated data extraction, supervised Machine Learning for intelligently assisted urban video surveillance; Extension to other components of a global security system are discussed. Applying Knowledge Management principles in this research helps with deep problem understanding and facilitates the implementation of efficient information and experience sharing decision support systems providing assistance to people on the field as well as in operations centers. The originality of this work is also the creation of ""common"" human-machine and machine to machine language and a security ontology.",Video Intelligence as a component of a Global Security system,"Dominique Verdejo, Eunika Mercier-Laurent (CRESTIC)",2019,Artificial Intelligence,2201.04349
"It is commonly acknowledged that the availability of the huge amount of (training) data is one of the most important factors for many recent advances in Artificial Intelligence (AI). However, datasets are often designed for specific tasks in narrow AI sub areas and there is no unified way to manage and access them. This not only creates unnecessary overheads when training or deploying Machine Learning models but also limits the understanding of the data, which is very important for data-centric AI. In this paper, we present our vision about a unified framework for different datasets so that they can be integrated and queried easily, e.g., using standard query languages. We demonstrate this in our ongoing work to create a framework for datasets in Computer Vision and show its advantages in different scenarios. Our demonstration is available at https://vision.semkg.org.",Fantastic Data and How to Query Them,"Trung-Kien Tran, Anh Le-Tuan, Manh Nguyen-Duc, Jicheng Yuan, Danh Le-Phuoc",2021,Artificial Intelligence,2201.05026
"Online advertising is a major source of income for many online companies. One common approach is to sell online advertisements via waterfall auctions, through which a publisher makes sequential price offers to ad networks. The publisher controls the order and prices of the waterfall in an attempt to maximize his revenue. In this work, we propose a methodology to learn a waterfall strategy from historical data by wisely searching in the space of possible waterfalls and selecting the one leading to the highest revenues. The contribution of this work is twofold; First, we propose a novel method to estimate the valuation distribution of each user, with respect to each ad network. Second, we utilize the valuation matrix to score our candidate waterfalls as part of a procedure that iteratively searches in local neighborhoods. Our framework guarantees that the waterfall revenue improves between iterations ultimately converging into a local optimum. Real-world demonstrations are provided to show that the proposed method improves the total revenue of real-world waterfalls, as compared to manual expert optimization. Finally, the code and the data are available here.",Search and Score-based Waterfall Auction Optimization,"Dan Halbersberg, Matan Halevi, Moshe Salhov",2022,Artificial Intelligence,2201.06409
"Due to extensive spread of fake news on social and news media it became an emerging research topic now a days that gained attention. In the news media and social media the information is spread highspeed but without accuracy and hence detection mechanism should be able to predict news fast enough to tackle the dissemination of fake news. It has the potential for negative impacts on individuals and society. Therefore, detecting fake news on social media is important and also a technically challenging problem these days. We knew that Machine learning is helpful for building Artificial intelligence systems based on tacit knowledge because it can help us to solve complex problems due to real word data. On the other side we knew that Knowledge engineering is helpful for representing experts knowledge which people aware of that knowledge. Due to this we proposed that integration of Machine learning and knowledge engineering can be helpful in detection of fake news. In this paper we present what is fake news, importance of fake news, overall impact of fake news on different areas, different ways to detect fake news on social media, existing detections algorithms that can help us to overcome the issue, similar application areas and at the end we proposed combination of data driven and engineered knowledge to combat fake news. We studied and compared three different modules text classifiers, stance detection applications and fact checking existing techniques that can help to detect fake news. Furthermore, we investigated the impact of fake news on society. Experimental evaluation of publically available datasets and our proposed fake news detection combination can serve better in detection of fake news.",Combining Machine Learning with Knowledge Engineering to detect Fake News in Social Networks-a survey,"Sajjad Ahmed, Knut Hinkelmann, Flavio Corradini",2019,Artificial Intelligence,2201.08032
"Stock Market can be easily seen as one of the most attractive places for investors, but it is also very complex in terms of making trading decisions. Predicting the market is a risky venture because of the uncertainties and nonlinear nature of the market. Deciding on the right time to trade is key to every successful trader as it can lead to either a huge gain of money or totally a loss in investment that will be recorded as a careless trade. The aim of this research is to develop a prediction system for stock market using Fuzzy Logic Type2 which will handle these uncertainties and complexities of human behaviour in general when it comes to buy, hold or sell decision making in stock trading. The proposed system was developed using VB.NET programming language as frontend and Microsoft SQL Server as backend. A total of four different technical indicators were selected for this research. The selected indicators are the Relative Strength Index, William Average, Moving Average Convergence and Divergence, and Stochastic Oscillator. These indicators serve as input variable to the Fuzzy System. The MACD and SO are deployed as primary indicators, while the RSI and WA are used as secondary indicators. Fibonacci retracement ratio was adopted for the secondary indicators to determine their support and resistance level in terms of making trading decisions. The input variables to the Fuzzy System is fuzzified to Low, Medium, and High using the Triangular and Gaussian Membership Function. The Mamdani Type Fuzzy Inference rules were used for combining the trading rules for each input variable to the fuzzy system. The developed system was tested using sample data collected from ten different companies listed on the Nigerian Stock Exchange for a total of fifty two periods. The dataset collected are Opening, High, Low, and Closing prices of each security.",Implementation of a Type-2 Fuzzy Logic Based Prediction System for the Nigerian Stock Exchange,"Isobo Nelson Davies, Donald Ene, Ibiere Boma Cookey, Godwin Fred Lenu",2022,Artificial Intelligence,2202.02107
"We compare four different `game-spaces' in terms of their usefulness in characterising multi-player tabletop games, with a particular interest in any underlying change to a game's characteristics as the number of players changes. In each case we take a 16-dimensional feature space, and reduce it to a 2-dimensional visualizable landscape. We find that a space obtained from optimization of parameters in Monte Carlo Tree Search (MCTS) is the most directly interpretable to characterise our set of games in terms of the relative importance of imperfect information, adversarial opponents and reward sparsity. These results do not correlate with a space defined using attributes of the game-tree. This dimensionality reduction does not show any general effect as the number of players. We therefore consider the question using the original features to classify the games into two sets; those for which the characteristics of the game changes significantly as the number of players changes, and those for which there is no such effect.",Visualising Multiplayer Game Spaces,"James Goodman, Diego Perez-Liebana, Simon Lucas",2021,Artificial Intelligence,2202.05773
"Development of new algorithms in the area of machine learning, especially clustering, comparative studies of such algorithms as well as testing according to software engineering principles requires availability of labeled data sets. While standard benchmarks are made available, a broader range of such data sets is necessary in order to avoid the problem of overfitting. In this context, theoretical works on axiomatization of clustering algorithms, especially axioms on clustering preserving transformations are quite a cheap way to produce labeled data sets from existing ones. However, the frequently cited axiomatic system of Kleinberg:2002, as we show in this paper, is not applicable for finite dimensional Euclidean spaces, in which many algorithms like $k$-means, operate. In particular, the so-called outer-consistency axiom fails upon making small changes in datapoint positions and inner-consistency axiom is valid only for identity transformation in general settings. Hence we propose an alternative axiomatic system, in which Kleinberg's inner consistency axiom is replaced by a centric consistency axiom and outer consistency axiom is replaced by motion consistency axiom. We demonstrate that the new system is satisfiable for a hierarchical version of $k$-means with auto-adjusted $k$, hence it is not contradictory. Additionally, as $k$-means creates convex clusters only, we demonstrate that it is possible to create a version detecting concave clusters and still the axiomatic system can be satisfied. The practical application area of such an axiomatic system may be the generation of new labeled test data from existent ones for clustering algorithm testing. %We propose the gravitational consistency as a replacement which does not have this deficiency.",Towards Continuous Consistency Axiom,Mieczyslaw A. Klopotek and Robert A. Klopotek,2022,Artificial Intelligence,2202.06015
"Edward C. Tolman found reinforcement learning unsatisfactory for explaining intelligence and proposed a clear distinction between learning and behavior. Tolman's ideas on latent learning and cognitive maps eventually led to what is now known as conceptual space, a geometric representation where concepts and ideas can form points or shapes.Active navigation between ideas - reasoning - can be expressed directly as purposive navigation in conceptual space. Assimilating the theory of conceptual space from modern neuroscience, we propose autonomous navigation as a valid approach for emulated cognition. However, achieving autonomous navigation in high-dimensional Euclidean spaces is not trivial in technology. In this work, we explore whether neoRL navigation is up for the task; adopting Kaelbling's concerns for efficient robot navigation, we test whether the neoRL approach is general across navigational modalities, compositional across considerations of experience, and effective when learning in multiple Euclidean dimensions. We find neoRL learning to be more resemblant of biological learning than of RL in AI, and propose neoRL navigation of conceptual space as a plausible new path toward emulated cognition.",Navigating Conceptual Space; A new take on Artificial General Intelligence,Per R. Leikanger,2021,Artificial Intelligence,2202.09646
"Knowledge graphs (KGs) are an important source repository for a wide range of applications and rule mining from KGs recently attracts wide research interest in the KG-related research community. Many solutions have been proposed for the rule mining from large-scale KGs, which however are limited in the inefficiency of rule generation and ineffectiveness of rule evaluation. To solve these problems, in this paper we propose a generation-then-evaluation rule mining approach guided by reinforcement learning. Specifically, a two-phased framework is designed. The first phase aims to train a reinforcement learning agent for rule generation from KGs, and the second is to utilize the value function of the agent to guide the step-by-step rule generation. We conduct extensive experiments on several datasets and the results prove that our rule mining solution achieves state-of-the-art performance in terms of efficiency and effectiveness.",Rule Mining over Knowledge Graphs via Reinforcement Learning,"Lihan Chen, Sihang Jiang, Jingping Liu, Chao Wang, Sheng Zhang, Chenhao Xie, Jiaqing Liang, Yanghua Xiao and Rui Song",2022,Artificial Intelligence,2202.10381
"Advocates for Neuro-Symbolic Artificial Intelligence (NeSy) assert that combining deep learning with symbolic reasoning will lead to stronger AI than either paradigm on its own. As successful as deep learning has been, it is generally accepted that even our best deep learning systems are not very good at abstract reasoning. And since reasoning is inextricably linked to language, it makes intuitive sense that Natural Language Processing (NLP), would be a particularly well-suited candidate for NeSy. We conduct a structured review of studies implementing NeSy for NLP, with the aim of answering the question of whether NeSy is indeed meeting its promises: reasoning, out-of-distribution generalization, interpretability, learning and reasoning from small data, and transferability to new domains. We examine the impact of knowledge representation, such as rules and semantic networks, language structure and relational structure, and whether implicit or explicit reasoning contributes to higher promise scores. We find that systems where logic is compiled into the neural network lead to the most NeSy goals being satisfied, while other factors such as knowledge representation, or type of neural architecture do not exhibit a clear correlation with goals being met. We find many discrepancies in how reasoning is defined, specifically in relation to human level reasoning, which impact decisions about model architectures and drive conclusions which are not always consistent across studies. Hence we advocate for a more methodical approach to the application of theories of human reasoning as well as the development of appropriate benchmarks, which we hope can lead to a better understanding of progress in the field. We make our data and code available on github for further analysis.",Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review,"Kyle Hamilton, Aparna Nayak, Bojan Bo\v{z}i\'c, Luca Longo",2022,Artificial Intelligence,2202.12205
"Language-guided Embodied AI benchmarks requiring an agent to navigate an environment and manipulate objects typically allow one-way communication: the human user gives a natural language command to the agent, and the agent can only follow the command passively. We present DialFRED, a dialogue-enabled embodied instruction following benchmark based on the ALFRED benchmark. DialFRED allows an agent to actively ask questions to the human user; the additional information in the user's response is used by the agent to better complete its task. We release a human-annotated dataset with 53K task-relevant questions and answers and an oracle to answer questions. To solve DialFRED, we propose a questioner-performer framework wherein the questioner is pre-trained with the human-annotated data and fine-tuned with reinforcement learning. We make DialFRED publicly available and encourage researchers to propose and evaluate their solutions to building dialog-enabled embodied agents.",DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following,"Xiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, Govind Thattai, Gaurav S. Sukhatme",2022,Artificial Intelligence,2202.13330
"Clinical Pathways (CP) are medical management plans developed to standardize patient treatment activities, optimize resource usage, reduce expenses, and improve the quality of healthcare services. Most CPs currently in use are paper-based documents (i.e., not computerized). CP computerization has been an active research topic since the inception of CP use in hospitals. This literature review research aims to examine studies that focused on CP computerization and offers recommendations for future research in this important research area. Some critical research suggestions include centralizing computerized CPs in Healthcare Information Systems (HIS), CP term standardization using international medical terminology systems, developing a global CP-specific digital coding system, creating a unified CP meta-ontology, developing independent Clinical Pathway Management Systems (CPMS), and supporting CPMSs with machine learning sub-systems.",Computerization of Clinical Pathways: A Literature Review and Directions for Future Research,Ayman Alahmar and Ola Alkhatib,2022,Artificial Intelligence,2203.00815
"The finetuning of pretrained transformer-based language generation models are typically conducted in an end-to-end manner, where the model learns to attend to relevant parts of the input by itself. However, there does not exist a mechanism to directly control the model's focus. This work aims to develop a control mechanism by which a user can select spans of context as ""highlights"" for the model to focus on, and generate relevant output. To achieve this goal, we augment a pretrained model with trainable ""focus vectors"" that are directly applied to the model's embeddings, while the model itself is kept fixed. These vectors, trained on automatic annotations derived from attribution methods, act as indicators for context importance. We test our approach on two core generation tasks: dialogue response generation and abstractive summarization. We also collect evaluation data where the highlight-generation pairs are annotated by humans. Our experiments show that the trained focus vectors are effective in steering the model to generate outputs that are relevant to user-selected highlights.",Controlling the Focus of Pretrained Language Generation Models,"Jiabao Ji, Yoon Kim, James Glass, Tianxing He",2022,Artificial Intelligence,2203.01146
"For communication to happen successfully, a common language is required between agents to understand information communicated by one another. Inducing the emergence of a common language has been a difficult challenge to multi-agent learning systems. In this work, we introduce an alternative perspective to the communicative messages sent between agents, considering them as different incomplete views of the environment state. Based on this perspective, we propose a simple approach to induce the emergence of a common language by maximizing the mutual information between messages of a given trajectory in a self-supervised manner. By evaluating our method in communication-essential environments, we empirically show how our method leads to better learning performance and speed, and learns a more consistent common language than existing methods, without introducing additional learning parameters.",Learning to Ground Decentralized Multi-Agent Communication with Contrastive Learning,Yat Long Lo and Biswa Sengupta,2022,Artificial Intelligence,2203.03344
"`gym-saturation` is an OpenAI Gym environment for reinforcement learning (RL) agents capable of proving theorems. Currently, only theorems written in a formal language of the Thousands of Problems for Theorem Provers (TPTP) library in clausal normal form (CNF) are supported. `gym-saturation` implements the 'given clause' algorithm (similar to the one used in Vampire and E Prover). Being written in Python, `gym-saturation` was inspired by PyRes. In contrast to the monolithic architecture of a typical Automated Theorem Prover (ATP), `gym-saturation` gives different agents opportunities to select clauses themselves and train from their experience. Combined with a particular agent, `gym-saturation` can work as an ATP. Even with a non trained agent based on heuristics, `gym-saturation` can find refutations for 688 (of 8257) CNF problems from TPTP v7.5.0.",Gym-saturation: an OpenAI Gym environment for saturation provers,Boris Shminke,2022,Artificial Intelligence,2203.04699
"This paper focuses on a dynamic aspect of responsible autonomy, namely, to make intelligent agents be responsible at run time. That is, it considers settings where decision making by agents impinges upon the outcomes perceived by other agents. For an agent to act responsibly, it must accommodate the desires and other attitudes of its users and, through other agents, of their users. The contribution of this paper is twofold. First, it provides a conceptual analysis of consent, its benefits and misuses, and how understanding consent can help achieve responsible autonomy. Second, it outlines challenges for AI (in particular, for agents and multiagent systems) that merit investigation to form as a basis for modeling consent in multiagent systems and applying consent to achieve responsible autonomy.",Consent as a Foundation for Responsible Autonomy,Munindar P. Singh,2022,Artificial Intelligence,2203.11420
"The symbolism, connectionism and behaviorism approaches of artificial intelligence have achieved a lot of successes in various tasks, while we still do not have a clear definition of ""intelligence"" with enough consensus in the community (although there are over 70 different ""versions"" of definitions). The nature of intelligence is still in darkness. In this work we do not take any of these three traditional approaches, instead we try to identify certain fundamental aspects of the nature of intelligence, and construct a mathematical model to represent and potentially reproduce these fundamental aspects. We first stress the importance of defining the scope of discussion and granularity of investigation. We carefully compare human and artificial intelligence, and qualitatively demonstrate an information abstraction process, which we propose to be the key to connect perception and cognition. We then present the broader idea of ""concept"", separate the idea of self model out of the world model, and construct a new model called world-self model (WSM). We show the mechanisms of creating and connecting concepts, and the flow of how the WSM receives, processes and outputs information with respect to an arbitrary type of problem to solve. We also consider and discuss the potential computer implementation issues of the proposed theoretical framework, and finally we propose a unified general framework of intelligence based on WSM.",A World-Self Model Towards Understanding Intelligence,Yutao Yue,2022,Artificial Intelligence,2203.13762
"Planning under uncertainty is an area of interest in artificial intelligence. We present a novel approach based on tree search and graph machine learning for the scheduling problem known as Disjunctive Temporal Networks with Uncertainty (DTNU). Dynamic Controllability (DC) of DTNUs seeks a reactive scheduling strategy to satisfy temporal constraints in response to uncontrollable action durations. We introduce new semantics for reactive scheduling: Time-based Dynamic Controllability (TDC) and a restricted subset of TDC, R-TDC. We design a tree search algorithm to determine whether or not a DTNU is R-TDC. Moreover, we leverage a graph neural network as a heuristic for tree search guidance. Finally, we conduct experiments on a known benchmark on which we show R-TDC to retain significant completeness with regard to DC, while being faster to prove. This results in the tree search processing fifty percent more DTNU problems in R-TDC than the state-of-the-art DC solver does in DC with the same time budget. We also observe that graph neural network search guidance leads to substantial performance gains on benchmarks of more complex DTNUs, with up to eleven times more problems solved than the baseline tree search.",Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability using Tree Search and Graph Neural Networks,"Kevin Osanlou, Jeremy Frank, Andrei Bursuc, Tristan Cazenave, Eric Jacopin, Christophe Guettier and J. Benton",2022,Artificial Intelligence,2203.15030
"Wind energy has emerged as a highly promising source of renewable energy in recent times. However, wind turbines regularly suffer from operational inconsistencies, leading to significant costs and challenges in operations and maintenance (O&M). Condition-based monitoring (CBM) and performance assessment/analysis of turbines are vital aspects for ensuring efficient O&M planning and cost minimisation. Data-driven decision making techniques have witnessed rapid evolution in the wind industry for such O&M tasks during the last decade, from applying signal processing methods in early 2010 to artificial intelligence (AI) techniques, especially deep learning in 2020. In this article, we utilise statistical computing to present a scientometric review of the conceptual and thematic evolution of AI in the wind energy sector, providing evidence-based insights into present strengths and limitations of data-driven decision making in the wind industry. We provide a perspective into the future and on current key challenges in data availability and quality, lack of transparency in black box-natured AI models, and prevailing issues in deploying models for real-time decision support, along with possible strategies to overcome these problems. We hope that a systematic analysis of the past, present and future of CBM and performance assessment can encourage more organisations to adopt data-driven decision making techniques in O&M towards making wind energy sources more reliable, contributing to the global efforts of tackling climate change.","Scientometric Review of Artificial Intelligence for Operations & Maintenance of Wind Turbines: The Past, Present and Future","Joyjit Chatterjee, Nina Dethlefs",2021,Artificial Intelligence,2204.02360
"Existing approaches to learning to prove theorems focus on particular logics and datasets. In this work, we propose Monte-Carlo simulations guided by reinforcement learning that can work in an arbitrarily specified logic, without any human knowledge or set of problems. Since the algorithm does not need any training dataset, it is able to learn to work with any logical foundation, even when there is no body of proofs or even conjectures available. We practically demonstrate the feasibility of the approach in multiple logical systems. The approach is stronger than training on randomly generated data but weaker than the approaches trained on tailored axiom and conjecture sets. It however allows us to apply machine learning to automated theorem proving for many logics, where no such attempts have been tried to date, such as intuitionistic logic or linear logic.",Adversarial Learning to Reason in an Arbitrary Logic,Stanis{\l}aw J. Purga{\l} and Cezary Kaliszyk,2022,Artificial Intelligence,2204.02737
"Accurate estimation of post-click conversion rate is critical for building recommender systems, which has long been confronted with sample selection bias and data sparsity issues. Methods in the Entire Space Multi-task Model (ESMM) family leverage the sequential pattern of user actions, i.e. $impression\rightarrow click \rightarrow conversion$ to address data sparsity issue. However, they still fail to ensure the unbiasedness of CVR estimates. In this paper, we theoretically demonstrate that ESMM suffers from the following two problems: (1) Inherent Estimation Bias (IEB), where the estimated CVR of ESMM is inherently higher than the ground truth; (2) Potential Independence Priority (PIP) for CTCVR estimation, where there is a risk that the ESMM overlooks the causality from click to conversion. To this end, we devise a principled approach named Entire Space Counterfactual Multi-task Modelling (ESCM$^2$), which employs a counterfactual risk miminizer as a regularizer in ESMM to address both IEB and PIP issues simultaneously. Extensive experiments on offline datasets and online environments demonstrate that our proposed ESCM$^2$ can largely mitigate the inherent IEB and PIP issues and achieve better performance than baseline models.",ESCM$^2$: Entire Space Counterfactual Multi-Task Model for Post-Click Conversion Rate Estimation,"Hao Wang, Tai-Wei Chang, Tianqiao Liu, Jianmin Huang, Zhichao Chen, Chao Yu, Ruopeng Li, Wei Chu",2022,Artificial Intelligence,2204.05125
"Creative Problem Solving (CPS) is a sub-area within Artificial Intelligence (AI) that focuses on methods for solving off-nominal, or anomalous problems in autonomous systems. Despite many advancements in planning and learning, resolving novel problems or adapting existing knowledge to a new context, especially in cases where the environment may change in unpredictable ways post deployment, remains a limiting factor in the safe and useful integration of intelligent systems. The emergence of increasingly autonomous systems dictates the necessity for AI agents to deal with environmental uncertainty through creativity. To stimulate further research in CPS, we present a definition and a framework of CPS, which we adopt to categorize existing AI methods in this field. Our framework consists of four main components of a CPS problem, namely, 1) problem formulation, 2) knowledge representation, 3) method of knowledge manipulation, and 4) method of evaluation. We conclude our survey with open research questions, and suggested directions for the future.",Creative Problem Solving in Artificially Intelligent Agents: A Survey and Framework,"Evana Gizzi, Lakshmi Nair, Sonia Chernova, Jivko Sinapov",2022,Artificial Intelligence,2204.10358
"Counterfactual (CF) explanations have been employed as one of the modes of explainability in explainable AI-both to increase the transparency of AI systems and to provide recourse. Cognitive science and psychology, however, have pointed out that people regularly use CFs to express causal relationships. Most AI systems are only able to capture associations or correlations in data so interpreting them as casual would not be justified. In this paper, we present two experiment (total N = 364) exploring the effects of CF explanations of AI system's predictions on lay people's causal beliefs about the real world. In Experiment 1 we found that providing CF explanations of an AI system's predictions does indeed (unjustifiably) affect people's causal beliefs regarding factors/features the AI uses and that people are more likely to view them as causal factors in the real world. Inspired by the literature on misinformation and health warning messaging, Experiment 2 tested whether we can correct for the unjustified change in causal beliefs. We found that pointing out that AI systems capture correlations and not necessarily causal relationships can attenuate the effects of CF explanations on people's causal beliefs.","Can counterfactual explanations of AI systems' predictions skew lay users' causal intuitions about the world? If so, can we correct for that?","Marko Tesic, Ulrike Hahn",2022,Artificial Intelligence,2205.06241
"In this paper, we revisit the solving bias when evaluating models on current Math Word Problem (MWP) benchmarks. However, current solvers exist solving bias which consists of data bias and learning bias due to biased dataset and improper training strategy. Our experiments verify MWP solvers are easy to be biased by the biased training datasets which do not cover diverse questions for each problem narrative of all MWPs, thus a solver can only learn shallow heuristics rather than deep semantics for understanding problems. Besides, an MWP can be naturally solved by multiple equivalent equations while current datasets take only one of the equivalent equations as ground truth, forcing the model to match the labeled ground truth and ignoring other equivalent equations. Here, we first introduce a novel MWP dataset named UnbiasedMWP which is constructed by varying the grounded expressions in our collected data and annotating them with corresponding multiple new questions manually. Then, to further mitigate learning bias, we propose a Dynamic Target Selection (DTS) Strategy to dynamically select more suitable target expressions according to the longest prefix match between the current model output and candidate equivalent equations which are obtained by applying commutative law during training. The results show that our UnbiasedMWP has significantly fewer biases than its original data and other datasets, posing a promising benchmark for fairly evaluating the solvers' reasoning skills rather than matching nearest neighbors. And the solvers trained with our DTS achieve higher accuracies on multiple MWP benchmarks. The source code is available at https://github.com/yangzhch6/UnbiasedMWP.",Unbiased Math Word Problems Benchmark for Mitigating Solving Bias,"Zhicheng Yang, Jinghui Qin, Jiaqi Chen, and Xiaodan Liang",2022,Artificial Intelligence,2205.08108
"Recently, deep learning models have made great progress in MWP solving on answer accuracy. However, they are uninterpretable since they mainly rely on shallow heuristics to achieve high performance without understanding and reasoning the grounded math logic. To address this issue and make a step towards interpretable MWP solving, we first construct a high-quality MWP dataset named InterMWP which consists of 11,495 MWPs and annotates interpretable logical formulas based on algebraic knowledge as the grounded linguistic logic of each solution equation. Different from existing MWP datasets, our InterMWP benchmark asks for a solver to not only output the solution expressions but also predict the corresponding logical formulas. We further propose a novel approach with logical prompt and interpretation generation, called LogicSolver. For each MWP, our LogicSolver first retrieves some highly-correlated algebraic knowledge and then passes them to the backbone model as prompts to improve the semantic representations of MWPs. With these improved semantic representations, our LogicSolver generates corresponding solution expressions and interpretable knowledge formulas in accord with the generated solution expressions, simultaneously. Experimental results show that our LogicSolver has stronger logical formula-based interpretability than baselines while achieving higher answer accuracy with the help of logical prompts, simultaneously. The source code and dataset is available at https://github.com/yangzhch6/InterMWP.",LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning,"Zhicheng Yang, Jinghui Qin, Jiaqi Chen, Liang Lin and Xiaodan Liang",2022,Artificial Intelligence,2205.08232
"This paper presents the power network reconfiguration algorithm HATSGA with a ""R"" modeling approach and evaluates its behavior in computing new reconfiguration topologies for the power network in the Smart Grid context. The modeling of the power distribution network with the language ""R"" is used to represent the network and support the computation of distinct algorithm configurations towards the evaluation of new reconfiguration topologies. The HATSGA algorithm adopts a hybrid Tabu Search and Genetic Algorithm strategy and can be configured in different ways to compute network reconfiguration solutions. The evaluation of power loss with HATSGA uses the IEEE 14-Bus topology as the power test scenario. The evaluation of reconfiguration topologies with minimum power loss with HATSGA indicates that an efficient solution can be reached with a feasible computational time. This suggests that HATSGA can be potentially used for computing reconfiguration network topologies and, beyond that, it can be used for autonomic self-healing management approaches where a feasible computational time is required.",On Evaluating Power Loss with HATSGA Algorithm for Power Network Reconfiguration in the Smart Grid,"Flavio Galvao Calhau, Alysson Pezzutti and Joberto S. B. Martins",2017,Artificial Intelligence,2205.10126
"Optimal experimental design seeks to determine the most informative allocation of experiments to infer an unknown statistical quantity. In this work, we investigate the optimal design of experiments for {\em estimation of linear functionals in reproducing kernel Hilbert spaces (RKHSs)}. This problem has been extensively studied in the linear regression setting under an estimability condition, which allows estimating parameters without bias. We generalize this framework to RKHSs, and allow for the linear functional to be only approximately inferred, i.e., with a fixed bias. This scenario captures many important modern applications, such as estimation of gradient maps, integrals, and solutions to differential equations. We provide algorithms for constructing bias-aware designs for linear functionals. We derive non-asymptotic confidence sets for fixed and adaptive designs under sub-Gaussian noise, enabling us to certify estimation with bounded error with high probability.",Experimental Design for Linear Functionals in Reproducing Kernel Hilbert Spaces,Mojm\'ir Mutn\'y and Andreas Krause,2022,Artificial Intelligence,2205.13627
"Static authentication methods, like passwords, grow increasingly weak with advancements in technology and attack strategies. Continuous authentication has been proposed as a solution, in which users who have gained access to an account are still monitored in order to continuously verify that the user is not an imposter who had access to the user credentials. Mouse dynamics is the behavior of a users mouse movements and is a biometric that has shown great promise for continuous authentication schemes. This article builds upon our previous published work by evaluating our dataset of 40 users using three machine learning and deep learning algorithms. Two evaluation scenarios are considered: binary classifiers are used for user authentication, with the top performer being a 1-dimensional convolutional neural network with a peak average test accuracy of 85.73% across the top 10 users. Multi class classification is also examined using an artificial neural network which reaches an astounding peak accuracy of 92.48% the highest accuracy we have seen for any classifier on this dataset.",Machine and Deep Learning Applications to Mouse Dynamics for Continuous User Authentication,"Nyle Siddiqui, Rushit Dave, Naeem Seliya, Mounika Vanamala",2022,Artificial Intelligence,2205.13646
"Artificial Intelligence in higher education opens new possibilities for improving the lecturing process, such as enriching didactic materials, helping in assessing students' works or even providing directions to the teachers on how to enhance the lectures. We follow this research path, and in this work, we explore how an academic lecture can be assessed automatically by quantitative features. First, we prepare a set of qualitative features based on teaching practices and then annotate the dataset of academic lecture videos collected for this purpose. We then show how these features could be detected automatically using machine learning and computer vision techniques. Our results show the potential usefulness of our work.",A Deep Learning Approach for Automatic Detection of Qualitative Features of Lecturing,"Anna Wroblewska, Jozef Jasek, Bogdan Jastrzebski, Stanislaw Pawlak, Anna Grzywacz, Cheong Siew Ann, Tan Seng Chee, Tomasz Trzcinski, Janusz Holyst",2022,Artificial Intelligence,2205.14919
"With Artificial intelligence (AI) to aid or automate decision-making advancing rapidly, a particular concern is its fairness. In order to create reliable, safe and trustworthy systems through human-centred artificial intelligence (HCAI) design, recent efforts have produced user interfaces (UIs) for AI experts to investigate the fairness of AI models. In this work, we provide a design space exploration that supports not only data scientists but also domain experts to investigate AI fairness. Using loan applications as an example, we held a series of workshops with loan officers and data scientists to elicit their requirements. We instantiated these requirements into FairHIL, a UI to support human-in-the-loop fairness investigations, and describe how this UI could be generalized to other use cases. We evaluated FairHIL through a think-aloud user study. Our work contributes better designs to investigate an AI model's fairness-and move closer towards responsible AI.",Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness,Yuri Nakao and Lorenzo Strappelli and Simone Stumpf and Aisha Naseer and Daniele Regoli and Giulia Del Gamba,2022,Artificial Intelligence,2206.00474
"We formalise some aspects of the neural-symbol design patterns of van Bekkum et al., such that we can formally define notions of refinement of patterns, as well as modular combination of larger patterns from smaller building blocks. These formal notions are being implemented in the heterogeneous tool set (Hets), such that patterns and refinements can be checked for well-formedness, and combinations can be computed.",Modular design patterns for neural-symbolic integration: refinement and combination,Till Mossakowski,2022,Artificial Intelligence,2206.04724
"The Hierarchical Task Network ({\sf HTN}) formalism is very expressive and used to express a wide variety of planning problems. In contrast to the classical {\sf STRIPS} formalism in which only the action model needs to be specified, the {\sf HTN} formalism requires to specify, in addition, the tasks of the problem and their decomposition into subtasks, called {\sf HTN} methods. For this reason, hand-encoding {\sf HTN} problems is considered more difficult and more error-prone by experts than classical planning problem. To tackle this problem, we propose a new approach (HierAMLSI) based on grammar induction to acquire {\sf HTN} planning domain knowledge, by learning action models and {\sf HTN} methods with their preconditions. Unlike other approaches, HierAMLSI is able to learn both actions and methods with noisy and partial inputs observation with a high level or accuracy.",An Accurate HDDL Domain Learning Algorithm from Partial and Noisy Observations,"M. Grand, H. Fiorino and D. Pellier",2022,Artificial Intelligence,2206.06882
"The Hierarchical Task Network (HTN) formalism is used to express a wide variety of planning problems in terms of decompositions of tasks into subtaks. Many techniques have been proposed to solve such hierarchical planning problems. A particular technique is to encode hierarchical planning problems as classical STRIPS planning problems. One advantage of this technique is to benefit directly from the constant improvements made by STRIPS planners. However, there are still few effective and expressive encodings. In this paper, we present a new HTN to STRIPS encoding allowing to generate concurrent plans. We show experimentally that this encoding outperforms previous approaches on hierarchical IPC benchmarks.",An Efficient HTN to STRIPS Encoding for Concurrent Plans,"N. Cavrel, D. Pellier, H. Fiorino",2022,Artificial Intelligence,2206.07084
"A large number of people suffer from life-threatening cardiac abnormalities, and electrocardiogram (ECG) analysis is beneficial to determining whether an individual is at risk of such abnormalities. Automatic ECG classification methods, especially the deep learning based ones, have been proposed to detect cardiac abnormalities using ECG records, showing good potential to improve clinical diagnosis and help early prevention of cardiovascular diseases. However, the predictions of the known neural networks still do not satisfactorily meet the needs of clinicians, and this phenomenon suggests that some information used in clinical diagnosis may not be well captured and utilized by these methods. In this paper, we introduce some rules into convolutional neural networks, which help present clinical knowledge to deep learning based ECG analysis, in order to improve automated ECG diagnosis performance. Specifically, we propose a Handcrafted-Rule-enhanced Neural Network (called HRNN) for ECG classification with standard 12-lead ECG input, which consists of a rule inference module and a deep learning module. Experiments on two large-scale public ECG datasets show that our new approach considerably outperforms existing state-of-the-art methods. Further, our proposed approach not only can improve the diagnosis performance, but also can assist in detecting mislabelled ECG samples. Our codes are available at https://github.com/alwaysbyx/ecg_processing.",Identifying Electrocardiogram Abnormalities Using a Handcrafted-Rule-Enhanced Neural Network,"Yuexin Bian, Jintai Chen, Xiaojun Chen, Xiaoxian Yang, Danny Z. Chen, JIan Wu",2022,Artificial Intelligence,2206.10592
"Object-centric process mining is a new paradigm with more realistic assumptions about underlying data by considering several case notions, e.g., an order handling process can be analyzed based on order, item, package, and route case notions. Including many case notions can result in a very complex model. To cope with such complexity, this paper introduces a new approach to cluster similar case notions based on Markov Directly-Follow Multigraph, which is an extended version of the well-known Directly-Follow Graph supported by many industrial and academic process mining tools. This graph is used to calculate a similarity matrix for discovering clusters of similar case notions based on a threshold. A threshold tuning algorithm is also defined to identify sets of different clusters that can be discovered based on different levels of similarity. Thus, the cluster discovery will not rely on merely analysts' assumptions. The approach is implemented and released as a part of a python library, called processmining, and it is evaluated through a Purchase to Pay (P2P) object-centric event log file. Some discovered clusters are evaluated by discovering Directly Follow-Multigraph by flattening the log based on the clusters. The similarity between identified clusters is also evaluated by calculating the similarity between the behavior of the process models discovered for each case notion using inductive miner based on footprints conformance checking.",Object Type Clustering using Markov Directly-Follow Multigraph in Object-Centric Process Mining,Amin Jalali,2022,Artificial Intelligence,2206.11017
"""Unless and until our society recognizes cyber bullying for what it is, the suffering of thousands of silent victims will continue."" ~ Anna Maria Chavez. There had been series of research on cyber bullying which are unable to provide reliable solution to cyber bullying. In this research work, we were able to provide a permanent solution to this by developing a model capable of detecting and intercepting bullying incoming and outgoing messages with 92% accuracy. We also developed a chatbot automation messaging system to test our model leading to the development of Artificial Intelligence powered anti-cyber bullying system using machine learning algorithm of Multinomial Naive Bayes (MNB) and optimized linear Support Vector Machine (SVM). Our model is able to detect and intercept bullying outgoing and incoming bullying messages and take immediate action.",AI Powered Anti-Cyber Bullying System using Machine Learning Algorithm of Multinomial Naive Bayes and Optimized Linear Support Vector Machine,"Tosin Ige, Sikiru Adewale",2022,Artificial Intelligence,2207.11897
"Current AI systems are designed to solve close-world problems with the assumption that the underlying world is remaining more or less the same. However, when dealing with real-world problems such assumptions can be invalid as sudden and unexpected changes can occur. To effectively deploy AI-powered systems in the real world, AI systems should be able to deal with open-world novelty quickly. Inevitably, dealing with open-world novelty raises an important question of novelty difficulty. Knowing whether one novelty is harder to deal with than another, can help researchers to train their systems systematically. In addition, it can also serve as a measurement of the performance of novelty robust AI systems. In this paper, we propose to define the novelty reaction difficulty as a relative difficulty of performing the known task after the introduction of the novelty. We propose a universal method that can be applied to approximate the difficulty. We present the approximations of the difficulty using our method and show how it aligns with the results of the evaluation of AI agents designed to deal with novelty.",Measuring Difficulty of Novelty Reaction,"Ekaterina Nikonova, Cheng Xue, Vimukthini Pinto, Chathura Gamage, Peng Zhang, Jochen Renz",2022,Artificial Intelligence,2207.13857
"In this work, we consider the problem of procedural content generation for video game levels. Prior approaches have relied on evolutionary search (ES) methods capable of generating diverse levels, but this generation procedure is slow, which is problematic in real-time settings. Reinforcement learning (RL) has also been proposed to tackle the same problem, and while level generation is fast, training time can be prohibitively expensive. We propose a framework to tackle the procedural content generation problem that combines the best of ES and RL. In particular, our approach first uses ES to generate a sequence of levels evolved over time, and then uses behaviour cloning to distil these levels into a policy, which can then be queried to produce new levels quickly. We apply our approach to a maze game and Super Mario Bros, with our results indicating that our approach does in fact decrease the time required for level generation, especially when an increasing number of valid levels are required.",Combining Evolutionary Search with Behaviour Cloning for Procedurally Generated Content,"Nicholas Muir, Steven James",2022,Artificial Intelligence,2207.14772
"A ''technology lottery'' describes a research idea or technology succeeding over others because it is suited to the available software and hardware, not necessarily because it is superior to alternative directions--examples abound, from the synergies of deep learning and GPUs to the disconnect of urban design and autonomous vehicles. The nascent field of Self-Driving Laboratories (SDL), particularly those implemented as Materials Acceleration Platforms (MAPs), is at risk of an analogous pitfall: the next logical step for building MAPs is to take existing lab equipment and workflows and mix in some AI and automation. In this whitepaper, we argue that the same simulation and AI tools that will accelerate the search for new materials, as part of the MAPs research program, also make possible the design of fundamentally new computing mediums. We need not be constrained by existing biases in science, mechatronics, and general-purpose computing, but rather we can pursue new vectors of engineering physics with advances in cyber-physical learning and closed-loop, self-optimizing systems. Here we outline a simulation-based MAP program to design computers that use physics itself to solve optimization problems. Such systems mitigate the hardware-software-substrate-user information losses present in every other class of MAPs and they perfect alignment between computing problems and computing mediums eliminating any technology lottery. We offer concrete steps toward early ''Physical Computing (PC) -MAP'' advances and the longer term cyber-physical R&D which we expect to introduce a new era of innovative collaboration between materials researchers and computer scientists.",Physical Computing for Materials Acceleration Platforms,"Erik Peterson, Alexander Lavin",2022,Artificial Intelligence,2208.08566
"We describe the first steps in the development of an artificial agent focused on the Brazilian maritime territory, a large region within the South Atlantic also known as the Blue Amazon. The ""BLue Amazon Brain"" (BLAB) integrates a number of services aimed at disseminating information about this region and its importance, functioning as a tool for environmental awareness. The main service provided by BLAB is a conversational facility that deals with complex questions about the Blue Amazon, called BLAB-Chat; its central component is a controller that manages several task-oriented natural language processing modules (e.g., question answering and summarizer systems). These modules have access to an internal data lake as well as to third-party databases. A news reporter (BLAB-Reporter) and a purposely-developed wiki (BLAB-Wiki) are also part of the BLAB service architecture. In this paper, we describe our current version of BLAB's architecture (interface, backend, web services, NLP modules, and resources) and comment on the challenges we have faced so far, such as the lack of training data and the scattered state of domain information. Solving these issues presents a considerable challenge in the development of artificial intelligence for technical domains.",The BLue Amazon Brain (BLAB): A Modular Architecture of Services about the Brazilian Maritime Territory,"Paulo Pirozelli, Ais B. R. Castro, Ana Luiza C. de Oliveira, Andr\'e S. Oliveira, Fl\'avio N. Ca\c{c}\~ao, Igor C. Silveira, Jo\~ao G. M. Campos, Laura C. Motheo, Leticia F. Figueiredo, Lucas F. A. O. Pellicer, Marcelo A. Jos\'e, Marcos M. Jos\'e, Pedro de M. Ligabue, Ricardo S. Grava, Rodrigo M. Tavares, Vin\'icius B. Matos, Yan V. Sym, Anna H. R. Costa, Anarosa A. F. Brand\~ao, Denis D. Mau\'a, Fabio G. Cozman, Sarajane M. Peres",2022,Artificial Intelligence,2209.07928
"Reasoning is a fundamental problem for computers and deeply studied in Artificial Intelligence. In this paper, we specifically focus on answering multi-hop logical queries on Knowledge Graphs (KGs). This is a complicated task because, in real-world scenarios, the graphs tend to be large and incomplete. Most previous works have been unable to create models that accept full First-Order Logical (FOL) queries, which include negative queries, and have only been able to process a limited set of query structures. Additionally, most methods present logic operators that can only perform the logical operation they are made for. We introduce a set of models that use Neural Networks to create one-point vector embeddings to answer the queries. The versatility of neural networks allows the framework to handle FOL queries with Conjunction ($\wedge$), Disjunction ($\vee$) and Negation ($\neg$) operators. We demonstrate experimentally the performance of our model through extensive experimentation on well-known benchmarking datasets. Besides having more versatile operators, the models achieve a 10\% relative increase over the best performing state of the art and more than 30\% over the original method based on single-point vector embeddings.",Neural Methods for Logical Reasoning Over Knowledge Graphs,"Alfonso Amayuelas, Shuai Zhang, Susie Xi Rao, Ce Zhang",2022,Artificial Intelligence,2209.14464
"We consider graph modeling for a knowledge graph for vehicle development, with a focus on crash safety. An organized schema that incorporates information from various structured and unstructured data sources is provided, which includes relevant concepts within the domain. In particular, we propose semantics for crash computer aided engineering (CAE) data, which enables searchability, filtering, recommendation, and prediction for crash CAE data during the development process. This graph modeling considers the CAE data in the context of the R\&D development process and vehicle safety. Consequently, we connect CAE data to the protocols that are used to assess vehicle safety performances. The R\&D process includes CAD engineering and safety attributes, with a focus on multidisciplinary problem-solving. We describe previous efforts in graph modeling in comparison to our proposal, discuss its strengths and limitations, and identify areas for future work.",Graph Modeling in Computer Assisted Automotive Development,"Anahita Pakiman, Jochen Garcke",2022,Artificial Intelligence,2209.14910
"Knowledge graph embedding aims to predict the missing relations between entities in knowledge graphs. Tensor-decomposition-based models, such as ComplEx, provide a good trade-off between efficiency and expressiveness, that is crucial because of the large size of real world knowledge graphs. The recent multi-partition embedding interaction (MEI) model subsumes these models by using the block term tensor format and provides a systematic solution for the trade-off. However, MEI has several drawbacks, some of which carried from its subsumed tensor-decomposition-based models. In this paper, we address these drawbacks and introduce the Multi-partition Embedding Interaction iMproved beyond block term format (MEIM) model, with independent core tensor for ensemble effects and soft orthogonality for max-rank mapping, in addition to multi-partition embedding. MEIM improves expressiveness while still being highly efficient, helping it to outperform strong baselines and achieve state-of-the-art results on difficult link prediction benchmarks using fairly small embedding sizes. The source code is released at https://github.com/tranhungnghiep/MEIM-KGE.",MEIM: Multi-partition Embedding Interaction Beyond Block Term Format for Efficient and Expressive Link Prediction,"Hung Nghiep Tran, Atsuhiro Takasu",2022,Artificial Intelligence,2209.15597
"Surrogate modeling has brought about a revolution in computation in the branches of science and engineering. Backed by Artificial Intelligence, a surrogate model can present highly accurate results with a significant reduction in computation time than computer simulation of actual models. Surrogate modeling techniques have found their use in numerous branches of science and engineering, energy system modeling being one of them. Since the idea of hybrid and sustainable energy systems is spreading rapidly in the modern world for the paradigm of the smart energy shift, researchers are exploring the future application of artificial intelligence-based surrogate modeling in analyzing and optimizing hybrid energy systems. One of the promising technologies for assessing applicability for the energy system is the digital twin, which can leverage surrogate modeling. This work presents a comprehensive framework/review on Artificial Intelligence-driven surrogate modeling and its applications with a focus on the digital twin framework and energy systems. The role of machine learning and artificial intelligence in constructing an effective surrogate model is explained. After that, different surrogate models developed for different sustainable energy sources are presented. Finally, digital twin surrogate models and associated uncertainties are described.",Digital Twin and Artificial Intelligence Incorporated With Surrogate Modeling for Hybrid and Sustainable Energy Systems,"Abid Hossain Khan, Salauddin Omar, Nadia Mushtary, Richa Verma, Dinesh Kumar, Syed Alam",2022,Artificial Intelligence,2210.00073
"The ability to reuse previous policies is an important aspect of human intelligence. To achieve efficient policy reuse, a Deep Reinforcement Learning (DRL) agent needs to decide when to reuse and which source policies to reuse. Previous methods solve this problem by introducing extra components to the underlying algorithm, such as hierarchical high-level policies over source policies, or estimations of source policies' value functions on the target task. However, training these components induces either optimization non-stationarity or heavy sampling cost, significantly impairing the effectiveness of transfer. To tackle this problem, we propose a novel policy reuse algorithm called Critic-gUided Policy reuse (CUP), which avoids training any extra components and efficiently reuses source policies. CUP utilizes the critic, a common component in actor-critic methods, to evaluate and choose source policies. At each state, CUP chooses the source policy that has the largest one-step improvement over the current target policy, and forms a guidance policy. The guidance policy is theoretically guaranteed to be a monotonic improvement over the current target policy. Then the target policy is regularized to imitate the guidance policy to perform efficient policy search. Empirical results demonstrate that CUP achieves efficient transfer and significantly outperforms baseline algorithms.",CUP: Critic-Guided Policy Reuse,"Jin Zhang, Siyuan Li, Chongjie Zhang",2022,Artificial Intelligence,2210.08153
"Ontology development methodologies emphasise knowledge gathering from domain experts and documentary resources, and knowledge representation using an ontology language such as OWL or FOL. However, working ontologists are often surprised by how challenging and slow it can be to develop ontologies. Here, with a particular emphasis on the sorts of ontologies that are content-heavy and intended to be shared across a community of users (reference ontologies), we propose that a significant and heretofore under-emphasised contributor of challenges during ontology development is the need to create, or bring about, consensus in the face of disagreement. For this reason reference ontology development cannot be automated, at least within the limitations of existing AI approaches. Further, for the same reason ontologists are required to have specific social-negotiating skills which are currently lacking in most technical curricula.","Ontology Development is Consensus Creation, Not (Merely) Representation",Fabian Neuhaus and Janna Hastings,2022,Artificial Intelligence,2210.12026
"In this paper, we propose a comprehensive benchmark to investigate models' logical reasoning capabilities in complex real-life scenarios. Current explanation datasets often employ synthetic data with simple reasoning structures. Therefore, it cannot express more complex reasoning processes, such as the rebuttal to a reasoning step and the degree of certainty of the evidence. To this end, we propose a comprehensive logical reasoning explanation form. Based on the multi-hop chain of reasoning, the explanation form includes three main components: (1) The condition of rebuttal that the reasoning node can be challenged; (2) Logical formulae that uncover the internal texture of reasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The fine-grained structure conforms to the real logical reasoning scenario, better fitting the human cognitive process but, simultaneously, is more challenging for the current models. We evaluate the current best models' performance on this new explanation form. The experimental results show that generating reasoning graphs remains a challenging task for current models, even with the help of giant pre-trained language models.",MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure,"Yinya Huang, Hongming Zhang, Ruixin Hong, Xiaodan Liang, Changshui Zhang and Dong Yu",2022,Artificial Intelligence,2210.12487
"Explainable Artificial Intelligence (XAI) is a paradigm that delivers transparent models and decisions, which are easy to understand, analyze, and augment by a non-technical audience. Fuzzy Logic Systems (FLS) based XAI can provide an explainable framework, while also modeling uncertainties present in real-world environments, which renders it suitable for applications where explainability is a requirement. However, most real-life processes are not characterized by high levels of uncertainties alone; they are inherently time-dependent as well, i.e., the processes change with time. In this work, we present novel Temporal Type-2 FLS Based Approach for time-dependent XAI (TXAI) systems, which can account for the likelihood of a measurement's occurrence in the time domain using (the measurement's) frequency of occurrence. In Temporal Type-2 Fuzzy Sets (TT2FSs), a four-dimensional (4D) time-dependent membership function is developed where relations are used to construct the inter-relations between the elements of the universe of discourse and its frequency of occurrence. The TXAI system manifested better classification prowess, with 10-fold test datasets, with a mean recall of 95.40\% than a standard XAI system (based on non-temporal general type-2 (GT2) fuzzy sets) that had a mean recall of 87.04\%. TXAI also performed significantly better than most non-explainable AI systems between 3.95\%, to 19.04\% improvement gain in mean recall. In addition, TXAI can also outline the most likely time-dependent trajectories using the frequency of occurrence values embedded in the TXAI model; viz. given a rule at a determined time interval, what will be the next most likely rule at a subsequent time interval. In this regard, the proposed TXAI system can have profound implications for delineating the evolution of real-life time-dependent processes, such as behavioural or biological processes.",A Temporal Type-2 Fuzzy System for Time-dependent Explainable Artificial Intelligence,"Mehrin Kiani, Javier Andreu-Perez, Hani Hagras",2022,Artificial Intelligence,2210.12571
"One of the most important AI research questions is to trade off computation versus performance since ``perfect rationality"" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9 \times 9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at \url{https://github.com/YeWR/V-MCTS.git}.",Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions,"Weirui Ye, Pieter Abbeel, Yang Gao",2022,Artificial Intelligence,2210.12628
"Sparsity of formal knowledge and roughness of non-ontological construction make sparsity problem particularly prominent in Open Knowledge Graphs (OpenKGs). Due to sparse links, learning effective representation for few-shot entities becomes difficult. We hypothesize that by introducing negative samples, a contrastive learning (CL) formulation could be beneficial in such scenarios. However, existing CL methods model KG triplets as binary objects of entities ignoring the relation-guided ternary propagation patterns and they are too generic, i.e., they ignore zero-shot, few-shot and synonymity problems that appear in OpenKGs. To address this, we propose TernaryCL, a CL framework based on ternary propagation patterns among head, relation and tail. TernaryCL designs Contrastive Entity and Contrastive Relation to mine ternary discriminative features with both negative entities and relations, introduces Contrastive Self to help zero- and few-shot entities learn discriminative features, Contrastive Synonym to model synonymous entities, and Contrastive Fusion to aggregate graph features from multiple paths. Extensive experiments on benchmarks demonstrate the superiority of TernaryCL over state-of-the-art models.",Alleviating Sparsity of Open Knowledge Graphs with Ternary Contrastive Learning,"Qian Li, Shafiq Joty, Daling Wang, Shi Feng and Yifei Zhang",2022,Artificial Intelligence,2211.03950
"In this paper, we propose a new method for knowledge base completion (KBC): instance-based learning (IBL). For example, to answer (Jill Biden, lived city,? ), instead of going directly to Washington D.C., our goal is to find Joe Biden, who has the same lived city as Jill Biden. Through prototype entities, IBL provides interpretability. We develop theories for modeling prototypes and combining IBL with translational models. Experiments on various tasks confirmed the IBL model's effectiveness and interpretability. In addition, IBL shed light on the mechanism of rule-based KBC models. Previous research has generally agreed that rule-based models provide rules with semantically compatible premises and hypotheses. We challenge this view. We begin by demonstrating that some logical rules represent {\it instance-based equivalence} (i.e. prototypes) rather than semantic compatibility. These are denoted as {\it IBL rules}. Surprisingly, despite occupying only a small portion of the rule space, IBL rules outperform non-IBL rules in all four benchmarks. We use a variety of experiments to demonstrate that rule-based models work because they have the ability to represent instance-based equivalence via IBL rules. The findings provide new insights of how rule-based models work and how to interpret their rules.",Instance-based Learning for Knowledge Base Completion,"Wanyun Cui, Xingran Chen",2022,Artificial Intelligence,2211.06807
"The present paper comes across the main steps that laid from Zadeh's fuzziness ana Atanassov's intuitionistic fuzzy sets to Smarandache's indeterminacy and to Molodstov's soft sets. Two hybrid methods for assessment and decision making respectively under fuzzy conditions are also presented through suitable examples that use soft sets and real intervals as tools. The decision making method improves an earlier method of Maji et al. Further, it is described how the concept of topological space, the most general category of mathematical spaces, can be extended to fuzzy structures and how to generalize the fundamental mathematical concepts of limit, continuity compactness and Hausdorff space within such kind of structures. In particular, fuzzy and soft topological spaces are defined and examples are given to illustrate these generalizations.","Fuzziness, Indeterminacy and Soft Sets: Frontiers and Perspectives",Michael Gr. Voskoglou,2022,Artificial Intelligence,2211.15408
"In recent years, unmanned aerial vehicle (UAV) related technology has expanded knowledge in the area, bringing to light new problems and challenges that require solutions. Furthermore, because the technology allows processes usually carried out by people to be automated, it is in great demand in industrial sectors. The automation of these vehicles has been addressed in the literature, applying different machine learning strategies. Reinforcement learning (RL) is an automation framework that is frequently used to train autonomous agents. RL is a machine learning paradigm wherein an agent interacts with an environment to solve a given task. However, learning autonomously can be time consuming, computationally expensive, and may not be practical in highly-complex scenarios. Interactive reinforcement learning allows an external trainer to provide advice to an agent while it is learning a task. In this study, we set out to teach an RL agent to control a drone using reward-shaping and policy-shaping techniques simultaneously. Two simulated scenarios were proposed for the training; one without obstacles and one with obstacles. We also studied the influence of each technique. The results show that an agent trained simultaneously with both techniques obtains a lower reward than an agent trained using only a policy-based approach. Nevertheless, the agent achieves lower execution times and less dispersion during training.",Reinforcement Learning for UAV control with Policy and Reward Shaping,"Cristian Mill\'an-Arias, Ruben Contreras, Francisco Cruz and Bruno Fernandes",2022,Artificial Intelligence,2212.03828
"We propose a hierarchical framework for collaborative intelligent systems. This framework organizes research challenges based on the nature of the collaborative activity and the information that must be shared, with each level building on capabilities provided by lower levels. We review research paradigms at each level, with a description of classical engineering-based approaches and modern alternatives based on machine learning, illustrated with a running example using a hypothetical personal service robot. We discuss cross-cutting issues that occur at all levels, focusing on the problem of communicating and sharing comprehension, the role of explanation and the social nature of collaboration. We conclude with a summary of research challenges and a discussion of the potential for economic and societal impact provided by technologies that enhance human abilities and empower people and society through collaboration with Intelligent Systems.",A Hierarchical Framework for Collaborative Artificial Intelligence,"James L. Crowley (LIG, UGA, MIAI@UGA, Grenoble INP ), Jo\""elle L Coutaz (UGA), Jasmin Grosinger, Javier V\'azquez-Salceda (UPC), Cecilio Angulo (UPC), Alberto Sanfeliu (UPC), Luca Iocchi (Sapienza University of Rome), Anthony G. Cohn",2022,Artificial Intelligence,2212.08659
"Bayesian networks (BNs) are a probabilistic graphical model widely used for representing expert knowledge and reasoning under uncertainty. Traditionally, they are based on directed acyclic graphs that capture dependencies between random variables. However, directed cycles can naturally arise when cross-dependencies between random variables exist, e.g., for modeling feedback loops. Existing methods to deal with such cross-dependencies usually rely on reductions to BNs without cycles. These approaches are fragile to generalize, since their justifications are intermingled with additional knowledge about the application context. In this paper, we present a foundational study regarding semantics for cyclic BNs that are generic and conservatively extend the cycle-free setting. First, we propose constraint-based semantics that specify requirements for full joint distributions over a BN to be consistent with the local conditional probabilities and independencies. Second, two kinds of limit semantics that formalize infinite unfolding approaches are introduced and shown to be computable by a Markov chain construction.",On the Foundations of Cycles in Bayesian Networks,"Christel Baier and Clemens Dubslaff and Holger Hermanns and Nikolai K\""afer",2022,Artificial Intelligence,2301.08608
A well known N P-hard problem called the Generalized Traveling Salesman Problem (GTSP) is considered. In GTSP the nodes of a complete undirected graph are partitioned into clusters. The objective is to find a minimum cost tour passing through exactly one node from each cluster. An exact exponential time algorithm and an effective meta-heuristic algorithm for the problem are presented. The meta-heuristic proposed is a modified Ant Colony System (ACS) algorithm called Reinforcing Ant Colony System (RACS) which introduces new correction rules in the ACS algorithm. Computational results are reported for many standard test problems. The proposed algorithm is competitive with the other already proposed heuristics for the GTSP in both solution quality and computational time.,The Generalized Traveling Salesman Problem solved with Ant Algorithms,"Camelia-M. Pintea, Petrica C. Pop, Camelia Chira",2017,Artificial Intelligence,1310.2350
"This paper describes a new method for classifying a dataset that partitions elements into their categories. It has relations with neural networks but a slightly different structure, requiring only a single pass through the classifier to generate the weight sets. A grid-like structure is required as part of a novel idea of converting a 1-D row of real values into a 2-D structure of value bands. Each cell in any band then stores a distinct set of weights, to represent its own importance and its relation to each output category. During classification, all of the output weight lists can be retrieved and summed to produce a probability for what the correct output category is. The bands possibly work like hidden layers of neurons, but they are variable specific, making the process orthogonal. The construction process can be a single update process without iterations, making it potentially much faster. It can also be compared with k-NN and may be practical for partial or competitive updating.",A Single-Pass Classifier for Categorical Data,Kieran Greer,2017,Artificial Intelligence,1503.02521
"Every day, billions of mobile network events (i.e. CDRs) are generated by cellular phone operator companies. Latent in this data are inspiring insights about human actions and behaviors, the discovery of which is important because context-aware applications and services hold the key to user-driven, intelligent services, which can enhance our everyday lives such as social and economic development, urban planning, and health prevention. The major challenge in this area is that interpreting such a big stream of data requires a deep understanding of mobile network events' context through available background knowledge. This article addresses the issues in context awareness given heterogeneous and uncertain data of mobile network events missing reliable information on the context of this activity. The contribution of this research is a model from a combination of logical and statistical reasoning standpoints for enabling human activity inference in qualitative terms from open geographical data that aimed at improving the quality of human behaviors recognition tasks from CDRs. We use open geographical data, Openstreetmap (OSM), as a proxy for predicting the content of human activity in the area. The user study performed in Trento shows that predicted human activities (top level) match the survey data with around 93% overall accuracy. The extensive validation for predicting a more specific economic type of human activity performed in Barcelona, by employing credit card transaction data. The analysis identifies that appropriately normalized data on points of interest (POI) is a good proxy for predicting human economical activities, with 84% accuracy on average. So the model is proven to be efficient for predicting the context of human activity, when its total level could be efficiently observed from cell phone data records, missing contextual information however.",Semantic Enrichment of Mobile Phone Data Records Using Background Knowledge,Zolzaya Dashdorj and Stanislav Sobolevsky and Luciano Serafini and Fabrizio Antonelli and Carlo Ratti,2018,Artificial Intelligence,1504.05895
"This paper describes a new method for reducing the error in a classifier. It uses an error correction update that includes the very simple rule of either adding or subtracting the error adjustment, based on whether the variable value is currently larger or smaller than the desired value. While a traditional neuron would sum the inputs together and then apply a function to the total, this new method can change the function decision for each input value. This gives added flexibility to the convergence procedure, where through a series of transpositions, variables that are far away can continue towards the desired value, whereas variables that are originally much closer can oscillate from one side to the other. Tests show that the method can successfully classify some benchmark datasets. It can also work in a batch mode, with reduced training times and can be used as part of a neural network architecture. Some comparisons with an earlier wave shape paper are also made.",A New Oscillating-Error Technique for Classifiers,Kieran Greer,2017,Artificial Intelligence,1505.05312
"We establish an equivalence between two seemingly different theories: one is the traditional axiomatisation of incomplete preferences on horse lotteries based on the mixture independence axiom; the other is the theory of desirable gambles developed in the context of imprecise probability. The equivalence allows us to revisit incomplete preferences from the viewpoint of desirability and through the derived notion of coherent lower previsions. On this basis, we obtain new results and insights: in particular, we show that the theory of incomplete preferences can be developed assuming only the existence of a worst act---no best act is needed---, and that a weakened Archimedean axiom suffices too; this axiom allows us also to address some controversy about the regularity assumption (that probabilities should be positive---they need not), which enables us also to deal with uncountable possibility spaces; we show that it is always possible to extend in a minimal way a preference relation to one with a worst act, and yet the resulting relation is never Archimedean, except in a trivial case; we show that the traditional notion of state independence coincides with the notion called strong independence in imprecise probability---this leads us to give much a weaker definition of state independence than the traditional one; we rework and uniform the notions of complete preferences, beliefs, values; we argue that Archimedeanity does not capture all the problems that can be modelled with sets of expected utilities and we provide a new notion that does precisely that. Perhaps most importantly, we argue throughout that desirability is a powerful and natural setting to model, and work with, incomplete preferences, even in case of non-Archimedean problems. This leads us to suggest that desirability, rather than preference, should be the primitive notion at the basis of decision-theoretic axiomatisations.",Desirability and the birth of incomplete preferences,Marco Zaffalon and Enrique Miranda,2017,Artificial Intelligence,1506.00529
"Detection of non-technical losses (NTL) which include electricity theft, faulty meters or billing errors has attracted increasing attention from researchers in electrical engineering and computer science. NTLs cause significant harm to the economy, as in some countries they may range up to 40% of the total electricity distributed. The predominant research direction is employing artificial intelligence to predict whether a customer causes NTL. This paper first provides an overview of how NTLs are defined and their impact on economies, which include loss of revenue and profit of electricity providers and decrease of the stability and reliability of electrical power grids. It then surveys the state-of-the-art research efforts in a up-to-date and comprehensive review of algorithms, features and data sets used. It finally identifies the key scientific and engineering challenges in NTL detection and suggests how they could be addressed in the future.",The Challenge of Non-Technical Loss Detection using Artificial Intelligence: A Survey,"Patrick Glauner, Jorge Augusto Meira, Petko Valtchev, Radu State, Franck Bettinger",2017,Artificial Intelligence,1606.00626
"A Concept Tree is a structure for storing knowledge where the trees are stored in a database called a Concept Base. It sits between the highly distributed neural architectures and the distributed information systems, with the intention of bringing brain-like and computer systems closer together. Concept Trees can grow from the semi-structured sources when consistent sequences of concepts are presented. Each tree ideally represents a single cohesive concept and the trees can link with each other for navigation and semantic purposes. The trees are therefore also a type of semantic network and would benefit from having a consistent level of context for each node. A consistent build process is managed through a 'counting rule' and some other rules that can normalise the database structure. This restricted structure can then be complimented and enriched by the more dynamic context. It is also suggested to use the linking structure of the licas system [15] as a basis for the context links, where the mathematical model is extended further to define this. A number of tests have demonstrated the soundness of the architecture. Building the trees from text documents shows that the tree structure could be inherent in natural language. Then, two types of query language are described. Both of these can perform consistent query processes to return knowledge to the user and even enhance the query with new knowledge. This is supported even further with direct comparisons to a cognitive model, also being developed by the author.",Adding Context to Concept Trees,Kieran Greer,2019,Artificial Intelligence,1606.05597
"Sequential data modeling and analysis have become indispensable tools for analyzing sequential data, such as time-series data, because larger amounts of sensed event data have become available. These methods capture the sequential structure of data of interest, such as input-output relations and correlation among datasets. However, because most studies in this area are specialized or limited to their respective applications, rigorous requirement analysis of such models has not been undertaken from a general perspective. Therefore, we particularly examine the structure of sequential data, and extract the necessity of `state duration' and `state interval' of events for efficient and rich representation of sequential data. Specifically addressing the hidden semi-Markov model (HSMM) that represents such state duration inside a model, we attempt to add representational capability of a state interval of events onto HSMM. To this end, we propose two extended models: an interval state hidden semi-Markov model (IS-HSMM) to express the length of a state interval with a special state node designated as ""interval state node""; and an interval length probability hidden semi-Markov model (ILP-HSMM) which represents the length of the state interval with a new probabilistic parameter ""interval length probability."" Exhaustive simulations have revealed superior performance of the proposed models in comparison with HSMM. These proposed models are the first reported extensions of HMM to support state interval representation as well as state duration representation.",State Duration and Interval Modeling in Hidden Semi-Markov Model for Sequential Data Analysis,Hiromi Narimatsu and Hiroyuki Kasai,2017,Artificial Intelligence,1608.06954
"This paper describes an approach to the methodology of answer set programming (ASP) that can facilitate the design of encodings that are easy to understand and provably correct. Under this approach, after appending a rule or a small group of rules to the emerging program we include a comment that states what has been ""achieved"" so far. This strategy allows us to set out our understanding of the design of the program by describing the roles of small parts of the program in a mathematically precise way.",Achievements in Answer Set Programming,Vladimir Lifschitz,2017,Artificial Intelligence,1608.08144
"Graph aggregation is the process of computing a single output graph that constitutes a good compromise between several input graphs, each provided by a different source. One needs to perform graph aggregation in a wide variety of situations, e.g., when applying a voting rule (graphs as preference orders), when consolidating conflicting views regarding the relationships between arguments in a debate (graphs as abstract argumentation frameworks), or when computing a consensus between several alternative clusterings of a given dataset (graphs as equivalence relations). In this paper, we introduce a formal framework for graph aggregation grounded in social choice theory. Our focus is on understanding which properties shared by the individual input graphs will transfer to the output graph returned by a given aggregation rule. We consider both common properties of graphs, such as transitivity and reflexivity, and arbitrary properties expressible in certain fragments of modal logic. Our results establish several connections between the types of properties preserved under aggregation and the choice-theoretic axioms satisfied by the rules used. The most important of these results is a powerful impossibility theorem that generalises Arrow's seminal result for the aggregation of preference orders to a large collection of different types of graphs.",Graph Aggregation,Ulle Endriss and Umberto Grandi,2017,Artificial Intelligence,1609.03765
"Pairwise comparisons between alternatives are a well-known method for measuring preferences of a decision-maker. Since these often do not exhibit consistency, a number of inconsistency indices has been introduced in order to measure the deviation from this ideal case. We axiomatically characterize the inconsistency ranking induced by the Koczkodaj inconsistency index: six independent properties are presented such that they determine a unique linear order on the set of all pairwise comparison matrices.",Characterization of an inconsistency ranking for pairwise comparison matrices,L\'aszl\'o Csat\'o,2018,Artificial Intelligence,1610.07388
"This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach's strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.",DeepBach: a Steerable Model for Bach Chorales Generation,"Ga\""etan Hadjeres and Fran\c{c}ois Pachet and Frank Nielsen",2017,Artificial Intelligence,1612.01010
"Crowdsourcing, a major economic issue, is the fact that the firm outsources internal task to the crowd. It is a form of digital subcontracting for the general public. The evaluation of the participants work quality is a major issue in crowdsourcing. Indeed, contributions must be controlled to ensure the effectiveness and relevance of the campaign. We are particularly interested in small, fast and not automatable tasks. Several methods have been proposed to solve this problem, but they are applicable when the ""golden truth"" is not always known. This work has the particularity to propose a method for calculating the degree of expertise in the presence of gold data in crowdsourcing. This method is based on the belief function theory and proposes a structuring of data using graphs. The proposed approach will be assessed and applied to the data.",Une mesure d'expertise pour le crowdsourcing,"Hosna Ouni (IRISA, DRUID), Arnaud Martin (IRISA, UR1, DRUID), Laetitia Gros, Mouloud Kharoune (IRISA, DRUID), Zoltan Miklos (IRISA, DRUID)",2017,Artificial Intelligence,1701.04645
"The 7th Symposium on Educational Advances in Artificial Intelligence (EAAI'17, co-chaired by Sven Koenig and Eric Eaton) launched the EAAI New and Future AI Educator Program to support the training of early-career university faculty, secondary school faculty, and future educators (PhD candidates or postdocs who intend a career in academia). As part of the program, awardees were asked to address one of the following ""blue sky"" questions: * How could/should Artificial Intelligence (AI) courses incorporate ethics into the curriculum? * How could we teach AI topics at an early undergraduate or a secondary school level? * AI has the potential for broad impact to numerous disciplines. How could we make AI education more interdisciplinary, specifically to benefit non-engineering fields? This paper is a collection of their responses, intended to help motivate discussion around these issues in AI education.",Blue Sky Ideas in Artificial Intelligence Education from the EAAI 2017 New and Future AI Educator Program,"Eric Eaton, Sven Koenig, Claudia Schulz, Francesco Maurelli, John Lee, Joshua Eckroth, Mark Crowley, Richard G. Freedman, Rogelio E. Cardona-Rivera, Tiago Machado, Tom Williams",2018,Artificial Intelligence,1702.00137
"Autonomous agents must often detect affordances: the set of behaviors enabled by a situation. Affordance detection is particularly helpful in domains with large action spaces, allowing the agent to prune its search space by avoiding futile behaviors. This paper presents a method for affordance extraction via word embeddings trained on a Wikipedia corpus. The resulting word vectors are treated as a common knowledge database which can be queried using linear algebra. We apply this method to a reinforcement learning agent in a text-only environment and show that affordance-based action selection improves performance most of the time. Our method increases the computational complexity of each learning step but significantly reduces the total number of steps needed. In addition, the agent's action selections begin to resemble those a human would choose.",What can you do with a rock? Affordance extraction via word embeddings,Nancy Fulda and Daniel Ricks and Ben Murdoch and David Wingate,2017,Artificial Intelligence,1703.03429
"Programming by Example (PBE) targets at automatically inferring a computer program for accomplishing a certain task from sample input and output. In this paper, we propose a deep neural networks (DNN) based PBE model called Neural Programming by Example (NPBE), which can learn from input-output strings and induce programs that solve the string manipulation problems. Our NPBE model has four neural network based components: a string encoder, an input-output analyzer, a program generator, and a symbol selector. We demonstrate the effectiveness of NPBE by training it end-to-end to solve some common string manipulation problems in spreadsheet systems. The results show that our model can induce string manipulation programs effectively. Our work is one step towards teaching DNN to generate computer programs.",Neural Programming by Example,"Chengxun Shu, Hongyu Zhang",2017,Artificial Intelligence,1703.04990
"Since Alan Turing envisioned Artificial Intelligence (AI) [1], a major driving force behind technical progress has been competition with human cognition. Historical milestones have been frequently associated with computers matching or outperforming humans in difficult cognitive tasks (e.g. face recognition [2], personality classification [3], driving cars [4], or playing video games [5]), or defeating humans in strategic zero-sum encounters (e.g. Chess [6], Checkers [7], Jeopardy! [8], Poker [9], or Go [10]). In contrast, less attention has been given to developing autonomous machines that establish mutually cooperative relationships with people who may not share the machine's preferences. A main challenge has been that human cooperation does not require sheer computational power, but rather relies on intuition [11], cultural norms [12], emotions and signals [13, 14, 15, 16], and pre-evolved dispositions toward cooperation [17], common-sense mechanisms that are difficult to encode in machines for arbitrary contexts. Here, we combine a state-of-the-art machine-learning algorithm with novel mechanisms for generating and acting on signals to produce a new learning algorithm that cooperates with people and other machines at levels that rival human cooperation in a variety of two-player repeated stochastic games. This is the first general-purpose algorithm that is capable, given a description of a previously unseen game environment, of learning to cooperate with people within short timescales in scenarios previously unanticipated by algorithm designers. This is achieved without complex opponent modeling or higher-order theories of mind, thus showing that flexible, fast, and general human-machine cooperation is computationally achievable using a non-trivial, but ultimately simple, set of algorithmic mechanisms.",Cooperating with Machines,"Jacob W. Crandall, Mayada Oudah, Tennom, Fatimah Ishowo-Oloko, Sherief Abdallah, Jean-Fran\c{c}ois Bonnefon, Manuel Cebrian, Azim Shariff, Michael A. Goodrich, and Iyad Rahwan",2018,Artificial Intelligence,1703.06207
"Catastrophic forgetting is of special importance in reinforcement learning, as the data distribution is generally non-stationary over time. We study and compare several pseudorehearsal approaches for Q-learning with function approximation in a pole balancing task. We have found that pseudorehearsal seems to assist learning even in such very simple problems, given proper initialization of the rehearsal parameters.",Pseudorehearsal in value function approximation,"Vladimir Marochko, Leonard Johard, Manuel Mazzara",2017,Artificial Intelligence,1703.07075
"Content marketing is todays one of the most remarkable approaches in the context of marketing processes of companies. Value of this kind of marketing has improved in time, thanks to the latest developments regarding to computer and communication technologies. Nowadays, especially social media based platforms have a great importance on enabling companies to design multimedia oriented, interactive content. But on the other hand, there is still something more to do for improved content marketing approaches. In this context, objective of this study is to focus on intelligent content marketing, which can be done by using artificial intelligence. Artificial Intelligence is todays one of the most remarkable research fields and it can be used easily as multidisciplinary. So, this study has aimed to discuss about its potential on improving content marketing. In detail, the study has enabled readers to improve their awareness about the intersection point of content marketing and artificial intelligence. Furthermore, the authors have introduced some example models of intelligent content marketing, which can be achieved by using current Web technologies and artificial intelligence techniques.",Improving content marketing processes with the approaches by artificial intelligence,"Utku Kose, Selcuk Sert",2017,Artificial Intelligence,1704.02114
"Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.",Stochastic Neural Networks for Hierarchical Reinforcement Learning,"Carlos Florensa, Yan Duan, Pieter Abbeel",2017,Artificial Intelligence,1704.03012
"The AGM model is the most remarkable framework for modeling belief revision. However, it is not perfect in all aspects. Paraconsistent belief revision, multi-agent belief revision and non-prioritized belief revision are three different extensions to AGM to address three important criticisms applied to it. In this article, we propose a framework based on AGM that takes a position in each of these categories. Also, we discuss some features of our framework and study the satisfiability of AGM postulates in this new context.",Source-Sensitive Belief Change,Shahab Ebrahimi,2017,Artificial Intelligence,1704.03396
"Monte Carlo Tree Search techniques have generally dominated General Video Game Playing, but recent research has started looking at Evolutionary Algorithms and their potential at matching Tree Search level of play or even outperforming these methods. Online or Rolling Horizon Evolution is one of the options available to evolve sequences of actions for planning in General Video Game Playing, but no research has been done up to date that explores the capabilities of the vanilla version of this algorithm in multiple games. This study aims to critically analyse the different configurations regarding population size and individual length in a set of 20 games from the General Video Game AI corpus. Distinctions are made between deterministic and stochastic games, and the implications of using superior time budgets are studied. Results show that there is scope for the use of these techniques, which in some configurations outperform Monte Carlo Tree Search, and also suggest that further research in these methods could boost their performance.",Analysis of Vanilla Rolling Horizon Evolution Parameters in General Video Game Playing,Raluca D. Gaina and Jialin Liu and Simon M. Lucas and Diego Perez-Liebana,2017,Artificial Intelligence,1704.07075
"The notion of events has occupied a central role in modeling and has an influence in computer science and philosophy. Recent developments in diagrammatic modeling have made it possible to examine conceptual representation of events. This paper explores some aspects of the notion of events that are produced by applying a new diagrammatic methodology with a focus on the interaction of events with such concepts as time and space, objects. The proposed description applies to abstract machines where events form the dynamic phases of a system. The results of this nontechnical research can be utilized in many fields where the notion of an event is typically used in interdisciplinary application.",Modeling Events as Machines,Sabah Al-Fedaghi,2017,Artificial Intelligence,1704.08588
"We present a rational analysis of curiosity, proposing that people's curiosity is driven by seeking stimuli that maximize their ability to make appropriate responses in the future. This perspective offers a way to unify previous theories of curiosity into a single framework. Experimental results confirm our model's predictions, showing how the relationship between curiosity and confidence can change significantly depending on the nature of the environment. Please refer to https://psyarxiv.com/wg5m6/ for a more updated version of this manuscript with a more detailed modeling section with extensive experiments.",A rational analysis of curiosity,Rachit Dubey and Thomas L. Griffiths,2017,Artificial Intelligence,1705.04351
"This paper proposes a design of hierarchical fuzzy inference tree (HFIT). An HFIT produces an optimum treelike structure, i.e., a natural hierarchical structure that accommodates simplicity by combining several low-dimensional fuzzy inference systems (FISs). Such a natural hierarchical structure provides a high degree of approximation accuracy. The construction of HFIT takes place in two phases. Firstly, a nondominated sorting based multiobjective genetic programming (MOGP) is applied to obtain a simple tree structure (a low complexity model) with a high accuracy. Secondly, the differential evolution algorithm is applied to optimize the obtained tree's parameters. In the derived tree, each node acquires a different input's combination, where the evolutionary process governs the input's combination. Hence, HFIT nodes are heterogeneous in nature, which leads to a high diversity among the rules generated by the HFIT. Additionally, the HFIT provides an automatic feature selection because it uses MOGP for the tree's structural optimization that accepts inputs only relevant to the knowledge contained in data. The HFIT was studied in the context of both type-1 and type-2 FISs, and its performance was evaluated through six application problems. Moreover, the proposed multiobjective HFIT was compared both theoretically and empirically with recently proposed FISs methods from the literature, such as McIT2FIS, TSCIT2FNN, SIT2FNN, RIT2FNS-WB, eT2FIS, MRIT2NFS, IT2FNN-SVR, etc. From the obtained results, it was found that the HFIT provided less complex and highly accurate models compared to the models produced by the most of other methods. Hence, the proposed HFIT is an efficient and competitive alternative to the other FISs for function approximation and feature selection.",Multiobjective Programming for Type-2 Hierarchical Fuzzy Inference Trees,"Varun Kumar Ojha, Vaclav Snasel, Ajith Abraham",2017,Artificial Intelligence,1705.05769
"In this work, we present a methodology that enables an agent to make efficient use of its exploratory actions by autonomously identifying possible objectives in its environment and learning them in parallel. The identification of objectives is achieved using an online and unsupervised adaptive clustering algorithm. The identified objectives are learned (at least partially) in parallel using Q-learning. Using a simulated agent and environment, it is shown that the converged or partially converged value function weights resulting from off-policy learning can be used to accumulate knowledge about multiple objectives without any additional exploration. We claim that the proposed approach could be useful in scenarios where the objectives are initially unknown or in real world scenarios where exploration is typically a time and energy intensive process. The implications and possible extensions of this work are also briefly discussed.",Identification and Off-Policy Learning of Multiple Objectives Using Adaptive Clustering,"Thommen George Karimpanal, Erik Wilhelm",2017,Artificial Intelligence,1705.06342
"The human reasoning process is seldom a one-way process from an input leading to an output. Instead, it often involves a systematic deduction by ruling out other possible outcomes as a self-checking mechanism. In this paper, we describe the design of a hybrid neural network for logical learning that is similar to the human reasoning through the introduction of an auxiliary input, namely the indicators, that act as the hints to suggest logical outcomes. We generate these indicators by digging into the hidden information buried underneath the original training data for direct or indirect suggestions. We used the MNIST data to demonstrate the design and use of these indicators in a convolutional neural network. We trained a series of such hybrid neural networks with variations of the indicators. Our results show that these hybrid neural networks are very robust in generating logical outcomes with inherently higher prediction accuracy than the direct use of the original input and output in apparent models. Such improved predictability with reassured logical confidence is obtained through the exhaustion of all possible indicators to rule out all illogical outcomes, which is not available in the apparent models. Our logical learning process can effectively cope with the unknown unknowns using a full exploitation of all existing knowledge available for learning. The design and implementation of the hints, namely the indicators, become an essential part of artificial intelligence for logical learning. We also introduce an ongoing application setup for this hybrid neural network in an autonomous grasping robot, namely as_DeepClaw, aiming at learning an optimized grasping pose through logical learning.",Logical Learning Through a Hybrid Neural Network with Auxiliary Inputs,Fang Wan and Chaoyang Song,2018,Artificial Intelligence,1705.08200
"In this paper we explore the theoretical boundaries of planning in a setting where no model of the agent's actions is given. Instead of an action model, a set of successfully executed plans are given and the task is to generate a plan that is safe, i.e., guaranteed to achieve the goal without failing. To this end, we show how to learn a conservative model of the world in which actions are guaranteed to be applicable. This conservative model is then given to an off-the-shelf classical planner, resulting in a plan that is guaranteed to achieve the goal. However, this reduction from a model-free planning to a model-based planning is not complete: in some cases a plan will not be found even when such exists. We analyze the relation between the number of observed plans and the likelihood that our conservative approach will indeed fail to solve a solvable problem. Our analysis show that the number of trajectories needed scales gracefully.","Efficient, Safe, and Probably Approximately Complete Learning of Action Models",Roni Stern and Brendan Juba,2017,Artificial Intelligence,1705.08961
"Humans are expert in the amount of sensory data they deal with each moment. Human brain not only analyses these data but also starts synthesizing new information from the existing data. The current age Big-data systems are needed not just to analyze data but also to come up new interpretation. We believe that the pivotal ability in human brain which enables us to do this is what is known as ""intuition"". Here, we present an intuition based architecture for big data analysis and synthesis.",ICABiDAS: Intuition Centred Architecture for Big Data Analysis and Synthesis,Amit Kumar Mishra,2018,Artificial Intelligence,1706.00638
"This paper introduces a cognitive architecture for a humanoid robot to engage in a proactive, mixed-initiative exploration and manipulation of its environment, where the initiative can originate from both the human and the robot. The framework, based on a biologically-grounded theory of the brain and mind, integrates a reactive interaction engine, a number of state-of-the-art perceptual and motor learning algorithms, as well as planning abilities and an autobiographical memory. The architecture as a whole drives the robot behavior to solve the symbol grounding problem, acquire language capabilities, execute goal-oriented behavior, and express a verbal narrative of its own experience in the world. We validate our approach in human-robot interaction experiments with the iCub humanoid robot, showing that the proposed cognitive architecture can be applied in real time within a realistic scenario and that it can be used with naive users.",DAC-h3: A Proactive Robot Cognitive Architecture to Acquire and Express Knowledge About the World and the Self,"Cl\'ement Moulin-Frier, Tobias Fischer, Maxime Petit, Gr\'egoire Pointeau, Jordi-Ysard Puigbo, Ugo Pattacini, Sock Ching Low, Daniel Camilleri, Phuong Nguyen, Matej Hoffmann, Hyung Jin Chang, Martina Zambelli, Anne-Laure Mealier, Andreas Damianou, Giorgio Metta, Tony J. Prescott, Yiannis Demiris, Peter Ford Dominey, Paul F. M. J. Verschure",2018,Artificial Intelligence,1706.03661
"Inductive Logic Programming (ILP) combines rule-based and statistical artificial intelligence methods, by learning a hypothesis comprising a set of rules given background knowledge and constraints for the search space. We focus on extending the XHAIL algorithm for ILP which is based on Answer Set Programming and we evaluate our extensions using the Natural Language Processing application of sentence chunking. With respect to processing natural language, ILP can cater for the constant change in how we use language on a daily basis. At the same time, ILP does not require huge amounts of training examples such as other statistical methods and produces interpretable results, that means a set of rules, which can be analysed and tweaked if necessary. As contributions we extend XHAIL with (i) a pruning mechanism within the hypothesis generalisation algorithm which enables learning from larger datasets, (ii) a better usage of modern solver technology using recently developed optimisation methods, and (iii) a time budget that permits the usage of suboptimal results. We evaluate these improvements on the task of sentence chunking using three datasets from a recent SemEval competition. Results show that our improvements allow for learning on bigger datasets with results that are of similar quality to state-of-the-art systems on the same task. Moreover, we compare the hypotheses obtained on datasets to gain insights on the structure of each dataset.",Improving Scalability of Inductive Logic Programming via Pruning and Best-Effort Optimisation,"Mishal Kazmi and Peter Sch\""uller and Y\""ucel Sayg{\i}n",2017,Artificial Intelligence,1706.05171
"Answer Set Programming (ASP) is a well-established declarative paradigm. One of the successes of ASP is the availability of efficient systems. State-of-the-art systems are based on the ground+solve approach. In some applications this approach is infeasible because the grounding of one or few constraints is expensive. In this paper, we systematically compare alternative strategies to avoid the instantiation of problematic constraints, that are based on custom extensions of the solver. Results on real and synthetic benchmarks highlight some strengths and weaknesses of the different strategies. (Under consideration for acceptance in TPLP, ICLP 2017 Special Issue.)","Constraints, Lazy Constraints, or Propagators in ASP Solving: An Empirical Analysis","Bernardo Cuteri, Carmine Dodaro, Francesco Ricca, Peter Sch\""uller",2017,Artificial Intelligence,1707.04027
"The paper investigates navigability with imperfect information. It shows that the properties of navigability with perfect recall are exactly those captured by Armstrong's axioms from the database theory. If the assumption of perfect recall is omitted, then Armstrong's transitivity axiom is not valid, but it can be replaced by two new weaker principles. The main technical results are soundness and completeness theorems for the logical systems describing properties of navigability with and without perfect recall.",Armstrong's Axioms and Navigation Strategies,Kaya Deuser and Pavel Naumov,2018,Artificial Intelligence,1707.04106
"As intelligent systems gain autonomy and capability, it becomes vital to ensure that their objectives match those of their human users; this is known as the value-alignment problem. In robotics, value alignment is key to the design of collaborative robots that can integrate into human workflows, successfully inferring and adapting to their users' objectives as they go. We argue that a meaningful solution to value alignment must combine multi-agent decision theory with rich mathematical models of human cognition, enabling robots to tap into people's natural collaborative capabilities. We present a solution to the cooperative inverse reinforcement learning (CIRL) dynamic game based on well-established cognitive models of decision making and theory of mind. The solution captures a key reciprocity relation: the human will not plan her actions in isolation, but rather reason pedagogically about how the robot might learn from them; the robot, in turn, can anticipate this and interpret the human's actions pragmatically. To our knowledge, this work constitutes the first formal analysis of value alignment grounded in empirically validated cognitive models.",Pragmatic-Pedagogic Value Alignment,"Jaime F. Fisac, Monica A. Gates, Jessica B. Hamrick, Chang Liu, Dylan Hadfield-Menell, Malayandi Palaniappan, Dhruv Malik, S. Shankar Sastry, Thomas L. Griffiths, and Anca D. Dragan",2017,Artificial Intelligence,1707.06354
"Sequential pattern mining algorithms are widely used to explore care pathways database, but they generate a deluge of patterns, mostly redundant or useless. Clinicians need tools to express complex mining queries in order to generate less but more significant patterns. These algorithms are not versatile enough to answer complex clinician queries. This article proposes to apply a declarative pattern mining approach based on Answer Set Programming paradigm. It is exemplified by a pharmaco-epidemiological study investigating the possible association between hospitalization for seizure and antiepileptic drug switch from a french medico-administrative database.",Declarative Sequential Pattern Mining of Care Pathways,"Thomas Guyet (1), Andr\'e Happe, Yann Dauxais (2) ((1) LACODAM, (2) UR1)",2017,Artificial Intelligence,1707.08342
"Nowadays, there are many approaches designed for the task of detecting communities in social networks. Among them, some methods only consider the topological graph structure, while others take use of both the graph structure and the node attributes. In real-world networks, there are many uncertain and noisy attributes in the graph. In this paper, we will present how we detect communities in graphs with uncertain attributes in the first step. The numerical, probabilistic as well as evidential attributes are generated according to the graph structure. In the second step, some noise will be added to the attributes. We perform experiments on graphs with different types of attributes and compare the detection results in terms of the Normalized Mutual Information (NMI) values. The experimental results show that the clustering with evidential attributes gives better results comparing to those with probabilistic and numerical attributes. This illustrates the advantages of evidential attributes.",The Advantage of Evidential Attributes in Social Networks,"Salma Ben Dhaou (LARODEC, DRUID), Kuang Zhou (NPU), Mouloud Kharoune (DRUID), Arnaud Martin (DRUID), Boutheina Ben Yaghlane (LARODEC)",2017,Artificial Intelligence,1707.08418
"Traffic speed is a key indicator for the efficiency of an urban transportation system. Accurate modeling of the spatiotemporally varying traffic speed thus plays a crucial role in urban planning and development. This paper addresses the problem of efficient fine-grained traffic speed prediction using big traffic data obtained from static sensors. Gaussian processes (GPs) have been previously used to model various traffic phenomena, including flow and speed. However, GPs do not scale with big traffic data due to their cubic time complexity. In this work, we address their efficiency issues by proposing local GPs to learn from and make predictions for correlated subsets of data. The main idea is to quickly group speed variables in both spatial and temporal dimensions into a finite number of clusters, so that future and unobserved traffic speed queries can be heuristically mapped to one of such clusters. A local GP corresponding to that cluster can then be trained on the fly to make predictions in real-time. We call this method localization. We use non-negative matrix factorization for localization and propose simple heuristics for cluster mapping. We additionally leverage on the expressiveness of GP kernel functions to model road network topology and incorporate side information. Extensive experiments using real-world traffic data collected in the two U.S. cities of Pittsburgh and Washington, D.C., show that our proposed local GPs significantly improve both runtime performances and prediction accuracies compared to the baseline global and local GPs.",Local Gaussian Processes for Efficient Fine-Grained Traffic Speed Prediction,"Truc Viet Le, Richard J. Oentaryo, Siyuan Liu, Hoong Chuin Lau",2017,Artificial Intelligence,1708.08079
"Traditionally psychometric tests were used for profiling incoming workers. These methods use DISC profiling method to classify people into distinct personality types, which are further used to predict if a person may be a possible fit to the organizational culture. This concept is taken further by introducing a novel technique to predict if a particular pair of an incoming worker and the manager being assigned are compatible at a psychological scale. This is done using multilayer perceptron neural network which can be adaptively trained to showcase the true nature of the compatibility index. The proposed prototype model is used to quantify the relevant attributes, use them to train the prediction engine, and to define the data pipeline required for it.",An Automated Compatibility Prediction Engine using DISC Theory Based Classification and Neural Networks,"Chandrasekaran Anirudh Bhardwaj, Megha Mishra and Sweetlin Hemalatha",2017,Artificial Intelligence,1709.00539
"We present theoretical analysis and a suite of tests and procedures for addressing a broad class of redundant and misleading association rules we call \emph{specious rules}. Specious dependencies, also known as \emph{spurious}, \emph{apparent}, or \emph{illusory associations}, refer to a well-known phenomenon where marginal dependencies are merely products of interactions with other variables and disappear when conditioned on those variables. The most extreme example is Yule-Simpson's paradox where two variables present positive dependence in the marginal contingency table but negative in all partial tables defined by different levels of a confounding factor. It is accepted wisdom that in data of any nontrivial dimensionality it is infeasible to control for all of the exponentially many possible confounds of this nature. In this paper, we consider the problem of specious dependencies in the context of statistical association rule mining. We define specious rules and show they offer a unifying framework which covers many types of previously proposed redundant or misleading association rules. After theoretical analysis, we introduce practical algorithms for detecting and pruning out specious association rules efficiently under many key goodness measures, including mutual information and exact hypergeometric probabilities. We demonstrate that the procedure greatly reduces the number of associations discovered, providing an elegant and effective solution to the problem of association mining discovering large numbers of misleading and redundant rules.",Specious rules: an efficient and effective unifying method for removing misleading and uninformative patterns in association rule mining,"Wilhelmiina H\""am\""al\""ainen and Geoffrey I. Webb",2017,Artificial Intelligence,1709.03915
"The incorporation of macro-actions (temporally extended actions) into multi-agent decision problems has the potential to address the curse of dimensionality associated with such decision problems. Since macro-actions last for stochastic durations, multiple agents executing decentralized policies in cooperative environments must act asynchronously. We present an algorithm that modifies generalized advantage estimation for temporally extended actions, allowing a state-of-the-art policy optimization algorithm to optimize policies in Dec-POMDPs in which agents act asynchronously. We show that our algorithm is capable of learning optimal policies in two cooperative domains, one involving real-time bus holding control and one involving wildfire fighting with unmanned aircraft. Our algorithm works by framing problems as ""event-driven decision processes,"" which are scenarios in which the sequence and timing of actions and events are random and governed by an underlying stochastic process. In addition to optimizing policies with continuous state and action spaces, our algorithm also facilitates the use of event-driven simulators, which do not require time to be discretized into time-steps. We demonstrate the benefit of using event-driven simulation in the context of multiple agents taking asynchronous actions. We show that fixed time-step simulation risks obfuscating the sequence in which closely separated events occur, adversely affecting the policies learned. In addition, we show that arbitrarily shrinking the time-step scales poorly with the number of agents.",Deep Reinforcement Learning for Event-Driven Multi-Agent Decision Processes,"Kunal Menda, Yi-Chun Chen, Justin Grana, James W. Bono, Brendan D. Tracey, Mykel J. Kochenderfer, and David Wolpert",2019,Artificial Intelligence,1709.06656
"We present PRM-RL, a hierarchical method for long-range navigation task completion that combines sampling based path planning with reinforcement learning (RL). The RL agents learn short-range, point-to-point navigation policies that capture robot dynamics and task constraints without knowledge of the large-scale topology. Next, the sampling-based planners provide roadmaps which connect robot configurations that can be successfully navigated by the RL agent. The same RL agents are used to control the robot under the direction of the planning, enabling long-range navigation. We use the Probabilistic Roadmaps (PRMs) for the sampling-based planner. The RL agents are constructed using feature-based and deep neural net policies in continuous state and action spaces. We evaluate PRM-RL, both in simulation and on-robot, on two navigation tasks with non-trivial robot dynamics: end-to-end differential drive indoor navigation in office environments, and aerial cargo delivery in urban environments with load displacement constraints. Our results show improvement in task completion over both RL agents on their own and traditional sampling-based planners. In the indoor navigation task, PRM-RL successfully completes up to 215 m long trajectories under noisy sensor conditions, and the aerial cargo delivery completes flights over 1000 m without violating the task constraints in an environment 63 million times larger than used in training.",PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-based Planning,"Aleksandra Faust, Oscar Ramirez, Marek Fiser, Kenneth Oslund, Anthony Francis, James Davidson, and Lydia Tapia",2018,Artificial Intelligence,1710.03937
"Social dilemmas, where mutual cooperation can lead to high payoffs but participants face incentives to cheat, are ubiquitous in multi-agent interaction. We wish to construct agents that cooperate with pure cooperators, avoid exploitation by pure defectors, and incentivize cooperation from the rest. However, often the actions taken by a partner are (partially) unobserved or the consequences of individual actions are hard to predict. We show that in a large class of games good strategies can be constructed by conditioning one's behavior solely on outcomes (ie. one's past rewards). We call this consequentialist conditional cooperation. We show how to construct such strategies using deep reinforcement learning techniques and demonstrate, both analytically and experimentally, that they are effective in social dilemmas beyond simple matrix games. We also show the limitations of relying purely on consequences and discuss the need for understanding both the consequences of and the intentions behind an action.",Consequentialist conditional cooperation in social dilemmas with imperfect information,"Alexander Peysakhovich, Adam Lerer",2018,Artificial Intelligence,1710.06975
"Little by little, newspapers are revealing the bright future that Artificial Intelligence (AI) is building. Intelligent machines will help everywhere. However, this bright future has a dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers will need financial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of AI is sustained by the biggest knowledge theft of the recent years. Learning AI machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, these workers are digging their own graves. In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI) as a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward aware and unaware knowledge producers with a different scheme: decisions of AI systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions. As modern Robin Hoods, HIT-AI researchers should fight for a fairer Artificial Intelligence that gives back what it steals.",Human-in-the-loop Artificial Intelligence,Fabio Massimo Zanzotto,2019,Artificial Intelligence,1710.08191
"The Advent of the Internet-of-Things (IoT) paradigm has brought opportunities to solve many real-world problems. Energy management, for example, has attracted huge interest from academia, industries, governments and regulatory bodies. It involves collecting energy usage data, analyzing it, and optimizing the energy consumption by applying control strategies. However, in industrial environments, performing such optimization is not trivial. The changes in business rules, process control, and customer requirements make it much more challenging. In this paper, a Semantic Rules Engine (SRE) for industrial gateways is presented that allows implementing dynamic and flexible rule-based control strategies. It is simple, expressive, and allows managing rules on-the-fly without causing any service interruption. Additionally, it can handle semantic queries and provide results by inferring additional knowledge from previously defined concepts in ontologies. SRE has been validated and tested on different hardware platforms and in commercial products. Performance evaluations are also presented to validate its conformance to the customer requirements.",SRE: Semantic Rules Engine For the Industrial Internet-Of-Things Gateways,"Charbel El Kaed, Imran Khan, Andre Van Den Berg, Hicham Hossayni and Christophe Saint-Marcel",2017,Artificial Intelligence,1710.09627
"The semantic web has received many contributions of researchers as ontologies which, in this context, i.e. within RDF linked data, are formalized conceptualizations that might use different protocols, such as RDFS, OWL DL and OWL FULL. In this article, we describe new expressive techniques which were found necessary after elaborating dozens of OWL ontologies for the scientific academy, the State and the civil society. They consist in: 1) stating possible uses a property might have without incurring into axioms or restrictions; 2) assigning a level of priority for an element (class, property, triple); 3) correct depiction in diagrams of relations between classes, between individuals which are imperative, and between individuals which are optional; 4) a convenient association between OWL classes and SKOS concepts. We propose specific rules to accomplish these enhancements and exemplify both its use and the difficulties that arise because these techniques are currently not established as standards to the ontology designer.",Enhancements of linked data expressiveness for ontologies,Renato Fabbri,2017,Artificial Intelligence,1710.09952
"There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.",Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR,"Sandra Wachter, Brent Mittelstadt, Chris Russell",2018,Artificial Intelligence,1711.00399
"In this paper we analyse the benefits of incorporating interval-valued fuzzy sets into the Bousi-Prolog system. A syntax, declarative semantics and im- plementation for this extension is presented and formalised. We show, by using potential applications, that fuzzy logic programming frameworks enhanced with them can correctly work together with lexical resources and ontologies in order to improve their capabilities for knowledge representation and reasoning.","On the incorporation of interval-valued fuzzy sets into the Bousi-Prolog system: declarative semantics, implementation and applications","Clemente Rubio-Manzano, Martin Pereira-Fari\~na",2018,Artificial Intelligence,1711.03147
"Cooperative multi-agent planning (MAP) is a relatively recent research field that combines technologies, algorithms and techniques developed by the Artificial Intelligence Planning and Multi-Agent Systems communities. While planning has been generally treated as a single-agent task, MAP generalizes this concept by considering multiple intelligent agents that work cooperatively to develop a course of action that satisfies the goals of the group. This paper reviews the most relevant approaches to MAP, putting the focus on the solvers that took part in the 2015 Competition of Distributed and Multi-Agent Planning, and classifies them according to their key features and relative performance.",Cooperative Multi-Agent Planning: A Survey,"Alejandro Torre\~no, Eva Onaindia, Anton\'in Komenda, Michal \v{S}tolba",2017,Artificial Intelligence,1711.09057
"Interval Pairwise Comparison Matrices have been widely used to account for uncertain statements concerning the preferences of decision makers. Several approaches have been proposed in the literature, such as multiplicative and fuzzy interval matrices. In this paper, we propose a general unified approach to Interval Pairwise Comparison Matrices, based on Abelian linearly ordered groups. In this framework, we generalize some consistency conditions provided for multiplicative and/or fuzzy interval pairwise comparison matrices and provide inclusion relations between them. Then, we provide a concept of distance between intervals that, together with a notion of mean defined over real continuous Abelian linearly ordered groups, allows us to provide a consistency index and an indeterminacy index. In this way, by means of suitable isomorphisms between Abelian linearly ordered groups, we will be able to compare the inconsistency and the indeterminacy of different kinds of Interval Pairwise Comparison Matrices, e.g. multiplicative, additive, and fuzzy, on a unique Cartesian coordinate system.",A general unified framework for interval pairwise comparison matrices,Bice Cavallo and Matteo Brunelli,2018,Artificial Intelligence,1711.09441
"Artificial Intelligence is a central topic in the computer science curriculum. From the year 2011 a project-based learning methodology based on computer games has been designed and implemented into the intelligence artificial course at the University of the Bio-Bio. The project aims to develop software-controlled agents (bots) which are programmed by using heuristic algorithms seen during the course. This methodology allows us to obtain good learning results, however several challenges have been founded during its implementation. In this paper we show how linguistic descriptions of data can help to provide students and teachers with technical and personalized feedback about the learned algorithms. Algorithm behavior profile and a new Turing test for computer games bots based on linguistic modelling of complex phenomena are also proposed in order to deal with such challenges. In order to show and explore the possibilities of this new technology, a web platform has been designed and implemented by one of authors and its incorporation in the process of assessment allows us to improve the teaching learning process.","How linguistic descriptions of data can help to the teaching-learning process in higher education, case of study: artificial intelligence","Clemente Rubio-Manzano, Tomas Lermanda Senoceain",2019,Artificial Intelligence,1711.09744
"Recently, model-free reinforcement learning algorithms have been shown to solve challenging problems by learning from extensive interaction with the environment. A significant issue with transferring this success to the robotics domain is that interaction with the real world is costly, but training on limited experience is prone to overfitting. We present a method for learning to navigate, to a fixed goal and in a known environment, on a mobile robot. The robot leverages an interactive world model built from a single traversal of the environment, a pre-trained visual feature encoder, and stochastic environmental augmentation, to demonstrate successful zero-shot transfer under real-world environmental variations without fine-tuning.",One-Shot Reinforcement Learning for Robot Navigation with Interactive Replay,"Jake Bruce, Niko Suenderhauf, Piotr Mirowski, Raia Hadsell, Michael Milford",2017,Artificial Intelligence,1711.10137
"Web-based human trafficking activity has increased in recent years but it remains sparsely dispersed among escort advertisements and difficult to identify due to its often-latent nature. The use of intelligent systems to detect trafficking can thus have a direct impact on investigative resource allocation and decision-making, and, more broadly, help curb a widespread social problem. Trafficking detection involves assigning a normalized score to a set of escort advertisements crawled from the Web -- a higher score indicates a greater risk of trafficking-related (involuntary) activities. In this paper, we define and study the problem of trafficking detection and present a trafficking detection pipeline architecture developed over three years of research within the DARPA Memex program. Drawing on multi-institutional data, systems, and experiences collected during this time, we also conduct post hoc bias analyses and present a bias mitigation plan. Our findings show that, while automatic trafficking detection is an important application of AI for social good, it also provides cautionary lessons for deploying predictive machine learning algorithms without appropriate de-biasing. This ultimately led to integration of an interpretable solution into a search system that contains over 100 million advertisements and is used by over 200 law enforcement agencies to investigate leads.",Always Lurking: Understanding and Mitigating Bias in Online Human Trafficking Detection,"Kyle Hundman, Thamme Gowda, Mayank Kejriwal, and Benedikt Boecking",2018,Artificial Intelligence,1712.00846
"Literature involving preferences of artificial agents or human beings often assume their preferences can be represented using a complete transitive binary relation. Much has been written however on different models of preferences. We review some of the reasons that have been put forward to justify more complex modeling, and review some of the techniques that have been proposed to obtain models of such preferences.",Reasons and Means to Model Preferences as Incomplete,"Olivier Cailloux (LAMSADE), S\'ebastien Destercke (Labex MS2T)",2017,Artificial Intelligence,1801.01657
"Artificial intelligence (AI) is an extensive scientific discipline which enables computer systems to solve problems by emulating complex biological processes such as learning, reasoning and self-correction. This paper presents a comprehensive review of the application of AI techniques for improving performance of optical communication systems and networks. The use of AI-based techniques is first studied in applications related to optical transmission, ranging from the characterization and operation of network components to performance monitoring, mitigation of nonlinearities, and quality of transmission estimation. Then, applications related to optical network control and management are also reviewed, including topics like optical network planning and operation in both transport and access networks. Finally, the paper also presents a summary of opportunities and challenges in optical networking where AI is expected to play a key role in the near future.",Artificial Intelligence (AI) Methods in Optical Networks: A Comprehensive Survey,"Javier Mata, Ignacio de Miguel, Ram\'o n J. Dur\'a n, Noem\'i Merayo, Sandeep Kumar Singh, Admela Jukan, Mohit Chamania",2018,Artificial Intelligence,1801.01704
"We propose a deep learning model - Probabilistic Prognostic Estimates of Survival in Metastatic Cancer Patients (PPES-Met) for estimating short-term life expectancy (3 months) of the patients by analyzing free-text clinical notes in the electronic medical record, while maintaining the temporal visit sequence. In a single framework, we integrated semantic data mapping and neural embedding technique to produce a text processing method that extracts relevant information from heterogeneous types of clinical notes in an unsupervised manner, and we designed a recurrent neural network to model the temporal dependency of the patient visits. The model was trained on a large dataset (10,293 patients) and validated on a separated dataset (1818 patients). Our method achieved an area under the ROC curve (AUC) of 0.89. To provide explain-ability, we developed an interactive graphical tool that may improve physician understanding of the basis for the model's predictions. The high accuracy and explain-ability of the PPES-Met model may enable our model to be used as a decision support tool to personalize metastatic cancer treatment and provide valuable assistance to the physicians.",Abstract: Probabilistic Prognostic Estimates of Survival in Metastatic Cancer Patients,"Imon Banerjee, Michael Francis Gensheimer, Douglas J. Wood, Solomon Henry, Daniel Chang, Daniel L. Rubin",2018,Artificial Intelligence,1801.03058
"Pairwise comparison matrices often exhibit inconsistency, therefore many indices have been suggested to measure their deviation from a consistent matrix. A set of axioms has been proposed recently that is required to be satisfied by any reasonable inconsistency index. This set seems to be not exhaustive as illustrated by an example, hence it is expanded by adding two new properties. All axioms are considered on the set of triads, pairwise comparison matrices with three alternatives, which is the simplest case of inconsistency. We choose the logically independent properties and prove that they characterize, that is, uniquely determine the inconsistency ranking induced by most inconsistency indices that coincide on this restricted domain. Since triads play a prominent role in a number of inconsistency indices, our results can also contribute to the measurement of inconsistency for pairwise comparison matrices with more than three alternatives.",Axiomatizations of inconsistency indices for triads,L\'aszl\'o Csat\'o,2019,Artificial Intelligence,1801.03355
"Humans use signs, e.g., sentences in a spoken language, for communication and thought. Hence, symbol systems like language are crucial for our communication with other agents and adaptation to our real-world environment. The symbol systems we use in our human society adaptively and dynamically change over time. In the context of artificial intelligence (AI) and cognitive systems, the symbol grounding problem has been regarded as one of the central problems related to {\it symbols}. However, the symbol grounding problem was originally posed to connect symbolic AI and sensorimotor information and did not consider many interdisciplinary phenomena in human communication and dynamic symbol systems in our society, which semiotics considered. In this paper, we focus on the symbol emergence problem, addressing not only cognitive dynamics but also the dynamics of symbol systems in society, rather than the symbol grounding problem. We first introduce the notion of a symbol in semiotics from the humanities, to leave the very narrow idea of symbols in symbolic AI. Furthermore, over the years, it became more and more clear that symbol emergence has to be regarded as a multifaceted problem. Therefore, secondly, we review the history of the symbol emergence problem in different fields, including both biological and artificial systems, showing their mutual relations. We summarize the discussion and provide an integrative viewpoint and comprehensive overview of symbol emergence in cognitive systems. Additionally, we describe the challenges facing the creation of cognitive systems that can be part of symbol emergence systems.",Symbol Emergence in Cognitive Developmental Systems: a Survey,"Tadahiro Taniguchi, Emre Ugur, Matej Hoffmann, Lorenzo Jamone, Takayuki Nagai, Benjamin Rosman, Toshihiko Matsuka, Naoto Iwahashi, Erhan Oztop, Justus Piater, Florentin W\""org\""otter",2019,Artificial Intelligence,1801.08829
"Semantic Web Rule Language (SWRL) combines OWL (Web Ontology Language) ontologies with Horn Logic rules of the Rule Markup Language (RuleML) family. Being supported by ontology editors, rule engines and ontology reasoners, it has become a very popular choice for developing rule-based applications on top of ontologies. However, SWRL is probably not go-ing to become a WWW Consortium standard, prohibiting industrial acceptance. On the other hand, SPIN (SPARQL Inferencing Notation) has become a de-facto industry standard to rep-resent SPARQL rules and constraints on Semantic Web models, building on the widespread acceptance of SPARQL (SPARQL Protocol and RDF Query Language). In this paper, we ar-gue that the life of existing SWRL rule-based ontology applications can be prolonged by con-verting them to SPIN. To this end, we have developed the SWRL2SPIN tool in Prolog that transforms SWRL rules into SPIN rules, considering the object-orientation of SPIN, i.e. linking rules to the appropriate ontology classes and optimizing them, as derived by analysing the rule conditions.",SWRL2SPIN: A tool for transforming SWRL rule bases in OWL ontologies to object-oriented SPIN rules,Nick Bassiliades,2020,Artificial Intelligence,1801.09061
"We consider a team of reinforcement learning agents that concurrently learn to operate in a common environment. We identify three properties - adaptivity, commitment, and diversity - which are necessary for efficient coordinated exploration and demonstrate that straightforward extensions to single-agent optimistic and posterior sampling approaches fail to satisfy them. As an alternative, we propose seed sampling, which extends posterior sampling in a manner that meets these requirements. Simulation results investigate how per-agent regret decreases as the number of agents grows, establishing substantial advantages of seed sampling over alternative exploration schemes.",Coordinated Exploration in Concurrent Reinforcement Learning,"Maria Dimakopoulou, Benjamin Van Roy",2018,Artificial Intelligence,1802.01282
"The Semantic Web aims at representing knowledge about the real world at web scale - things, their attributes and relationships among them can be represented as nodes and edges in an inter-linked semantic graph. In the presence of noisy data, as is typical of data on the Semantic Web, a software Agent needs to be able to robustly infer one or more associated actionable classes for the individuals in order to act automatically on it. We model this problem as a multi-label classification task where we want to robustly identify types of the individuals in a semantic graph such as DBpedia, which we use as an exemplary dataset on the Semantic Web. Our approach first extracts multiple features for the individuals using random walks and then performs multi-label classification using fully-connected Neural Networks. Through systematic exploration and experimentation, we identify the effect of hyper-parameters of the feature extraction and the fully-connected Neural Network structure on the classification performance. Our final results show that our method performs better than state-of-the-art inferencing systems like SDtype and SLCN, from which we can conclude that random-walk-based feature extraction of individuals and their multi-label classification using Deep Neural Networks is a promising alternative to these systems for type classification of individuals on the Semantic Web. The main contribution of our work is to introduce a novel approach that allows us to use Deep Neural Networks to identify types of individuals in a noisy semantic graph by extracting features using random walks",Classification of Things in DBpedia using Deep Neural Networks,Rahul Parundekar,2018,Artificial Intelligence,1802.02528
"We propose to use a supervised machine learning technique to track the location of a mobile agent in real time. Hidden Markov Models are used to build artificial intelligence that estimates the unknown position of a mobile target moving in a defined environment. This narrow artificial intelligence performs two distinct tasks. First, it provides real-time estimation of the mobile agent's position using the forward algorithm. Second, it uses the Baum-Welch algorithm as a statistical learning tool to gain knowledge of the mobile target. Finally, an experimental environment is proposed, namely a video game that we use to test our artificial intelligence. We present statistical and graphical results to illustrate the efficiency of our method.",Narrow Artificial Intelligence with Machine Learning for Real-Time Estimation of a Mobile Agents Location Using Hidden Markov Models,C\'edric Beaulac and Fabrice Larribe,2017,Artificial Intelligence,1802.03417
"The problem of detecting bots, automated social media accounts governed by software but disguising as human users, has strong implications. For example, bots have been used to sway political elections by distorting online discourse, to manipulate the stock market, or to push anti-vaccine conspiracy theories that caused health epidemics. Most techniques proposed to date detect bots at the account level, by processing large amount of social media posts, and leveraging information from network structure, temporal dynamics, sentiment analysis, etc. In this paper, we propose a deep neural network based on contextual long short-term memory (LSTM) architecture that exploits both content and metadata to detect bots at the tweet level: contextual features are extracted from user metadata and fed as auxiliary input to LSTM deep nets processing the tweet text. Another contribution that we make is proposing a technique based on synthetic minority oversampling to generate a large labeled dataset, suitable for deep nets training, from a minimal amount of labeled data (roughly 3,000 examples of sophisticated Twitter bots). We demonstrate that, from just one single tweet, our architecture can achieve high classification accuracy (AUC > 96%) in separating bots from humans. We apply the same architecture to account-level bot detection, achieving nearly perfect classification accuracy (AUC > 99%). Our system outperforms previous state of the art while leveraging a small and interpretable set of features yet requiring minimal training data.",Deep Neural Networks for Bot Detection,"Sneha Kudugunta, Emilio Ferrara",2018,Artificial Intelligence,1802.04289
"We address the problem of inferring the causal direction between two variables by comparing the least-squares errors of the predictions in both possible directions. Under the assumption of an independence between the function relating cause and effect, the conditional noise distribution, and the distribution of the cause, we show that the errors are smaller in causal direction if both variables are equally scaled and the causal relation is close to deterministic. Based on this, we provide an easily applicable algorithm that only requires a regression in both possible causal directions and a comparison of the errors. The performance of the algorithm is compared with various related causal inference methods in different artificial and real-world data sets.",Analysis of cause-effect inference by comparing regression errors,"Patrick Bl\""obaum, Dominik Janzing, Takashi Washio, Shohei Shimizu, Bernhard Sch\""olkopf",2019,Artificial Intelligence,1802.06698
"This paper proposes a class of well-conditioned neural networks in which a unit amount of change in the inputs causes at most a unit amount of change in the outputs or any of the internal layers. We develop the known methodology of controlling Lipschitz constants to realize its full potential in maximizing robustness, with a new regularization scheme for linear layers, new ways to adapt nonlinearities and a new loss function. With MNIST and CIFAR-10 classifiers, we demonstrate a number of advantages. Without needing any adversarial training, the proposed classifiers exceed the state of the art in robustness against white-box L2-bounded adversarial attacks. They generalize better than ordinary networks from noisy data with partially random labels. Their outputs are quantitatively meaningful and indicate levels of confidence and generalization, among other desirable properties.",L2-Nonexpansive Neural Networks,"Haifeng Qian, Mark N. Wegman",2019,Artificial Intelligence,1802.07896
"Reasoning systems with too simple a model of the world and human intent are unable to consider potential negative side effects of their actions and modify their plans to avoid them (e.g., avoiding potential errors). However, hand-encoding the enormous and subtle body of facts that constitutes common sense into a knowledge base has proved too difficult despite decades of work. Distributed semantic vector spaces learned from large text corpora, on the other hand, can learn representations that capture shades of meaning of common-sense concepts and perform analogical and associational reasoning in ways that knowledge bases are too rigid to perform, by encoding concepts and the relations between them as geometric structures. These have, however, the disadvantage of being unreliable, poorly understood, and biased in their view of the world by the source material. This chapter will discuss how these approaches may be combined in a way that combines the best properties of each for understanding the world and human intentions in a richer way.",Semantic Vector Spaces for Broadening Consideration of Consequences,Douglas Summers Stay,2017,Artificial Intelligence,1802.08554
"What makes humans so good at solving seemingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play. Videos and the game manipulations are available at https://rach0012.github.io/humanRL_website/",Investigating Human Priors for Playing Video Games,"Rachit Dubey, Pulkit Agrawal, Deepak Pathak, Thomas L. Griffiths, and Alexei A. Efros",2018,Artificial Intelligence,1802.10217
"The tasks that an agent will need to solve often are not known during training. However, if the agent knows which properties of the environment are important then, after learning how its actions affect those properties, it may be able to use this knowledge to solve complex tasks without training specifically for them. Towards this end, we consider a setup in which an environment is augmented with a set of user defined attributes that parameterize the features of interest. We propose a method that learns a policy for transitioning between ""nearby"" sets of attributes, and maintains a graph of possible transitions. Given a task at test time that can be expressed in terms of a target set of attributes, and a current state, our model infers the attributes of the current state and searches over paths through attribute space to get a high level plan, and then uses its low level policy to execute the plan. We show in 3D block stacking, grid-world games, and StarCraft that our model is able to generalize to longer, more complex tasks at test time by composing simpler learned policies.",Composable Planning with Attributes,"Amy Zhang, Adam Lerer, Sainbayar Sukhbaatar, Rob Fergus, Arthur Szlam",2018,Artificial Intelligence,1803.00512
"The 6th International Workshop on Theorem proving components for Educational software (ThEdu'17) was held in Gothenburg, Sweden, on 6 Aug 2017. It was associated to the conference CADE26. Topics of interest include: methods of automated deduction applied to checking students' input; methods of automated deduction applied to prove post-conditions for particular problem solutions; combinations of deduction and computation enabling systems to propose next steps; automated provers specific for dynamic geometry systems; proof and proving in mathematics education. ThEdu'17 was a vibrant workshop, with one invited talk and eight contributions. It triggered the post-proceedings at hand.",Proceedings 6th International Workshop on Theorem proving components for Educational software,"Pedro Quaresma (University of Coimbra), Walther Neuper (IICM at Graz University of Technology)",2018,Artificial Intelligence,1803.00722
"This paper describes a process for combining patterns and features, to guide a search process and make predictions. It is based on the functionality that a human brain might have, which is a highly distributed network of simple neuronal components that can apply some level of matching and cross-referencing over retrieved patterns. The process uses memory in a dynamic way and it is directed through the pattern matching. The paper firstly describes the mechanisms for neuronal search, memory and prediction. The paper then presents a formal language for defining cognitive processes, that is, pattern-based sequences and transitions. The language can define an outer framework for concept sets that are linked to perform the cognitive act. The language also has a mathematical basis, allowing for the rule construction to be consistent. Now, both static memory and dynamic process hierarchies can be built as tree structures. The new information can also be used to further integrate the cognitive model and the ensemble-hierarchy structure becomes an essential part. A theory about linking can suggest that nodes in different regions link together when generally they represent the same thing.",New Ideas for Brain Modelling 5,Kieran Greer,2021,Artificial Intelligence,1803.01690
"Highly automated driving requires precise models of traffic participants. Many state of the art models are currently based on machine learning techniques. Among others, the required amount of labeled data is one major challenge. An autonomous learning process addressing this problem is proposed. The initial models are iteratively refined in three steps: (1) detection and context identification, (2) novelty detection and active learning and (3) online model adaption.",Highly Automated Learning for Improved Active Safety of Vulnerable Road Users,"Maarten Bieshaar and G\""unther Reitberger and Viktor Kre{\ss} and Stefan Zernetsch and Konrad Doll and Erich Fuchs and Bernhard Sick",2017,Artificial Intelligence,1803.03479
"This article explores the ideas that went into George Boole's development of an algebra for logical inference in his book The Laws of Thought. We explore in particular his wife Mary Boole's claim that he was deeply influenced by Indian logic and argue that his work was more than a framework for processing propositions. By exploring parallels between his work and Indian logic, we are able to explain several peculiarities of this work.",On the Algebra in Boole's Laws of Thought,Subhash Kak,2018,Artificial Intelligence,1803.04994
"The authors present an overview of a hierarchical framework for coordinating task- and motion-level operations in multirobot systems. Their framework is based on the idea of using simple temporal networks to simultaneously reason about precedence/causal constraints required for task-level coordination and simple temporal constraints required to take some kinematic constraints of robots into account. In the plan-generation phase, the framework provides a computationally scalable method for generating plans that achieve high-level tasks for groups of robots and take some of their kinematic constraints into account. In the plan-execution phase, the framework provides a method for absorbing an imperfect plan execution to avoid time-consuming re-planning in many cases. The authors use the multirobot path-planning problem as a case study to present the key ideas behind their framework for the long-term autonomy of multirobot systems.",Overview: A Hierarchical Framework for Plan Generation and Execution in Multi-Robot Systems,"Hang Ma, Wolfgang H\""onig, Liron Cohen, Tansel Uras, Hong Xu, T. K. Satish Kumar, Nora Ayanian, Sven Koenig",2017,Artificial Intelligence,1804.00038
"Navigating through unstructured environments is a basic capability of intelligent creatures, and thus is of fundamental interest in the study and development of artificial intelligence. Long-range navigation is a complex cognitive task that relies on developing an internal representation of space, grounded by recognisable landmarks and robust visual processing, that can simultaneously support continuous self-localisation (""I am here"") and a representation of the goal (""I am going there""). Building upon recent research that applies deep reinforcement learning to maze navigation problems, we present an end-to-end deep reinforcement learning approach that can be applied on a city scale. Recognising that successful navigation relies on integration of general policies with locale-specific knowledge, we propose a dual pathway architecture that allows locale-specific features to be encapsulated, while still enabling transfer to multiple cities. We present an interactive navigation environment that uses Google StreetView for its photographic content and worldwide coverage, and demonstrate that our learning method allows agents to learn to navigate multiple cities and to traverse to target destinations that may be kilometres away. The project webpage http://streetlearn.cc contains a video summarising our research and showing the trained agent in diverse city environments and on the transfer task, the form to request the StreetLearn dataset and links to further resources. The StreetLearn environment code is available at https://github.com/deepmind/streetlearn",Learning to Navigate in Cities Without a Map,"Piotr Mirowski, Matthew Koichi Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, Koray Kavukcuoglu, Andrew Zisserman, Raia Hadsell",2018,Artificial Intelligence,1804.00168
Fuzzy relation equations (FRE)are associated with the composition of binary fuzzy relations. In the present work FRE are used as a tool for studying the process of learning a new subject matter by a student class. A classroom application and other csuitable examples connected to the student learning of the derivative are also presented illustrating our results and useful conclusions are obtained.,A Study of Student Learning Skills Using Fuzzy Relation Equations,Michael Gr. Voskoglou,2018,Artificial Intelligence,1804.00421
"The theory of grey systems plays an important role in science,engineering and in the everyday life in general for handling approximate data. In the present paper grey numbers are used as a tool for assessing with linguistic expressions the mean performance of a group of objects participating in a certain activity. Two applications to student and football player assessment are also presented illustrating our results.",Application of Grey Numbers to Assessment Processes,"Michael Gr. Voskoglou, Yiannis Theodorou",2017,Artificial Intelligence,1804.00423
"We implement a automated tactical prover TacticToe on top of the HOL4 interactive theorem prover. TacticToe learns from human proofs which mathematical technique is suitable in each proof situation. This knowledge is then used in a Monte Carlo tree search algorithm to explore promising tactic-level proof paths. On a single CPU, with a time limit of 60 seconds, TacticToe proves 66.4 percent of the 7164 theorems in HOL4's standard library, whereas E prover with auto-schedule solves 34.5 percent. The success rate rises to 69.0 percent by combining the results of TacticToe and E prover.",TacticToe: Learning to Prove with Tactics,"Thibault Gauthier, Cezary Kaliszyk, Josef Urban, Ramana Kumar, Michael Norrish",2021,Artificial Intelligence,1804.00596
"In the past several years, we have taken advantage of a number of opportunities to advance the intersection of next generation high-performance computing AI and big data technologies through partnerships in precision medicine. Today we are in the throes of piecing together what is likely the most unique convergence of medical data and computer technologies. But more deeply, we observe that the traditional paradigm of computer simulation and prediction needs fundamental revision. This is the time for a number of reasons. We will review what the drivers are, why now, how this has been approached over the past several years, and where we are heading.",Precision Medicine as an Accelerator for Next Generation Cognitive Supercomputing,"Edmon Begoli, Jim Brase, Bambi DeLaRosa, Penelope Jones, Dimitri Kusnezov, Jason Paragas, Rick Stevens, Fred Streitz, Georgia Tourassi",2018,Artificial Intelligence,1804.11002
"The sine-cosine algorithm (SCA) is a new population-based meta-heuristic algorithm. In addition to exploiting sine and cosine functions to perform local and global searches (hence the name sine-cosine), the SCA introduces several random and adaptive parameters to facilitate the search process. Although it shows promising results, the search process of the SCA is vulnerable to local minima/maxima due to the adoption of a fixed switch probability and the bounded magnitude of the sine and cosine functions (from -1 to 1). In this paper, we propose a new hybrid Q-learning sine-cosine- based strategy, called the Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the switching probability. Instead, we rely on the Q-learning algorithm (based on the penalty and reward mechanism) to dynamically identify the best operation during runtime. Additionally, we integrate two new operations (L\'evy flight motion and crossover) into the QLSCA to facilitate jumping out of local minima/maxima and enhance the solution diversity. To assess its performance, we adopt the QLSCA for the combinatorial test suite minimization problem. Experimental results reveal that the QLSCA is statistically superior with regard to test suite size reduction compared to recent state-of-the-art strategies, including the original SCA, the particle swarm test generator (PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search strategy (CS) at the 95% confidence level. However, concerning the comparison with discrete particle swarm optimization (DPSO), there is no significant difference in performance at the 95% confidence level. On a positive note, the QLSCA statistically outperforms the DPSO in certain configurations at the 90% confidence level.",A Hybrid Q-Learning Sine-Cosine-based Strategy for Addressing the Combinatorial Test Suite Minimization Problem,Kamal Z. Zamli and Fakhrud Din and Bestoun S. Ahmed and Miroslav Bures,2018,Artificial Intelligence,1805.00873
"In many applications that involve processing high-dimensional data, it is important to identify a small set of entities that account for a significant fraction of detections. Rather than formalize this as a clustering problem, in which all detections must be grouped into hard or soft categories, we formalize it as an instance of the frequent items or heavy hitters problem, which finds groups of tightly clustered objects that have a high density in the feature space. We show that the heavy hitters formulation generates solutions that are more accurate and effective than the clustering formulation. In addition, we present a novel online algorithm for heavy hitters, called HAC, which addresses problems in continuous space, and demonstrate its effectiveness on real video and household domains.",Finding Frequent Entities in Continuous Data,"Ferran Alet, Rohan Chitnis, Leslie P. Kaelbling, Tomas Lozano-Perez",2018,Artificial Intelligence,1805.02874
"End-to-end trained neural networks (NNs) are a compelling approach to autonomous vehicle control because of their ability to learn complex tasks without manual engineering of rule-based decisions. However, challenging road conditions, ambiguous navigation situations, and safety considerations require reliable uncertainty estimation for the eventual adoption of full-scale autonomous vehicles. Bayesian deep learning approaches provide a way to estimate uncertainty by approximating the posterior distribution of weights given a set of training data. Dropout training in deep NNs approximates Bayesian inference in a deep Gaussian process and can thus be used to estimate model uncertainty. In this paper, we propose a Bayesian NN for end-to-end control that estimates uncertainty by exploiting feature map correlation during training. This approach achieves improved model fits, as well as tighter uncertainty estimates, than traditional element-wise dropout. We evaluate our algorithms on a challenging dataset collected over many different road types, times of day, and weather conditions, and demonstrate how uncertainties can be used in conjunction with a human controller in a parallel autonomous setting.",Spatial Uncertainty Sampling for End-to-End Control,"Alexander Amini, Ava Soleimany, Sertac Karaman, Daniela Rus",2018,Artificial Intelligence,1805.04829
"Structural Causal Models (SCMs) provide a popular causal modeling framework. In this work, we show that SCMs are not flexible enough to give a complete causal representation of dynamical systems at equilibrium. Instead, we propose a generalization of the notion of an SCM, that we call Causal Constraints Model (CCM), and prove that CCMs do capture the causal semantics of such systems. We show how CCMs can be constructed from differential equations and initial conditions and we illustrate our ideas further on a simple but ubiquitous (bio)chemical reaction. Our framework also allows to model functional laws, such as the ideal gas law, in a sensible and intuitive way.",Beyond Structural Causal Models: Causal Constraints Models,"Tineke Blom, Stephan Bongers, Joris M. Mooij",2019,Artificial Intelligence,1805.06539
"{Radio Frequency Identification technology has gained popularity for cheap and easy deployment. In the realm of manufacturing shopfloor, it can be used to track the location of manufacturing objects to achieve better efficiency. The underlying challenge of localization lies in the non-stationary characteristics of manufacturing shopfloor which calls for an adaptive life-long learning strategy in order to arrive at accurate localization results. This paper presents an evolving model based on a novel evolving intelligent system, namely evolving Type-2 Quantum Fuzzy Neural Network (eT2QFNN), which features an interval type-2 quantum fuzzy set with uncertain jump positions. The quantum fuzzy set possesses a graded membership degree which enables better identification of overlaps between classes. The eT2QFNN works fully in the evolving mode where all parameters including the number of rules are automatically adjusted and generated on the fly. The parameter adjustment scenario relies on decoupled extended Kalman filter method. Our numerical study shows that eT2QFNN is able to deliver comparable accuracy compared to state-of-the-art algorithms.",An Online RFID Localization in the Manufacturing Shopfloor,"Andri Ashfahani, Mahardhika Pratama, Edwin Lughofer, Qing Cai, and Huang Sheng",2019,Artificial Intelligence,1805.07715
"The Sensor, Observation, Sample, and Actuator (SOSA) ontology provides a formal but lightweight general-purpose specification for modeling the interaction between the entities involved in the acts of observation, actuation, and sampling. SOSA is the result of rethinking the W3C-XG Semantic Sensor Network (SSN) ontology based on changes in scope and target audience, technical developments, and lessons learned over the past years. SOSA also acts as a replacement of SSN's Stimulus Sensor Observation (SSO) core. It has been developed by the first joint working group of the Open Geospatial Consortium (OGC) and the World Wide Web Consortium (W3C) on \emph{Spatial Data on the Web}. In this work, we motivate the need for SOSA, provide an overview of the main classes and properties, and briefly discuss its integration with the new release of the SSN ontology as well as various other alignments to specifications such as OGC's Observations and Measurements (O\&M), Dolce-Ultralite (DUL), and other prominent ontologies. We will also touch upon common modeling problems and application areas related to publishing and searching observation, sampling, and actuation data on the Web. The SOSA ontology and standard can be accessed at \url{https://www.w3.org/TR/vocab-ssn/}.","SOSA: A Lightweight Ontology for Sensors, Observations, Samples, and Actuators","Krzysztof Janowicz, Armin Haller, Simon J D Cox, Danh Le Phuoc, Maxime Lefrancois",2018,Artificial Intelligence,1805.09979
"We consider the problem of how to improve automatic target recognition by fusing the naive sensor-level classification decisions with ""intuition,"" or context, in a mathematically principled way. This is a general approach that is compatible with many definitions of context, but for specificity, we consider context as co-occurrence in imagery. In particular, we consider images that contain multiple objects identified at various confidence levels. We learn the patterns of co-occurrence in each context, then use these patterns as hyper-parameters for a Hierarchical Bayesian Model. The result is that low-confidence sensor classification decisions can be dramatically improved by fusing those readings with context. We further use hyperpriors to address the case where multiple contexts may be appropriate. We also consider the Bayesian Network, an alternative to the Hierarchical Bayesian Model, which is computationally more efficient but assumes that context and sensor readings are uncorrelated.",Context Exploitation using Hierarchical Bayesian Models,"Christopher A. George, Pranab Banerjee, Kendra E. Moore",2018,Artificial Intelligence,1805.12183
"During the 60s and 70s, AI researchers explored intuitions about intelligence by writing programs that displayed intelligent behavior. Many good ideas came out from this work but programs written by hand were not robust or general. After the 80s, research increasingly shifted to the development of learners capable of inferring behavior and functions from experience and data, and solvers capable of tackling well-defined but intractable models like SAT, classical planning, Bayesian networks, and POMDPs. The learning approach has achieved considerable success but results in black boxes that do not have the flexibility, transparency, and generality of their model-based counterparts. Model-based approaches, on the other hand, require models and scalable algorithms. Model-free learners and model-based solvers have close parallels with Systems 1 and 2 in current theories of the human mind: the first, a fast, opaque, and inflexible intuitive mind; the second, a slow, transparent, and flexible analytical mind. In this paper, I review developments in AI and draw on these theories to discuss the gap between model-free learners and model-based solvers, a gap that needs to be bridged in order to have intelligent systems that are robust and general.","Model-free, Model-based, and General Intelligence",Hector Geffner,2018,Artificial Intelligence,1806.02308
"Machine learning practitioners are often ambivalent about the ethical aspects of their products. We believe anything that gets us from that current state to one in which our systems are achieving some degree of fairness is an improvement that should be welcomed. This is true even when that progress does not get us 100% of the way to the goal of ""complete"" fairness or perfectly align with our personal belief on which measure of fairness is used. Some measure of fairness being built would still put us in a better position than the status quo. Impediments to getting fairness and ethical concerns applied in real applications, whether they are abstruse philosophical debates or technical overhead such as the introduction of ever more hyper-parameters, should be avoided. In this paper we further elaborate on our argument for this viewpoint and its importance.",What About Applied Fairness?,"Jared Sylvester, Edward Raff",2018,Artificial Intelligence,1806.05250
"In many real-world problems, there is the possibility to configure, to a limited extent, some environmental parameters to improve the performance of a learning agent. In this paper, we propose a novel framework, Configurable Markov Decision Processes (Conf-MDPs), to model this new type of interaction with the environment. Furthermore, we provide a new learning algorithm, Safe Policy-Model Iteration (SPMI), to jointly and adaptively optimize the policy and the environment configuration. After having introduced our approach and derived some theoretical results, we present the experimental evaluation in two explicative problems to show the benefits of the environment configurability on the performance of the learned policy.",Configurable Markov Decision Processes,"Alberto Maria Metelli, Mirco Mutti and Marcello Restelli",2018,Artificial Intelligence,1806.05415
"The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult. Bayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs) extend POMDPs to allow the model to be learned during execution. BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between exploitation and exploration. Unfortunately, BA-POMDPs are currently impractical to solve for any non-trivial domain. In this paper, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle problems that previous solution methods have been unable to solve. Additionally, we introduce several techniques that exploit the BA-POMDP structure to improve the efficiency of BA-POMCP along with proof of their convergence.",Learning in POMDPs with Monte Carlo Tree Search,"Sammie Katt, Frans A. Oliehoek, Christopher Amato",2017,Artificial Intelligence,1806.05631
"The well-known Late Acceptance Hill Climbing (LAHC) search aims to overcome the main downside of traditional Hill Climbing (HC) search, which is often quickly trapped in a local optimum due to strictly accepting only non-worsening moves within each iteration. In contrast, LAHC also accepts worsening moves, by keeping a circular array of fitness values of previously visited solutions and comparing the fitness values of candidate solutions against the least recent element in the array. While this straightforward strategy has proven effective, there are nevertheless situations where LAHC can unfortunately behave in a similar manner to HC. For example, when a new local optimum is found, often the same fitness value is stored many times in the array. To address this shortcoming, we propose new acceptance and replacement strategies to take into account worsening, improving, and sideways movement scenarios with the aim to improve the diversity of values in the array. Compared to LAHC, the proposed Diversified Late Acceptance Search approach is shown to lead to better quality solutions that are obtained with a lower number of iterations on benchmark Travelling Salesman Problems and Quadratic Assignment Problems.",Diversified Late Acceptance Search,"Majid Namazi, Conrad Sanderson, M.A. Hakim Newton, M.M.A. Polash, Abdul Sattar",2018,Artificial Intelligence,1806.09328
"Fully observable non-deterministic (FOND) planning is becoming increasingly important as an approach for computing proper policies in probabilistic planning, extended temporal plans in LTL planning, and general plans in generalized planning. In this work, we introduce a SAT encoding for FOND planning that is compact and can produce compact strong cyclic policies. Simple variations of the encodings are also introduced for strong planning and for what we call, dual FOND planning, where some non-deterministic actions are assumed to be fair (e.g., probabilistic) and others unfair (e.g., adversarial). The resulting FOND planners are compared empirically with existing planners over existing and new benchmarks. The notion of ""probabilistic interesting problems"" is also revisited to yield a more comprehensive picture of the strengths and limitations of current FOND planners and the proposed SAT approach.",Compact Policies for Fully-Observable Non-Deterministic Planning as SAT,Tomas Geffner and Hector Geffner,2018,Artificial Intelligence,1806.09455
"Combinatorial preference aggregation has many applications in AI. Given the exponential nature of these preferences, compact representations are needed and ($m$)CP-nets are among the most studied ones. Sequential and global voting are two ways to aggregate preferences over CP-nets. In the former, preferences are aggregated feature-by-feature. Hence, when preferences have specific feature dependencies, sequential voting may exhibit voting paradoxes, i.e., it might select sub-optimal outcomes. To avoid paradoxes in sequential voting, one has often assumed the $\mathcal{O}$-legality restriction, which imposes a shared topological order among all the CP-nets. On the contrary, in global voting, CP-nets are considered as a whole during preference aggregation. For this reason, global voting is immune from paradoxes, and there is no need to impose restrictions over the CP-nets' topological structure. Sequential voting over $\mathcal{O}$-legal CP-nets has extensively been investigated. On the other hand, global voting over non-$\mathcal{O}$-legal CP-nets has not carefully been analyzed, despite it was stated in the literature that a theoretical comparison between global and sequential voting was promising and a precise complexity analysis for global voting has been asked for multiple times. In quite few works, very partial results on the complexity of global voting over CP-nets have been given. We start to fill this gap by carrying out a thorough complexity analysis of Pareto and majority global voting over not necessarily $\mathcal{O}$-legal acyclic binary polynomially connected (m)CP-nets. We settle these problems in the polynomial hierarchy, and some of them in PTIME or LOGSPACE, whereas EXPTIME was the previously known upper bound for most of them. We show various tight lower bounds and matching upper bounds for problems that up to date did not have any explicit non-obvious lower bound.",Complexity Results for Preference Aggregation over (m)CP-nets: Pareto and Majority Voting,"Thomas Lukasiewicz, Enrico Malizia",2019,Artificial Intelligence,1806.10018
"Human posture recognition provides a dynamic field that has produced many methods. Using fuzzy subsets based data fusion methods to aggregate the results given by different types of recognition processes is a convenient way to improve recognition methods. Nevertheless, choosing a defuzzification method to imple-ment the decision is a crucial point of this approach. The goal of this paper is to present an approach where the choice of the defuzzification method is driven by the constraints of the final data user, which are expressed as limitations on indica-tors like confidence or accuracy. A practical experimentation illustrating this ap-proach is presented: from a depth camera sensor, human posture is interpreted and the defuzzification method is selected in accordance with the constraints of the final information consumer. The paper illustrates the interest of the approach in a context of postures based human robot communication.",Decision method choice in a human posture recognition context,"St\'ephane Perrin (LISTIC), Eric Benoit (LISTIC), Didier Coquin (LISTIC)",2018,Artificial Intelligence,1807.04170
"This paper introduces a fully automatic method for generating video game tutorials. The AtDELFI system (AuTomatically DEsigning Legible, Full Instructions for games) was created to investigate procedural generation of instructions that teach players how to play video games. We present a representation of game rules and mechanics using a graph system as well as a tutorial generation method that uses said graph representation. We demonstrate the concept by testing it on games within the General Video Game Artificial Intelligence (GVG-AI) framework; the paper discusses tutorials generated for eight different games. Our findings suggest that a graph representation scheme works well for simple arcade style games such as Space Invaders and Pacman, but it appears that tutorials for more complex games might require higher-level understanding of the game than just single mechanics.","AtDelfi: Automatically Designing Legible, Full Instructions For Games","Michael Cerny Green, Ahmed Khalifa, Gabriella A.B. Barros, Tiago Machado, Andy Nealen and Julian Togelius",2018,Artificial Intelligence,1807.04375
"We introduce a new generative model for human planning under the Bayesian Inverse Reinforcement Learning (BIRL) framework which takes into account the fact that humans often plan using hierarchical strategies. We describe the Bayesian Inverse Hierarchical RL (BIHRL) algorithm for inferring the values of hierarchical planners, and use an illustrative toy model to show that BIHRL retains accuracy where standard BIRL fails. Furthermore, BIHRL is able to accurately predict the goals of `Wikispeedia' game players, with inclusion of hierarchical structure in the model resulting in a large boost in accuracy. We show that BIHRL is able to significantly outperform BIRL even when we only have a weak prior on the hierarchical structure of the plans available to the agent, and discuss the significant challenges that remain for scaling up this framework to more realistic settings.",Exploring Hierarchy-Aware Inverse Reinforcement Learning,"Chris Cundy, Daniel Filan",2018,Artificial Intelligence,1807.05037
"It is the focus of this work to extend and study the previously proposed quantum-like Bayesian networks to deal with decision-making scenarios by incorporating the notion of maximum expected utility in influence diagrams. The general idea is to take advantage of the quantum interference terms produced in the quantum-like Bayesian Network to influence the probabilities used to compute the expected utility of some action. This way, we are not proposing a new type of expected utility hypothesis. On the contrary, we are keeping it under its classical definition. We are only incorporating it as an extension of a probabilistic graphical model in a compact graphical representation called an influence diagram in which the utility function depends on the probabilistic influences of the quantum-like Bayesian network. Our findings suggest that the proposed quantum-like influence digram can indeed take advantage of the quantum interference effects of quantum-like Bayesian Networks to maximise the utility of a cooperative behaviour in detriment of a fully rational defect behaviour under the prisoner's dilemma game.",Introducing Quantum-Like Influence Diagrams for Violations of the Sure Thing Principle,Catarina Moreira and Andreas Wichert,2018,Artificial Intelligence,1807.06142
Recommendation systems are an integral part of Artificial Intelligence (AI) and have become increasingly important in the growing age of commercialization in AI. Deep learning (DL) techniques for recommendation systems (RS) provide powerful latent-feature models for effective recommendation but suffer from the major drawback of being non-interpretable. In this paper we describe a framework for explainable temporal recommendations in a DL model. We consider an LSTM based Recurrent Neural Network (RNN) architecture for recommendation and a neighbourhood-based scheme for generating explanations in the model. We demonstrate the effectiveness of our approach through experiments on the Netflix dataset by jointly optimizing for both prediction accuracy and explainability.,Explanations for Temporal Recommendations,"Homanga Bharadhwaj, Shruti Joshi",2018,Artificial Intelligence,1807.06161
"Monte Carlo tree search (MCTS) is a popular choice for solving sequential anytime problems. However, it depends on a numeric feedback signal, which can be difficult to define. Real-time MCTS is a variant which may only rarely encounter states with an explicit, extrinsic reward. To deal with such cases, the experimenter has to supply an additional numeric feedback signal in the form of a heuristic, which intrinsically guides the agent. Recent work has shown evidence that in different areas the underlying structure is ordinal and not numerical. Hence erroneous and biased heuristics are inevitable, especially in such domains. In this paper, we propose a MCTS variant which only depends on qualitative feedback, and therefore opens up new applications for MCTS. We also find indications that translating absolute into ordinal feedback may be beneficial. Using a puzzle domain, we show that our preference-based MCTS variant, wich only receives qualitative feedback, is able to reach a performance level comparable to a regular MCTS baseline, which obtains quantitative feedback.",Preference-Based Monte Carlo Tree Search,"Tobias Joppen, Christian Wirth, and Johannes F\""urnkranz",2018,Artificial Intelligence,1807.06286
"We present the design of a competitive artificial intelligence for Scopone, a popular Italian card game. We compare rule-based players using the most established strategies (one for beginners and two for advanced players) against players using Monte Carlo Tree Search (MCTS) and Information Set Monte Carlo Tree Search (ISMCTS) with different reward functions and simulation strategies. MCTS requires complete information about the game state and thus implements a cheating player while ISMCTS can deal with incomplete information and thus implements a fair player. Our results show that, as expected, the cheating MCTS outperforms all the other strategies; ISMCTS is stronger than all the rule-based players implementing well-known and most advanced strategies and it also turns out to be a challenging opponent for human players.",Traditional Wisdom and Monte Carlo Tree Search Face-to-Face in the Card Game Scopone,Stefano Di Palma and Pier Luca Lanzi,2018,Artificial Intelligence,1807.06813
"Many distributed machine learning frameworks have recently been built to speed up the large-scale data learning process. However, most distributed machine learning used in these frameworks still uses an offline algorithm model which cannot cope with the data stream problems. In fact, large-scale data are mostly generated by the non-stationary data stream where its pattern evolves over time. To address this problem, we propose a novel Evolving Large-scale Data Stream Analytics framework based on a Scalable Parsimonious Network based on Fuzzy Inference System (Scalable PANFIS), where the PANFIS evolving algorithm is distributed over the worker nodes in the cloud to learn large-scale data stream. Scalable PANFIS framework incorporates the active learning (AL) strategy and two model fusion methods. The AL accelerates the distributed learning process to generate an initial evolving large-scale data stream model (initial model), whereas the two model fusion methods aggregate an initial model to generate the final model. The final model represents the update of current large-scale data knowledge which can be used to infer future data. Extensive experiments on this framework are validated by measuring the accuracy and running time of four combinations of Scalable PANFIS and other Spark-based built in algorithms. The results indicate that Scalable PANFIS with AL improves the training time to be almost two times faster than Scalable PANFIS without AL. The results also show both rule merging and the voting mechanisms yield similar accuracy in general among Scalable PANFIS algorithms and they are generally better than Spark-based algorithms. In terms of running time, the Scalable PANFIS training time outperforms all Spark-based algorithms when classifying numerous benchmark datasets.",Evolving Large-Scale Data Stream Analytics based on Scalable PANFIS,"Mahardhika Pratama, Choiru Za'in, Eric Pardede",2018,Artificial Intelligence,1807.06996
"Unmanned aerial vehicles (UAVs), also known as drones, have emerged as a promising mode of fast, energy-efficient, and cost-effective package delivery. A considerable number of works have studied different aspects of drone package delivery service by a supplier, one of which is delivery planning. However, existing works addressing the planning issues consider a simple case of perfect delivery without service interruption, e.g., due to accident which is common and realistic. Therefore, this paper introduces the joint ground and aerial delivery service optimization and planning (GADOP) framework. The framework explicitly incorporates uncertainty of drone package delivery, i.e., takeoff and breakdown conditions. The GADOP framework aims to minimize the total delivery cost given practical constraints, e.g., traveling distance limit. Specifically, we formulate the GADOP framework as a three-stage stochastic integer programming model. To deal with the high complexity issue of the problem, a decomposition method is adopted. Then, the performance of the GADOP framework is evaluated by using two data sets including Solomon benchmark suite and the real data from one of the Singapore logistics companies. The performance evaluation clearly shows that the GADOP framework can achieve significantly lower total payment than that of the baseline methods which do not take uncertainty into account.",Joint Ground and Aerial Package Delivery Services: A Stochastic Optimization Approach,"Suttinee Sawadsitang, Dusit Niyato, Puay-Siew Tan, and Ping Wang",2018,Artificial Intelligence,1808.04617
"Approaches to decision-making under uncertainty in the belief function framework are reviewed. Most methods are shown to blend criteria for decision under ignorance with the maximum expected utility principle of Bayesian decision theory. A distinction is made between methods that construct a complete preference relation among acts, and those that allow incomparability of some acts due to lack of information. Methods developed in the imprecise probability framework are applicable in the Dempster-Shafer context and are also reviewed. Shafer's constructive decision theory, which substitutes the notion of goal for that of utility, is described and contrasted with other approaches. The paper ends by pointing out the need to carry out deeper investigation of fundamental issues related to decision-making with belief functions and to assess the descriptive, normative and prescriptive values of the different approaches.",Decision-Making with Belief Functions: a Review,Thierry Denoeux,2019,Artificial Intelligence,1808.05322
"With the increasing need of personalised decision making, such as personalised medicine and online recommendations, a growing attention has been paid to the discovery of the context and heterogeneity of causal relationships. Most existing methods, however, assume a known cause (e.g. a new drug) and focus on identifying from data the contexts of heterogeneous effects of the cause (e.g. patient groups with different responses to the new drug). There is no approach to efficiently detecting directly from observational data context specific causal relationships, i.e. discovering the causes and their contexts simultaneously. In this paper, by taking the advantages of highly efficient decision tree induction and the well established causal inference framework, we propose the Tree based Context Causal rule discovery (TCC) method, for efficient exploration of context specific causal relationships from data. Experiments with both synthetic and real world data sets show that TCC can effectively discover context specific causal rules from the data.",Discovering Context Specific Causal Relationships,"Saisai Ma, Jiuyong Li, Lin Liu, Thuc Duy Le",2019,Artificial Intelligence,1808.06316
"Artificial intelligence (AI) is the core technology of technological revolution and industrial transformation. As one of the new intelligent needs in the AI 2.0 era, financial intelligence has elicited much attention from the academia and industry. In our current dynamic capital market, financial intelligence demonstrates a fast and accurate machine learning capability to handle complex data and has gradually acquired the potential to become a ""financial brain"". In this work, we survey existing studies on financial intelligence. First, we describe the concept of financial intelligence and elaborate on its position in the financial technology field. Second, we introduce the development of financial intelligence and review state-of-the-art techniques in wealth management, risk management, financial security, financial consulting, and blockchain. Finally, we propose a research framework called FinBrain and summarize four open issues, namely, explainable financial agents and causality, perception and prediction under uncertainty, risk-sensitive and robust decision making, and multi-agent game and mechanism design. We believe that these research directions can lay the foundation for the development of AI 2.0 in the finance field.",FinBrain: When Finance Meets AI 2.0,"Xiaolin Zheng, Mengying Zhu, Qibing Li, Chaochao Chen, Yanchao Tan",2018,Artificial Intelligence,1808.08497
"Developing visual perception models for active agents and sensorimotor control are cumbersome to be done in the physical world, as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we are concerned with the problem of developing real-world perception for active agents, propose Gibson Virtual Environment for this purpose, and showcase sample perceptual tasks learned therein. Gibson is based on virtualizing real spaces, rather than using artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism, ""Goggles"", enabling deploying the trained models in real-world without needing further domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.",Gibson Env: Real-World Perception for Embodied Agents,"Fei Xia, Amir Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik, Silvio Savarese",2018,Artificial Intelligence,1808.10654
"Using a game engine, we have developed a virtual environment which models important aspects of critical incident scenarios. We focused on modelling phenomena relating to the identification and gathering of key forensic evidence, in order to develop and test a system which can handle chemical, biological, radiological/nuclear or explosive (CBRNe) events autonomously. This allows us to build and validate AI-based technologies, which can be trained and tested in our custom virtual environment before being deployed in real-world scenarios. We have used our virtual scenario to rapidly prototype a system which can use simulated Remote Aerial Vehicles (RAVs) to gather images from the environment for the purpose of mapping. Our environment provides us with an effective medium through which we can develop and test various AI methodologies for critical incident scene assessment, in a safe and controlled manner",Using a Game Engine to Simulate Critical Incidents and Data Collection by Autonomous Drones,"David L. Smyth, Frank G. Glavin, Michael G. Madden",2018,Artificial Intelligence,1808.10784
"This paper introduces an information-theoretic method for selecting a subset of problems which gives the most information about a group of problem-solving algorithms. This method was tested on the games in the General Video Game AI (GVGAI) framework, allowing us to identify a smaller set of games that still gives a large amount of information about the abilities of different game-playing agents. This approach can be used to make agent testing more efficient. We can achieve almost as good discriminatory accuracy when testing on only a handful of games as when testing on more than a hundred games, something which is often computationally infeasible. Furthermore, this method can be extended to study the dimensions of the effective variance in game design between these games, allowing us to identify which games differentiate between agents in the most complementary ways.",A Continuous Information Gain Measure to Find the Most Discriminatory Problems for AI Benchmarking,"Matthew Stephenson, Damien Anderson, Ahmed Khalifa, John Levine, Jochen Renz, Julian Togelius, Christoph Salge",2020,Artificial Intelligence,1809.02904
"As an exquisite and concise literary form, poetry is a gem of human culture. Automatic poetry generation is an essential step towards computer creativity. In recent years, several neural models have been designed for this task. However, among lines of a whole poem, the coherence in meaning and topics still remains a big challenge. In this paper, inspired by the theoretical concept in cognitive psychology, we propose a novel Working Memory model for poetry generation. Different from previous methods, our model explicitly maintains topics and informative limited history in a neural memory. During the generation process, our model reads the most relevant parts from memory slots to generate the current line. After each line is generated, it writes the most salient parts of the previous line into memory slots. By dynamic manipulation of the memory, our model keeps a coherent information flow and learns to express each topic flexibly and naturally. We experiment on three different genres of Chinese poetry: quatrain, iambic and chinoiserie lyric. Both automatic and human evaluation results show that our model outperforms current state-of-the-art methods.",Chinese Poetry Generation with a Working Memory Model,"Xiaoyuan Yi, Maosong Sun, Ruoyu Li, Zonghan Yang",2018,Artificial Intelligence,1809.04306
"When modeling real world domains we have to deal with information that is incomplete or that comes from sources with different trust levels. This motivates the need for managing uncertainty in the Semantic Web. To this purpose, we introduced a probabilistic semantics, named DISPONTE, in order to combine description logics with probability theory. The probability of a query can be then computed from the set of its explanations by building a Binary Decision Diagram (BDD). The set of explanations can be found using the tableau algorithm, which has to handle non-determinism. Prolog, with its efficient handling of non-determinism, is suitable for implementing the tableau algorithm. TRILL and TRILLP are systems offering a Prolog implementation of the tableau algorithm. TRILLP builds a pinpointing formula, that compactly represents the set of explanations and can be directly translated into a BDD. Both reasoners were shown to outperform state-of-the-art DL reasoners. In this paper, we present an improvement of TRILLP, named TORNADO, in which the BDD is directly built during the construction of the tableau, further speeding up the overall inference process. An experimental comparison shows the effectiveness of TORNADO. All systems can be tried online in the TRILL on SWISH web application at http://trill.ml.unife.it/.",Probabilistic DL Reasoning with Pinpointing Formulas: A Prolog-based Approach,"Riccardo Zese, Giuseppe Cota, Evelina Lamma, Elena Bellodi, Fabrizio Riguzzi",2019,Artificial Intelligence,1809.06180
"Autonomous robotics and artificial intelligence techniques can be used to support human personnel in the event of critical incidents. These incidents can pose great danger to human life. Some examples of such assistance include: multi-robot surveying of the scene; collection of sensor data and scene imagery, real-time risk assessment and analysis; object identification and anomaly detection; and retrieval of relevant supporting documentation such as standard operating procedures (SOPs). These incidents, although often rare, can involve chemical, biological, radiological/nuclear or explosive (CBRNE) substances and can be of high consequence. Real-world training and deployment of these systems can be costly and sometimes not feasible. For this reason, we have developed a realistic 3D model of a CBRNE scenario to act as a testbed for an initial set of assisting AI tools that we have developed.","A Virtual Testbed for Critical Incident Investigation with Autonomous Remote Aerial Vehicle Surveying, Artificial Intelligence, and Decision Support","David L. Smyth, Sai Abinesh, Nazli B. Karimi, Brett Drury, Ihsan Ullah, Frank G. Glavin, Michael G. Madden",2018,Artificial Intelligence,1809.06244
"The growing influence and decision-making capacities of Autonomous systems and Artificial Intelligence in our lives force us to consider the values embedded in these systems. But how ethics should be implemented into these systems? In this study, the solution is seen on philosophical conceptualization as a framework to form practical implementation model for ethics of AI. To take the first steps on conceptualization main concepts used on the field needs to be identified. A keyword based Systematic Mapping Study (SMS) on the keywords used in AI and ethics was conducted to help in identifying, defying and comparing main concepts used in current AI ethics discourse. Out of 1062 papers retrieved SMS discovered 37 re-occurring keywords in 83 academic papers. We suggest that the focus on finding keywords is the first step in guiding and providing direction for future research in the AI ethics field.",The Key Concepts of Ethics of Artificial Intelligence - A Keyword based Systematic Mapping Study,Ville Vakkuri and Pekka Abrahamsson,2018,Artificial Intelligence,1809.07027
"The Winograd Schema (WS) challenge, proposed as an al-ternative to the Turing Test, has become the new standard for evaluating progress in natural language understanding (NLU). In this paper we will not however be concerned with how this challenge might be addressed. Instead, our aim here is threefold: (i) we will first formally 'situate' the WS challenge in the data-information-knowledge continuum, suggesting where in that continuum a good WS resides; (ii) we will show that a WS is just special case of a more general phenomenon in language understanding, namely the missing text phenomenon (henceforth, MTP) - in particular, we will argue that what we usually call thinking in the process of language understanding involves discovering a significant amount of 'missing text' - text that is not explicitly stated, but is often implicitly assumed as shared background knowledge; and (iii) we conclude by a brief discussion on why MTP is inconsistent with the data-driven and machine learning approach to language understanding.",On the Winograd Schema: Situating Language Understanding in the Data-Information-Knowledge Continuum,Walid S. Saba,2019,Artificial Intelligence,1810.00324
"This paper introduces DATA Agent, a system which creates murder mystery adventures from open data. In the game, the player takes on the role of a detective tasked with finding the culprit of a murder. All characters, places, and items in DATA Agent games are generated using open data as source content. The paper discusses the general game design and user interface of DATA Agent, and provides details on the generative algorithms which transform linked data into different game objects. Findings from a user study with 30 participants playing through two games of DATA Agent show that the game is easy and fun to play, and that the mysteries it generates are straightforward to solve.",DATA Agent,"Michael Cerny Green, Gabriella A.B. Barros, Antonios Liapis, Julian Togelius",2018,Artificial Intelligence,1810.02251
"Perceiving the surrounding environment in terms of objects is useful for any general purpose intelligent agent. In this paper, we investigate a fundamental mechanism making object perception possible, namely the identification of spatio-temporally invariant structures in the sensorimotor experience of an agent. We take inspiration from the Sensorimotor Contingencies Theory to define a computational model of this mechanism through a sensorimotor, unsupervised and predictive approach. Our model is based on processing the unsupervised interaction of an artificial agent with its environment. We show how spatio-temporally invariant structures in the environment induce regularities in the sensorimotor experience of an agent, and how this agent, while building a predictive model of its sensorimotor experience, can capture them as densely connected subgraphs in a graph of sensory states connected by motor commands. Our approach is focused on elementary mechanisms, and is illustrated with a set of simple experiments in which an agent interacts with an environment. We show how the agent can build an internal model of moving but spatio-temporally invariant structures by performing a Spectral Clustering of the graph modeling its overall sensorimotor experiences. We systematically examine properties of the model, shedding light more globally on the specificities of the paradigm with respect to methods based on the supervised processing of collections of static images.",Identification of Invariant Sensorimotor Structures as a Prerequisite for the Discovery of Objects,"Nicolas Le Hir, Olivier Sigaud, Alban Laflaqui\`ere",2018,Artificial Intelligence,1810.05057
"Most of agents that learn policy for tasks with reinforcement learning (RL) lack the ability to communicate with people, which makes human-agent collaboration challenging. We believe that, in order for RL agents to comprehend utterances from human colleagues, RL agents must infer the mental states that people attribute to them because people sometimes infer an interlocutor's mental states and communicate on the basis of this mental inference. This paper proposes PublicSelf model, which is a model of a person who infers how the person's own behavior appears to their colleagues. We implemented the PublicSelf model for an RL agent in a simulated environment and examined the inference of the model by comparing it with people's judgment. The results showed that the agent's intention that people attributed to the agent's movement was correctly inferred by the model in scenes where people could find certain intentionality from the agent's behavior.",Bayesian Inference of Self-intention Attributed by Observer,"Yosuke Fukuchi, Masahiko Osawa, Hiroshi Yamakawa, Tatsuji Takahashi, Michita Imai",2018,Artificial Intelligence,1810.05564
"This paper presents a technology for simple and computationally efficient improvements of a generic Artificial Intelligence (AI) system, including Multilayer and Deep Learning neural networks. The improvements are, in essence, small network ensembles constructed on top of the existing AI architectures. Theoretical foundations of the technology are based on Stochastic Separation Theorems and the ideas of the concentration of measure. We show that, subject to mild technical assumptions on statistical properties of internal signals in the original AI system, the technology enables instantaneous and computationally efficient removal of spurious and systematic errors with probability close to one on the datasets which are exponentially large in dimension. The method is illustrated with numerical examples and a case study of ten digits recognition from American Sign Language.",Fast Construction of Correcting Ensembles for Legacy Artificial Intelligence Systems: Algorithms and a Case Study,"Ivan Y. Tyukin, Alexander N. Gorban, Stephen Green, Danil Prokhorov",2019,Artificial Intelligence,1810.05593
"In open-ended environments, autonomous learning agents must set their own goals and build their own curriculum through an intrinsically motivated exploration. They may consider a large diversity of goals, aiming to discover what is controllable in their environments, and what is not. Because some goals might prove easy and some impossible, agents must actively select which goal to practice at any moment, to maximize their overall mastery on the set of learnable goals. This paper proposes CURIOUS, an algorithm that leverages 1) a modular Universal Value Function Approximator with hindsight learning to achieve a diversity of goals of different kinds within a unique policy and 2) an automated curriculum learning mechanism that biases the attention of the agent towards goals maximizing the absolute learning progress. Agents focus sequentially on goals of increasing complexity, and focus back on goals that are being forgotten. Experiments conducted in a new modular-goal robotic environment show the resulting developmental self-organization of a learning curriculum, and demonstrate properties of robustness to distracting goals, forgetting and changes in body properties.",CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning,"C\'edric Colas, Pierre Fournier, Olivier Sigaud, Mohamed Chetouani, Pierre-Yves Oudeyer",2019,Artificial Intelligence,1810.06284
"Traditional data quality control methods are based on users experience or previously established business rules, and this limits performance in addition to being a very time consuming process with lower than desirable accuracy. Utilizing deep learning, we can leverage computing resources and advanced techniques to overcome these challenges and provide greater value to users. In this paper, we, the authors, first review relevant works and discuss machine learning techniques, tools, and statistical quality models. Second, we offer a creative data quality framework based on deep learning and statistical model algorithm for identifying data quality. Third, we use data involving salary levels from an open dataset published by the state of Arkansas to demonstrate how to identify outlier data and how to improve data quality via deep learning. Finally, we discuss future work.",Improving Data Quality through Deep Learning and Statistical Models,"Wei Dai, Kenji Yoshigoe, William Parsley",2018,Artificial Intelligence,1810.07132
"The increasing presence of robots in industries has not gone unnoticed. Large industrial players have incorporated them into their production lines, but smaller companies hesitate due to high initial costs and the lack of programming expertise. In this work we introduce a framework that combines two disciplines, Programming by Demonstration and Automated Planning, to allow users without any programming knowledge to program a robot. The user teaches the robot atomic actions together with their semantic meaning and represents them in terms of preconditions and effects. Using these atomic actions the robot can generate action sequences autonomously to reach any goal given by the user. We evaluated the usability of our framework in terms of user experiments with a Baxter Research Robot and showed that it is well-adapted to users without any programming experience.",A Framework for Robot Programming in Cobotic Environments: First user experiments,Ying Siu Liang and Damien Pellier and Humbert Fiorino and Sylvie Pesty,2017,Artificial Intelligence,1810.08492
"In cooperation, the workers must know how co-workers behave. However, an agent's policy, which is embedded in a statistical machine learning model, is hard to understand, and requires much time and knowledge to comprehend. Therefore, it is difficult for people to predict the behavior of machine learning robots, which makes Human Robot Cooperation challenging. In this paper, we propose Instruction-based Behavior Explanation (IBE), a method to explain an autonomous agent's future behavior. In IBE, an agent can autonomously acquire the expressions to explain its own behavior by reusing the instructions given by a human expert to accelerate the learning of the agent's policy. IBE also enables a developmental agent, whose policy may change during the cooperation, to explain its own behavior with sufficient time granularity.",Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents,"Yosuke Fukuchi, Masahiko Osawa, Hiroshi Yamakawa, Michita Imai",2017,Artificial Intelligence,1810.08811
"Counterfactual Regret Minimization (CFR) is the leading framework for solving large imperfect-information games. It converges to an equilibrium by iteratively traversing the game tree. In order to deal with extremely large games, abstraction is typically applied before running CFR. The abstracted game is solved with tabular CFR, and its solution is mapped back to the full game. This process can be problematic because aspects of abstraction are often manual and domain specific, abstraction algorithms may miss important strategic nuances of the game, and there is a chicken-and-egg problem because determining a good abstraction requires knowledge of the equilibrium of the game. This paper introduces Deep Counterfactual Regret Minimization, a form of CFR that obviates the need for abstraction by instead using deep neural networks to approximate the behavior of CFR in the full game. We show that Deep CFR is principled and achieves strong performance in large poker games. This is the first non-tabular variant of CFR to be successful in large games.",Deep Counterfactual Regret Minimization,"Noam Brown, Adam Lerer, Sam Gross, Tuomas Sandholm",2019,Artificial Intelligence,1811.00164
"This article deals with the problem of the uncertainty in rule-based systems (RBS), but from the perspective of quantum computing (QC). In this work we first remember the characteristics of Quantum Rule-Based Systems (QRBS), a concept defined in a previous article by one of the authors of this paper, and we introduce the problem of quantum uncertainty. We assume that the subjective uncertainty that affects the facts of classical RBSs can be treated as a direct consequence of the probabilistic nature of quantum mechanics (QM), and we also assume that the uncertainty associated with a given hypothesis is a consequence of the propagation of the imprecision through the inferential circuits of RBSs. This article does not intend to contribute anything new to the QM field: it is a work of artificial intelligence (AI) that uses QC techniques to solve the problem of uncertainty in RBSs. Bearing the above arguments in mind a quantum model is proposed. This model has been applied to a problem already defined by one of the authors of this work in a previous publication and which is briefly described in this article. Then the model is generalized, and it is thoroughly evaluated. The results obtained show that QC is a valid, effective and efficient method to deal with the inherent uncertainty of RBSs",Uncertainty in Quantum Rule-Based Systems,"Vicente Moret-Bonillo, Isaac Fern\'andez-Varela, Diego Alvarez-Estevez",2021,Artificial Intelligence,1811.02782
"Item response theory (IRT) can be applied to the analysis of the evaluation of results from AI benchmarks. The two-parameter IRT model provides two indicators (difficulty and discrimination) on the side of the item (or AI problem) while only one indicator (ability) on the side of the respondent (or AI agent). In this paper we analyse how to make this set of indicators dual, by adding a fourth indicator, generality, on the side of the respondent. Generality is meant to be dual to discrimination, and it is based on difficulty. Namely, generality is defined as a new metric that evaluates whether an agent is consistently good at easy problems and bad at difficult ones. With the addition of generality, we see that this set of four key indicators can give us more insight on the results of AI benchmarks. In particular, we explore two popular benchmarks in AI, the Arcade Learning Environment (Atari 2600 games) and the General Video Game AI competition. We provide some guidelines to estimate and interpret these indicators for other AI benchmarks and competitions.",Analysing Results from AI Benchmarks: Key Indicators and How to Obtain Them,Fernando Mart\'inez-Plumed and Jos\'e Hern\'andez-Orallo,2018,Artificial Intelligence,1811.08186
"Embedding models for deterministic Knowledge Graphs (KG) have been extensively studied, with the purpose of capturing latent semantic relations between entities and incorporating the structured knowledge into machine learning. However, there are many KGs that model uncertain knowledge, which typically model the inherent uncertainty of relations facts with a confidence score, and embedding such uncertain knowledge represents an unresolved challenge. The capturing of uncertain knowledge will benefit many knowledge-driven applications such as question answering and semantic search by providing more natural characterization of the knowledge. In this paper, we propose a novel uncertain KG embedding model UKGE, which aims to preserve both structural and uncertainty information of relation facts in the embedding space. Unlike previous models that characterize relation facts with binary classification techniques, UKGE learns embeddings according to the confidence scores of uncertain relation facts. To further enhance the precision of UKGE, we also introduce probabilistic soft logic to infer confidence scores for unseen relation facts during training. We propose and evaluate two variants of UKGE based on different learning objectives. Experiments are conducted on three real-world uncertain KGs via three tasks, i.e. confidence prediction, relation fact ranking, and relation fact classification. UKGE shows effectiveness in capturing uncertain knowledge by achieving promising results on these tasks, and consistently outperforms baselines on these tasks.",Embedding Uncertain Knowledge Graphs,"Xuelu Chen, Muhao Chen, Weijia Shi, Yizhou Sun, Carlo Zaniolo",2019,Artificial Intelligence,1811.10667
"Predicting the time to build software is a very complex task for software engineering managers. There are complex factors that can directly interfere with the productivity of the development team. Factors directly related to the complexity of the system to be developed drastically change the time necessary for the completion of the works with the software factories. This work proposes the use of a hybrid system based on artificial neural networks and fuzzy systems to assist in the construction of an expert system based on rules to support in the prediction of hours destined to the development of software according to the complexity of the elements present in the same. The set of fuzzy rules obtained by the system helps the management and control of software development by providing a base of interpretable estimates based on fuzzy rules. The model was submitted to tests on a real database, and its results were promissory in the construction of an aid mechanism in the predictability of the software construction.",Regularized Fuzzy Neural Networks to Aid Effort Forecasting in the Construction and Software Development,"Paulo Vitor de Campos Souza, Augusto Junio Guimaraes, Vanessa Souza Araujo, Thiago Silva Rezende, Vinicius Jonathan Silva Araujo",2018,Artificial Intelligence,1812.01351
"As artificial intelligence (AI) systems become increasingly ubiquitous, the topic of AI governance for ethical decision-making by AI has captured public imagination. Within the AI research community, this topic remains less familiar to many researchers. In this paper, we complement existing surveys, which largely focused on the psychological, social and legal discussions of the topic, with an analysis of recent advances in technical solutions for AI governance. By reviewing publications in leading AI conferences including AAAI, AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions. We highlight the intuitions and key techniques used in each approach, and discuss promising future research directions towards successful integration of ethical AI systems into human societies.",Building Ethics into Artificial Intelligence,"Han Yu, Zhiqi Shen, Chunyan Miao, Cyril Leung, Victor R. Lesser and Qiang Yang",2018,Artificial Intelligence,1812.02953
"The problem of allocating students to supervisors for the development of a personal project or a dissertation is a crucial activity in the higher education environment, as it enables students to get feedback on their work from an expert and improve their personal, academic, and professional abilities. In this article, we propose a multi-objective and near Pareto optimal genetic algorithm for the allocation of students to supervisors. The allocation takes into consideration the students and supervisors' preferences on research/project topics, the lower and upper supervision quotas of supervisors, as well as the workload balance amongst supervisors. We introduce novel mutation and crossover operators for the student-supervisor allocation problem. The experiments carried out show that the components of the genetic algorithm are more apt for the problem than classic components, and that the genetic algorithm is capable of producing allocations that are near Pareto optimal in a reasonable time.",A near Pareto optimal approach to student-supervisor allocation with two sided preferences and workload balance,"Victor Sanchez-Anguix, Rithin Chalumuri, Reyhan Aydogan, Vicente Julian",2018,Artificial Intelligence,1812.06474
"We propose a novel approach for trip prediction by analyzing user's trip histories. We augment users' (self-) trip histories by adding 'similar' trips from other users, which could be informative and useful for predicting future trips for a given user. This also helps to cope with noisy or sparse trip histories, where the self-history by itself does not provide a reliable prediction of future trips. We show empirical evidence that by enriching the users' trip histories with additional trips, one can improve the prediction error by 15%-40%, evaluated on multiple subsets of the Nancy2012 dataset. This real-world dataset is collected from public transportation ticket validations in the city of Nancy, France. Our prediction tool is a central component of a trip simulator system designed to analyze the functionality of public transportation in the city of Nancy.",Trip Prediction by Leveraging Trip Histories from Neighboring Users,Yuxin Chen and Morteza Haghir Chehreghani,2022,Artificial Intelligence,1812.10097
"In artificial intelligence (AI) mediated workforce management systems (e.g., crowdsourcing), long-term success depends on workers accomplishing tasks productively and resting well. This dual objective can be summarized by the concept of productive laziness. Existing scheduling approaches mostly focus on efficiency but overlook worker wellbeing through proper rest. In order to enable workforce management systems to follow the IEEE Ethically Aligned Design guidelines to prioritize worker wellbeing, we propose a distributed Computational Productive Laziness (CPL) approach in this paper. It intelligently recommends personalized work-rest schedules based on local data concerning a worker's capabilities and situational factors to incorporate opportunistic resting and achieve superlinear collective productivity without the need for explicit coordination messages. Extensive experiments based on a real-world dataset of over 5,000 workers demonstrate that CPL enables workers to spend 70% of the effort to complete 90% of the tasks on average, providing more ethically aligned scheduling than existing approaches.",Ethically Aligned Opportunistic Scheduling for Productive Laziness,"Han Yu, Chunyan Miao, Yongqing Zheng, Lizhen Cui, Simon Fauvel and Cyril Leung",2019,Artificial Intelligence,1901.00298
Predicting the behavior of surrounding vehicles is a critical problem in automated driving. We present a novel game theoretic behavior prediction model that achieves state of the art prediction accuracy by explicitly reasoning about possible future interaction between agents. We evaluate our approach on the NGSIM vehicle trajectory data set and demonstrate lower root mean square error than state-of-the-art methods.,Multi-Fidelity Recursive Behavior Prediction,"Mihir Jain, Kyle Brown, Ahmed K. Sadek",2018,Artificial Intelligence,1901.01831
"Its constant technological evolution characterizes the contemporary world, and every day the processes, once manual, become computerized. Data are stored in the cyberspace, and as a consequence, one must increase the concern with the security of this environment. Cyber-attacks are represented by a growing worldwide scale and are characterized as one of the significant challenges of the century. This article aims to propose a computational system based on intelligent hybrid models, which through fuzzy rules allows the construction of expert systems in cybernetic data attacks, focusing on the SQL Injection attack. The tests were performed with real bases of SQL Injection attacks on government computers, using fuzzy neural networks. According to the results obtained, the feasibility of constructing a system based on fuzzy rules, with the classification accuracy of cybernetic invasions within the margin of the standard deviation (compared to the state-of-the-art model in solving this type of problem) is real. The model helps countries prepare to protect their data networks and information systems, as well as create opportunities for expert systems to automate the identification of attacks in cyberspace.",Fuzzy neural networks to create an expert system for detecting attacks by SQL Injection,"Lucas Oliveira Batista, Gabriel Adriano de Silva, Vanessa Souza Ara\'ujo, Vin\'icius Jonathan Silva Ara\'ujo, Thiago Silva Rezende, Augusto Junio Guimar\~aes, Paulo Vitor de Campos Souza",2018,Artificial Intelligence,1901.02868
"In many problem settings, most notably in game playing, an agent receives a possibly delayed reward for its actions. Often, those rewards are handcrafted and not naturally given. Even simple terminal-only rewards, like winning equals 1 and losing equals -1, can not be seen as an unbiased statement, since these values are chosen arbitrarily, and the behavior of the learner may change with different encodings, such as setting the value of a loss to -0:5, which is often done in practice to encourage learning. It is hard to argue about good rewards and the performance of an agent often depends on the design of the reward signal. In particular, in domains where states by nature only have an ordinal ranking and where meaningful distance information between game state values are not available, a numerical reward signal is necessarily biased. In this paper, we take a look at Monte Carlo Tree Search (MCTS), a popular algorithm to solve MDPs, highlight a reoccurring problem concerning its use of rewards, and show that an ordinal treatment of the rewards overcomes this problem. Using the General Video Game Playing framework we show a dominance of our newly proposed ordinal MCTS algorithm over preference-based MCTS, vanilla MCTS and various other MCTS variants.",Ordinal Monte Carlo Tree Search,"Tobias Joppen and Johannes F\""urnkranz",2020,Artificial Intelligence,1901.04274
"In this paper, We Apply Reinforcement learning (RL) techniques to train a realistic biomechanical model to work with different people and on different walking environments. We benchmarking 3 RL algorithms: Deep Deterministic Policy Gradient (DDPG), Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO) in OpenSim environment, Also we apply imitation learning to a prosthetics domain to reduce the training time needed to design customized prosthetics. We use DDPG algorithm to train an original expert agent. We then propose a modification to the Dataset Aggregation (DAgger) algorithm to reuse the expert knowledge and train a new target agent to replicate that behaviour in fewer than 5 iterations, compared to the 100 iterations taken by the expert agent which means reducing training time by 95%. Our modifications to the DAgger algorithm improve the balance between exploiting the expert policy and exploring the environment. We show empirically that these improve convergence time of the target agent, particularly when there is some degree of variation between expert and naive agent.",Transfer Learning for Prosthetics Using Imitation Learning,"Montaser Mohammedalamen, Waleed D. Khamies, Benjamin Rosman",2018,Artificial Intelligence,1901.04772
"The availability of high-fidelity energy networks brings significant value to academic and commercial research. However, such releases also raise fundamental concerns related to privacy and security as they can reveal sensitive commercial information and expose system vulnerabilities. This paper investigates how to release power networks where the parameters of transmission lines and transformers are obfuscated. It does so by using the framework of Differential Privacy (DP), that provides strong privacy guarantees and has attracted significant attention in recent years. Unfortunately, simple DP mechanisms often result in AC-infeasible networks. To address these concerns, this paper presents a novel differential privacy mechanism that guarantees AC-feasibility and largely preserves the fidelity of the obfuscated network. Experimental results also show that the obfuscation significantly reduces the potential damage of an attacker exploiting the release of the dataset.",Differential Privacy for Power Grid Obfuscation,"Ferdinando Fioretto, Terrence W.K. Mak, Pascal Van Hentenryck",2020,Artificial Intelligence,1901.06949
"Collaborative filtering (CF) is the key technique for recommender systems (RSs). CF exploits user-item behavior interactions (e.g., clicks) only and hence suffers from the data sparsity issue. One research thread is to integrate auxiliary information such as product reviews and news titles, leading to hybrid filtering methods. Another thread is to transfer knowledge from other source domains such as improving the movie recommendation with the knowledge from the book domain, leading to transfer learning methods. In real-world life, no single service can satisfy a user's all information needs. Thus it motivates us to exploit both auxiliary and source information for RSs in this paper. We propose a novel neural model to smoothly enable Transfer Meeting Hybrid (TMH) methods for cross-domain recommendation with unstructured text in an end-to-end manner. TMH attentively extracts useful content from unstructured text via a memory module and selectively transfers knowledge from a source domain via a transfer network. On two real-world datasets, TMH shows better performance in terms of three ranking metrics by comparing with various baselines. We conduct thorough analyses to understand how the text content and transferred knowledge help the proposed model.",Transfer Meets Hybrid: A Synthetic Approach for Cross-Domain Collaborative Filtering with Text,"Guangneng Hu, Yu Zhang, and Qiang Yang",2019,Artificial Intelligence,1901.07199
"Learning in multi-agent scenarios is a fruitful research direction, but current approaches still show scalability problems in multiple games with general reward settings and different opponent types. The Multi-Agent Reinforcement Learning in Malm\""O (MARL\""O) competition is a new challenge that proposes research in this domain using multiple 3D games. The goal of this contest is to foster research in general agents that can learn across different games and opponent types, proposing a challenge as a milestone in the direction of Artificial General Intelligence.","The Multi-Agent Reinforcement Learning in Malm\""O (MARL\""O) Competition","Diego Perez-Liebana, Katja Hofmann, Sharada Prasanna Mohanty, Noburu Kuno, Andre Kramer, Sam Devlin, Raluca D. Gaina, Daniel Ionita",2018,Artificial Intelligence,1901.08129
"Under the project Maccoy Critical, we would like to train individuals, in virtual environments, to handle critical situations such as dilemmas. These latter refer to situations where there is no ``good'' solution. In other words, situations that lead to negative consequences whichever choice is made. Our objective is to use Knowledge Models to extract necessary properties for dilemmas to emerge. To do so, our approach consists in developing a Scenario Orchestration System that generates dilemma situations dynamically without having to write them beforehand. In this paper we present this approach and expose a proof of concept of the generation process.",A model for prohibition and obligation dilemmas generation in virtual environments,"Azzeddine Benabbou (Heudiasyc), Domitile Lourdeaux (Heudiasyc), Dominique Lenne (Heudiasyc)",2018,Artificial Intelligence,1901.09790
"Robotic mobile fulfillment systems (RMFSs) are a new type of warehousing system, which has received more attention recently, due to increasing growth in the e-commerce sector. Instead of sending pickers to the inventory area to search for and pick the ordered items, robots carry shelves (called ""pods"") including ordered items from the inventory area to picking stations. In the picking stations, human pickers put ordered items into totes; then these items are transported by a conveyor to the packing stations. This type of warehousing system relieves the human pickers and improves the picking process. In this paper, we concentrate on decisions about the assignment of pods to stations and orders to stations to fulfill picking for each incoming customer's order. In previous research for an RMFS with multiple picking stations, these decisions are made sequentially. Instead, we present a new integrated model. To improve the system performance even more, we extend our model by splitting orders. This means parts of an order are allowed to be picked at different stations. To the best of the authors' knowledge, this is the first publication on split orders in an RMFS. We analyze different performance metrics, such as pile-on, pod-station visits, robot moving distance and order turn-over time. We compare the results of our models in different instances with the sequential method in our open-source simulation framework RAWSim-O.",Efficient order picking methods in robotic mobile fulfillment systems,"Lin Xie, Nils Thieme, Ruslan Krenzler, Hanyi Li",2021,Artificial Intelligence,1902.03092
"Today's AI still faces two major challenges. One is that in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated learning framework, which includes horizontal federated learning, vertical federated learning and federated transfer learning. We provide definitions, architectures and applications for the federated learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allow knowledge to be shared without compromising user privacy.",Federated Machine Learning: Concept and Applications,"Qiang Yang, Yang Liu, Tianjian Chen, Yongxin Tong",2019,Artificial Intelligence,1902.04885
"Numerous, artificially intelligent, networked things will populate the battlefield of the future, operating in close collaboration with human warfighters, and fighting as teams in highly adversarial environments. This chapter explores the characteristics, capabilities and intelli-gence required of such a network of intelligent things and humans - Internet of Battle Things (IOBT). The IOBT will experience unique challenges that are not yet well addressed by the current generation of AI and machine learning.",Intelligent Autonomous Things on the Battlefield,"Alexander Kott, Ethan Stump",2019,Artificial Intelligence,1902.10086
"Navigating and understanding the real world remains a key challenge in machine learning and inspires a great variety of research in areas such as language grounding, planning, navigation and computer vision. We propose an instruction-following task that requires all of the above, and which combines the practicality of simulated environments with the challenges of ambiguous, noisy real world data. StreetNav is built on top of Google Street View and provides visually accurate environments representing real places. Agents are given driving instructions which they must learn to interpret in order to successfully navigate in this environment. Since humans equipped with driving instructions can readily navigate in previously unseen cities, we set a high bar and test our trained agents for similar cognitive capabilities. Although deep reinforcement learning (RL) methods are frequently evaluated only on data that closely follow the training distribution, our dataset extends to multiple cities and has a clean train/test separation. This allows for thorough testing of generalisation ability. This paper presents the StreetNav environment and tasks, models that establish strong baselines, and extensive analysis of the task and the trained agents.",Learning To Follow Directions in Street View,"Karl Moritz Hermann, Mateusz Malinowski, Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, Raia Hadsell",2020,Artificial Intelligence,1903.00401
"This paper presents a novel approach for learning STRIPS action models from examples that compiles this inductive learning task into a classical planning task. Interestingly, the compilation approach is flexible to different amounts of available input knowledge; the learning examples can range from a set of plans (with their corresponding initial and final states) to just a pair of initial and final states (no intermediate action or state is given). Moreover, the compilation accepts partially specified action models and it can be used to validate whether the observation of a plan execution follows a given STRIPS action model, even if this model is not fully specified.",Learning STRIPS Action Models with Classical Planning,"Diego Aineto, Sergio Jim\'enez and Eva Onaindia",2018,Artificial Intelligence,1903.01153
"Learning models of user behaviour is an important problem that is broadly applicable across many application domains requiring human-robot interaction. In this work we show that it is possible to learn a generative model for distinct user behavioral types, extracted from human demonstrations, by enforcing clustering of preferred task solutions within the latent space. We use this model to differentiate between user types and to find cases with overlapping solutions. Moreover, we can alter an initially guessed solution to satisfy the preferences that constitute a particular user type by backpropagating through the learned differentiable model. An advantage of structuring generative models in this way is that it allows us to extract causal relationships between symbols that might form part of the user's specification of the task, as manifested in the demonstrations. We show that the proposed method is capable of correctly distinguishing between three user types, who differ in degrees of cautiousness in their motion, while performing the task of moving objects with a kinesthetically driven robot in a tabletop environment. Our method successfully identifies the correct type, within the specified time, in 99% [97.8 - 99.8] of the cases, which outperforms an IRL baseline. We also show that our proposed method correctly changes a default trajectory to one satisfying a particular user specification even with unseen objects. The resulting trajectory is shown to be directly implementable on a PR2 humanoid robot completing the same task.",Using Causal Analysis to Learn Specifications from Task Demonstrations,"Daniel Angelov, Yordan Hristov, Subramanian Ramamoorthy",2019,Artificial Intelligence,1903.01267
"Argumentation theory is a powerful paradigm that formalizes a type of commonsense reasoning that aims to simulate the human ability to resolve a specific problem in an intelligent manner. A classical argumentation process takes into account only the properties related to the intrinsic logical soundness of an argument in order to determine its acceptability status. However, these properties are not always the only ones that matter to establish the argument's acceptability---there exist other qualities, such as strength, weight, social votes, trust degree, relevance level, and certainty degree, among others.",An Approach to Characterize Graded Entailment of Arguments through a Label-based Framework,"Maximiliano C. D. Bud\'an, Gerardo I. Simari, Ignacio Viglizzo and Guillermo R. Simari",2017,Artificial Intelligence,1903.01865
"A Timed Argumentation Framework (TAF) is a formalism where arguments are only valid for consideration in a given period of time, called availability intervals, which are defined for every individual argument. The original proposal is based on a single, abstract notion of attack between arguments that remains static and permanent in time. Thus, in general, when identifying the set of acceptable arguments, the outcome associated with a TAF will vary over time. In this work we introduce an extension of TAF adding the capability of modeling a support relation between arguments. In this sense, the resulting framework provides a suitable model for different time-dependent issues. Thus, the main contribution here is to provide an enhanced framework for modeling a positive (support) and negative (attack) interaction varying over time, which are relevant in many real-world situations. This leads to a Timed Bipolar Argumentation Framework (T-BAF), where classical argument extensions can be defined. The proposal aims at advancing in the integration of temporal argumentation in different application domain.",Bipolar in Temporal Argumentation Framework,"Maximiliano C. D. Bud\'an, Maria Laura Cobo, Diego C. Martinez and Guillermo R. Simari",2017,Artificial Intelligence,1903.01874
"In this work, we enrich a formalism for argumentation by including a formal characterization of features related to the knowledge, in order to capture proper reasoning in legal domains. We add meta-data information to the arguments in the form of labels representing quantitative and qualitative data about them. These labels are propagated through an argumentative graph according to the relations of support, conflict, and aggregation between arguments.",Dealing with Qualitative and Quantitative Features in Legal Domains,"Maximiliano C. D. Bud\'an, Mar\'ia Laura Cobo, Diego I. Mart\'inez and Antonino Rotolo",2018,Artificial Intelligence,1903.01966
"Current advances in research, development and application of artificial intelligence (AI) systems have yielded a far-reaching discourse on AI ethics. In consequence, a number of ethics guidelines have been released in recent years. These guidelines comprise normative principles and recommendations aimed to harness the ""disruptive"" potentials of new AI technologies. Designed as a comprehensive evaluation, this paper analyzes and compares these guidelines highlighting overlaps but also omissions. As a result, I give a detailed overview of the field of AI ethics. Finally, I also examine to what extent the respective ethical principles and values are implemented in the practice of research, development and application of AI systems - and how the effectiveness in the demands of AI ethics can be improved.",The Ethics of AI Ethics -- An Evaluation of Guidelines,Thilo Hagendorff,2020,Artificial Intelligence,1903.03425
"Interactive reinforcement learning has become an important apprenticeship approach to speed up convergence in classic reinforcement learning problems. In this regard, a variant of interactive reinforcement learning is policy shaping which uses a parent-like trainer to propose the next action to be performed and by doing so reduces the search space by advice. On some occasions, the trainer may be another artificial agent which in turn was trained using reinforcement learning methods to afterward becoming an advisor for other learner-agents. In this work, we analyze internal representations and characteristics of artificial agents to determine which agent may outperform others to become a better trainer-agent. Using a polymath agent, as compared to a specialist agent, an advisor leads to a larger reward and faster convergence of the reward signal and also to a more stable behavior in terms of the state visit frequency of the learner-agents. Moreover, we analyze system interaction parameters in order to determine how influential they are in the apprenticeship process, where the consistency of feedback is much more relevant when dealing with different learner obedience parameters.",Improving interactive reinforcement learning: What makes a good teacher?,"Francisco Cruz, Sven Magg, Yukie Nagai, Stefan Wermter",2018,Artificial Intelligence,1904.06879
"Many computer models such as cellular automata and artificial neural networks have been developed and successfully applied. However, in some cases, these models might be restrictive on the possible solutions or their solutions might be difficult to interpret. To overcome this problem, we outline a new approach, the so-called allagmatic method, that automatically programs and executes models with as little limitations as possible while maintaining human interpretability. Earlier we described a metamodel and its building blocks according to the philosophical concepts of structure (spatial dimension) and operation (temporal dimension). They are entity, milieu, and update function that together abstractly describe cellular automata, artificial neural networks, and possibly any kind of computer model. By automatically combining these building blocks in an evolutionary computation, interpretability might be increased by the relationship to the metamodel, and models might be translated into more interpretable models via the metamodel. We propose generic and object-oriented programming to implement the entities and their milieus as dynamic and generic arrays and the update function as a method. We show two experiments where a simple cellular automaton and an artificial neural network are automatically programmed, compiled, and executed. A target state is successfully evolved and learned in the cellular automaton and artificial neural network, respectively. We conclude that the allagmatic method can create and execute cellular automaton and artificial neural network models in an automated manner with the guidance of philosophy.",Automatic Programming of Cellular Automata and Artificial Neural Networks Guided by Philosophy,Patrik Christen and Olivier Del Fabbro,2020,Artificial Intelligence,1905.04232
"This paper surveys an approach to the XAI problem, using post-hoc explanation by example, that hinges on twinning Artificial Neural Networks (ANNs) with Case-Based Reasoning (CBR) systems, so-called ANN-CBR twins. A systematic survey of 1100+ papers was carried out to identify the fragmented literature on this topic and to trace it influence through to more recent work involving Deep Neural Networks (DNNs). The paper argues that this twin-system approach, especially using ANN-CBR twins, presents one possible coherent, generic solution to the XAI problem (and, indeed, XCBR problem). The paper concludes by road-mapping some future directions for this XAI solution involving (i) further tests of feature-weighting techniques, (iii) explorations of how explanatory cases might best be deployed (e.g., in counterfactuals, near-miss cases, a fortori cases), and (iii) the raising of the unwelcome and, much ignored, issue of human user evaluation.",How Case Based Reasoning Explained Neural Networks: An XAI Survey of Post-Hoc Explanation-by-Example in ANN-CBR Twins,Mark T Keane and Eoin M Kenny,2019,Artificial Intelligence,1905.07186
"Over the past decade, knowledge graphs became popular for capturing structured domain knowledge. Relational learning models enable the prediction of missing links inside knowledge graphs. More specifically, latent distance approaches model the relationships among entities via a distance between latent representations. Translating embedding models (e.g., TransE) are among the most popular latent distance approaches which use one distance function to learn multiple relation patterns. However, they are mostly inefficient in capturing symmetric relations since the representation vector norm for all the symmetric relations becomes equal to zero. They also lose information when learning relations with reflexive patterns since they become symmetric and transitive. We propose the Multiple Distance Embedding model (MDE) that addresses these limitations and a framework to collaboratively combine variant latent distance-based terms. Our solution is based on two principles: 1) we use a limit-based loss instead of a margin ranking loss and, 2) by learning independent embedding vectors for each of the terms we can collectively train and predict using contradicting distance terms. We further demonstrate that MDE allows modeling relations with (anti)symmetry, inversion, and composition patterns. We propose MDE as a neural network model that allows us to map non-linear relations between the embedding vectors and the expected output of the score function. Our empirical results show that MDE performs competitively to state-of-the-art embedding models on several benchmark datasets.",MDE: Multiple Distance Embeddings for Link Prediction in Knowledge Graphs,"Afshin Sadeghi, Damien Graux, Hamed Shariat Yazdi, Jens Lehmann",2020,Artificial Intelligence,1905.10702
"We propose a set of compositional design patterns to describe a large variety of systems that combine statistical techniques from machine learning with symbolic techniques from knowledge representation. As in other areas of computer science (knowledge engineering, software engineering, ontology engineering, process mining and others), such design patterns help to systematize the literature, clarify which combinations of techniques serve which purposes, and encourage re-use of software components. We have validated our set of compositional design patterns against a large body of recent literature.",A Boxology of Design Patterns for Hybrid Learning and Reasoning Systems,Frank van Harmelen and Annette ten Teije,2019,Artificial Intelligence,1905.12389
"In this paper, we present a simple and cheap ordinal bucketing algorithm that approximately generates $q$-quantiles from an incremental data stream. The bucketing is done dynamically in the sense that the amount of buckets $q$ increases with the number of seen samples. We show how this can be used in Ordinal Monte Carlo Tree Search (OMCTS) to yield better bounds on time and space complexity, especially in the presence of noisy rewards. Besides complexity analysis and quality tests of quantiles, we evaluate our method using OMCTS in the General Video Game Framework (GVGAI). Our results demonstrate its dominance over vanilla Monte Carlo Tree Search in the presence of noise, where OMCTS without bucketing has a very bad time and space complexity.",Ordinal Bucketing for Game Trees using Dynamic Quantile Approximation,"Tobias Joppen, Tilman Str\""ubig, Johannes F\""urnkranz",2019,Artificial Intelligence,1905.13449
"Kandinsky Figures and Kandinsky Patterns are mathematically describable, simple self-contained hence controllable test data sets for the development, validation and training of explainability in artificial intelligence. Whilst Kandinsky Patterns have these computationally manageable properties, they are at the same time easily distinguishable from human observers. Consequently, controlled patterns can be described by both humans and computers. We define a Kandinsky Pattern as a set of Kandinsky Figures, where for each figure an ""infallible authority"" defines that the figure belongs to the Kandinsky Pattern. With this simple principle we build training and validation data sets for automatic interpretability and context learning. In this paper we describe the basic idea and some underlying principles of Kandinsky Patterns and provide a Github repository to invite the international machine learning research community to a challenge to experiment with our Kandinsky Patterns to expand and thus make progress in the field of explainable AI and to contribute to the upcoming field of explainability and causability.",Kandinsky Patterns,Heimo Mueller and Andreas Holzinger,2021,Artificial Intelligence,1906.00657
"In this paper, we define and apply representational stability analysis (ReStA), an intuitive way of analyzing neural language models. ReStA is a variant of the popular representational similarity analysis (RSA) in cognitive neuroscience. While RSA can be used to compare representations in models, model components, and human brains, ReStA compares instances of the same model, while systematically varying single model parameter. Using ReStA, we study four recent and successful neural language models, and evaluate how sensitive their internal representations are to the amount of prior context. Using RSA, we perform a systematic study of how similar the representational spaces in the first and second (or higher) layers of these models are to each other and to patterns of activation in the human brain. Our results reveal surprisingly strong differences between language models, and give insights into where the deep linguistic processing, that integrates information over multiple sentences, is happening in these models. The combination of ReStA and RSA on models and brains allows us to start addressing the important question of what kind of linguistic processes we can hope to observe in fMRI brain imaging data. In particular, our results suggest that the data on story reading from Wehbe et al. (2014) contains a signal of shallow linguistic processing, but show no evidence on the more interesting deep linguistic processing.",Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains,"Samira Abnar, Lisa Beinborn, Rochelle Choenni, Willem Zuidema",2019,Artificial Intelligence,1906.01539
"In the last decades we have witnessed the success of applications of Artificial Intelligence to playing games. In this work we address the challenging field of games with hidden information and card games in particular. Jass is a very popular card game in Switzerland and is closely connected with Swiss culture. To the best of our knowledge, performances of Artificial Intelligence agents in the game of Jass do not outperform top players yet. Our contribution to the community is two-fold. First, we provide an overview of the current state-of-the-art of Artificial Intelligence methods for card games in general. Second, we discuss their application to the use-case of the Swiss card game Jass. This paper aims to be an entry point for both seasoned researchers and new practitioners who want to join in the Jass challenge.",Survey of Artificial Intelligence for Card Games and Its Application to the Swiss Game Jass,"Joel Niklaus, Michele Alberti, Vinaychandran Pondenkandath, Rolf Ingold, Marcus Liwicki",2019,Artificial Intelligence,1906.04439
"This article describes the application of soft computing methods for solving the problem of locating garbage accumulation points in urban scenarios. This is a relevant problem in modern smart cities, in order to reduce negative environmental and social impacts in the waste management process, and also to optimize the available budget from the city administration to install waste bins. A specific problem model is presented, which accounts for reducing the investment costs, enhance the number of citizens served by the installed bins, and the accessibility to the system. A family of single- and multi-objective heuristics based on the PageRank method and two mutiobjective evolutionary algorithms are proposed. Experimental evaluation performed on real scenarios on the cities of Montevideo (Uruguay) and Bahia Blanca (Argentina) demonstrates the effectiveness of the proposed approaches. The methods allow computing plannings with different trade-off between the problem objectives. The computed results improve over the current planning in Montevideo and provide a reasonable budget cost and quality of service for Bahia Blanca.",Soft computing methods for multiobjective location of garbage accumulation points in smart cities,"Jamal Toutouh, Diego Rossit, and Sergio Nesmachnow",2019,Artificial Intelligence,1906.10689
"Ontology-based knowledge bases (KBs) like DBpedia are very valuable resources, but their usefulness and usability is limited by various quality issues. One such issue is the use of string literals instead of semantically typed entities. In this paper we study the automated canonicalization of such literals, i.e., replacing the literal with an existing entity from the KB or with a new entity that is typed using classes from the KB. We propose a framework that combines both reasoning and machine learning in order to predict the relevant entities and types, and we evaluate this framework against state-of-the-art baselines for both semantic typing and entity matching.",Canonicalizing Knowledge Base Literals,Jiaoyan Chen and Ernesto Jimenez-Ruiz and Ian Horrocks,2019,Artificial Intelligence,1906.11180
"Making decisions in complex environments is a key challenge in artificial intelligence (AI). Situations involving multiple decision makers are particularly complex, leading to computational intractability of principled solution methods. A body of work in AI has tried to mitigate this problem by trying to distill interaction to its essence: how does the policy of one agent influence another agent? If we can find more compact representations of such influence, this can help us deal with the complexity, for instance by searching the space of influences rather than the space of policies. However, so far these notions of influence have been restricted in their applicability to special cases of interaction. In this paper we formalize influence-based abstraction (IBA), which facilitates the elimination of latent state factors without any loss in value, for a very general class of problems described as factored partially observable stochastic games (fPOSGs). On the one hand, this generalizes existing descriptions of influence, and thus can serve as the foundation for improvements in scalability and other insights in decision making in complex multiagent settings. On the other hand, since the presence of other agents can be seen as a generalization of single agent settings, our formulation of IBA also provides a sufficient statistic for decision making under abstraction for a single agent. We also give a detailed discussion of the relations to such previous works, identifying new insights and interpretations of these approaches. In these ways, this paper deepens our understanding of abstraction in a wide range of sequential decision making settings, providing the basis for new approaches and algorithms for a large class of problems.",A Sufficient Statistic for Influence in Structured Multiagent Environments,"Frans A. Oliehoek, Stefan Witwicki, Leslie P. Kaelbling",2021,Artificial Intelligence,1907.09278
"This paper deals with robust optimization applied to network flows. Two robust variants of the minimum-cost integer flow problem are considered. Thereby, uncertainty in problem formulation is limited to arc unit costs and expressed by a finite set of explicitly given scenarios. It is shown that both problem variants are NP-hard. To solve the considered variants, several heuristics based on local search or evolutionary computing are proposed. The heuristics are experimentally evaluated on appropriate problem instances.",Heuristic solutions to robust variants of the minimum-cost integer flow problem,"Marko \v{S}poljarec, Robert Manger",2020,Artificial Intelligence,1907.09468
"Humans as designers have quite versatile problem-solving strategies. Computer agents on the other hand can access large scale computational resources to solve certain design problems. Hence, if agents can learn from human behavior, a synergetic human-agent problem solving team can be created. This paper presents an approach to extract human design strategies and implicit rules, purely from historical human data, and use that for design generation. A two-step framework that learns to imitate human design strategies from observation is proposed and implemented. This framework makes use of deep learning constructs to learn to generate designs without any explicit information about objective and performance metrics. The framework is designed to interact with the problem through a visual interface as humans did when solving the problem. It is trained to imitate a set of human designers by observing their design state sequences without inducing problem-specific modelling bias or extra information about the problem. Furthermore, an end-to-end agent is developed that uses this deep learning framework as its core in conjunction with image processing to map pixel-to-design moves as a mechanism to generate designs. Finally, the designs generated by a computational team of these agents are then compared to actual human data for teams solving a truss design problem. Results demonstrates that these agents are able to create feasible and efficient truss designs without guidance, showing that this methodology allows agents to learn effective design strategies.",Learning to design from humans: Imitating human designers through deep learning,"Ayush Raina, Christopher McComb and Jonathan Cagan",2019,Artificial Intelligence,1907.11813
"This paper is the preprint of an invited commentary on Lake et al's Behavioral and Brain Sciences article titled ""Building machines that learn and think like people"". Lake et al's paper offers a timely critique on the recent accomplishments in artificial intelligence from the vantage point of human intelligence, and provides insightful suggestions about research directions for building more human-like intelligence. Since we agree with most of the points raised in that paper, we will offer a few points that are complementary.",What can the brain teach us about building artificial intelligence?,Dileep George,2017,Artificial Intelligence,1909.01561
"Over the last year, the amount of research in hierarchical planning has increased, leading to significant improvements in the performance of planners. However, the research is diverging and planners are somewhat hard to compare against each other. This is mostly caused by the fact that there is no standard set of benchmark domains, nor even a common description language for hierarchical planning problems. As a consequence, the available planners support a widely varying set of features and (almost) none of them can solve (or even parse) any problem developed for another planner. With this paper, we propose to create a new track for the IPC in which hierarchical planners will compete. This competition will result in a standardised description language, broader support for core features of that language among planners, a set of benchmark problems, a means to fairly and objectively compare HTN planners, and for new challenges for planners.",Hierarchical Planning in the IPC,"D. H\""oller, G. Behnke, P. Bercher, S. Biundo, H. Fiorino, D. Pellier, R. Alford",2019,Artificial Intelligence,1909.04405
"This paper introduces Strict Partial Order Networks (SPON), a novel neural network architecture designed to enforce asymmetry and transitive properties as soft constraints. We apply it to induce hypernymy relations by training with is-a pairs. We also present an augmented variant of SPON that can generalize type information learned for in-vocabulary terms to previously unseen ones. An extensive evaluation over eleven benchmarks across different tasks shows that SPON consistently either outperforms or attains the state of the art on all but one of these benchmarks.",Hypernym Detection Using Strict Partial Order Networks,"Sarthak Dash, Md Faisal Mahbub Chowdhury, Alfio Gliozzo, Nandana Mihindukulasooriya, Nicolas Rodolfo Fauceglia",2020,Artificial Intelligence,1909.10572
"Active inference is a first principle account of how autonomous agents operate in dynamic, non-stationary environments. This problem is also considered in reinforcement learning (RL), but limited work exists on comparing the two approaches on the same discrete-state environments. In this paper, we provide: 1) an accessible overview of the discrete-state formulation of active inference, highlighting natural behaviors in active inference that are generally engineered in RL; 2) an explicit discrete-state comparison between active inference and RL on an OpenAI gym baseline. We begin by providing a condensed overview of the active inference literature, in particular viewing the various natural behaviors of active inference agents through the lens of RL. We show that by operating in a pure belief-based setting, active inference agents can carry out epistemic exploration, and account for uncertainty about their environment in a Bayes-optimal fashion. Furthermore, we show that the reliance on an explicit reward signal in RL is removed in active inference, where reward can simply be treated as another observation; even in the total absence of rewards, agent behaviors are learned through preference learning. We make these properties explicit by showing two scenarios in which active inference agents can infer behaviors in reward-free environments compared to both Q-learning and Bayesian model-based RL agents; by placing zero prior preferences over rewards and by learning the prior preferences over the observations corresponding to reward. We conclude by noting that this formalism can be applied to more complex settings if appropriate generative models can be formulated. In short, we aim to demystify the behavior of active inference agents by presenting an accessible discrete state-space and time formulation, and demonstrate these behaviors in a OpenAI gym environment, alongside RL agents.",Active inference: demystified and compared,"Noor Sajid, Philip J. Ball, Thomas Parr, Karl J. Friston",2021,Artificial Intelligence,1909.10863
"We present ""AutoJudge"", an automated evaluation method for conversational dialogue systems. The method works by first generating dialogues based on self-talk, i.e. dialogue systems talking to itself. Then, it uses human ratings on these dialogues to train an automated judgement model. Our experiments show that AutoJudge correlates well with the human ratings and can be used to automatically evaluate dialogue systems, even in deployed systems. In a second part, we attempt to apply AutoJudge to improve existing systems. This works well for re-ranking a set of candidate utterances. However, our experiments show that AutoJudge cannot be applied as reward for reinforcement learning, although the metric can distinguish good from bad dialogues. We discuss potential reasons, but state here already that this is still an open question for further research.",Towards a Metric for Automated Conversational Dialogue System Evaluation and Improvement,"Jan Deriu, Mark Cieliebak",2019,Artificial Intelligence,1909.12066
"Dense urban traffic environments can produce situations where accurate prediction and dynamic models are insufficient for successful autonomous vehicle motion planning. We investigate how an autonomous agent can safely negotiate with other traffic participants, enabling the agent to handle potential deadlocks. Specifically we consider merges where the gap between cars is smaller than the size of the ego vehicle. We propose a game theoretic framework capable of generating and responding to interactive behaviors. Our main contribution is to show how game-tree decision making can be executed by an autonomous vehicle, including approximations and reasoning that make the tree-search computationally tractable. Additionally, to test our model we develop a stochastic rule-based traffic agent capable of generating interactive behaviors that can be used as a benchmark for simulating traffic participants in a crowded merge setting.",Interactive Decision Making for Autonomous Vehicles in Dense Traffic,David Isele,2019,Artificial Intelligence,1909.12914
"In a multi-agent setting, the optimal policy of a single agent is largely dependent on the behavior of other agents. We investigate the problem of multi-agent reinforcement learning, focusing on decentralized learning in non-stationary domains for mobile robot navigation. We identify a cause for the difficulty in training non-stationary policies: mutual adaptation to sub-optimal behaviors, and we use this to motivate a curriculum-based strategy for learning interactive policies. The curriculum has two stages. First, the agent leverages policy gradient algorithms to learn a policy that is capable of achieving multiple goals. Second, the agent learns a modifier policy to learn how to interact with other agents in a multi-agent setting. We evaluated our approach on both an autonomous driving lane-change domain and a robot navigation domain.",Interaction-Aware Multi-Agent Reinforcement Learning for Mobile Agents with Individual Goals,"Anahita Mohseni-Kabir, David Isele, and Kikuo Fujimura",2019,Artificial Intelligence,1909.12925
"Robotic navigation through crowds or herds requires the ability to both predict the future motion of nearby individuals and understand how these predictions might change in response to a robot's future action. State of the art trajectory prediction models using Recurrent Neural Networks (RNNs) do not currently account for a planned future action of a robot, and so cannot predict how an individual will move in response to a robot's planned path. We propose an approach that adapts RNNs to use a robot's next planned action as an input alongside the current position of nearby individuals. This allows the model to learn the response of individuals with regards to a robot's motion from real world observations. By linking a robot's actions to the response of those around it in training, we show that we are able to not only improve prediction accuracy in close range interactions, but also to predict the likely response of surrounding individuals to simulated actions. This allows the use of the model to simulate state transitions, without requiring any assumptions on agent interaction. We apply this model to varied datasets, including crowds of pedestrians interacting with vehicles and bicycles, and livestock interacting with a robotic vehicle.",Predicting Responses to a Robot's Future Motion using Generative Recurrent Neural Networks,Stuart Eiffert and Salah Sukkarieh,2019,Artificial Intelligence,1909.13486
"Often, when modeling human decision-making behaviors in the context of human-robot teaming, the emotion aspect of human is ignored. Nevertheless, the influence of emotion, in some cases, is not only undeniable but beneficial. This work studies the human-like characteristics brought by regret emotion in one-human-multi-robot teaming for the application of domain search. In such application, the task management load is outsourced to the robots to reduce the human's workload, freeing the human to do more important work. The regret decision model is first used by each robot for deciding whether to request human service, then is extended for optimally queuing the requests from multiple robots. For the movement of the robots in the domain search, we designed a path planning algorithm based on dynamic programming for each robot. The simulation shows that the human-like characteristics, namely, risk-seeking and risk-aversion, indeed bring some appealing effects for balancing the workload and performance in the human-multi-robot team.",Respect Your Emotion: Human-Multi-Robot Teaming based on Regret Decision Model,"Longsheng Jiang, Yue Wang",2019,Artificial Intelligence,1910.00087
"Combining neural networks with continuous logic and multicriteria decision making tools can reduce the black box nature of neural models. In this study, we show that nilpotent logical systems offer an appropriate mathematical framework for a hybridization of continuous nilpotent logic and neural models, helping to improve the interpretability and safety of machine learning. In our concept, perceptrons model soft inequalities; namely membership functions and continuous logical operators. We design the network architecture before training, using continuous logical operators and multicriteria decision tools with given weights working in the hidden layers. Designing the structure appropriately leads to a drastic reduction in the number of parameters to be learned. The theoretical basis offers a straightforward choice of activation functions (the cutting function or its differentiable approximation, the squashing function), and also suggests an explanation to the great success of the rectified linear unit (ReLU). In this study, we focus on the architecture of a hybrid model and introduce the building blocks for future application in deep neural networks. The concept is illustrated with some toy examples taken from an extended version of the tensorflow playground.",Interpretable neural networks based on continuous-valued logic and multicriteria decision operators,"Orsolya Csisz\'ar, G\'abor Csisz\'ar, J\'ozsef Dombi",2020,Artificial Intelligence,1910.02486
"Legg and Hutter, as well as subsequent authors, considered intelligent agents through the lens of interaction with reward-giving environments, attempting to assign numeric intelligence measures to such agents, with the guiding principle that a more intelligent agent should gain higher rewards from environments in some aggregate sense. In this paper, we consider a related question: rather than measure numeric intelligence of one Legg- Hutter agent, how can we compare the relative intelligence of two Legg-Hutter agents? We propose an elegant answer based on the following insight: we can view Legg-Hutter agents as candidates in an election, whose voters are environments, letting each environment vote (via its rewards) which agent (if either) is more intelligent. This leads to an abstract family of comparators simple enough that we can prove some structural theorems about them. It is an open question whether these structural theorems apply to more practical intelligence measures.",Intelligence via ultrafilters: structural properties of some intelligence comparators of deterministic Legg-Hutter agents,Samuel Allen Alexander,2019,Artificial Intelligence,1910.09721
"Using neural networks in the reinforcement learning (RL) framework has achieved notable successes. Yet, neural networks tend to forget what they learned in the past, especially when they learn online and fully incrementally, a setting in which the weights are updated after each sample is received and the sample is then discarded. Under this setting, an update can lead to overly global generalization by changing too many weights. The global generalization interferes with what was previously learned and deteriorates performance, a phenomenon known as catastrophic interference. Many previous works use mechanisms such as experience replay (ER) buffers to mitigate interference by performing minibatch updates, ensuring the data distribution is approximately independent-and-identically-distributed (i.i.d.). But using ER would become infeasible in terms of memory as problem complexity increases. Thus, it is crucial to look for more memory-efficient alternatives. Interference can be averted if we replace global updates with more local ones, so only weights responsible for the observed data sample are updated. In this work, we propose the use of dynamic self-organizing map (DSOM) with neural networks to induce such locality in the updates without ER buffers. Our method learns a DSOM to produce a mask to reweigh each hidden unit's output, modulating its degree of use. It prevents interference by replacing global updates with local ones, conditioned on the agent's state. We validate our method on standard RL benchmarks including Mountain Car and Lunar Lander, where existing methods often fail to learn without ER. Empirically, we show that our online and fully incremental method is on par with and in some cases, better than state-of-the-art in terms of final performance and learning speed. We provide visualizations and quantitative measures to show that our method indeed mitigates interference.",Overcoming Catastrophic Interference in Online Reinforcement Learning with Dynamic Self-Organizing Maps,Yat Long Lo and Sina Ghiassian,2019,Artificial Intelligence,1910.13213
"The fourth edition of the international workshop on Causation, Responsibility and Explanation took place in Prague (Czech Republic) as part of ETAPS 2019. The program consisted in 5 invited speakers and 4 regular papers, whose selection was based on a careful reviewing process and that are included in these proceedings.","Proceedings of the 4th Workshop on Formal Reasoning about Causation, Responsibility, and Explanations in Science and Technology","Georgiana Caltais (Konstanz University), Jean Krivine (CNRS)",2019,Artificial Intelligence,1910.13641
"Information retrieval (IR) systems need to constantly update their knowledge as target objects and user queries change over time. Due to the power-law nature of linguistic data, learning lexical concepts is a problem resisting standard machine learning approaches: while manual intervention is always possible, a more general and automated solution is desirable. In this work, we propose a novel end-to-end framework that models the interaction between a search engine and users as a virtuous human-in-the-loop inference. The proposed framework is the first to our knowledge combining ideas from psycholinguistics and experiment design to maximize efficiency in IR. We provide a brief overview of the main components and initial simulations in a toy world, showing how inference works end-to-end and discussing preliminary results and next steps.",Lexical Learning as an Online Optimal Experiment: Building Efficient Search Engines through Human-Machine Collaboration,"Jacopo Tagliabue, Reuben Cohn-Gordon",2019,Artificial Intelligence,1910.14164
"The Shapes Constraint Language (SHACL) has been recently introduced as a W3C recommendation to define constraints that can be validated against RDF graphs. Interactions of SHACL with other Semantic Web technologies, such as ontologies or reasoners, is a matter of ongoing research. In this paper we study the interaction of a subset of SHACL with inference rules expressed in datalog. On the one hand, SHACL constraints can be used to define a ""schema"" for graph datasets. On the other hand, inference rules can lead to the discovery of new facts that do not match the original schema. Given a set of SHACL constraints and a set of datalog rules, we present a method to detect which constraints could be violated by the application of the inference rules on some graph instance of the schema, and update the original schema, i.e, the set of SHACL constraints, in order to capture the new facts that can be inferred. We provide theoretical and experimental results of the various components of our approach.",SHACL Constraints with Inference Rules,"Paolo Pareti, George Konstantinidis, Timothy J. Norman, Murat \c{S}ensoy",2019,Artificial Intelligence,1911.00598
"The debate on AI ethics largely focuses on technical improvements and stronger regulation to prevent accidents or misuse of AI, with solutions relying on holding individual actors accountable for responsible AI development. While useful and necessary, we argue that this ""agency"" approach disregards more indirect and complex risks resulting from AI's interaction with the socio-economic and political context. This paper calls for a ""structural"" approach to assessing AI's effects in order to understand and prevent such systemic risks where no individual can be held accountable for the broader negative impacts. This is particularly relevant for AI applied to systemic issues such as climate change and food security which require political solutions and global cooperation. To properly address the wide range of AI risks and ensure 'AI for social good', agency-focused policies must be complemented by policies informed by a structural approach.",AI Ethics for Systemic Issues: A Structural Approach,"Agnes Schim van der Loeff, Iggy Bassi, Sachin Kapila, Jevgenij Gamper",2019,Artificial Intelligence,1911.03216
"In level co-creation an AI and human work together to create a video game level. One open challenge in level co-creation is how to empower human users to ensure particular qualities of the final level, such as challenge. There has been significant prior research into automated pathing and automated playtesting for video game levels, but not in how to incorporate these into tools. In this demonstration we present an improvement of the Morai Maker mixed-initiative level editor for Super Mario Bros. that includes automated pathing and challenge approximation features.",Integrating Automated Play in Level Co-Creation,"Andrew Hoyt, Matthew Guzdial, Yalini Kumar, Gillian Smith, and Mark O. Riedl",2019,Artificial Intelligence,1911.09219
"Recent superhuman results in games have largely been achieved in a variety of zero-sum settings, such as Go and Poker, in which agents need to compete against others. However, just like humans, real-world AI systems have to coordinate and communicate with other agents in cooperative partially observable environments as well. These settings commonly require participants to both interpret the actions of others and to act in a way that is informative when being interpreted. Those abilities are typically summarized as theory f mind and are seen as crucial for social interactions. In this paper we propose two different search techniques that can be applied to improve an arbitrary agreed-upon policy in a cooperative partially observable game. The first one, single-agent search, effectively converts the problem into a single agent setting by making all but one of the agents play according to the agreed-upon policy. In contrast, in multi-agent search all agents carry out the same common-knowledge search procedure whenever doing so is computationally feasible, and fall back to playing according to the agreed-upon policy otherwise. We prove that these search procedures are theoretically guaranteed to at least maintain the original performance of the agreed-upon policy (up to a bounded approximation error). In the benchmark challenge problem of Hanabi, our search technique greatly improves the performance of every agent we tested and when applied to a policy trained using RL achieves a new state-of-the-art score of 24.61 / 25 in the game, compared to a previous-best of 24.08 / 25.",Improving Policies via Search in Cooperative Partially Observable Games,"Adam Lerer, Hengyuan Hu, Jakob Foerster, Noam Brown",2020,Artificial Intelligence,1912.02318
"SUMMARY: Recently, novel machine-learning algorithms have shown potential for predicting undiscovered links in biomedical knowledge networks. However, dedicated benchmarks for measuring algorithmic progress have not yet emerged. With OpenBioLink, we introduce a large-scale, high-quality and highly challenging biomedical link prediction benchmark to transparently and reproducibly evaluate such algorithms. Furthermore, we present preliminary baseline evaluation results. AVAILABILITY AND IMPLEMENTATION: Source code, data and supplementary files are openly available at https://github.com/OpenBioLink/OpenBioLink CONTACT: matthias.samwald ((at)) meduniwien.ac.at",OpenBioLink: A benchmarking framework for large-scale biomedical link prediction,"Anna Breit, Simon Ott, Asan Agibetov, Matthias Samwald",2020,Artificial Intelligence,1912.04616
"The main goal of this paper is to describe an axiomatic utility theory for Dempster-Shafer belief function lotteries. The axiomatic framework used is analogous to von Neumann-Morgenstern's utility theory for probabilistic lotteries as described by Luce and Raiffa. Unlike the probabilistic case, our axiomatic framework leads to interval-valued utilities, and therefore, to a partial (incomplete) preference order on the set of all belief function lotteries. If the belief function reference lotteries we use are Bayesian belief functions, then our representation theorem coincides with Jaffray's representation theorem for his linear utility theory for belief functions. We illustrate our representation theorem using some examples discussed in the literature, and we propose a simple model for assessing utilities based on an interval-valued pessimism index representing a decision-maker's attitude to ambiguity and indeterminacy. Finally, we compare our decision theory with those proposed by Jaffray, Smets, Dubois et al., Giang and Shenoy, and Shafer.",An Interval-Valued Utility Theory for Decision Making with Dempster-Shafer Belief Functions,Thierry Denoeux and Prakash P. Shenoy,2020,Artificial Intelligence,1912.06594
"Counterfactual Thinking is a human cognitive ability studied in a wide variety of domains. It captures the process of reasoning about a past event that did not occur, namely what would have happened had this event occurred, or, otherwise, to reason about an event that did occur but what would ensue had it not. Given the wide cognitive empowerment of counterfactual reasoning in the human individual, the question arises of how the presence of individuals with this capability may improve cooperation in populations of self-regarding individuals. Here we propose a mathematical model, grounded on Evolutionary Game Theory, to examine the population dynamics emerging from the interplay between counterfactual thinking and social learning (i.e., individuals that learn from the actions and success of others) whenever the individuals in the population face a collective dilemma. Our results suggest that counterfactual reasoning fosters coordination in collective action problems occurring in large populations, and has a limited impact on cooperation dilemmas in which coordination is not required. Moreover, we show that a small prevalence of individuals resorting to counterfactual thinking is enough to nudge an entire population towards highly cooperative standards.",Counterfactual thinking in cooperation dynamics,Luis Moniz Pereira and Francisco C. Santos,2019,Artificial Intelligence,1912.08946
"Recent work on fairness in machine learning has primarily emphasized how to define, quantify, and encourage ""fair"" outcomes. Less attention has been paid, however, to the ethical foundations which underlie such efforts. Among the ethical perspectives that should be taken into consideration is consequentialism, the position that, roughly speaking, outcomes are all that matter. Although consequentialism is not free from difficulties, and although it does not necessarily provide a tractable way of choosing actions (because of the combined problems of uncertainty, subjectivity, and aggregation), it nevertheless provides a powerful foundation from which to critique the existing literature on machine learning fairness. Moreover, it brings to the fore some of the tradeoffs involved, including the problem of who counts, the pros and cons of using a policy, and the relative value of the distant future. In this paper we provide a consequentialist critique of common definitions of fairness within machine learning, as well as a machine learning perspective on consequentialism. We conclude with a broader discussion of the issues of learning and randomization, which have important implications for the ethics of automated decision making systems.",On Consequentialism and Fairness,Dallas Card and Noah A. Smith,2020,Artificial Intelligence,2001.00329
"The wide adoption of machine learning in the critical domains such as medical diagnosis, law, education had propelled the need for interpretable techniques due to the need for end users to understand the reasoning behind decisions due to learning systems. The computational intractability of interpretable learning led practitioners to design heuristic techniques, which fail to provide sound handles to tradeoff accuracy and interpretability. Motivated by the success of MaxSAT solvers over the past decade, recently MaxSAT-based approach, called MLIC, was proposed that seeks to reduce the problem of learning interpretable rules expressed in Conjunctive Normal Form (CNF) to a MaxSAT query. While MLIC was shown to achieve accuracy similar to that of other state of the art black-box classifiers while generating small interpretable CNF formulas, the runtime performance of MLIC is significantly lagging and renders approach unusable in practice. In this context, authors raised the question: Is it possible to achieve the best of both worlds, i.e., a sound framework for interpretable learning that can take advantage of MaxSAT solvers while scaling to real-world instances? In this paper, we take a step towards answering the above question in affirmation. We propose IMLI: an incremental approach to MaxSAT based framework that achieves scalable runtime performance via partition-based training methodology. Extensive experiments on benchmarks arising from UCI repository demonstrate that IMLI achieves up to three orders of magnitude runtime improvement without loss of accuracy and interpretability.",IMLI: An Incremental Framework for MaxSAT-Based Learning of Interpretable Classification Rules,Bishwamittra Ghosh and Kuldeep S. Meel,2019,Artificial Intelligence,2001.01891
"This work presents an architecture that generates curiosity-driven goal-directed exploration behaviours for an image sensor of a microfarming robot. A combination of deep neural networks for offline unsupervised learning of low-dimensional features from images, and of online learning of shallow neural networks representing the inverse and forward kinematics of the system have been used. The artificial curiosity system assigns interest values to a set of pre-defined goals, and drives the exploration towards those that are expected to maximise the learning progress. We propose the integration of an episodic memory in intrinsic motivation systems to face catastrophic forgetting issues, typically experienced when performing online updates of artificial neural networks. Our results show that adopting an episodic memory system not only prevents the computational models from quickly forgetting knowledge that has been previously acquired, but also provides new avenues for modulating the balance between plasticity and stability of the models.",Intrinsic Motivation and Episodic Memories for Robot Exploration of High-Dimensional Sensory Spaces,"Guido Schillaci, Antonio Pico Villalpando, Verena Vanessa Hafner, Peter Hanappe, David Colliaux, Timoth\'ee Wintz",2020,Artificial Intelligence,2001.01982
"Autonomous systems are often required to operate in partially observable environments. They must reliably execute a specified objective even with incomplete information about the state of the environment. We propose a methodology to synthesize policies that satisfy a linear temporal logic formula in a partially observable Markov decision process (POMDP). By formulating a planning problem, we show how to use point-based value iteration methods to efficiently approximate the maximum probability of satisfying a desired logical formula and compute the associated belief state policy. We demonstrate that our method scales to large POMDP domains and provides strong bounds on the performance of the resulting policy.",Point-Based Methods for Model Checking in Partially Observable Markov Decision Processes,"Maxime Bouton, Jana Tumova, and Mykel J. Kochenderfer",2020,Artificial Intelligence,2001.03809
"Explaining sophisticated machine-learning based systems is an important issue at the foundations of AI. Recent efforts have shown various methods for providing explanations. These approaches can be broadly divided into two schools: those that provide a local and human interpreatable approximation of a machine learning algorithm, and logical approaches that exactly characterise one aspect of the decision. In this paper we focus upon the second school of exact explanations with a rigorous logical foundation. There is an epistemological problem with these exact methods. While they can furnish complete explanations, such explanations may be too complex for humans to understand or even to write down in human readable form. Interpretability requires epistemically accessible explanations, explanations humans can grasp. Yet what is a sufficiently complete epistemically accessible explanation still needs clarification. We do this here in terms of counterfactuals, following [Wachter et al., 2017]. With counterfactual explanations, many of the assumptions needed to provide a complete explanation are left implicit. To do so, counterfactual explanations exploit the properties of a particular data point or sample, and as such are also local as well as partial explanations. We explore how to move from local partial explanations to what we call complete local explanations and then to global ones. But to preserve accessibility we argue for the need for partiality. This partiality makes it possible to hide explicit biases present in the algorithm that may be injurious or unfair.We investigate how easy it is to uncover these biases in providing complete and fair explanations by exploiting the structure of the set of counterfactuals providing a complete local explanation.",Adequate and fair explanations,"Nicholas Asher, Soumya Paul, Chris Russell",2021,Artificial Intelligence,2001.07578
"Automated decision making based on big data and machine learning (ML) algorithms can result in discriminatory decisions against certain protected groups defined upon personal data like gender, race, sexual orientation etc. Such algorithms designed to discover patterns in big data might not only pick up any encoded societal biases in the training data, but even worse, they might reinforce such biases resulting in more severe discrimination. The majority of thus far proposed fairness-aware machine learning approaches focus solely on the pre-, in- or post-processing steps of the machine learning process, that is, input data, learning algorithms or derived models, respectively. However, the fairness problem cannot be isolated to a single step of the ML process. Rather, discrimination is often a result of complex interactions between big data and algorithms, and therefore, a more holistic approach is required. The proposed FAE (Fairness-Aware Ensemble) framework combines fairness-related interventions at both pre- and postprocessing steps of the data analysis process. In the preprocessing step, we tackle the problems of under-representation of the protected group (group imbalance) and of class-imbalance by generating balanced training samples. In the post-processing step, we tackle the problem of class overlapping by shifting the decision boundary in the direction of fairness.",FAE: A Fairness-Aware Ensemble Framework,"Vasileios Iosifidis, Besnik Fetahu, Eirini Ntoutsi",2019,Artificial Intelligence,2002.00695
"Task-allocation is an important problem in multi-agent systems. It becomes more challenging when the team-members are humans with imperfect knowledge about their teammates' costs and the overall performance metric. While distributed task-allocation methods let the team-members engage in iterative dialog to reach a consensus, the process can take a considerable amount of time and communication. On the other hand, a centralized method that simply outputs an allocation may result in discontented human team-members who, due to their imperfect knowledge and limited computation capabilities, perceive the allocation to be unfair. To address these challenges, we propose a centralized Artificial Intelligence Task Allocation (AITA) that simulates a negotiation and produces a negotiation-aware task allocation that is fair. If a team-member is unhappy with the proposed allocation, we allow them to question the proposed allocation using a counterfactual. By using parts of the simulated negotiation, we are able to provide contrastive explanations that providing minimum information about other's costs to refute their foil. With human studies, we show that (1) the allocation proposed using our method does indeed appear fair to the majority, and (2) when a counterfactual is raised, explanations generated are easy to comprehend and convincing. Finally, we empirically study the effect of different kinds of incompleteness on the explanation-length and find that underestimation of a teammate's costs often increases it.",`Why didn't you allocate this task to them?' Negotiation-Aware Task Allocation and Contrastive Explanation Generation,"Zahra Zahedi, Sailik Sengupta, Subbarao Kambhampati",2020,Artificial Intelligence,2002.01640
"Monte-Carlo Tree Search (MCTS) is one of the most-widely used methods for planning, and has powered many recent advances in artificial intelligence. In MCTS, one typically performs computations (i.e., simulations) to collect statistics about the possible future consequences of actions, and then chooses accordingly. Many popular MCTS methods such as UCT and its variants decide which computations to perform by trading-off exploration and exploitation. In this work, we take a more direct approach, and explicitly quantify the value of a computation based on its expected impact on the quality of the action eventually chosen. Our approach goes beyond the ""myopic"" limitations of existing computation-value-based methods in two senses: (I) we are able to account for the impact of non-immediate (ie, future) computations (II) on non-immediate actions. We show that policies that greedily optimize computation values are optimal under certain assumptions and obtain results that are competitive with the state-of-the-art.",Static and Dynamic Values of Computation in MCTS,Eren Sezener and Peter Dayan,2020,Artificial Intelligence,2002.04335
"Information theory can be used to analyze the cost-benefit of visualization processes. However, the current measure of benefit contains an unbounded term that is neither easy to estimate nor intuitive to interpret. In this work, we propose to revise the existing cost-benefit measure by replacing the unbounded term with a bounded one. We examine a number of bounded measures that include the Jenson-Shannon divergence and a new divergence measure formulated as part of this work. We use visual analysis to support the multi-criteria comparison, narrowing the search down to those options with better mathematical properties. We apply those remaining options to two visualization case studies to instantiate their uses in practical scenarios, while the collected real world data further informs the selection of a bounded measure, which can be used to estimate the benefit of visualization.",A Bounded Measure for Estimating the Benefit of Visualization,"Min Chen, Mateu Sbert, Alfie Abdul-Rahman, and Deborah Silver",2022,Artificial Intelligence,2002.05282
"Developmental machine learning studies how artificial agents can model the way children learn open-ended repertoires of skills. Such agents need to create and represent goals, select which ones to pursue and learn to achieve them. Recent approaches have considered goal spaces that were either fixed and hand-defined or learned using generative models of states. This limited agents to sample goals within the distribution of known effects. We argue that the ability to imagine out-of-distribution goals is key to enable creative discoveries and open-ended learning. Children do so by leveraging the compositionality of language as a tool to imagine descriptions of outcomes they never experienced before, targeting them as goals during play. We introduce IMAGINE, an intrinsically motivated deep reinforcement learning architecture that models this ability. Such imaginative agents, like children, benefit from the guidance of a social peer who provides language descriptions. To take advantage of goal imagination, agents must be able to leverage these descriptions to interpret their imagined out-of-distribution goals. This generalization is made possible by modularity: a decomposition between learned goal-achievement reward function and policy relying on deep sets, gated attention and object-centered representations. We introduce the Playground environment and study how this form of goal imagination improves generalization and exploration over agents lacking this capacity. In addition, we identify the properties of goal imagination that enable these results and study the impacts of modularity and social interactions.",Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven Exploration,"C\'edric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Cl\'ement Moulin-Frier, Peter Ford Dominey, Pierre-Yves Oudeyer",2020,Artificial Intelligence,2002.09253
"In this paper I argue that the search for explainable models and interpretable decisions in AI must be reformulated in terms of the broader project of offering a pragmatic and naturalistic account of understanding in AI. Intuitively, the purpose of providing an explanation of a model or a decision is to make it understandable to its stakeholders. But without a previous grasp of what it means to say that an agent understands a model or a decision, the explanatory strategies will lack a well-defined goal. Aside from providing a clearer objective for XAI, focusing on understanding also allows us to relax the factivity condition on explanation, which is impossible to fulfill in many machine learning models, and to focus instead on the pragmatic conditions that determine the best fit between a model and the methods and devices deployed to understand it. After an examination of the different types of understanding discussed in the philosophical and psychological literature, I conclude that interpretative or approximation models not only provide the best way to achieve the objectual understanding of a machine learning model, but are also a necessary condition to achieve post-hoc interpretability. This conclusion is partly based on the shortcomings of the purely functionalist approach to post-hoc interpretability that seems to be predominant in most recent literature.",The Pragmatic Turn in Explainable Artificial Intelligence (XAI),Andr\'es P\'aez,2019,Artificial Intelligence,2002.09595
"Knowledge Graphs (KGs) are graph-structured knowledge bases storing factual information about real-world entities. Understanding the uniqueness of each entity is crucial to the analyzing, sharing, and reusing of KGs. Traditional profiling technologies encompass a vast array of methods to find distinctive features in various applications, which can help to differentiate entities in the process of human understanding of KGs. In this work, we present a novel profiling approach to identify distinctive entity features. The distinctiveness of features is carefully measured by a HAS model, which is a scalable representation learning model to produce a multi-pattern entity embedding. We fully evaluate the quality of entity profiles generated from real KGs. The results show that our approach facilitates human understanding of entities in KGs.",Entity Profiling in Knowledge Graphs,"Xiang Zhang, Qingqing Yang, Jinru Ding and Ziyue Wang",2020,Artificial Intelligence,2003.00172
"In this paper we discuss how systems with Artificial Intelligence (AI) can undergo safety assessment. This is relevant, if AI is used in safety related applications. Taking a deeper look into AI models, we show, that many models of artificial intelligence, in particular machine learning, are statistical models. Safety assessment would then have t o concentrate on the model that is used in AI, besides the normal assessment procedure. Part of the budget of dangerous random failures for the relevant safety integrity level needs to be used for the probabilistic faulty behavior of the AI system. We demonstrate our thoughts with a simple example and propose a research challenge that may be decisive for the use of AI in safety related systems.",On Safety Assessment of Artificial Intelligence,"Jens Braband and Hendrik Sch\""abe",2020,Artificial Intelligence,2003.00260
"Explainability and interpretability of AI models is an essential factor affecting the safety of AI. While various explainable AI (XAI) approaches aim at mitigating the lack of transparency in deep networks, the evidence of the effectiveness of these approaches in improving usability, trust, and understanding of AI systems are still missing. We evaluate multimodal explanations in the setting of a Visual Question Answering (VQA) task, by asking users to predict the response accuracy of a VQA agent with and without explanations. We use between-subjects and within-subjects experiments to probe explanation effectiveness in terms of improving user prediction accuracy, confidence, and reliance, among other factors. The results indicate that the explanations help improve human prediction accuracy, especially in trials when the VQA system's answer is inaccurate. Furthermore, we introduce active attention, a novel method for evaluating causal attentional effects through intervention by editing attention maps. User explanation ratings are strongly correlated with human prediction accuracy and suggest the efficacy of these explanations in human-machine AI collaboration tasks.",A Study on Multimodal and Interactive Explanations for Visual Question Answering,"Kamran Alipour, Jurgen P. Schulze, Yi Yao, Avi Ziskind, Giedrius Burachas",2020,Artificial Intelligence,2003.00431
"Given the large variety of existing logical formalisms it is of utmost importance to select the most adequate one for a specific purpose, e.g. for representing the knowledge relevant for a particular application or for using the formalism as a modeling tool for problem solving. Awareness of the nature of a logical formalism, in other words, of its fundamental intrinsic properties, is indispensable and provides the basis of an informed choice. In this treatise we consider the existence characterization logics as well as properties like existence and uniqueness, expressibility, replaceability and verifiability in the realm of abstract argumentation",On the Existence of Characterization Logics and Fundamental Properties of Argumentation Semantics,Ringo Baumann,2019,Artificial Intelligence,2003.00767
"ML models are increasingly deployed in settings with real world interactions such as vehicles, but unfortunately, these models can fail in systematic ways. To prevent errors, ML engineering teams monitor and continuously improve these models. We propose a new abstraction, model assertions, that adapts the classical use of program assertions as a way to monitor and improve ML models. Model assertions are arbitrary functions over a model's input and output that indicate when errors may be occurring, e.g., a function that triggers if an object rapidly changes its class in a video. We propose methods of using model assertions at all stages of ML system deployment, including runtime monitoring, validating labels, and continuously improving ML models. For runtime monitoring, we show that model assertions can find high confidence errors, where a model returns the wrong output with high confidence, which uncertainty-based monitoring techniques would not detect. For training, we propose two methods of using model assertions. First, we propose a bandit-based active learning algorithm that can sample from data flagged by assertions and show that it can reduce labeling costs by up to 40% over traditional uncertainty-based methods. Second, we propose an API for generating ""consistency assertions"" (e.g., the class change example) and weak labels for inputs where the consistency assertions fail, and show that these weak labels can improve relative model quality by up to 46%. We evaluate model assertions on four real-world tasks with video, LIDAR, and ECG data.",Model Assertions for Monitoring and Improving ML Models,"Daniel Kang, Deepti Raghavan, Peter Bailis, Matei Zaharia",2020,Artificial Intelligence,2003.01668
"We present a novel technique called Dynamic Experience Replay (DER) that allows Reinforcement Learning (RL) algorithms to use experience replay samples not only from human demonstrations but also successful transitions generated by RL agents during training and therefore improve training efficiency. It can be combined with an arbitrary off-policy RL algorithm, such as DDPG or DQN, and their distributed versions. We build upon Ape-X DDPG and demonstrate our approach on robotic tight-fitting joint assembly tasks, based on force/torque and Cartesian pose observations. In particular, we run experiments on two different tasks: peg-in-hole and lap-joint. In each case, we compare different replay buffer structures and how DER affects them. Our ablation studies show that Dynamic Experience Replay is a crucial ingredient that either largely shortens the training time in these challenging environments or solves the tasks that the vanilla Ape-X DDPG cannot solve. We also show that our policies learned purely in simulation can be deployed successfully on the real robot. The video presenting our experiments is available at https://sites.google.com/site/dynamicexperiencereplay",Dynamic Experience Replay,Jieliang Luo and Hui Li,2020,Artificial Intelligence,2003.02372
"We propose the Interactive Constrained MAP-Elites, a quality-diversity solution for game content generation, implemented as a new feature of the Evolutionary Dungeon Designer: a mixed-initiative co-creativity tool for designing dungeons. The feature uses the MAP-Elites algorithm, an illumination algorithm that segregates the population among several cells depending on their scores with respect to different behavioral dimensions. Users can flexibly and dynamically alternate between these dimensions anytime, thus guiding the evolutionary process in an intuitive way, and then incorporate suggestions produced by the algorithm in their room designs. At the same time, any modifications performed by the human user will feed back into MAP-Elites, closing a circular workflow of constant mutual inspiration. This paper presents the algorithm followed by an in-depth analysis of its behaviour, with the aims of evaluating the expressive range of all possible dimension combinations in several scenarios, as well as discussing their influence in the fitness landscape and in the overall performance of the mixed-initiative procedural content generation.",Interactive Constrained MAP-Elites: Analysis and Evaluation of the Expressiveness of the Feature Dimensions,"Alberto Alvarez, Steve Dahlskog, Jose Font and Julian Togelius",2020,Artificial Intelligence,2003.03377
"We describe nearly fifteen years of General Game Playing experimental research history in the context of reproducibility and fairness of comparisons between various GGP agents and systems designed to play games described by different formalisms. We think our survey may provide an interesting perspective of how chaotic methods were allowed when nothing better was possible. Finally, from our experience-based view, we would like to propose a few recommendations of how such specific heterogeneous branch of research should be handled appropriately in the future. The goal of this note is to point out common difficulties and problems in the experimental research in the area. We hope that our recommendations will help in avoiding them in future works and allow more fair and reproducible comparisons.",Experimental Studies in General Game Playing: An Experience Report,"Jakub Kowalski, Marek Szyku{\l}a",2020,Artificial Intelligence,2003.03410
"Continual learning studies agents that learn from streams of tasks without forgetting previous ones while adapting to new ones. Two recent continual-learning scenarios have opened new avenues of research. In meta-continual learning, the model is pre-trained to minimize catastrophic forgetting of previous tasks. In continual-meta learning, the aim is to train agents for faster remembering of previous tasks through adaptation. In their original formulations, both methods have limitations. We stand on their shoulders to propose a more general scenario, OSAKA, where an agent must quickly solve new (out-of-distribution) tasks, while also requiring fast remembering. We show that current continual learning, meta-learning, meta-continual learning, and continual-meta learning techniques fail in this new scenario. We propose Continual-MAML, an online extension of the popular MAML algorithm as a strong baseline for this scenario. We empirically show that Continual-MAML is better suited to the new scenario than the aforementioned methodologies, as well as standard continual learning and meta-learning approaches.",Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning,"Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Caccia, Issam Laradji, Irina Rish, Alexandre Lacoste, David Vazquez, Laurent Charlin",2020,Artificial Intelligence,2003.05856
"Smart home environments equipped with distributed sensor networks are capable of helping people by providing services related to health, emergency detection or daily routine management. A backbone to these systems relies often on the system's ability to track and detect activities performed by the users in their home. Despite the continuous progress in the area of activity recognition in smart homes, many systems make a strong underlying assumption that the number of occupants in the home at any given moment of time is always known. Estimating the number of persons in a Smart Home at each time step remains a challenge nowadays. Indeed, unlike most (crowd) counting solution which are based on computer vision techniques, the sensors considered in a Smart Home are often very simple and do not offer individually a good overview of the situation. The data gathered needs therefore to be fused in order to infer useful information. This paper aims at addressing this challenge and presents a probabilistic approach able to estimate the number of persons in the environment at each time step. This approach works in two steps: first, an estimate of the number of persons present in the environment is done using a Constraint Satisfaction Problem solver, based on the topology of the sensor network and the sensor activation pattern at this time point. Then, a Hidden Markov Model refines this estimate by considering the uncertainty related to the sensors. Using both simulated and real data, our method has been tested and validated on two smart homes of different sizes and configuration and demonstrates the ability to accurately estimate the number of inhabitants.",Online Guest Detection in a Smart Home using Pervasive Sensors and Probabilistic Reasoning,"Jennifer Renoux, Uwe K\""ockemann, Amy Loutfi",2018,Artificial Intelligence,2003.06347
"Autonomous driving is of great interest to industry and academia alike. The use of machine learning approaches for autonomous driving has long been studied, but mostly in the context of perception. In this paper we take a deeper look on the so called end-to-end approaches for autonomous driving, where the entire driving pipeline is replaced with a single neural network. We review the learning methods, input and output modalities, network architectures and evaluation schemes in end-to-end driving literature. Interpretability and safety are discussed separately, as they remain challenging for this approach. Beyond providing a comprehensive overview of existing methods, we conclude the review with an architecture that combines the most promising elements of the end-to-end autonomous driving systems.",A Survey of End-to-End Driving: Architectures and Training Methods,"Ardi Tampuu, Maksym Semikin, Naveed Muhammad, Dmytro Fishman and Tambet Matiisen",2020,Artificial Intelligence,2003.06404
"Decentralized online planning can be an attractive paradigm for cooperative multi-agent systems, due to improved scalability and robustness. A key difficulty of such approach lies in making accurate predictions about the decisions of other agents. In this paper, we present a trainable online decentralized planning algorithm based on decentralized Monte Carlo Tree Search, combined with models of teammates learned from previous episodic runs. By only allowing one agent to adapt its models at a time, under the assumption of ideal policy approximation, successive iterations of our method are guaranteed to improve joint policies, and eventually lead to convergence to a Nash equilibrium. We test the efficiency of the algorithm by performing experiments in several scenarios of the spatial task allocation environment introduced in [Claes et al., 2015]. We show that deep learning and convolutional neural networks can be employed to produce accurate policy approximators which exploit the spatial features of the problem, and that the proposed algorithm improves over the baseline planning performance for particularly challenging domain configurations.",Decentralized MCTS via Learned Teammate Models,"Aleksander Czechowski, Frans A. Oliehoek",2020,Artificial Intelligence,2003.08727
"We present a system that utilizes machine learning for tactic proof search in the Coq Proof Assistant. In a similar vein as the TacticToe project for HOL4, our system predicts appropriate tactics and finds proofs in the form of tactic scripts. To do this, it learns from previous tactic scripts and how they are applied to proof states. The performance of the system is evaluated on the Coq Standard Library. Currently, our predictor can identify the correct tactic to be applied to a proof state 23.4% of the time. Our proof searcher can fully automatically prove 39.3% of the lemmas. When combined with the CoqHammer system, the two systems together prove 56.7% of the library's lemmas.",Tactic Learning and Proving for the Coq Proof Assistant,"Lasse Blaauwbroek, Josef Urban, and Herman Geuvers",2020,Artificial Intelligence,2003.09140
"In process mining, process models are extracted from event logs using process discovery algorithms and are commonly assessed using multiple quality dimensions. While the metrics that measure the relationship of an extracted process model to its event log are well-studied, quantifying the level by which a process model can describe the unobserved behavior of its underlying system falls short in the literature. In this paper, a novel deep learning-based methodology called Adversarial System Variant Approximation (AVATAR) is proposed to overcome this issue. Sequence Generative Adversarial Networks are trained on the variants contained in an event log with the intention to approximate the underlying variant distribution of the system behavior. Unobserved realistic variants are sampled either directly from the Sequence Generative Adversarial Network or by leveraging the Metropolis-Hastings algorithm. The degree by which a process model relates to its underlying unknown system behavior is then quantified based on the realistic observed and estimated unobserved variants using established process model quality metrics. Significant performance improvements in revealing realistic unobserved variants are demonstrated in a controlled experiment on 15 ground truth systems. Additionally, the proposed methodology is experimentally tested and evaluated to quantify the generalization of 60 discovered process models with respect to their systems.",Adversarial System Variant Approximation to Quantify Process Model Generalization,Julian Theis and Houshang Darabi,2020,Artificial Intelligence,2003.12168
"The question of whether artificial beings or machines could become self-aware or consciousness has been a philosophical question for centuries. The main problem is that self-awareness cannot be observed from an outside perspective and the distinction of whether something is really self-aware or merely a clever program that pretends to do so cannot be answered without access to accurate knowledge about the mechanism's inner workings. We review the current state-of-the-art regarding these developments and investigate common machine learning approaches with respect to their potential ability to become self-aware. We realise that many important algorithmic steps towards machines with a core consciousness have already been devised. For human-level intelligence, however, many additional techniques have to be discovered.",Will we ever have Conscious Machines?,"Patrick Krauss, Andreas Maier",2020,Artificial Intelligence,2003.14132
"Two indicators are classically used to evaluate the quality of rule-based classification systems: predictive accuracy, i.e. the system's ability to successfully reproduce learning data and coverage, i.e. the proportion of possible cases for which the logical rules constituting the system apply. In this work, we claim that these two indicators may be insufficient, and additional measures of quality may need to be developed. We theoretically show that classification systems presenting ""good"" predictive accuracy and coverage can, nonetheless, be trivially improved and illustrate this proposition with examples.",On Evaluating the Quality of Rule-Based Classification Systems,Nassim Dehouche,2017,Artificial Intelligence,2004.02671
"Reinforcement Learning (RL) methods have emerged as a popular choice for training an efficient and effective dialogue policy. However, these methods suffer from sparse and unstable reward signals returned by a user simulator only when a dialogue finishes. Besides, the reward signal is manually designed by human experts, which requires domain knowledge. Recently, a number of adversarial learning methods have been proposed to learn the reward function together with the dialogue policy. However, to alternatively update the dialogue policy and the reward model on the fly, we are limited to policy-gradient-based algorithms, such as REINFORCE and PPO. Moreover, the alternating training of a dialogue agent and the reward model can easily get stuck in local optima or result in mode collapse. To overcome the listed issues, we propose to decompose the adversarial training into two steps. First, we train the discriminator with an auxiliary dialogue generator and then incorporate a derived reward model into a common RL method to guide the dialogue policy learning. This approach is applicable to both on-policy and off-policy RL methods. Based on our extensive experimentation, we can conclude the proposed method: (1) achieves a remarkable task success rate using both on-policy and off-policy RL methods; and (2) has the potential to transfer knowledge from existing domains to a new domain.",Guided Dialog Policy Learning without Adversarial Learning in the Loop,"Ziming Li, Sungjin Lee, Baolin Peng, Jinchao Li, Julia Kiseleva, Maarten de Rijke, Shahin Shayandeh, Jianfeng Gao",2020,Artificial Intelligence,2004.03267
"Purpose: In this study, the recently emerged advances in Fuzzy Cognitive Maps (FCM) are investigated and employed, for achieving the automatic and non-invasive diagnosis of Coronary Artery Disease (CAD). Methods: A Computer-Aided Diagnostic model for the acceptable and non-invasive prediction of CAD using the State Space Advanced FCM (AFCM) approach is proposed. Also, a rule-based mechanism is incorporated, to further increase the knowledge of the system and the interpretability of the decision mechanism. The proposed method is tested utilizing a CAD dataset from the Laboratory of Nuclear Medicine of the University of Patras. More specifically, two architectures of AFCMs are designed, and different parameter testing is performed. Furthermore, the proposed AFCMs, which are based on the new equations proposed recently, are compared with the traditional FCM approach. Results: The experiments highlight the effectiveness of the AFCM approach and the new equations over the traditional approach, which obtained an accuracy of 78.21%, achieving an increase of seven percent (+7%) on the classification task, and obtaining 85.47% accuracy. Conclusions: It is demonstrated that the AFCM approach in developing Fuzzy Cognitive Maps outperforms the conventional approach, while it constitutes a reliable method for the diagnosis of Coronary Artery Disease. Conclusions and future research related to recent pandemic of coronavirus are provided.",State Space Advanced Fuzzy Cognitive Map approach for automatic and non Invasive diagnosis of Coronary Artery Disease,"Ioannis D. Apostolopoulos, Peter P. Groumpos, Dimitris I. Apostolopoulos",2021,Artificial Intelligence,2004.03372
"In reinforcement learning (RL), dealing with non-stationarity is a challenging issue. However, some domains such as traffic optimization are inherently non-stationary. Causes for and effects of this are manifold. In particular, when dealing with traffic signal controls, addressing non-stationarity is key since traffic conditions change over time and as a function of traffic control decisions taken in other parts of a network. In this paper we analyze the effects that different sources of non-stationarity have in a network of traffic signals, in which each signal is modeled as a learning agent. More precisely, we study both the effects of changing the \textit{context} in which an agent learns (e.g., a change in flow rates experienced by it), as well as the effects of reducing agent observability of the true environment state. Partial observability may cause distinct states (in which distinct actions are optimal) to be seen as the same by the traffic signal agents. This, in turn, may lead to sub-optimal performance. We show that the lack of suitable sensors to provide a representative observation of the real state seems to affect the performance more drastically than the changes to the underlying traffic patterns.",Quantifying the Impact of Non-Stationarity in Reinforcement Learning-Based Traffic Signal Control,"Lucas N. Alegre, Ana L. C. Bazzan, Bruno C. da Silva",2021,Artificial Intelligence,2004.04778
"Reinforcement learning agents learn by encouraging behaviours which maximize their total reward, usually provided by the environment. In many environments, however, the reward is provided after a series of actions rather than each single action, leading the agent to experience ambiguity in terms of whether those actions are effective, an issue known as the credit assignment problem. In this paper, we propose two strategies inspired by behavioural psychology to enable the agent to intrinsically estimate more informative reward values for actions with no reward. The first strategy, called self-punishment (SP), discourages the agent from making mistakes that lead to undesirable terminal states. The second strategy, called the rewards backfill (RB), backpropagates the rewards between two rewarded actions. We prove that, under certain assumptions and regardless of the reinforcement learning algorithm used, these two strategies maintain the order of policies in the space of all possible policies in terms of their total reward, and, by extension, maintain the optimal policy. Hence, our proposed strategies integrate with any reinforcement learning algorithm that learns a value or action-value function through experience. We incorporated these two strategies into three popular deep reinforcement learning approaches and evaluated the results on thirty Atari games. After parameter tuning, our results indicate that the proposed strategies improve the tested methods in over 65 percent of tested games by up to over 25 times performance improvement.",Self Punishment and Reward Backfill for Deep Q-Learning,"Mohammad Reza Bonyadi, Rui Wang, Maryam Ziaei",2022,Artificial Intelligence,2004.05002
"In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. For this, we introduce an intermediate representation that is based on the logical query plan in a database called Operation Trees (OT). This representation allows us to invert the annotation process without losing flexibility in the types of queries that we generate. Furthermore, it allows for fine-grained alignment of query tokens to OT operations. In our method, we randomly generate OTs from a context-free grammar. Afterwards, annotators have to write the appropriate natural language question that is represented by the OT. Finally, the annotators assign the tokens to the OT operations. We apply the method to create a new corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases. We compare OTTA to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries. Finally, we train a state-of-the-art semantic parsing model on our data and show that our corpus is a challenging dataset and that the token alignment can be leveraged to increase the performance significantly.",A Methodology for Creating Question Answering Corpora Using Inverse Data Annotation,"Jan Deriu, Katsiaryna Mlynchyk, Philippe Schl\""apfer, Alvaro Rodrigo, Dirk von Gr\""unigen, Nicolas Kaiser, Kurt Stockinger, Eneko Agirre, and Mark Cieliebak",2020,Artificial Intelligence,2004.07633
"Recent progress in deep learning is essentially based on a ""big data for small tasks"" paradigm, under which massive amounts of data are used to train a classifier for a single narrow task. In this paper, we call for a shift that flips this paradigm upside down. Specifically, we propose a ""small data for big tasks"" paradigm, wherein a single artificial intelligence (AI) system is challenged to develop ""common sense"", enabling it to solve a wide range of tasks with little training data. We illustrate the potential power of this new paradigm by reviewing models of common sense that synthesize recent breakthroughs in both machine and human vision. We identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense. When taken as a unified concept, FPICU is concerned with the questions of ""why"" and ""how"", beyond the dominant ""what"" and ""where"" framework for understanding vision. They are invisible in terms of pixels but nevertheless drive the creation, maintenance, and development of visual scenes. We therefore coin them the ""dark matter"" of vision. Just as our universe cannot be understood by merely studying observable matter, we argue that vision cannot be understood without studying FPICU. We demonstrate the power of this perspective to develop cognitive AI systems with humanlike common sense by showing how to observe and apply FPICU with little training data to solve a wide range of challenging tasks, including tool use, planning, utility inference, and social learning. In summary, we argue that the next generation of AI must embrace ""dark"" humanlike common sense for solving novel tasks.","Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense","Yixin Zhu, Tao Gao, Lifeng Fan, Siyuan Huang, Mark Edmonds, Hangxin Liu, Feng Gao, Chi Zhang, Siyuan Qi, Ying Nian Wu, Joshua B. Tenenbaum, Song-Chun Zhu",2020,Artificial Intelligence,2004.09044
"Recently, AlphaZero has achieved landmark results in deep reinforcement learning, by providing a single self-play architecture that learned three different games at super human level. AlphaZero is a large and complicated system with many parameters, and success requires much compute power and fine-tuning. Reproducing results in other games is a challenge, and many researchers are looking for ways to improve results while reducing computational demands. AlphaZero's design is purely based on self-play and makes no use of labeled expert data ordomain specific enhancements; it is designed to learn from scratch. We propose a novel approach to deal with this cold-start problem by employing simple search enhancements at the beginning phase of self-play training, namely Rollout, Rapid Action Value Estimate (RAVE) and dynamically weighted combinations of these with the neural network, and Rolling Horizon Evolutionary Algorithms (RHEA). Our experiments indicate that most of these enhancements improve the performance of their baseline player in three different (small) board games, with especially RAVE based variants playing strongly.",Warm-Start AlphaZero Self-Play Search Enhancements,"Hui Wang, Mike Preuss, Aske Plaat",2020,Artificial Intelligence,2004.12357
"Black-box Artificial Intelligence (AI) methods, e.g. deep neural networks, have been widely utilized to build predictive models that can extract complex relationships in a dataset and make predictions for new unseen data records. However, it is difficult to trust decisions made by such methods since their inner working and decision logic is hidden from the user. Explainable Artificial Intelligence (XAI) refers to systems that try to explain how a black-box AI model produces its outcomes. Post-hoc XAI methods approximate the behavior of a black-box by extracting relationships between feature values and the predictions. Perturbation-based and decision set methods are among commonly used post-hoc XAI systems. The former explanators rely on random perturbations of data records to build local or global linear models that explain individual predictions or the whole model. The latter explanators use those feature values that appear more frequently to construct a set of decision rules that produces the same outcomes as the target black-box. However, these two classes of XAI methods have some limitations. Random perturbations do not take into account the distribution of feature values in different subspaces, leading to misleading approximations. Decision sets only pay attention to frequent feature values and miss many important correlations between features and class labels that appear less frequently but accurately represent decision boundaries of the model. In this paper, we address the above challenges by proposing an explanation method named Confident Itemsets Explanation (CIE). We introduce confident itemsets, a set of feature values that are highly correlated to a specific class label. CIE utilizes confident itemsets to discretize the whole decision space of a model to smaller subspaces.",Post-hoc explanation of black-box classifiers using confident itemsets,"Milad Moradi, Matthias Samwald",2021,Artificial Intelligence,2005.01992
"This paper analyses the application of artificial intelligence techniques to various areas of archaeology and more specifically: a) The use of software tools as a creative stimulus for the organization of exhibitions; the use of humanoid robots and holographic displays as guides that interact and involve museum visitors; b) The analysis of methods for the classification of fragments found in archaeological excavations and for the reconstruction of ceramics, with the recomposition of the parts of text missing from historical documents and epigraphs; c) The cataloguing and study of human remains to understand the social and historical context of belonging with the demonstration of the effectiveness of the AI techniques used; d) The detection of particularly difficult terrestrial archaeological sites with the analysis of the architectures of the Artificial Neural Networks most suitable for solving the problems presented by the site; the design of a study for the exploration of marine archaeological sites, located at depths that cannot be reached by man, through the construction of a freely explorable 3D version.",The computerization of archaeology: survey on AI techniques,Lorenzo Mantovan and Loris Nanni,2020,Artificial Intelligence,2005.02863
"Artificial behavioral agents are often evaluated based on their consistent behaviors and performance to take sequential actions in an environment to maximize some notion of cumulative reward. However, human decision making in real life usually involves different strategies and behavioral trajectories that lead to the same empirical outcome. Motivated by clinical literature of a wide range of neurological and psychiatric disorders, we propose here a more general and flexible parametric framework for sequential decision making that involves a two-stream reward processing mechanism. We demonstrated that this framework is flexible and unified enough to incorporate a family of problems spanning multi-armed bandits (MAB), contextual bandits (CB) and reinforcement learning (RL), which decompose the sequential decision making process in different levels. Inspired by the known reward processing abnormalities of many mental disorders, our clinically-inspired agents demonstrated interesting behavioral trajectories and comparable performance on simulated tasks with particular reward distributions, a real-world dataset capturing human decision-making in gambling tasks, and the PacMan game across different reward stationarities in a lifelong learning setting.","Unified Models of Human Behavioral Agents in Bandits, Contextual Bandits and RL","Baihan Lin, Guillermo Cecchi, Djallel Bouneffouf, Jenna Reinen, Irina Rish",2021,Artificial Intelligence,2005.04544
"Information gathering in a partially observable environment can be formulated as a reinforcement learning (RL), problem where the reward depends on the agent's uncertainty. For example, the reward can be the negative entropy of the agent's belief over an unknown (or hidden) variable. Typically, the rewards of an RL agent are defined as a function of the state-action pairs and not as a function of the belief of the agent; this hinders the direct application of deep RL methods for such tasks. This paper tackles the challenge of using belief-based rewards for a deep RL agent, by offering a simple insight that maximizing any convex function of the belief of the agent can be approximated by instead maximizing a prediction reward: a reward based on prediction accuracy. In particular, we derive the exact error between negative entropy and the expected prediction reward. This insight provides theoretical motivation for several fields using prediction rewards---namely visual attention, question answering systems, and intrinsic motivation---and highlights their connection to the usually distinct fields of active perception, active sensing, and sensor placement. Based on this insight we present deep anticipatory networks (DANs), which enables an agent to take actions to reduce its uncertainty without performing explicit belief inference. We present two applications of DANs: building a sensor selection system for tracking people in a shopping mall and learning discrete models of attention on fashion MNIST and MNIST digit classification.",Maximizing Information Gain in Partially Observable Environments via Prediction Reward,"Yash Satsangi, Sungsu Lim, Shimon Whiteson, Frans Oliehoek, Martha White",2020,Artificial Intelligence,2005.04912
"We explore state-of-the-art neural models for question answering on electronic medical records and improve their ability to generalize better on previously unseen (paraphrased) questions at test time. We enable this by learning to predict logical forms as an auxiliary task along with the main task of answer span detection. The predicted logical forms also serve as a rationale for the answer. Further, we also incorporate medical entity information in these models via the ERNIE architecture. We train our models on the large-scale emrQA dataset and observe that our multi-task entity-enriched models generalize to paraphrased questions ~5% better than the baseline BERT model.",Entity-Enriched Neural Models for Clinical Question Answering,"Bhanu Pratap Singh Rawat, Wei-Hung Weng, So Yeon Min, Preethi Raghavan, Peter Szolovits",2020,Artificial Intelligence,2005.06587
"The travelling thief problem (TTP) is a multi-component optimisation problem involving two interdependent NP-hard components: the travelling salesman problem (TSP) and the knapsack problem (KP). Recent state-of-the-art TTP solvers modify the underlying TSP and KP solutions in an iterative and interleaved fashion. The TSP solution (cyclic tour) is typically changed in a deterministic way, while changes to the KP solution typically involve a random search, effectively resulting in a quasi-meandering exploration of the TTP solution space. Once a plateau is reached, the iterative search of the TTP solution space is restarted by using a new initial TSP tour. We propose to make the search more efficient through an adaptive surrogate model (based on a customised form of Support Vector Regression) that learns the characteristics of initial TSP tours that lead to good TTP solutions. The model is used to filter out non-promising initial TSP tours, in effect reducing the amount of time spent to find a good TTP solution. Experiments on a broad range of benchmark TTP instances indicate that the proposed approach filters out a considerable number of non-promising initial tours, at the cost of omitting only a small number of the best TTP solutions.",Surrogate Assisted Optimisation for Travelling Thief Problems,"Majid Namazi, Conrad Sanderson, M.A. Hakim Newton, Abdul Sattar",2020,Artificial Intelligence,2005.06695
"Maneuvering in dense traffic is a challenging task for autonomous vehicles because it requires reasoning about the stochastic behaviors of many other participants. In addition, the agent must achieve the maneuver within a limited time and distance. In this work, we propose a combination of reinforcement learning and game theory to learn merging behaviors. We design a training curriculum for a reinforcement learning agent using the concept of level-$k$ behavior. This approach exposes the agent to a broad variety of behaviors during training, which promotes learning policies that are robust to model discrepancies. We show that our approach learns more efficient policies than traditional training methods.",Reinforcement Learning with Iterative Reasoning for Merging in Dense Traffic,"Maxime Bouton, Alireza Nakhaei, David Isele, Kikuo Fujimura, and Mykel J. Kochenderfer",2020,Artificial Intelligence,2005.11895
"Recently, a groundswell of research has identified the use of counterfactual explanations as a potentially significant solution to the Explainable AI (XAI) problem. It is argued that (a) technically, these counterfactual cases can be generated by permuting problem-features until a class change is found, (b) psychologically, they are much more causally informative than factual explanations, (c) legally, they are GDPR-compliant. However, there are issues around the finding of good counterfactuals using current techniques (e.g. sparsity and plausibility). We show that many commonly-used datasets appear to have few good counterfactuals for explanation purposes. So, we propose a new case based approach for generating counterfactuals using novel ideas about the counterfactual potential and explanatory coverage of a case-base. The new technique reuses patterns of good counterfactuals, present in a case-base, to generate analogous counterfactuals that can explain new problems and their solutions. Several experiments show how this technique can improve the counterfactual potential and explanatory coverage of case-bases that were previously found wanting.",Good Counterfactuals and Where to Find Them: A Case-Based Technique for Generating Counterfactuals for Explainable AI (XAI),"Mark T. Keane, Barry Smyth",2020,Artificial Intelligence,2005.13997
"Recommendation system or also known as a recommender system is a tool to help the user in providing a suggestion of a specific dilemma. Thus, recently, the interest in developing a recommendation system in many fields has increased. Fuzzy Logic system (FLSs) is one of the approaches that can be used to model the recommendation systems as it can deal with uncertainty and imprecise information. However, one of the fundamental issues in FLS is the problem of the curse of dimensionality. That is, the number of rules in FLSs is increasing exponentially with the number of input variables. One effective way to overcome this problem is by using Hierarchical Fuzzy System (HFSs). This paper aims to explore the use of HFSs for Recommendation system. Specifically, we are interested in exploring and comparing the HFS and FLS for the Career path recommendation system (CPRS) based on four key criteria, namely topology, the number of rules, the rules structures and interpretability. The findings suggested that the HFS has advantages over FLS towards improving the interpretability models, in the context of a recommendation system example. This study contributes to providing an insight into the development of interpretable HFSs in the Recommendation systems.",An Exploratory Study of Hierarchical Fuzzy Systems Approach in Recommendation System,"Tajul Rosli Razak, Iman Hazwam Abd Halim, Muhammad Nabil Fikri Jamaludin, Mohammad Hafiz Ismail, Shukor Sanim Mohd Fauzi",2019,Artificial Intelligence,2005.14026
"There is a growing recognition that artists use valuable ways to understand and work with cognitive and perceptual mechanisms to convey desired experiences and narrative in their created artworks (DiPaola et al., 2010; Zeki, 2001). This paper documents our attempt to computationally model the creative process of a portrait painter, who relies on understanding human traits (i.e., personality and emotions) to inform their art. Our system includes an empathic conversational interaction component to capture the dominant personality category of the user and a generative AI Portraiture system that uses this categorization to create a personalized stylization of the user's portrait. This paper includes the description of our systems and the real-time interaction results obtained during the demonstration session of the NeurIPS 2019 Conference.",Empathic AI Painter: A Computational Creativity System with Embodied Conversational Interaction,"Ozge Nilay Yalcin, Nouf Abukhodair and Steve DiPaola",2020,Artificial Intelligence,2005.14223
"This paper provides the foundations of a unified cognitive decision-making framework (QulBIT) which is derived from quantum theory. The main advantage of this framework is that it can cater for paradoxical and irrational human decision making. Although quantum approaches for cognition have demonstrated advantages over classical probabilistic approaches and bounded rationality models, they still lack explanatory power. To address this, we introduce a novel explanatory analysis of the decision-maker's belief space. This is achieved by exploiting quantum interference effects as a way of both quantifying and explaining the decision-maker's uncertainty. We detail the main modules of the unified framework, the explanatory analysis method, and illustrate their application in situations violating the Sure Thing Principle.",QuLBIT: Quantum-Like Bayesian Inference Technologies for Cognition and Decision,Catarina Moreira and Matheus Hammes and Rasim Serdar Kurdoglu and Peter Bruza,2020,Artificial Intelligence,2006.02256
"Humans possess an inherent ability to chunk sequences into their constituent parts. In fact, this ability is thought to bootstrap language skills and learning of image patterns which might be a key to a more animal-like type of intelligence. Here, we propose a continual generalization of the chunking problem (an unsupervised problem), encompassing fixed and probabilistic chunks, discovery of temporal and causal structures and their continual variations. Additionally, we propose an algorithm called SyncMap that can learn and adapt to changes in the problem by creating a dynamic map which preserves the correlation between variables. Results of SyncMap suggest that the proposed algorithm learn near optimal solutions, despite the presence of many types of structures and their continual variation. When compared to Word2vec, PARSER and MRIL, SyncMap surpasses or ties with the best algorithm on $66\%$ of the scenarios while being the second best in the remaining $34\%$. SyncMap's model-free simple dynamics and the absence of loss functions reveal that, perhaps surprisingly, much can be done with self-organization alone. Code available at https://github.com/zweifel/SyncMap.",Continual General Chunking Problem and SyncMap,Danilo Vasconcellos Vargas and Toshitake Asabuki,2021,Artificial Intelligence,2006.07853
"Decision and policy-makers in multi-criteria decision-making analysis take into account some strategies in order to analyze outcomes and to finally make an effective and more precise decision. Among those strategies, the modification of the normalization process in the multiple-criteria decision-making algorithm is still a question due to the confrontation of many normalization tools. Normalization is the basic action in defining and solving a MADM problem and a MADM model. Normalization is the first, also necessary, step in solving, i.e. the application of a MADM method. It is a fact that the selection of normalization methods has a direct effect on the results. One of the latest normalization methods introduced is the Logarithmic Normalization (LN) method. This new method has a distinguished advantage, reflecting in that a sum of the normalized values of criteria always equals 1. This normalization method had never been applied in any MADM methods before. This research study is focused on the analysis of the classical MADM methods based on logarithmic normalization. VIKOR and TOPSIS, as the two famous MADM methods, were selected for this reanalysis research study. Two numerical examples were checked in both methods, based on both the classical and the novel ways based on the LN. The results indicate that there are differences between the two approaches. Eventually, a sensitivity analysis is also designed to illustrate the reliability of the final results.",A VIKOR and TOPSIS focused reanalysis of the MADM methods based on logarithmic normalization,"Sarfaraz Zolfani, Morteza Yazdani, Dragan Pamucar, Pascale Zarat\'e (IRIT-ADRIA, IRIT, UT1)",2020,Artificial Intelligence,2006.08150
"Fuzzy rule-based model is a powerful tool for imitating the human way of thinking and solving uncertainty-related problems as it allows for understandable and interpretable rule bases. The objective of this paper is to study the applicability of fuzzy rule-based modelling to quantify soil classification for engineering purposes by qualitatively considering soil index properties. The classification system of the Highway Research Board is considered to illustrate a fuzzy rule-based model. The soil's index properties are fuzzified using triangular functions, and the fuzzy membership values are calculated. Fuzzy arithmetical operators are then applied to the membership values obtained for classification. Fuzzy decision tree classification algorithm is used to derive fuzzy if-then rules to quantify qualitative soil classification. The proposed system is implemented in MATLAB. The results obtained are checked and the implementation of the proposed model is measured against the outcomes of the laboratory tests.",Application of Fuzzy Rule based System for Highway Research Board Classification of Soils,"Sujatha A, L Govindaraju and N Shivakumar",2020,Artificial Intelligence,2006.08347
"This paper describes an entropy equation, but one that should be used for measuring energy and not information. In relation to the human brain therefore, both of these quantities can be used to represent the stored information. The human brain makes use of energy efficiency to form its structures, which is likely to be linked to the neuron wiring. This energy efficiency can also be used as the basis for a clustering algorithm, which is described in a different paper. This paper is more of a discussion about global properties, where the rules used for the clustering algorithm can also create the entropy equation E = (mean * variance). This states that work is done through the energy released by the 'change' in entropy. The equation is so simplistic and generic that it can offer arguments for completely different domains, where the journey ends with a discussion about global energy properties in physics and beyond. A comparison with Einstein's relativity equation is made and also the audacious suggestion that a black hole has zero-energy inside.",An Entropy Equation for Energy,Kieran Greer,2020,Artificial Intelligence,2007.03286
"Advances in hardware technology have enabled more integration of sophisticated software, triggering progress in the development and employment of Unmanned Vehicles (UVs), and mitigating restraints for onboard intelligence. As a result, UVs can now take part in more complex mission where continuous transformation in environmental condition calls for a higher level of situational responsiveness. This paper serves as an introduction to UVs mission planning and management systems aiming to highlight some of the recent developments in the field of autonomous underwater and aerial vehicles in addition to stressing some possible future directions and discussing the learned lessons. A comprehensive survey over autonomy assessment of UVs, and different aspects of autonomy such as situation awareness, cognition, and decision-making has been provided in this study. The paper separately explains the humanoid and autonomous system's performance and highlights the role and impact of a human in UVs operations.",Current Advancements on Autonomous Mission Planning and Management Systems: an AUV and UAV perspective,"Adham Atyabi, Somaiyeh MahmoudZadeh, Samia Nefti-Meziani",2018,Artificial Intelligence,2007.05179
"Tabletop roleplaying games (TTRPGs) and procedural content generators can both be understood as systems of rules for producing content. In this paper, we argue that TTRPG design can usefully be viewed as procedural content generator design. We present several case studies linking key concepts from PCG research -- including possibility spaces, expressive range analysis, and generative pipelines -- to key concepts in TTRPG design. We then discuss the implications of these relationships and suggest directions for future work uniting research in TTRPGs and PCG.",Tabletop Roleplaying Games as Procedural Content Generators,"Matthew Guzdial, Devi Acharya, Max Kreminski, Michael Cook, Mirjam Eladhari, Antonios Liapis and Anne Sullivan",2020,Artificial Intelligence,2007.06108
"Real-world complex systems are often modelled by sets of equations with endogenous and exogenous variables. What can we say about the causal and probabilistic aspects of variables that appear in these equations without explicitly solving the equations? We make use of Simon's causal ordering algorithm (Simon, 1953) to construct a causal ordering graph and prove that it expresses the effects of soft and perfect interventions on the equations under certain unique solvability assumptions. We further construct a Markov ordering graph and prove that it encodes conditional independences in the distribution implied by the equations with independent random exogenous variables, under a similar unique solvability assumption. We discuss how this approach reveals and addresses some of the limitations of existing causal modelling frameworks, such as causal Bayesian networks and structural causal models.",Conditional independences and causal relations implied by sets of equations,Tineke Blom and Mirthe M. van Diepen and Joris M. Mooij,2021,Artificial Intelligence,2007.07183
"Advances in hardware technology have facilitated more integration of sophisticated software toward augmenting the development of Unmanned Vehicles (UVs) and mitigating constraints for onboard intelligence. As a result, UVs can operate in complex missions where continuous trans-formation in environmental condition calls for a higher level of situational responsiveness and autonomous decision making. This book is a research monograph that aims to provide a comprehensive survey of UVs autonomy and its related properties in internal and external situation awareness to-ward robust mission planning in severe conditions. An advance level of intelligence is essential to minimize the reliance on the human supervisor, which is a main concept of autonomy. A self-controlled system needs a robust mission management strategy to push the boundaries towards autonomous structures, and the UV should be aware of its internal state and capabilities to assess whether current mission goal is achievable or find an alternative solution. In this book, the AUVs will become the major case study thread but other cases/types of vehicle will also be considered. In-deed the research monograph, the review chapters and the new approaches we have developed would be appropriate for use as a reference in upper years or postgraduate degrees for its coverage of literature and algorithms relating to Robot/Vehicle planning, tasking, routing, and trust.",Autonomy and Unmanned Vehicles Augmented Reactive Mission-Motion Planning Architecture for Autonomous Vehicles,"Somaiyeh MahmoudZadeh, David MW Powers, Reza Bairam Zadeh",2019,Artificial Intelligence,2007.09563
"Social network structure is one of the key determinants of human language evolution. Previous work has shown that the network of social interactions shapes decentralized learning in human groups, leading to the emergence of different kinds of communicative conventions. We examined the effects of social network organization on the properties of communication systems emerging in decentralized, multi-agent reinforcement learning communities. We found that the global connectivity of a social network drives the convergence of populations on shared and symmetric communication systems, preventing the agents from forming many local ""dialects"". Moreover, the agent's degree is inversely related to the consistency of its use of communicative conventions. These results show the importance of the basic properties of social network structure on reinforcement communication learning and suggest a new interpretation of findings on human convergence on word conventions.",Reinforcement Communication Learning in Different Social Network Structures,"Marina Dubova, Arseny Moskvichev, Robert Goldstone",2020,Artificial Intelligence,2007.09820
"To provide a foundation for conceptual modeling, ontologies have been introduced to specify the entities, the existences of which are acknowledged in the model. Ontologies are essential components as mechanisms to model a portion of reality in software engineering. In this context, a model refers to a description of objects and processes that populate a system. Developing such a description constrains and directs the design, development, and use of the corresponding system, thus avoiding such difficulties as conflicts and lack of a common understanding. In this cross-area research between modeling and ontology, there has been a growing interest in the development and use of domain ontologies (e.g., Resource Description Framework, Ontology Web Language). This paper contributes to the establishment of a broad ontological foundation for conceptual modeling in a specific domain through proposing a workable ontology (abbreviated as TM). A TM is a one-category ontology called a thimac (things/machines) that is used to elaborate the design and analysis of ontological presumptions. The focus of the study is on such notions as change, event, and time. Several current ontological difficulties are reviewed and remodeled in the TM. TM modeling is also contrasted with time representation in SysML. The results demonstrate that a TM is a useful tool for addressing these ontological problems.",Conceptual Modeling of Time for Computational Ontologies,Sabah Al-Fedaghi,2020,Artificial Intelligence,2007.10151
"We present Tactician, a tactic learner and prover for the Coq Proof Assistant. Tactician helps users make tactical proof decisions while they retain control over the general proof strategy. To this end, Tactician learns from previously written tactic scripts and gives users either suggestions about the next tactic to be executed or altogether takes over the burden of proof synthesis. Tactician's goal is to provide users with a seamless, interactive, and intuitive experience together with robust and adaptive proof automation. In this paper, we give an overview of Tactician from the user's point of view, regarding both day-to-day usage and issues of package dependency management while learning in the large. Finally, we give a peek into Tactician's implementation as a Coq plugin and machine learning platform.","The Tactician (extended version): A Seamless, Interactive Tactic Learner and Prover for Coq","Lasse Blaauwbroek, Josef Urban and Herman Geuvers",2020,Artificial Intelligence,2008.00120
"Ortus is a simple virtual organism that also serves as an initial framework for investigating and developing biologically-based artificial intelligence. Born from a goal to create complex virtual intelligence and an initial attempt to model C. elegans, Ortus implements a number of mechanisms observed in organic nervous systems, and attempts to fill in unknowns based upon plausible biological implementations and psychological observations. Implemented mechanisms include excitatory and inhibitory chemical synapses, bidirectional gap junctions, and Hebbian learning with its Stentian extension. We present an initial experiment that showcases Ortus' fundamental principles; specifically, a cyclic respiratory circuit, and emotionally-driven associative learning with respect to an input stimulus. Finally, we discuss the implications and future directions for Ortus and similar systems.",Ortus: an Emotion-Driven Approach to (artificial) Biological Intelligence,"Andrew W.E. McDonald, Sean Grimes, David E. Breen",2017,Artificial Intelligence,2008.04875
"Red light running at signalised intersections is a growing road safety issue worldwide, leading to the rapid development of advanced intelligent transportation technologies and countermeasures. However, existing studies have yet to summarise and present the effect of these technology based innovations in improving safety. This paper represents a comprehensive review of red light running behaviour prediction methodologies and technology-based countermeasures. Specifically, the major focus of this study is to provide a comprehensive review on two streams of literature targeting red light running and stop and go behaviour at signalised intersection (1) studies focusing on modelling and predicting the red light running and stop and go related driver behaviour and (2) studies focusing on the effectiveness of different technology based countermeasures which combat such unsafe behaviour. The study provides a systematic guide to assist researchers and stakeholders in understanding how to best identify red light running and stop and go associated driving behaviour and subsequently implement countermeasures to combat such risky behaviour and improve the associated safety.",A Review on Drivers Red Light Running Behavior Predictions and Technology Based Countermeasures,"Md Mostafizur Rahman Komol, Jack Pinnow, Mohammed Elhenawy, Shamsunnahar Yasmin, Mahmoud Masoud, Sebastien Glaser and Andry Rakotonirainy",2022,Artificial Intelligence,2008.06727
"Relevant research has been highlighted in the computing community to develop machine learning models capable of predicting the occurrence of crimes, analyzing contexts of crimes, extracting profiles of individuals linked to crime, and analyzing crimes over time. However, models capable of predicting specific crimes, such as homicide, are not commonly found in the current literature. This research presents a machine learning model to predict homicide crimes, using a dataset that uses generic data (without study location dependencies) based on incident report records for 34 different types of crimes, along with time and space data from crime reports. Experimentally, data from the city of Bel\'em - Par\'a, Brazil was used. These data were transformed to make the problem generic, enabling the replication of this model to other locations. In the research, analyses were performed with simple and robust algorithms on the created dataset. With this, statistical tests were performed with 11 different classification methods and the results are related to the prediction's occurrence and non-occurrence of homicide crimes in the month subsequent to the occurrence of other registered crimes, with 76% assertiveness for both classes of the problem, using Random Forest. Results are considered as a baseline for the proposed problem.",Prediction of Homicides in Urban Centers: A Machine Learning Approach,"Jos\'e Ribeiro, Lair Meneses, Denis Costa, Wando Miranda, Ronnie Alves",2021,Artificial Intelligence,2008.06979
"Autonomous car racing is a major challenge in robotics. It raises fundamental problems for classical approaches such as planning minimum-time trajectories under uncertain dynamics and controlling the car at the limits of its handling. Besides, the requirement of minimizing the lap time, which is a sparse objective, and the difficulty of collecting training data from human experts have also hindered researchers from directly applying learning-based approaches to solve the problem. In the present work, we propose a learning-based system for autonomous car racing by leveraging a high-fidelity physical car simulation, a course-progress proxy reward, and deep reinforcement learning. We deploy our system in Gran Turismo Sport, a world-leading car simulator known for its realistic physics simulation of different race cars and tracks, which is even used to recruit human race car drivers. Our trained policy achieves autonomous racing performance that goes beyond what had been achieved so far by the built-in AI, and, at the same time, outperforms the fastest driver in a dataset of over 50,000 human players.",Super-Human Performance in Gran Turismo Sport Using Deep Reinforcement Learning,"Florian Fuchs, Yunlong Song, Elia Kaufmann, Davide Scaramuzza, Peter Duerr",2021,Artificial Intelligence,2008.07971
"Consumer electronic devices such as mobile handsets, goods tagged with RFID labels, location and position sensors are continuously generating a vast amount of location enriched data called geospatial data. Conventionally such geospatial data is used for military applications. In recent times, many useful civilian applications have been designed and deployed around such geospatial data. For example, a recommendation system to suggest restaurants or places of attraction to a tourist visiting a particular locality. At the same time, civic bodies are harnessing geospatial data generated through remote sensing devices to provide better services to citizens such as traffic monitoring, pothole identification, and weather reporting. Typically such applications are leveraged upon non-hierarchical machine learning techniques such as Naive-Bayes Classifiers, Support Vector Machines, and decision trees. Recent advances in the field of deep-learning showed that Neural Network-based techniques outperform conventional techniques and provide effective solutions for many geospatial data analysis tasks such as object recognition, image classification, and scene understanding. The chapter presents a survey on the current state of the applications of deep learning techniques for analyzing geospatial data. The chapter is organized as below: (i) A brief overview of deep learning algorithms. (ii)Geospatial Analysis: a Data Science Perspective (iii) Deep-learning techniques for Remote Sensing data analytics tasks (iv) Deep-learning techniques for GPS data analytics(iv) Deep-learning techniques for RFID data analytics.",Deep Learning Techniques for Geospatial Data Analysis,"Arvind W. Kiwelekar, Geetanjali S. Mahamunkar, Laxman D. Netak, Valmik B Nikam",2020,Artificial Intelligence,2008.13146
"Due to advances in machine learning and artificial intelligence (AI), a new role is emerging for machines as intelligent assistants to radiologists in their clinical workflows. But what systematic clinical thought processes are these machines using? Are they similar enough to those of radiologists to be trusted as assistants? A live demonstration of such a technology was conducted at the 2016 Scientific Assembly and Annual Meeting of the Radiological Society of North America (RSNA). The demonstration was presented in the form of a question-answering system that took a radiology multiple choice question and a medical image as inputs. The AI system then demonstrated a cognitive workflow, involving text analysis, image analysis, and reasoning, to process the question and generate the most probable answer. A post demonstration survey was made available to the participants who experienced the demo and tested the question answering system. Of the reported 54,037 meeting registrants, 2,927 visited the demonstration booth, 1,991 experienced the demo, and 1,025 completed a post-demonstration survey. In this paper, the methodology of the survey is shown and a summary of its results are presented. The results of the survey show a very high level of receptiveness to cognitive computing technology and artificial intelligence among radiologists.",Receptivity of an AI Cognitive Assistant by the Radiology Community: A Report on Data Collected at RSNA,"Karina Kanjaria, Anup Pillai, Chaitanya Shivade, Marina Bendersky, Ashutosh Jadhav, Vandana Mukherjee, Tanveer Syeda-Mahmood",2020,Artificial Intelligence,2009.06082
"Equipping machines with comprehensive knowledge of the world's entities and their relationships has been a long-standing goal of AI. Over the last decade, large-scale knowledge bases, also known as knowledge graphs, have been automatically constructed from web contents and text sources, and have become a key asset for search engines. This machine knowledge can be harnessed to semantically interpret textual phrases in news, social media and web tables, and contributes to question answering, natural language processing and data analytics. This article surveys fundamental concepts and practical methods for creating and curating large knowledge bases. It covers models and methods for discovering and canonicalizing entities and their semantic types and organizing them into clean taxonomies. On top of this, the article discusses the automatic extraction of entity-centric properties. To support the long-term life-cycle and the quality assurance of machine knowledge, the article presents methods for constructing open schemas and for knowledge curation. Case studies on academic projects and industrial knowledge graphs complement the survey of concepts and methods.",Machine Knowledge: Creation and Curation of Comprehensive Knowledge Bases,"Gerhard Weikum, Luna Dong, Simon Razniewski, Fabian Suchanek",2021,Artificial Intelligence,2009.11564
"Within intelligent tutoring systems, considerable research has investigated hints, including how to generate data-driven hints, what hint content to present, and when to provide hints for optimal learning outcomes. However, less attention has been paid to how hints are presented. In this paper, we propose a new hint delivery mechanism called ""Assertions"" for providing unsolicited hints in a data-driven intelligent tutor. Assertions are partially-worked example steps designed to appear within a student workspace, and in the same format as student-derived steps, to show students a possible subgoal leading to the solution. We hypothesized that Assertions can help address the well-known hint avoidance problem. In systems that only provide hints upon request, hint avoidance results in students not receiving hints when they are needed. Our unsolicited Assertions do not seek to improve student help-seeking, but rather seek to ensure students receive the help they need. We contrast Assertions with Messages, text-based, unsolicited hints that appear after student inactivity. Our results show that Assertions significantly increase unsolicited hint usage compared to Messages. Further, they show a significant aptitude-treatment interaction between Assertions and prior proficiency, with Assertions leading students with low prior proficiency to generate shorter (more efficient) posttest solutions faster. We also present a clustering analysis that shows patterns of productive persistence among students with low prior knowledge when the tutor provides unsolicited help in the form of Assertions. Overall, this work provides encouraging evidence that hint presentation can significantly impact how students use them and using Assertions can be an effective way to address help avoidance.",Avoiding Help Avoidance: Using Interface Design Changes to Promote Unsolicited Hint Usage in an Intelligent Tutor,"Mehak Maniktala, Christa Cody, Tiffany Barnes, and Min Chi",2020,Artificial Intelligence,2009.13371
"We propose a vision for directing research and education in the ICT field. Our Smart and Sustainable World vision targets at prosperity for the people and the planet through better awareness and control of both human-made and natural environment. The needs of the society, individuals, and industries are fulfilled with intelligent systems that sense their environment, make proactive decisions on actions advancing their goals, and perform the actions on the environment. We emphasize artificial intelligence, feedback loops, human acceptance and control, intelligent use of basic resources, performance parameters, mission-oriented interdisciplinary research, and a holistic systems view complementing the conventional analytical reductive view as a research paradigm especially for complex problems. To serve a broad audience, we explain these concepts and list the essential literature. We suggest planning research and education by specifying, in a step-wise manner, scenarios, performance criteria, system models, research problems and education content, resulting in common goals and a coherent project portfolio as well as education curricula. Research and education produce feedback to support evolutionary development and encourage creativity in research. Finally, we propose concrete actions for realizing this approach.",Research and Education Towards Smart and Sustainable World,"Jukka Riekki and Aarne M\""ammel\""a",2021,Artificial Intelligence,2009.13849
"In this paper, we proposed a transfer learning-based English language learning chatbot, whose output generated by GPT-2 can be explained by corresponding ontology graph rooted by fine-tuning dataset. We design three levels for systematically English learning, including phonetics level for speech recognition and pronunciation correction, semantic level for specific domain conversation, and the simulation of free-style conversation in English - the highest level of language chatbot communication as free-style conversation agent. For academic contribution, we implement the ontology graph to explain the performance of free-style conversation, following the concept of XAI (Explainable Artificial Intelligence) to visualize the connections of neural network in bionics, and explain the output sentence from language model. From implementation perspective, our Language Learning agent integrated the mini-program in WeChat as front-end, and fine-tuned GPT-2 model of transfer learning as back-end to interpret the responses by ontology graph.",The design and implementation of Language Learning Chatbot with XAI using Ontology and Transfer Learning,"Nuobei Shi, Qin Zeng and Raymond Lee",2020,Artificial Intelligence,2009.13984
"We addressed the problem of a lack of semantic representation for user-centric explanations and different explanation types in our Explanation Ontology (https://purl.org/heals/eo). Such a representation is increasingly necessary as explainability has become an important problem in Artificial Intelligence with the emergence of complex methods and an uptake in high-precision and user-facing settings. In this submission, we provide step-by-step guidance for system designers to utilize our ontology, introduced in our resource track paper, to plan and model for explanations during the design of their Artificial Intelligence systems. We also provide a detailed example with our utilization of this guidance in a clinical setting.",Explanation Ontology in Action: A Clinical Use-Case,"Shruthi Chari, Oshani Seneviratne, Daniel M. Gruen, Morgan A. Foreman, Amar K. Das, Deborah L. McGuinness",2020,Artificial Intelligence,2010.01478
"Explainability has been a goal for Artificial Intelligence (AI) systems since their conception, with the need for explainability growing as more complex AI models are increasingly used in critical, high-stakes settings such as healthcare. Explanations have often added to an AI system in a non-principled, post-hoc manner. With greater adoption of these systems and emphasis on user-centric explainability, there is a need for a structured representation that treats explainability as a primary consideration, mapping end user needs to specific explanation types and the system's AI capabilities. We design an explanation ontology to model both the role of explanations, accounting for the system and user attributes in the process, and the range of different literature-derived explanation types. We indicate how the ontology can support user requirements for explanations in the domain of healthcare. We evaluate our ontology with a set of competency questions geared towards a system designer who might use our ontology to decide which explanation types to include, given a combination of users' needs and a system's capabilities, both in system design settings and in real-time operations. Through the use of this ontology, system designers will be able to make informed choices on which explanations AI systems can and should provide.",Explanation Ontology: A Model of Explanations for User-Centered AI,"Shruthi Chari, Oshani Seneviratne, Daniel M. Gruen, Morgan A. Foreman, Amar K. Das, Deborah L. McGuinness",2020,Artificial Intelligence,2010.01479
"Determining when and whether to provide personalized support is a well-known challenge called the assistance dilemma. A core problem in solving the assistance dilemma is the need to discover when students are unproductive so that the tutor can intervene. Such a task is particularly challenging for open-ended domains, even those that are well-structured with defined principles and goals. In this paper, we present a set of data-driven methods to classify, predict, and prevent unproductive problem-solving steps in the well-structured open-ended domain of logic. This approach leverages and extends the Hint Factory, a set of methods that leverages prior student solution attempts to build data-driven intelligent tutors. We present a HelpNeed classification, that uses prior student data to determine when students are likely to be unproductive and need help learning optimal problem-solving strategies. We present a controlled study to determine the impact of an Adaptive pedagogical policy that provides proactive hints at the start of each step based on the outcomes of our HelpNeed predictor: productive vs. unproductive. Our results show that the students in the Adaptive condition exhibited better training behaviors, with lower help avoidance, and higher help appropriateness (a higher chance of receiving help when it was likely to be needed), as measured using the HelpNeed classifier, when compared to the Control. Furthermore, the results show that the students who received Adaptive hints based on HelpNeed predictions during training significantly outperform their Control peers on the posttest, with the former producing shorter, more optimal solutions in less time. We conclude with suggestions on how these HelpNeed methods could be applied in other well-structured open-ended domains.","Extending the Hint Factory for the assistance dilemma: A novel, data-driven HelpNeed Predictor for proactive problem-solving help","Mehak Maniktala, Christa Cody, Amy Isvik, Nicholas Lytle, Min Chi, Tiffany Barnes",2020,Artificial Intelligence,2010.04124
"The recent advances in artificial intelligence namely in machine learning and deep learning, have boosted the performance of intelligent systems in several ways. This gave rise to human expectations, but also created the need for a deeper understanding of how intelligent systems think and decide. The concept of explainability appeared, in the extent of explaining the internal system mechanics in human terms. Recommendation systems are intelligent systems that support human decision making, and as such, they have to be explainable in order to increase user trust and improve the acceptance of recommendations. In this work, we focus on a context-aware recommendation system for energy efficiency and develop a mechanism for explainable and persuasive recommendations, which are personalized to user preferences and habits. The persuasive facts either emphasize on the economical saving prospects (Econ) or on a positive ecological impact (Eco) and explanations provide the reason for recommending an energy saving action. Based on a study conducted using a Telegram bot, different scenarios have been validated with actual data and human feedback. Current results show a total increase of 19\% on the recommendation acceptance ratio when both economical and ecological persuasive facts are employed. This revolutionary approach on recommendation systems, demonstrates how intelligent recommendations can effectively encourage energy saving behavior.",The emergence of Explainability of Intelligent Systems: Delivering Explainable and Personalised Recommendations for Energy Efficiency,Christos Sardianos and Iraklis Varlamis and Christos Chronis and George Dimitrakopoulos and Abdullah Alsalemi and Yassine Himeur and Faycal Bensaali and Abbes Amira,2020,Artificial Intelligence,2010.04990
"Anginal symptoms can connote increased cardiac risk and a need for change in cardiovascular management. This study evaluated the potential to extract these symptoms from physician notes using the Bidirectional Encoder from Transformers language model fine-tuned on a domain-specific corpus. The history of present illness section of 459 expert annotated primary care physician notes from consecutive patients referred for cardiac testing without known atherosclerotic cardiovascular disease were included. Notes were annotated for positive and negative mentions of chest pain and shortness of breath characterization. The results demonstrate high sensitivity and specificity for the detection of chest pain or discomfort, substernal chest pain, shortness of breath, and dyspnea on exertion. Small sample size limited extracting factors related to provocation and palliation of chest pain. This study provides a promising starting point for the natural language processing of physician notes to characterize clinically actionable anginal symptoms.",Extracting Angina Symptoms from Clinical Notes Using Pre-Trained Transformer Architectures,"Aaron S. Eisman, Nishant R. Shah, Carsten Eickhoff, George Zerveas, Elizabeth S. Chen, Wen-Chih Wu, Indra Neil Sarkar",2020,Artificial Intelligence,2010.05757
"We describe the design of a reproducing kernel suitable for attributed graphs, in which the similarity between the two graphs is defined based on the neighborhood information of the graph nodes with the aid of a product graph formulation. We represent the proposed kernel as the weighted sum of two other kernels of which one is an R-convolution kernel that processes the attribute information of the graph and the other is an optimal assignment kernel that processes label information. They are formulated in such a way that the edges processed as part of the kernel computation have the same neighborhood properties and hence the kernel proposed makes a well-defined correspondence between regions processed in graphs. These concepts are also extended to the case of the shortest paths. We identified the state-of-the-art kernels that can be mapped to such a neighborhood preserving framework. We found that the kernel value of the argument graphs in each iteration of the Weisfeiler-Lehman color refinement algorithm can be obtained recursively from the product graph formulated in our method. By incorporating the proposed kernel on support vector machines we analyzed the real-world data sets and it has shown superior performance in comparison with that of the other state-of-the-art graph kernels.",Neighborhood Preserving Kernels for Attributed Graphs,"Asif Salim, Shiju. S. S, and Sumitra. S",2022,Artificial Intelligence,2010.06261
"In classical causal inference, inferring cause-effect relations from data relies on the assumption that units are independent and identically distributed. This assumption is violated in settings where units are related through a network of dependencies. An example of such a setting is ad placement in sponsored search advertising, where the clickability of a particular ad is potentially influenced by where it is placed and where other ads are placed on the search result page. In such scenarios, confounding arises due to not only the individual ad-level covariates but also the placements and covariates of other ads in the system. In this paper, we leverage the language of causal inference in the presence of interference to model interactions among the ads. Quantification of such interactions allows us to better understand the click behavior of users, which in turn impacts the revenue of the host search engine and enhances user satisfaction. We illustrate the utility of our formalization through experiments carried out on the ad placement system of the Bing search engine.",Causal Inference in the Presence of Interference in Sponsored Search Advertising,"Razieh Nabi, Joel Pfeiffer, Murat Ali Bayir, Denis Charles, Emre K{\i}c{\i}man",2022,Artificial Intelligence,2010.07458
"Transfer learning is an effective technique to improve a target recommender system with the knowledge from a source domain. Existing research focuses on the recommendation performance of the target domain while ignores the privacy leakage of the source domain. The transferred knowledge, however, may unintendedly leak private information of the source domain. For example, an attacker can accurately infer user demographics from their historical purchase provided by a source domain data owner. This paper addresses the above privacy-preserving issue by learning a privacy-aware neural representation by improving target performance while protecting source privacy. The key idea is to simulate the attacks during the training for protecting unseen users' privacy in the future, modeled by an adversarial game, so that the transfer learning model becomes robust to attacks. Experiments show that the proposed PrivNet model can successfully disentangle the knowledge benefitting the transfer from leaking the privacy.",PrivNet: Safeguarding Private Attributes in Transfer Learning for Recommendation,"Guangneng Hu, Qiang Yang",2020,Artificial Intelligence,2010.08187
"Game AI competitions are important to foster research and development on Game AI and AI in general. These competitions supply different challenging problems that can be translated into other contexts, virtual or real. They provide frameworks and tools to facilitate the research on their core topics and provide means for comparing and sharing results. A competition is also a way to motivate new researchers to study these challenges. In this document, we present the Geometry Friends Game AI Competition. Geometry Friends is a two-player cooperative physics-based puzzle platformer computer game. The concept of the game is simple, though its solving has proven to be difficult. While the main and apparent focus of the game is cooperation, it also relies on other AI-related problems such as planning, plan execution, and motion control, all connected to situational awareness. All of these must be solved in real-time. In this paper, we discuss the competition and the challenges it brings, and present an overview of the current solutions.",A Game AI Competition to foster Collaborative AI research and development,Ana Salta and Rui Prada and Francisco S. Melo,2020,Artificial Intelligence,2010.08885
"The Dead Sea Scrolls are tangible evidence of the Bible's ancient scribal culture. Palaeography - the study of ancient handwriting - can provide access to this scribal culture. However, one of the problems of traditional palaeography is to determine writer identity when the writing style is near uniform. This is exemplified by the Great Isaiah Scroll (1QIsaa). To this end, we used pattern recognition and artificial intelligence techniques to innovate the palaeography of the scrolls regarding writer identification and to pioneer the microlevel of individual scribes to open access to the Bible's ancient scribal culture. Although many scholars believe that 1QIsaa was written by one scribe, we report new evidence for a breaking point in the series of columns in this scroll. Without prior assumption of writer identity, based on point clouds of the reduced-dimensionality feature-space, we found that columns from the first and second halves of the manuscript ended up in two distinct zones of such scatter plots, notably for a range of digital palaeography tools, each addressing very different featural aspects of the script samples. In a secondary, independent, analysis, now assuming writer difference and using yet another independent feature method and several different types of statistical testing, a switching point was found in the column series. A clear phase transition is apparent around column 27. Given the statistically significant differences between the two halves, a tertiary, post-hoc analysis was performed. Demonstrating that two main scribes were responsible for the Great Isaiah Scroll, this study sheds new light on the Bible's ancient scribal culture by providing new, tangible evidence that ancient biblical texts were not copied by a single scribe only but that multiple scribes could closely collaborate on one particular manuscript.",Artificial intelligence based writer identification generates new evidence for the unknown scribes of the Dead Sea Scrolls exemplified by the Great Isaiah Scroll (1QIsaa),"Mladen Popovi\'c, Maruf A. Dhali, Lambert Schomaker",2021,Artificial Intelligence,2010.14476
"The 9th International Workshop on Theorem-Proving Components for Educational Software (ThEdu'20) was scheduled to happen on June 29 as a satellite of the IJCAR-FSCD 2020 joint meeting, in Paris. The COVID-19 pandemic came by surprise, though, and the main conference was virtualised. Fearing that an online meeting would not allow our community to fully reproduce the usual face-to-face networking opportunities of the ThEdu initiative, the Steering Committee of ThEdu decided to cancel our workshop. Given that many of us had already planned and worked for that moment, we decided that ThEdu'20 could still live in the form of an EPTCS volume. The EPTCS concurred with us, recognising this very singular situation, and accepted our proposal of organising a special issue with papers submitted to ThEdu'20. An open call for papers was then issued, and attracted five submissions, all of which have been accepted by our reviewers, who produced three careful reports on each of the contributions. The resulting revised papers are collected in the present volume. We, the volume editors, hope that this collection of papers will help further promoting the development of theorem-proving-based software, and that it will collaborate to improve the mutual understanding between computer mathematicians and stakeholders in education. With some luck, we would actually expect that the very special circumstances set up by the worst sanitary crisis in a century will happen to reinforce the need for the application of certified components and of verification methods for the production of educational software that would be available even when the traditional on-site learning experiences turn out not to be recommendable.",Proceedings 9th International Workshop on Theorem Proving Components for Educational Software,"Pedro Quaresma (University of Coimbra, Portugal), Walther Neuper (JKU Johannes Kepler University, Linz, Austria), Jo\~ao Marcos (UFRN, Brazil)",2020,Artificial Intelligence,2010.15832
"We study the novel problem of blackbox optimization of multiple objectives via multi-fidelity function evaluations that vary in the amount of resources consumed and their accuracy. The overall goal is to approximate the true Pareto set of solutions by minimizing the resources consumed for function evaluations. For example, in power system design optimization, we need to find designs that trade-off cost, size, efficiency, and thermal tolerance using multi-fidelity simulators for design evaluations. In this paper, we propose a novel approach referred as Multi-Fidelity Output Space Entropy Search for Multi-objective Optimization (MF-OSEMO) to solve this problem. The key idea is to select the sequence of candidate input and fidelity-vector pairs that maximize the information gained about the true Pareto front per unit resource cost. Our experiments on several synthetic and real-world benchmark problems show that MF-OSEMO, with both approximations, significantly improves over the state-of-the-art single-fidelity algorithms for multi-objective optimization.",Multi-Fidelity Multi-Objective Bayesian Optimization: An Output Space Entropy Search Approach,"Syrine Belakaria, Aryan Deshwal and Janardhan Rao Doppa",2020,Artificial Intelligence,2011.01542
"This paper updates the cognitive model, firstly by creating two systems and then unifying them over the same structure. It represents information at the semantic level only, where labelled patterns are aggregated into a 'type-set-match' form. It is described that the aggregations can be used to match across regions with potentially different functionality and therefore give the structure a required amount of flexibility. The theory is that if the model stores information which can be transposed in consistent ways, then that will result in knowledge and some level of intelligence. As part of the design, patterns have to become distinct and that is realised by unique paths through shared aggregated structures. An ensemble-hierarchy relation also helps to define uniqueness through local feedback that may even be an action potential. The earlier models are still consistent in terms of their proposed functionality, but some of the architecture boundaries have been moved to match them up more closely. After pattern optimisation and tree-like aggregations, the two main models differ only in their upper, more intelligent level. One provides a propositional logic for mutually inclusive or exclusive pattern groups and sequences, while the other provides a behaviour script that is constructed from node types. It can be seen that these two views are complimentary and would allow some control over behaviours, as well as memories, that might get selected.",New Ideas for Brain Modelling 7,Kieran Greer,2021,Artificial Intelligence,2011.02223
"Analytic software tools and workflows are increasing in capability, complexity, number, and scale, and the integrity of our workflows is as important as ever. Specifically, we must be able to inspect the process of analytic workflows to assess (1) confidence of the conclusions, (2) risks and biases of the operations involved, (3) sensitivity of the conclusions to sources and agents, (4) impact and pertinence of various sources and agents, and (5) diversity of the sources that support the conclusions. We present an approach that tracks agents' provenance with PROV-O in conjunction with agents' appraisals and evidence links (expressed in our novel DIVE ontology). Together, PROV-O and DIVE enable dynamic propagation of confidence and counter-factual refutation to improve human-machine trust and analytic integrity. We demonstrate representative software developed for user interaction with that provenance, and discuss key needs for organizations adopting such approaches. We demonstrate all of these assessments in a multi-agent analysis scenario, using an interactive web-based information validation UI.",Provenance-Based Interpretation of Multi-Agent Information Analysis,"Scott Friedman, Jeff Rye, David LaVergne, Dan Thomsen, Matthew Allen, Kyle Tunis",2020,Artificial Intelligence,2011.04016
Floor space optimization is a critical revenue management problem commonly encountered by retailers. It maximizes store revenue by optimally allocating floor space to product categories which are assigned to their most appropriate planograms. We formulate the problem as a connected multi-choice knapsack problem with an additional global constraint and propose a tabu search based meta-heuristic that exploits the multiple special neighborhood structures. We also incorporate a mechanism to determine how to combine the multiple neighborhood moves. A candidate list strategy based on learning from prior search history is also employed to improve the search quality. The results of computational testing with a set of test problems show that our tabu search heuristic can solve all problems within a reasonable amount of time. Analyses of individual contributions of relevant components of the algorithm were conducted with computational experiments.,Maximizing Store Revenues using Tabu Search for Floor Space Optimization,Jiefeng Xu and Evren Gul and Alvin Lim,2021,Artificial Intelligence,2011.04422
"This paper studies the scheduling of jobs of different families on parallel machines with qualification constraints. Originating from semiconductor manufacturing, this constraint imposes a time threshold between the execution of two jobs of the same family. Otherwise, the machine becomes disqualified for this family. The goal is to minimize both the flow time and the number of disqualifications. Recently, an efficient constraint programming model has been proposed. However, when priority is given to the flow time objective, the efficiency of the model can be improved. This paper uses a polynomial-time algorithm which minimize the flow time for a single machine relaxation where disqualifications are not considered. Using this algorithm one can derived filtering rules on different variables of the model. Experimental results are presented showing the effectiveness of these rules. They improve the competitiveness with the mixed integer linear program of the literature.",Filtering Rules for Flow Time Minimization in a Parallel Machine Scheduling Problem,"Margaux Nattaf (G-SCOP), Arnaud Malapert",2020,Artificial Intelligence,2011.10307
"A new fuzzy method is developed using triangular/trapezoidal fuzzy numbers for evaluating a group's mean performance, when qualitative grades instead of numerical scores are used for assessing its members' individual performance. Also, a new technique is developed for solving Linear Programming problems with fuzzy coefficients and everyday life applications are presented to illustrate our results.",Assessment and Linear Programming under Fuzzy Conditions,Michael Voskoglou,2020,Artificial Intelligence,2011.10640
"Artificial Intelligence (AI) governance regulates the exercise of authority and control over the management of AI. It aims at leveraging AI through effective use of data and minimization of AI-related cost and risk. While topics such as AI governance and AI ethics are thoroughly discussed on a theoretical, philosophical, societal and regulatory level, there is limited work on AI governance targeted to companies and corporations. This work views AI products as systems, where key functionality is delivered by machine learning (ML) models leveraging (training) data. We derive a conceptual framework by synthesizing literature on AI and related fields such as ML. Our framework decomposes AI governance into governance of data, (ML) models and (AI) systems along four dimensions. It relates to existing IT and data governance frameworks and practices. It can be adopted by practitioners and academics alike. For practitioners the synthesis of mainly research papers, but also practitioner publications and publications of regulatory bodies provides a valuable starting point to implement AI governance, while for academics the paper highlights a number of areas of AI governance that deserve more attention.",AI Governance for Businesses,Johannes Schneider and Rene Abraham and Christian Meske and Jan vom Brocke,2022,Artificial Intelligence,2011.10672
"Planning - the ability to analyze the structure of a problem in the large and decompose it into interrelated subproblems - is a hallmark of human intelligence. While deep reinforcement learning (RL) has shown great promise for solving relatively straightforward control tasks, it remains an open problem how to best incorporate planning into existing deep RL paradigms to handle increasingly complex environments. One prominent framework, Model-Based RL, learns a world model and plans using step-by-step virtual rollouts. This type of world model quickly diverges from reality when the planning horizon increases, thus struggling at long-horizon planning. How can we learn world models that endow agents with the ability to do temporally extended reasoning? In this work, we propose to learn graph-structured world models composed of sparse, multi-step transitions. We devise a novel algorithm to learn latent landmarks that are scattered (in terms of reachability) across the goal space as the nodes on the graph. In this same graph, the edges are the reachability estimates distilled from Q-functions. On a variety of high-dimensional continuous control tasks ranging from robotic manipulation to navigation, we demonstrate that our method, named L3P, significantly outperforms prior work, and is oftentimes the only method capable of leveraging both the robustness of model-free RL and generalization of graph-search algorithms. We believe our work is an important step towards scalable planning in reinforcement learning.",World Model as a Graph: Learning Latent Landmarks for Planning,"Lunjun Zhang, Ge Yang, Bradly C. Stadie",2021,Artificial Intelligence,2011.12491
"Large software systems tune hundreds of 'constants' to optimize their runtime performance. These values are commonly derived through intuition, lab tests, or A/B tests. A 'one-size-fits-all' approach is often sub-optimal as the best value depends on runtime context. In this paper, we provide an experimental approach to replace constants with learned contextual functions for Skype - a widely used real-time communication (RTC) application. We present Resonance, a system based on contextual bandits (CB). We describe experiences from three real-world experiments: applying it to the audio, video, and transport components in Skype. We surface a unique and practical challenge of performing machine learning (ML) inference in large software systems written using encapsulation principles. Finally, we open-source FeatureBroker, a library to reduce the friction in adopting ML models in such development environments",Resonance: Replacing Software Constants with Context-Aware Models in Real-time Communication,"Jayant Gupchup, Ashkan Aazami, Yaran Fan, Senja Filipi, Tom Finley, Scott Inglis, Marcus Asteborg, Luke Caroll, Rajan Chari, Markus Cozowicz, Vishak Gopal, Vinod Prakash, Sasikanth Bendapudi, Jack Gerrits, Eric Lau, Huazhou Liu, Marco Rossi, Dima Slobodianyk, Dmitri Birjukov, Matty Cooper, Nilesh Javar, Dmitriy Perednya, Sriram Srinivasan, John Langford, Ross Cutler, Johannes Gehrke",2020,Artificial Intelligence,2011.12715
"In this paper, we outline the implementation of the TFD (Totally Ordered Fast Downward) and the PFD (Partially ordered Fast Downward) hierarchical planners that participated in the first HTN IPC competition in 2020. These two planners are based on forward-chaining task decomposition coupled with a compact grounding of actions, methods, tasks and HTN problems.",Totally and Partially Ordered Hierarchical Planners in PDDL4J Library,"Damien Pellier, Humbert Fiorino",2020,Artificial Intelligence,2011.13297
"For a long time the ability to solve abstract reasoning tasks was considered one of the hallmarks of human intelligence. Recent advances in application of deep learning (DL) methods led, as in many other domains, to surpassing human abstract reasoning performance, specifically in the most popular type of such problems - the Raven's Progressive Matrices (RPMs). While the efficacy of DL systems is indeed impressive, the way they approach the RPMs is very different from that of humans. State-of-the-art systems solving RPMs rely on massive pattern-based training and sometimes on exploiting biases in the dataset, whereas humans concentrate on identification of the rules / concepts underlying the RPM (or generally a visual reasoning task) to be solved. Motivated by this cognitive difference, this work aims at combining DL with human way of solving RPMs and getting the best of both worlds. Specifically, we cast the problem of solving RPMs into multi-label classification framework where each RPM is viewed as a multi-label data point, with labels determined by the set of abstract rules underlying the RPM. For efficient training of the system we introduce a generalisation of the Noise Contrastive Estimation algorithm to the case of multi-label samples. Furthermore, we propose a new sparse rule encoding scheme for RPMs which, besides the new training algorithm, is the key factor contributing to the state-of-the-art performance. The proposed approach is evaluated on two most popular benchmark datasets (Balanced-RAVEN and PGM) and on both of them demonstrates an advantage over the current state-of-the-art results. Contrary to applications of contrastive learning methods reported in other domains, the state-of-the-art performance reported in the paper is achieved with no need for large batch sizes or strong data augmentation.",Multi-Label Contrastive Learning for Abstract Visual Reasoning,"Miko{\l}aj Ma{\l}ki\'nski, Jacek Ma\'ndziuk",2022,Artificial Intelligence,2012.01944
"While the potential of deep learning(DL) for automating simple tasks is already well explored, recent research started investigating the use of deep learning for creative design, both for complete artifact creation and supporting humans in the creation process. In this paper, we use insights from computational creativity to conceptualize and assess current applications of generative deep learning in creative domains identified in a literature review. We highlight parallels between current systems and different models of human creativity as well as their shortcomings. While deep learning yields results of high value, such as high quality images, their novelity is typically limited due to multiple reasons such a being tied to a conceptual space defined by training data and humans. Current DL methods also do not allow for changes in the internal problem representation and they lack the capability to identify connections across highly different domains, both of which are seen as major drivers of human creativity.",Creativity of Deep Learning: Conceptualization and Assessment,Johannes Schneider and Marcus Basalla,2022,Artificial Intelligence,2012.02282
"Having a comprehensive, high-quality dataset of road sign annotation is critical to the success of AI-based Road Sign Recognition (RSR) systems. In practice, annotators often face difficulties in learning road sign systems of different countries; hence, the tasks are often time-consuming and produce poor results. We propose a novel approach using knowledge graphs and a machine learning algorithm - variational prototyping-encoder (VPE) - to assist human annotators in classifying road signs effectively. Annotators can query the Road Sign Knowledge Graph using visual attributes and receive closest matching candidates suggested by the VPE model. The VPE model uses the candidates from the knowledge graph and a real sign image patch as inputs. We show that our knowledge graph approach can reduce sign search space by 98.9%. Furthermore, with VPE, our system can propose the correct single candidate for 75% of signs in the tested datasets, eliminating the human search effort entirely in those cases.",Accelerating Road Sign Ground Truth Construction with Knowledge Graph and Machine Learning,"Ji Eun Kim, Cory Henson, Kevin Huang, Tuan A. Tran, Wan-Yi Lin",2021,Artificial Intelligence,2012.02672
"With the popularity of the Internet, traditional offline resource allocation has evolved into a new form, called online resource allocation. It features the online arrivals of agents in the system and the real-time decision-making requirement upon the arrival of each online agent. Both offline and online resource allocation have wide applications in various real-world matching markets ranging from ridesharing to crowdsourcing. There are some emerging applications such as rebalancing in bike sharing and trip-vehicle dispatching in ridesharing, which involve a two-stage resource allocation process. The process consists of an offline phase and another sequential online phase, and both phases compete for the same set of resources. In this paper, we propose a unified model which incorporates both offline and online resource allocation into a single framework. Our model assumes non-uniform and known arrival distributions for online agents in the second online phase, which can be learned from historical data. We propose a parameterized linear programming (LP)-based algorithm, which is shown to be at most a constant factor of $1/4$ from the optimal. Experimental results on the real dataset show that our LP-based approaches outperform the LP-agnostic heuristics in terms of robustness and effectiveness.",A Unified Model for the Two-stage Offline-then-Online Resource Allocation,"Yifan Xu, Pan Xu, Jianping Pan and Jun Tao",2020,Artificial Intelligence,2012.06845
"Several scientific studies have reported the existence of the income gap among rideshare drivers based on demographic factors such as gender, age, race, etc. In this paper, we study the income inequality among rideshare drivers due to discriminative cancellations from riders, and the tradeoff between the income inequality (called fairness objective) with the system efficiency (called profit objective). We proposed an online bipartite-matching model where riders are assumed to arrive sequentially following a distribution known in advance. The highlight of our model is the concept of acceptance rate between any pair of driver-rider types, where types are defined based on demographic factors. Specially, we assume each rider can accept or cancel the driver assigned to her, each occurs with a certain probability which reflects the acceptance degree from the rider type towards the driver type. We construct a bi-objective linear program as a valid benchmark and propose two LP-based parameterized online algorithms. Rigorous online competitive ratio analysis is offered to demonstrate the flexibility and efficiency of our online algorithms in balancing the two conflicting goals, promotions of fairness and profit. Experimental results on a real-world dataset are provided as well, which confirm our theoretical predictions.",Trading the System Efficiency for the Income Equality of Drivers in Rideshare,Yifan Xu and Pan Xu,2020,Artificial Intelligence,2012.06850
"Ranking vertices of multidimensional networks is crucial in many areas of research, including selecting and determining the importance of decisions. Some decisions are significantly more important than others, and their weight categorization is also imortant. This paper defines a completely new method for determining the weight decisions using artificial intelligence for importance ranking of three-dimensional network vertices, improving the existing Ordered Statistics Vertex Extraction and Tracking Algorithm (OSVETA) based on modulation of quantized indices (QIM) and error correction codes. The technique we propose in this paper offers significant improvements the efficiency of determination the importance of network vertices in relation to statistical OSVETA criteria, replacing heuristic methods with methods of precise prediction of modern neural networks. The new artificial intelligence technique enables a significantly better definition of the 3D meshes and a better assessment of their topological features. The new method contributions result in a greater precision in defining stable vertices, significantly reducing the probability of deleting mesh vertices.",Artificial Intelligence ordered 3D vertex importance,"Iva Vasic, Bata Vasic, and Zorica Nikolic",2020,Artificial Intelligence,2012.10232
"Previous neural solvers of math word problems (MWPs) are learned with full supervision and fail to generate diverse solutions. In this paper, we address this issue by introducing a \textit{weakly-supervised} paradigm for learning MWPs. Our method only requires the annotations of the final answers and can generate various solutions for a single problem. To boost weakly-supervised learning, we propose a novel \textit{learning-by-fixing} (LBF) framework, which corrects the misperceptions of the neural network via symbolic reasoning. Specifically, for an incorrect solution tree generated by the neural network, the \textit{fixing} mechanism propagates the error from the root node to the leaf nodes and infers the most probable fix that can be executed to get the desired answer. To generate more diverse solutions, \textit{tree regularization} is applied to guide the efficient shrinkage and exploration of the solution space, and a \textit{memory buffer} is designed to track and save the discovered various fixes for each problem. Experimental results on the Math23K dataset show the proposed LBF framework significantly outperforms reinforcement learning baselines in weakly-supervised learning. Furthermore, it achieves comparable top-1 and much better top-3/5 answer accuracies than fully-supervised methods, demonstrating its strength in producing diverse solutions.",Learning by Fixing: Solving Math Word Problems with Weak Supervision,"Yining Hong, Qing Li, Daniel Ciao, Siyuan Huang, Song-Chun Zhu",2021,Artificial Intelligence,2012.10582
"The paper relies on the clinical data of a previously published study. We identify two very questionable assumptions of said work, namely confusing evidence of absence and absence of evidence, and neglecting the ordinal nature of attributes' domains. We then show that using an adequate ordinal methodology such as the dominance-based rough sets approach (DRSA) can significantly improve the predictive accuracy of the expert system, resulting in almost complete accuracy for a dataset of 100 instances. Beyond the performance of DRSA in solving the diagnosis problem at hand, these results suggest the inadequacy and triviality of the underlying dataset. We provide links to open data from the UCI machine learning repository to allow for an easy verification/refutation of the claims made in this paper.",Predicting Seminal Quality with the Dominance-Based Rough Sets Approach,Nassim Dehouche,2020,Artificial Intelligence,2012.13204
"Textile manufacturing is a typical traditional industry involving high complexity in interconnected processes with limited capacity on the application of modern technologies. Decision-making in this domain generally takes multiple criteria into consideration, which usually arouses more complexity. To address this issue, the present paper proposes a decision support system that combines the intelligent data-based random forest (RF) models and a human knowledge based analytical hierarchical process (AHP) multi-criteria structure in accordance to the objective and the subjective factors of the textile manufacturing process. More importantly, the textile manufacturing process is described as the Markov decision process (MDP) paradigm, and a deep reinforcement learning scheme, the Deep Q-networks (DQN), is employed to optimize it. The effectiveness of this system has been validated in a case study of optimizing a textile ozonation process, showing that it can better master the challenging decision-making tasks in textile manufacturing processes.",A Deep Reinforcement Learning Based Multi-Criteria Decision Support System for Textile Manufacturing Process Optimization,"Zhenglei He (GEMTEX), Kim Phuc Tran (GEMTEX), Sebastien Thomassey (GEMTEX), Xianyi Zeng (GEMTEX), Jie Xu, Chang Haiyi",2020,Artificial Intelligence,2012.14794
"Simulations, along with other similar applications like virtual worlds and video games, require computational models of intelligence that generate realistic and credible behavior for the participating synthetic characters. Cognitive architectures, which are models of the fixed structure underlying intelligent behavior in both natural and artificial systems, provide a conceptually valid common basis, as evidenced by the current efforts towards a standard model of the mind, to generate human-like intelligent behavior for these synthetic characters. Sigma is a cognitive architecture and system that strives to combine what has been learned from four decades of independent work on symbolic cognitive architectures, probabilistic graphical models, and more recently neural models, under its graphical architecture hypothesis. Sigma leverages an extended form of factor graphs towards a uniform grand unification of not only traditional cognitive capabilities but also key non-cognitive aspects, creating unique opportunities for the construction of new kinds of cognitive models that possess a Theory-of-Mind and that are perceptual, autonomous, interactive, affective, and adaptive. In this paper, we will introduce Sigma along with its diverse capabilities and then use three distinct proof-of-concept Sigma models to highlight combinations of these capabilities: (1) Distributional reinforcement learning models in; (2) A pair of adaptive and interactive agent models that demonstrate rule-based, probabilistic, and social reasoning; and (3) A knowledge-free exploration model in which an agent leverages only architectural appraisal variables, namely attention and curiosity, to locate an item while building up a map in a Unity environment.",Controlling Synthetic Characters in Simulations: A Case for Cognitive Architectures and Sigma,"Volkan Ustun, Paul S. Rosenbloom, Seyed Sajjadi, Jeremy Nuttal",2018,Artificial Intelligence,2101.02231
"The Coronavirus (COVID-19) pandemic has led to a rapidly growing 'infodemic' of health information online. This has motivated the need for accurate semantic search and retrieval of reliable COVID-19 information across millions of documents, in multiple languages. To address this challenge, this paper proposes a novel high precision and high recall neural Multistage BiCross encoder approach. It is a sequential three-stage ranking pipeline which uses the Okapi BM25 retrieval algorithm and transformer-based bi-encoder and cross-encoder to effectively rank the documents with respect to the given query. We present experimental results from our participation in the Multilingual Information Access (MLIA) shared task on COVID-19 multilingual semantic search. The independently evaluated MLIA results validate our approach and demonstrate that it outperforms other state-of-the-art approaches according to nearly all evaluation metrics in cases of both monolingual and bilingual runs.",Multistage BiCross encoder for multilingual access to COVID-19 health information,"Iknoor Singh, Carolina Scarton, Kalina Bontcheva",2021,Artificial Intelligence,2101.03013
"We present DEGARI (Dynamic Emotion Generator And ReclassIfier), an explainable system for emotion attribution and recommendation. This system relies on a recently introduced commonsense reasoning framework, the TCL logic, which is based on a human-like procedure for the automatic generation of novel concepts in a Description Logics knowledge base. Starting from an ontological formalization of emotions based on the Plutchik model, known as ArsEmotica, the system exploits the logic TCL to automatically generate novel commonsense semantic representations of compound emotions (e.g. Love as derived from the combination of Joy and Trust according to Plutchik). The generated emotions correspond to prototypes, i.e. commonsense representations of given concepts, and have been used to reclassify emotion-related contents in a variety of artistic domains, ranging from art datasets to the editorial contents available in RaiPlay, the online platform of RAI Radiotelevisione Italiana (the Italian public broadcasting company). We show how the reported results (evaluated in the light of the obtained reclassifications, the user ratings assigned to such reclassifications, and their explainability) are encouraging, and pave the way to many further research directions.","A Commonsense Reasoning Framework for Explanatory Emotion Attribution, Generation and Re-classification","Antonio Lieto, Gian Luca Pozzato, Stefano Zoia, Viviana Patti, Rossana Damiano",2021,Artificial Intelligence,2101.04017
"Commonsense knowledge is essential for many AI applications, including those in natural language processing, visual processing, and planning. Consequently, many sources that include commonsense knowledge have been designed and constructed over the past decades. Recently, the focus has been on large text-based sources, which facilitate easier integration with neural (language) models and application to textual tasks, typically at the expense of the semantics of the sources and their harmonization. Efforts to consolidate commonsense knowledge have yielded partial success, with no clear path towards a comprehensive solution. We aim to organize these sources around a common set of dimensions of commonsense knowledge. We survey a wide range of popular commonsense sources with a special focus on their relations. We consolidate these relations into 13 knowledge dimensions. This consolidation allows us to unify the separate sources and to compute indications of their coverage, overlap, and gaps with respect to the knowledge dimensions. Moreover, we analyze the impact of each dimension on downstream reasoning tasks that require commonsense knowledge, observing that the temporal and desire/goal dimensions are very beneficial for reasoning on current downstream tasks, while distinctness and lexical knowledge have little impact. These results reveal preferences for some dimensions in current evaluation, and potential neglect of others.",Dimensions of Commonsense Knowledge,"Filip Ilievski, Alessandro Oltramari, Kaixin Ma, Bin Zhang, Deborah L. McGuinness, Pedro Szekely",2021,Artificial Intelligence,2101.04640
"With the powerful learning ability of deep convolutional networks, deep clustering methods can extract the most discriminative information from individual data and produce more satisfactory clustering results. However, existing deep clustering methods usually ignore the relationship between the data. Fortunately, the graph convolutional network can handle such relationship, opening up a new research direction for deep clustering. In this paper, we propose a cross-attention based deep clustering framework, named Cross-Attention Fusion based Enhanced Graph Convolutional Network (CaEGCN), which contains four main modules: the cross-attention fusion module which innovatively concatenates the Content Auto-encoder module (CAE) relating to the individual data and Graph Convolutional Auto-encoder module (GAE) relating to the relationship between the data in a layer-by-layer manner, and the self-supervised model that highlights the discriminative information for clustering tasks. While the cross-attention fusion module fuses two kinds of heterogeneous representation, the CAE module supplements the content information for the GAE module, which avoids the over-smoothing problem of GCN. In the GAE module, two novel loss functions are proposed that reconstruct the content and relationship between the data, respectively. Finally, the self-supervised module constrains the distributions of the middle layer representations of CAE and GAE to be consistent. Experimental results on different types of datasets prove the superiority and robustness of the proposed CaEGCN.",CaEGCN: Cross-Attention Fusion based Enhanced Graph Convolutional Network for Clustering,"Guangyu Huo, Yong Zhang, Junbin Gao, Boyue Wang, Yongli Hu, and Baocai Yin",2021,Artificial Intelligence,2101.06883
"Recent advances have shown how decision trees are apt data structures for concisely representing strategies (or controllers) satisfying various objectives. Moreover, they also make the strategy more explainable. The recent tool dtControl had provided pipelines with tools supporting strategy synthesis for hybrid systems, such as SCOTS and Uppaal Stratego. We present dtControl 2.0, a new version with several fundamentally novel features. Most importantly, the user can now provide domain knowledge to be exploited in the decision tree learning process and can also interactively steer the process based on the dynamically provided information. To this end, we also provide a graphical user interface. It allows for inspection and re-computation of parts of the result, suggesting as well as receiving advice on predicates, and visual simulation of the decision-making process. Besides, we interface model checkers of probabilistic systems, namely Storm and PRISM and provide dedicated support for categorical enumeration-type state variables. Consequently, the controllers are more explainable and smaller.",dtControl 2.0: Explainable Strategy Representation via Decision Tree Learning Steered by Experts,"Pranav Ashok, Mathias Jackermeier, Jan K\v{r}et\'insk\'y, Christoph Weinhuber, Maximilian Weininger, Mayank Yadav",2021,Artificial Intelligence,2101.07202
"For some scientific questions, empirical data are essential to develop reliable simulation models. These data usually come from different sources with diverse and heterogeneous formats. The design of complex data-driven models is often shaped by the structure of the data available in research projects. Hence, applying such models to other case studies requires either to get similar data or to transform new data to fit the model inputs. It is the case of agent-based models (ABMs) that use advanced data structures such as Geographic Information Systems data. We faced this problem in the LittoSIM-GEN project when generalizing our participatory flooding model (LittoSIM) to new territories. From this experience, we provide a mapping approach to structure, describe, and automatize the integration of geospatial data into ABMs.",Mapping and Describing Geospatial Data to Generalize Complex Mapping and Describing Geospatial Data to Generalize Complex Models: The Case of LittoSIM-GEN Models,"Ahmed Laatabi, Nicolas Becu (LIENSs), Nicolas Marilleau (UMMISCO), C\'ecilia Pignon-Mussaud (LIENSs), Marion Amalric (CITERES), X. Bertin (LIENSs), Brice Anselme (PRODIG), Elise Beck (PACTE)",2020,Artificial Intelligence,2101.07523
"In 2011, Hibbard suggested an intelligence measure for agents who compete in an adversarial sequence prediction game. We argue that Hibbard's idea should actually be considered as two separate ideas: first, that the intelligence of such agents can be measured based on the growth rates of the runtimes of the competitors that they defeat; and second, one specific (somewhat arbitrary) method for measuring said growth rates. Whereas Hibbard's intelligence measure is based on the latter growth-rate-measuring method, we survey other methods for measuring function growth rates, and exhibit the resulting Hibbard-like intelligence measures and taxonomies. Of particular interest, we obtain intelligence taxonomies based on Big-O and Big-Theta notation systems, which taxonomies are novel in that they challenge conventional notions of what an intelligence measure should look like. We discuss how intelligence measurement of sequence predictors can indirectly serve as intelligence measurement for agents with Artificial General Intelligence (AGIs).",Measuring Intelligence and Growth Rate: Variations on Hibbard's Intelligence Measure,"Samuel Alexander, Bill Hibbard",2021,Artificial Intelligence,2101.12047
"Experimental studies are prevalent in Evolutionary Computation (EC), and concerns about the reproducibility and replicability of such studies have increased in recent times, reflecting similar concerns in other scientific fields. In this article, we discuss, within the context of EC, the different types of reproducibility and suggest a classification that refines the badge system of the Association of Computing Machinery (ACM) adopted by ACM Transactions on Evolutionary Learning and Optimization (https://dlnext.acm.org/journal/telo). We identify cultural and technical obstacles to reproducibility in the EC field. Finally, we provide guidelines and suggest tools that may help to overcome some of these reproducibility obstacles.",Reproducibility in Evolutionary Computation,"Manuel L\'opez-Ib\'a\~nez (University of M\'alaga, Spain), Juergen Branke (University of Warwick, UK), Lu\'is Paquete (University of Coimbra, Portugal)",2021,Artificial Intelligence,2102.03380
"Real-world knowledge graphs are often characterized by low-frequency relations - a challenge that has prompted an increasing interest in few-shot link prediction methods. These methods perform link prediction for a set of new relations, unseen during training, given only a few example facts of each relation at test time. In this work, we perform a systematic study on a spectrum of models derived by generalizing the current state of the art for few-shot link prediction, with the goal of probing the limits of learning in this few-shot setting. We find that a simple zero-shot baseline - which ignores any relation-specific information - achieves surprisingly strong performance. Moreover, experiments on carefully crafted synthetic datasets show that having only a few examples of a relation fundamentally limits models from using fine-grained structural information and only allows for exploiting the coarse-grained positional information of entities. Together, our findings challenge the implicit assumptions and inductive biases of prior work and highlight new directions for research in this area.",Exploring the Limits of Few-Shot Link Prediction in Knowledge Graphs,"Dora Jambor, Komal Teru, Joelle Pineau, William L. Hamilton",2021,Artificial Intelligence,2102.03419
AI systems often rely on two key components: a specified goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial specification of the principal's goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the $L$ attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on $J < L$ attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal-agent problem from artificial intelligence; 2) we provide necessary and sufficient conditions under which indefinitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identifies a theoretical scenario where some degree of interactivity is desirable.,Consequences of Misaligned AI,"Simon Zhuang, Dylan Hadfield-Menell",2020,Artificial Intelligence,2102.03896
"Understanding the principles of real-world biological multi-agent behaviors is a current challenge in various scientific and engineering fields. The rules regarding the real-world biological multi-agent behaviors such as team sports are often largely unknown due to their inherently higher-order interactions, cognition, and body dynamics. Estimation of the rules from data, i.e., data-driven approaches such as machine learning, provides an effective way for the analysis of such behaviors. Although most data-driven models have non-linear structures and high prediction performances, it is sometimes hard to interpret them. This survey focuses on data-driven analysis for quantitative understanding of invasion team sports behaviors such as basketball and football, and introduces two main approaches for understanding such multi-agent behaviors: (1) extracting easily interpretable features or rules from data and (2) generating and controlling behaviors in visually-understandable ways. The first approach involves the visualization of learned representations and the extraction of mathematical structures behind the behaviors. The second approach can be used to test hypotheses by simulating and controlling future and counterfactual behaviors. Lastly, the potential practical applications of extracted rules, features, and generated behaviors are discussed. These approaches can contribute to a better understanding of multi-agent behaviors in the real world.",Data-driven Analysis for Understanding Team Sports Behaviors,Keisuke Fujii,2021,Artificial Intelligence,2102.07545
"Accurate forecasting of medical service requirements is an important big data problem that is crucial for resource management in critical times such as natural disasters and pandemics. With the global spread of coronavirus disease 2019 (COVID-19), several concerns have been raised regarding the ability of medical systems to handle sudden changes in the daily routines of healthcare providers. One significant problem is the management of ambulance dispatch and control during a pandemic. To help address this problem, we first analyze ambulance dispatch data records from April 2014 to August 2020 for Nagoya City, Japan. Significant changes were observed in the data during the pandemic, including the state of emergency (SoE) declared across Japan. In this study, we propose a deep learning framework based on recurrent neural networks to estimate the number of emergency ambulance dispatches (EADs) during a SoE. The fusion of data includes environmental factors, the localization data of mobile phone users, and the past history of EADs, thereby providing a general framework for knowledge discovery and better resource management. The results indicate that the proposed blend of training data can be used efficiently in a real-world estimation of EAD requirements during periods of high uncertainties such as pandemics.","Knowledge discovery from emergency ambulance dispatch during COVID-19: A case study of Nagoya City, Japan","Essam A. Rashed, Sachiko Kodera, Hidenobu Shirakami, Ryotetsu Kawaguchi, Kazuhiro Watanabe, Akimasa Hirata",2021,Artificial Intelligence,2102.08628
"Drug discovery and development is a complex and costly process. Machine learning approaches are being investigated to help improve the effectiveness and speed of multiple stages of the drug discovery pipeline. Of these, those that use Knowledge Graphs (KG) have promise in many tasks, including drug repurposing, drug toxicity prediction and target gene-disease prioritisation. In a drug discovery KG, crucial elements including genes, diseases and drugs are represented as entities, whilst relationships between them indicate an interaction. However, to construct high-quality KGs, suitable data is required. In this review, we detail publicly available sources suitable for use in constructing drug discovery focused KGs. We aim to help guide machine learning and KG practitioners who are interested in applying new techniques to the drug discovery field, but who may be unfamiliar with the relevant data sources. The datasets are selected via strict criteria, categorised according to the primary type of information contained within and are considered based upon what information could be extracted to build a KG. We then present a comparative analysis of existing public drug discovery KGs and a evaluation of selected motivating case studies from the literature. Additionally, we raise numerous and unique challenges and issues associated with the domain and its datasets, whilst also highlighting key future research directions. We hope this review will motivate KGs use in solving key and emerging questions in the drug discovery domain.",A Review of Biomedical Datasets Relating to Drug Discovery: A Knowledge Graph Perspective,Stephen Bonner and Ian P Barrett and Cheng Ye and Rowan Swiers and Ola Engkvist and Andreas Bender and Charles Tapley Hoyt and William L Hamilton,2022,Artificial Intelligence,2102.10062
"A key challenge for reinforcement learning is solving long-horizon planning problems. Recent work has leveraged programs to guide reinforcement learning in these settings. However, these approaches impose a high manual burden on the user since they must provide a guiding program for every new task. Partially observed environments further complicate the programming task because the program must implement a strategy that correctly, and ideally optimally, handles every possible configuration of the hidden regions of the environment. We propose a new approach, model predictive program synthesis (MPPS), that uses program synthesis to automatically generate the guiding programs. It trains a generative model to predict the unobserved portions of the world, and then synthesizes a program based on samples from this model in a way that is robust to its uncertainty. In our experiments, we show that our approach significantly outperforms non-program-guided approaches on a set of challenging benchmarks, including a 2D Minecraft-inspired environment where the agent must complete a complex sequence of subtasks to achieve its goal, and achieves a similar performance as using handcrafted programs to guide the agent. Our results demonstrate that our approach can obtain the benefits of program-guided reinforcement learning without requiring the user to provide a new guiding program for every new task.",Program Synthesis Guided Reinforcement Learning for Partially Observed Environments,"Yichen David Yang, Jeevana Priya Inala, Osbert Bastani, Yewen Pu, Armando Solar-Lezama, Martin Rinard",2021,Artificial Intelligence,2102.11137
"Despite many proposed algorithms to provide robustness to deep learning (DL) models, DL models remain susceptible to adversarial attacks. We hypothesize that the adversarial vulnerability of DL models stems from two factors. The first factor is data sparsity which is that in the high dimensional input data space, there exist large regions outside the support of the data distribution. The second factor is the existence of many redundant parameters in the DL models. Owing to these factors, different models are able to come up with different decision boundaries with comparably high prediction accuracy. The appearance of the decision boundaries in the space outside the support of the data distribution does not affect the prediction accuracy of the model. However, it makes an important difference in the adversarial robustness of the model. We hypothesize that the ideal decision boundary is as far as possible from the support of the data distribution. In this paper, we develop a training framework to observe if DL models are able to learn such a decision boundary spanning the space around the class distributions further from the data points themselves. Semi-supervised learning was deployed during training by leveraging unlabeled data generated in the space outside the support of the data distribution. We measured adversarial robustness of the models trained using this training framework against well-known adversarial attacks and by using robustness metrics. We found that models trained using our framework, as well as other regularization methods and adversarial training support our hypothesis of data sparsity and that models trained with these methods learn to have decision boundaries more similar to the aforementioned ideal decision boundary. The code for our training framework is available at https://github.com/MahsaPaknezhad/AdversariallyRobustTraining.",Explaining Adversarial Vulnerability with a Data Sparsity Hypothesis,"Mahsa Paknezhad, Cuong Phuc Ngo, Amadeus Aristo Winarto, Alistair Cheong, Chuen Yang Beh, Jiayang Wu, Hwee Kuan Lee",2022,Artificial Intelligence,2103.00778
"Multiplayer Online Battle Area (MOBA) games are a recent huge success both in the video game industry and the international eSports scene. These games encourage team coordination and cooperation, short and long-term planning, within a real-time combined action and strategy gameplay. Artificial Intelligence and Computational Intelligence in Games research competitions offer a wide variety of challenges regarding the study and application of AI techniques to different game genres. These events are widely accepted by the AI/CI community as a sort of AI benchmarking that strongly influences many other research areas in the field. This paper presents and describes in detail the Dota 2 Bot competition and the Dota 2 AI framework that supports it. This challenge aims to join both, MOBAs and AI/CI game competitions, inviting participants to submit AI controllers for the successful MOBA \textit{Defense of the Ancients 2} (Dota 2) to play in 1v1 matches, which aims for fostering research on AI techniques for real-time games. The Dota 2 AI framework makes use of the actual Dota 2 game modding capabilities to enable to connect external AI controllers to actual Dota 2 game matches using the original Free-to-Play game.se of the actual Dota 2 game modding capabilities to enable to connect external AI controllers to actual Dota 2 game matches using the original Free-to-Play game.",The Dota 2 Bot Competition,Jose M. Font and Tobias Mahlmann,2018,Artificial Intelligence,2103.02943
"Deep deterministic policy gradient (DDPG)-based car-following strategy can break through the constraints of the differential equation model due to the ability of exploration on complex environments. However, the car-following performance of DDPG is usually degraded by unreasonable reward function design, insufficient training, and low sampling efficiency. In order to solve this kind of problem, a hybrid car-following strategy based on DDPG and cooperative adaptive cruise control (CACC) is proposed. First, the car-following process is modeled as the Markov decision process to calculate CACC and DDPG simultaneously at each frame. Given a current state, two actions are obtained from CACC and DDPG, respectively. Then, an optimal action, corresponding to the one offering a larger reward, is chosen as the output of the hybrid strategy. Meanwhile, a rule is designed to ensure that the change rate of acceleration is smaller than the desired value. Therefore, the proposed strategy not only guarantees the basic performance of car-following through CACC but also makes full use of the advantages of exploration on complex environments via DDPG. Finally, simulation results show that the car-following performance of the proposed strategy is improved compared with that of DDPG and CACC.",Hybrid Car-Following Strategy based on Deep Deterministic Policy Gradient and Cooperative Adaptive Cruise Control,"Ruidong Yan, Rui Jiang, Bin Jia, Jin Huang, and Diange Yang",2021,Artificial Intelligence,2103.03796
"Self-driving Autonomous Vehicles (SAVs) are gaining more interest each passing day by the industry as well as the general public. Tech and automobile companies are investing huge amounts of capital in research and development of SAVs to make sure they have a head start in the SAV market in the future. One of the major hurdles in the way of SAVs making it to the public roads is the lack of confidence of public in the safety aspect of SAVs. In order to assure safety and provide confidence to the public in the safety of SAVs, researchers around the world have used coverage-based testing for Verification and Validation (V&V) and safety assurance of SAVs. The objective of this paper is to investigate the coverage criteria proposed and coverage maximizing techniques used by researchers in the last decade up till now, to assure safety of SAVs. We conduct a Systematic Literature Review (SLR) for this investigation in our paper. We present a classification of existing research based on the coverage criteria used. Several research gaps and research directions are also provided in this SLR to enable further research in this domain. This paper provides a body of knowledge in the domain of safety assurance of SAVs. We believe the results of this SLR will be helpful in the progression of V&V and safety assurance of SAVs.",Coverage based testing for V&V and Safety Assurance of Self-driving Autonomous Vehicles: A Systematic Literature Review,"Zaid Tahir, Rob Alexander",2020,Artificial Intelligence,2103.04364
"Vehicle trajectory prediction tasks have been commonly tackled from two distinct perspectives: either with knowledge-driven methods or more recently with data-driven ones. On the one hand, we can explicitly implement domain-knowledge or physical priors such as anticipating that vehicles will follow the middle of the roads. While this perspective leads to feasible outputs, it has limited performance due to the difficulty to hand-craft complex interactions in urban environments. On the other hand, recent works use data-driven approaches which can learn complex interactions from the data leading to superior performance. However, generalization, \textit{i.e.}, having accurate predictions on unseen data, is an issue leading to unrealistic outputs. In this paper, we propose to learn a ""Realistic Residual Block"" (RRB), which effectively connects these two perspectives. Our RRB takes any off-the-shelf knowledge-driven model and finds the required residuals to add to the knowledge-aware trajectory. Our proposed method outputs realistic predictions by confining the residual range and taking into account its uncertainty. We also constrain our output with Model Predictive Control (MPC) to satisfy kinematic constraints. Using a publicly available dataset, we show that our method outperforms previous works in terms of accuracy and generalization to new scenes. We will release our code and data split here: https://github.com/vita-epfl/RRB.",Injecting Knowledge in Data-driven Vehicle Trajectory Predictors,"Mohammadhossein Bahari, Ismail Nejjar, Alexandre Alahi",2021,Artificial Intelligence,2103.04854
"Monte Carlo Tree Search (MCTS) is a powerful approach to designing game-playing bots or solving sequential decision problems. The method relies on intelligent tree search that balances exploration and exploitation. MCTS performs random sampling in the form of simulations and stores statistics of actions to make more educated choices in each subsequent iteration. The method has become a state-of-the-art technique for combinatorial games, however, in more complex games (e.g. those with high branching factor or real-time ones), as well as in various practical domains (e.g. transportation, scheduling or security) an efficient MCTS application often requires its problem-dependent modification or integration with other techniques. Such domain-specific modifications and hybrid approaches are the main focus of this survey. The last major MCTS survey has been published in 2012. Contributions that appeared since its release are of particular interest for this review.",Monte Carlo Tree Search: A Review of Recent Modifications and Applications,"Maciej \'Swiechowski, Konrad Godlewski, Bartosz Sawicki, Jacek Ma\'ndziuk",2022,Artificial Intelligence,2103.04931
"In this short paper, we outline nine classical benchmarks submitted to the first hierarchical planning track of the International Planning competition in 2020. All of these benchmarks are based on the HDDL language. The choice of the benchmarks was based on a questionnaire sent to the HTN community. They are the following: Barman, Childsnack, Rover, Satellite, Blocksworld, Depots, Gripper, and Hiking. In the rest of the paper we give a short description of these benchmarks. All are totally ordered.",From Classical to Hierarchical: benchmarks for the HTN Track of the International Planning Competition,"Damien Pellier, Humbert Fiorino",2020,Artificial Intelligence,2103.05481
"High capacity end-to-end approaches for human motion (behavior) prediction have the ability to represent subtle nuances in human behavior, but struggle with robustness to out of distribution inputs and tail events. Planning-based prediction, on the other hand, can reliably output decent-but-not-great predictions: it is much more stable in the face of distribution shift (as we verify in this work), but it has high inductive bias, missing important aspects that drive human decisions, and ignoring cognitive biases that make human behavior suboptimal. In this work, we analyze one family of approaches that strive to get the best of both worlds: use the end-to-end predictor on common cases, but do not rely on it for tail events / out-of-distribution inputs -- switch to the planning-based predictor there. We contribute an analysis of different approaches for detecting when to make this switch, using an autonomous driving domain. We find that promising approaches based on ensembling or generative modeling of the training distribution might not be reliable, but that there very simple methods which can perform surprisingly well -- including training a classifier to pick up on tell-tale issues in predicted trajectories.",On complementing end-to-end human behavior predictors with planning,"Liting Sun, Xiaogang Jia, Anca D. Dragan",2021,Artificial Intelligence,2103.05661
"This paper outlines a perspective on the future of AI, discussing directions for machines models of human-like intelligence. We explain how developmental and evolutionary theories of human cognition should further inform artificial intelligence. We emphasize the role of ecological niches in sculpting intelligent behavior, and in particular that human intelligence was fundamentally shaped to adapt to a constantly changing socio-cultural environment. We argue that a major limit of current work in AI is that it is missing this perspective, both theoretically and experimentally. Finally, we discuss the promising approach of developmental artificial intelligence, modeling infant development through multi-scale interaction between intrinsically motivated learning, embodiment and a fastly changing socio-cultural environment. This paper takes the form of an interview of Pierre-Yves Oudeyer by Mandred Eppe, organized within the context of a KI - K{\""{u}}nstliche Intelligenz special issue in developmental robotics.",Intelligent behavior depends on the ecological niche: Scaling up AI to human-like intelligence in socio-cultural environments,Manfred Eppe and Pierre-Yves Oudeyer,2021,Artificial Intelligence,2103.06769
"In this paper we establish a link between fuzzy and preferential semantics for description logics and Self-Organising Maps, which have been proposed as possible candidates to explain the psychological mechanisms underlying category generalisation. In particular, we show that the input/output behavior of a Self-Organising Map after training can be described by a fuzzy description logic interpretation as well as by a preferential interpretation, based on a concept-wise multipreference semantics, which takes into account preferences with respect to different concepts and has been recently proposed for ranked and for weighted defeasible description logics. Properties of the network can be proven by model checking on the fuzzy or on the preferential interpretation. Starting from the fuzzy interpretation, we also provide a probabilistic account for this neural network model.","A conditional, a fuzzy and a probabilistic interpretation of self-organising maps","Laura Giordano, Valentina Gliozzi, Daniele Theseider Dupr\'e",2022,Artificial Intelligence,2103.06854
"The reporting and the analysis of current events around the globe has expanded from professional, editor-lead journalism all the way to citizen journalism. Nowadays, politicians and other key players enjoy direct access to their audiences through social media, bypassing the filters of official cables or traditional media. However, the multiple advantages of free speech and direct communication are dimmed by the misuse of media to spread inaccurate or misleading claims. These phenomena have led to the modern incarnation of the fact-checker -- a professional whose main aim is to examine claims using available evidence and to assess their veracity. As in other text forensics tasks, the amount of information available makes the work of the fact-checker more difficult. With this in mind, starting from the perspective of the professional fact-checker, we survey the available intelligent technologies that can support the human expert in the different steps of her fact-checking endeavor. These include identifying claims worth fact-checking, detecting relevant previously fact-checked claims, retrieving relevant evidence to fact-check a claim, and actually verifying a claim. In each case, we pay attention to the challenges in future work and the potential impact on real-world fact-checking.",Automated Fact-Checking for Assisting Human Fact-Checkers,"Preslav Nakov, David Corney, Maram Hasanain, Firoj Alam, Tamer Elsayed, Alberto Barr\'on-Cede\~no, Paolo Papotti, Shaden Shaar, Giovanni Da San Martino",2021,Artificial Intelligence,2103.07769
"We consider the problem of reaching a propositional goal condition in fully-observable non-deterministic (FOND) planning under a general class of fairness assumptions that are given explicitly. The fairness assumptions are of the form A/B and say that state trajectories that contain infinite occurrences of an action a from A in a state s and finite occurrence of actions from B, must also contain infinite occurrences of action a in s followed by each one of its possible outcomes. The infinite trajectories that violate this condition are deemed as unfair, and the solutions are policies for which all the fair trajectories reach a goal state. We show that strong and strong-cyclic FOND planning, as well as QNP planning, a planning model introduced recently for generalized planning, are all special cases of FOND planning with fairness assumptions of this form which can also be combined. FOND+ planning, as this form of planning is called, combines the syntax of FOND planning with some of the versatility of LTL for expressing fairness constraints. A new planner is implemented by reducing FOND+ planning to answer set programs, and the performance of the planner is evaluated in comparison with FOND and QNP planners, and LTL synthesis tools.",Flexible FOND Planning with Explicit Fairness Assumptions,Ivan D. Rodriguez and Blai Bonet and Sebastian Sardina and Hector Geffner,2022,Artificial Intelligence,2103.08391
"Most conversational recommendation approaches are either not explainable, or they require external user's knowledge for explaining or their explanations cannot be applied in real time due to computational limitations. In this work, we present a real time category based conversational recommendation approach, which can provide concise explanations without prior user knowledge being required. We first perform an explainable user model in the form of preferences over the items' categories, and then use the category preferences to recommend items. The user model is performed by applying a BERT-based neural architecture on the conversation. Then, we translate the user model into item recommendation scores using a Feed Forward Network. User preferences during the conversation in our approach are represented by category vectors which are directly interpretable. The experimental results on the real conversational recommendation dataset ReDial demonstrate comparable performance to the state-of-the-art, while our approach is explainable. We also show the potential power of our framework by involving an oracle setting of category preference prediction.",Category Aware Explainable Conversational Recommendation,"Nikolaos Kondylidis, Jie Zou and Evangelos Kanoulas",2021,Artificial Intelligence,2103.08733
"With the development of measurement technology, data on the movements of actual games in various sports can be obtained and used for planning and evaluating the tactics and strategy. Defense in team sports is generally difficult to be evaluated because of the lack of statistical data. Conventional evaluation methods based on predictions of scores are considered unreliable because they predict rare events throughout the game. Besides, it is difficult to evaluate various plays leading up to a score. In this study, we propose a method to evaluate team defense from a comprehensive perspective related to team performance by predicting ball recovery and being attacked, which occur more frequently than goals, using player actions and positional data of all players and the ball. Using data from 45 soccer matches, we examined the relationship between the proposed index and team performance in actual matches and throughout a season. Results show that the proposed classifiers predicted the true events (mean F1 score $>$ 0.483) better than the existing classifiers which were based on rare events or goals (mean F1 score $<$ 0.201). Also, the proposed index had a moderate correlation with the long-term outcomes of the season ($r =$ 0.397). These results suggest that the proposed index might be a more reliable indicator rather than winning or losing with the inclusion of accidental factors.",Evaluation of soccer team defense based on prediction models of ball recovery and being attacked: A pilot study,"Kosuke Toda, Masakiyo Teranishi, Keisuke Kushiro, Keisuke Fujii",2022,Artificial Intelligence,2103.09627
"While artificial intelligence has been applied to control players' decisions in board games for over half a century, little attention is given to games with no player competition. Pandemic is an exemplar collaborative board game where all players coordinate to overcome challenges posed by events occurring during the game's progression. This paper proposes an artificial agent which controls all players' actions and balances chances of winning versus risk of losing in this highly stochastic environment. The agent applies a Rolling Horizon Evolutionary Algorithm on an abstraction of the game-state that lowers the branching factor and simulates the game's stochasticity. Results show that the proposed algorithm can find winning strategies more consistently in different games of varying difficulty. The impact of a number of state evaluation metrics is explored, balancing between optimistic strategies that favor winning and pessimistic strategies that guard against losing.",Collaborative Agent Gameplay in the Pandemic Board Game,Konstantinos Sfikas and Antonios Liapis,2020,Artificial Intelligence,2103.11388
"We discuss how over the last 30 to 50 years, Artificial Intelligence (AI) systems that focused only on data have been handicapped, and how knowledge has been critical in developing smarter, intelligent, and more effective systems. In fact, the vast progress in AI can be viewed in terms of the three waves of AI as identified by DARPA. During the first wave, handcrafted knowledge has been at the center-piece, while during the second wave, the data-driven approaches supplanted knowledge. Now we see a strong role and resurgence of knowledge fueling major breakthroughs in the third wave of AI underpinning future intelligent systems as they attempt human-like decision making, and seek to become trusted assistants and companions for humans. We find a wider availability of knowledge created from diverse sources, using manual to automated means both by repurposing as well as by extraction. Using knowledge with statistical learning is becoming increasingly indispensable to help make AI systems more transparent and auditable. We will draw a parallel with the role of knowledge and experience in human intelligence based on cognitive science, and discuss emerging neuro-symbolic or hybrid AI systems in which knowledge is the critical enabler for combining capabilities of the data-intensive statistical AI systems with those of symbolic AI systems, resulting in more capable AI systems that support more human-like intelligence.",The Duality of Data and Knowledge Across the Three Waves of AI,Amit Sheth and Krishnaprasad Thirunarayan,2021,Artificial Intelligence,2103.13520
"We propose a new classifier based on Dempster-Shafer (DS) theory and a convolutional neural network (CNN) architecture for set-valued classification. In this classifier, called the evidential deep-learning classifier, convolutional and pooling layers first extract high-dimensional features from input data. The features are then converted into mass functions and aggregated by Dempster's rule in a DS layer. Finally, an expected utility layer performs set-valued classification based on mass functions. We propose an end-to-end learning strategy for jointly updating the network parameters. Additionally, an approach for selecting partial multi-class acts is proposed. Experiments on image recognition, signal processing, and semantic-relationship classification tasks demonstrate that the proposed combination of deep CNN, DS layer, and expected utility layer makes it possible to improve classification accuracy and to make cautious decisions by assigning confusing patterns to multi-class sets.",An evidential classifier based on Dempster-Shafer theory and deep learning,"Zheng Tong, Philippe Xu, Thierry Den{\oe}ux",2021,Artificial Intelligence,2103.13549
"This article outlines what we learned from the first year of the AI Settlement Generation Competition in Minecraft, a competition about producing AI programs that can generate interesting settlements in Minecraft for an unseen map. This challenge seeks to focus research into adaptive and holistic procedural content generation. Generating Minecraft towns and villages given existing maps is a suitable task for this, as it requires the generated content to be adaptive, functional, evocative and aesthetic at the same time. Here, we present the results from the first iteration of the competition. We discuss the evaluation methodology, present the different technical approaches by the competitors, and outline the open problems.",The AI Settlement Generation Challenge in Minecraft: First Year Report,"Christoph Salge, Michael Cerny Green, Rodrigo Canaan, Filip Skwarski, Rafael Fritsch, Adrian Brightmoore, Shaofang Ye, Changxing Cao and Julian Togelius",2020,Artificial Intelligence,2103.14950
"In the family of Intelligent Transportation Systems (ITS), Multimodal Transport Systems (MMTS) have placed themselves as a mainstream transportation mean of our time as a feasible integrative transportation process. The Global Economy progressed with the help of transportation. The volume of goods and distances covered have doubled in the last ten years, so there is a high demand of an optimized transportation, fast but with low costs, saving resources but also safe, with low or zero emissions. Thus, it is important to have an overview of existing research in this field, to know what was already done and what is to be studied next. The main objective is to explore a beneficent selection of the existing research, methods and information in the field of multimodal transportation research, to identify industry needs and gaps in research and provide context for future research. The selective survey covers multimodal transport design and optimization in terms of: cost, time, and network topology. The multimodal transport theoretical aspects, context and resources are also covering various aspects. The survey's selection includes nowadays best methods and solvers for Intelligent Transportation Systems (ITS). The gap between theory and real-world applications should be further solved in order to optimize the global multimodal transportation system.",Selective Survey: Most Efficient Models and Solvers for Integrative Multimodal Transport,"Oliviu Matei, Erdei Rudolf, Camelia-M. Pintea",2021,Artificial Intelligence,2103.15555
"Along with the development of modern computing technology and social sciences, both theoretical research and practical applications of social computing have been continuously extended. In particular with the boom of artificial intelligence (AI), social computing is significantly influenced by AI. However, the conventional technologies of AI have drawbacks in dealing with more complicated and dynamic problems. Such deficiency can be rectified by hybrid human-artificial intelligence (H-AI) which integrates both human intelligence and AI into one unity, forming a new enhanced intelligence. H-AI in dealing with social problems shows the advantages that AI can not surpass. This paper firstly introduces the concept of H-AI. AI is the intelligence in the transition stage of H-AI, so the latest research progresses of AI in social computing are reviewed. Secondly, it summarizes typical challenges faced by AI in social computing, and makes it possible to introduce H-AI to solve these challenges. Finally, the paper proposes a holistic framework of social computing combining with H-AI, which consists of four layers: object layer, base layer, analysis layer, and application layer. It represents H-AI has significant advantages over AI in solving social problems.",A Survey of Hybrid Human-Artificial Intelligence for Social Computing,"Wenxi Wang, Huansheng Ning, Feifei Shi, Sahraoui Dhelim, Weishan Zhang, Liming Chen",2021,Artificial Intelligence,2103.15558
"The scope of data-driven fault diagnosis models is greatly extended through deep learning (DL). However, the classical convolution and recurrent structure have their defects in computational efficiency and feature representation, while the latest Transformer architecture based on attention mechanism has not yet been applied in this field. To solve these problems, we propose a novel time-frequency Transformer (TFT) model inspired by the massive success of vanilla Transformer in sequence processing. Specially, we design a fresh tokenizer and encoder module to extract effective abstractions from the time-frequency representation (TFR) of vibration signals. On this basis, a new end-to-end fault diagnosis framework based on time-frequency Transformer is presented in this paper. Through the case studies on bearing experimental datasets, we construct the optimal Transformer structure and verify its fault diagnosis performance. The superiority of the proposed method is demonstrated in comparison with the benchmark models and other state-of-the-art methods.",A novel time-frequency Transformer based on self-attention mechanism and its application in fault diagnosis of rolling bearings,"Yifei Ding, Minping Jia, Qiuhua Miao, Yudong Cao",2022,Artificial Intelligence,2104.09079
"Recently, the research on protecting the intellectual properties (IP) of deep neural networks (DNN) has attracted serious concerns. A number of DNN copyright protection methods have been proposed. However, most of the existing watermarking methods focus on verifying the copyright of the model, which do not support the authentication and management of users' fingerprints, thus can not satisfy the requirements of commercial copyright protection. In addition, the query modification attack which was proposed recently can invalidate most of the existing backdoor-based watermarking methods. To address these challenges, in this paper, we propose a method to protect the intellectual properties of DNN models by using an additional class and steganographic images. Specifically, we use a set of watermark key samples to embed an additional class into the DNN, so that the watermarked DNN will classify the watermark key sample as the predefined additional class in the copyright verification stage. We adopt the least significant bit (LSB) image steganography to embed users' fingerprints into watermark key images. Each user will be assigned with a unique fingerprint image so that the user's identity can be authenticated later. Experimental results demonstrate that, the proposed method can protect the copyright of DNN models effectively. On Fashion-MNIST and CIFAR-10 datasets, the proposed method can obtain 100% watermark accuracy and 100% fingerprint authentication success rate. In addition, the proposed method is demonstrated to be robust to the model fine-tuning attack, model pruning attack, and the query modification attack. Compared with three existing watermarking methods (the logo-based, noise-based, and adversarial frontier stitching watermarking methods), the proposed method has better performance on watermark accuracy and robustness against the query modification attack.",Protecting the Intellectual Properties of Deep Neural Networks with an Additional Class and Steganographic Images,"Shichang Sun, Mingfu Xue, Jian Wang, Weiqiang Liu",2022,Artificial Intelligence,2104.09203
"Recent machine-learning approaches to deterministic search and domain-independent planning employ policy learning to speed up search. Unfortunately, when attempting to solve a search problem by successively applying a policy, no guarantees can be given on solution quality. The problem of how to effectively use a learned policy within a bounded-suboptimal search algorithm remains largely as an open question. In this paper, we propose various ways in which such policies can be integrated into Focal Search, assuming that the policy is a neural network classifier. Furthermore, we provide mathematical foundations for some of the resulting algorithms. To evaluate the resulting algorithms over a number of policies with varying accuracy, we use synthetic policies which can be generated for a target accuracy for problems where the search space can be held in memory. We evaluate our focal search variants over three benchmark domains using our synthetic approach, and on the 15-puzzle using a neural network learned using 1.5 million examples. We observe that Discrepancy Focal Search, which we show expands the node which maximizes an approximation of the probability that its corresponding path is a prefix of an optimal path, obtains, in general, the best results in terms of runtime and solution quality.",Exploiting Learned Policies in Focal Search,"Pablo Araneda, Matias Greco, Jorge A. Baier",2021,Artificial Intelligence,2104.10535
"As of 2020, the international workshop on Procedural Content Generation enters its second decade. The annual workshop, hosted by the international conference on the Foundations of Digital Games, has collected a corpus of 95 papers published in its first 10 years. This paper provides an overview of the workshop's activities and surveys the prevalent research topics emerging over the years.",10 Years of the PCG workshop: Past and Future Trends,Antonios Liapis,2020,Artificial Intelligence,2104.11037
"Objects are a centerpiece of the mathematical realm and our interaction with and reasoning about it, just as they are of the physical one (if not more). And humans' mathematical reasoning must ultimately be grounded in our general intelligence. Yet in contemporary cognitive science and A.I., the physical and mathematical domains are customarily explored separately, which allows for baking in assumptions for what objects are for the system - and missing potential connections. In this paper, I put the issue into its philosophical and cognitive context. I then describe an abstract theoretical framework for learning object representations, that makes room for mathematical objects on par with non-mathematical ones. Finally, I describe a case study that builds on that view to show how our general ability for integrating different aspects of objects effects our conception of the natural numbers.",The Role of General Intelligence in Mathematical Reasoning,Aviv Keren,2021,Artificial Intelligence,2104.13468
"Recently, it has been proposed that fruitful synergies may exist between Deep Learning (DL) and Case Based Reasoning (CBR); that there are insights to be gained by applying CBR ideas to problems in DL (what could be called DeepCBR). In this paper, we report on a program of research that applies CBR solutions to the problem of Explainable AI (XAI) in the DL. We describe a series of twin-systems pairings of opaque DL models with transparent CBR models that allow the latter to explain the former using factual, counterfactual and semi-factual explanation strategies. This twinning shows that functional abstractions of DL (e.g., feature weights, feature importance and decision boundaries) can be used to drive these explanatory solutions. We also raise the prospect that this research also applies to the problem of Data Augmentation in DL, underscoring the fecundity of these DeepCBR ideas.",Twin Systems for DeepCBR: A Menagerie of Deep Learning and Case-Based Reasoning Pairings for Explanation and Data Augmentation,Mark T Keane and Eoin M Kenny and Mohammed Temraz and Derek Greene and Barry Smyth,2021,Artificial Intelligence,2104.14461
"AI has the potential to revolutionize many areas of healthcare. Radiology, dermatology, and ophthalmology are some of the areas most likely to be impacted in the near future, and they have received significant attention from the broader research community. But AI techniques are now also starting to be used in in vitro fertilization (IVF), in particular for selecting which embryos to transfer to the woman. The contribution of AI to IVF is potentially significant, but must be done carefully and transparently, as the ethical issues are significant, in part because this field involves creating new people. We first give a brief introduction to IVF and review the use of AI for embryo selection. We discuss concerns with the interpretation of the reported results from scientific and practical perspectives. We then consider the broader ethical issues involved. We discuss in detail the problems that result from the use of black-box methods in this context and advocate strongly for the use of interpretable models. Importantly, there have been no published trials of clinical effectiveness, a problem in both the AI and IVF communities, and we therefore argue that clinical implementation at this point would be premature. Finally, we discuss ways for the broader AI community to become involved to ensure scientifically sound and ethically responsible development of AI in IVF.",Ethical Implementation of Artificial Intelligence to Select Embryos in In Vitro Fertilization,"Michael Anis Mihdi Afnan, Cynthia Rudin, Vincent Conitzer, Julian Savulescu, Abhishek Mishra, Yanhe Liu, Masoud Afnan",2021,Artificial Intelligence,2105.00060
"By successfully solving the problem of forecasting, the processes in the work of various companies are optimized and savings are achieved. In this process, the analysis of time series data is of particular importance. Since the creation of Facebook's Prophet, and Amazon's DeepAR+ and CNN-QR forecasting models, algorithms have attracted a great deal of attention. The paper presents the application and comparison of the above algorithms for sales forecasting in distribution companies. A detailed comparison of the performance of algorithms over real data with different lengths of sales history was made. The results show that Prophet gives better results for items with a longer history and frequent sales, while Amazon's algorithms show superiority for items without a long history and items that are rarely sold.","Comparison Analysis of Facebook's Prophet, Amazon's DeepAR+ and CNN-QR Algorithms for Successful Real-World Sales Forecasting","Emir Zunic, Kemal Korjenic, Sead Delalic, Zlatko Subara",2021,Artificial Intelligence,2105.00694
"We present Arianna+, a framework to design networks of ontologies for representing knowledge enabling smart homes to perform human activity recognition online. In the network, nodes are ontologies allowing for various data contextualisation, while edges are general-purpose computational procedures elaborating data. Arianna+ provides a flexible interface between the inputs and outputs of procedures and statements, which are atomic representations of ontological knowledge. Arianna+ schedules procedures on the basis of events by employing logic-based reasoning, i.e., by checking the classification of certain statements in the ontologies. Each procedure involves input and output statements that are differently contextualised in the ontologies based on specific prior knowledge. Arianna+ allows to design networks that encode data within multiple contexts and, as a reference scenario, we present a modular network based on a spatial context shared among all activities and a temporal context specialised for each activity to be recognised. In the paper, we argue that a network of small ontologies is more intelligible and has a reduced computational load than a single ontology encoding the same knowledge. Arianna+ integrates in the same architecture heterogeneous data processing techniques, which may be better suited to different contexts. Thus, we do not propose a new algorithmic approach to activity recognition, instead, we focus on the architectural aspects for accommodating logic-based and data-driven activity models in a context-oriented way. Also, we discuss how to leverage data contextualisation and reasoning for activity recognition, and to support an iterative development process driven by domain experts.",Human Activity Recognition Models in Ontology Networks,"Luca Buoncompagni, Syed Yusha Kareem and Fulvio Mastrogiovanni",2021,Artificial Intelligence,2105.02264
"Learning disentangled representations of textual data is essential for many natural language tasks such as fair classification, style transfer and sentence generation, among others. The existent dominant approaches in the context of text data {either rely} on training an adversary (discriminator) that aims at making attribute values difficult to be inferred from the latent code {or rely on minimising variational bounds of the mutual information between latent code and the value attribute}. {However, the available methods suffer of the impossibility to provide a fine-grained control of the degree (or force) of disentanglement.} {In contrast to} {adversarial methods}, which are remarkably simple, although the adversary seems to be performing perfectly well during the training phase, after it is completed a fair amount of information about the undesired attribute still remains. This paper introduces a novel variational upper bound to the mutual information between an attribute and the latent code of an encoder. Our bound aims at controlling the approximation error via the Renyi's divergence, leading to both better disentangled representations and in particular, a precise control of the desirable degree of disentanglement {than state-of-the-art methods proposed for textual data}. Furthermore, it does not suffer from the degeneracy of other losses in multi-class scenarios. We show the superiority of this method on fair classification and on textual style transfer tasks. Additionally, we provide new insights illustrating various trade-offs in style transfer when attempting to learn disentangled representations and quality of the generated sentence.",A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations,Pierre Colombo and Chloe Clavel and Pablo Piantanida,2021,Artificial Intelligence,2105.02685
"The manpower scheduling problem is a critical research field in the resource management area. Based on the existing studies on scheduling problem solutions, this paper transforms the manpower scheduling problem into a combinational optimization problem under multi-constraint conditions from a new perspective. It also uses logical paradigms to build a mathematical model for problem solution and an improved multi-dimensional evolution algorithm for solving the model. Moreover, the constraints discussed in this paper basically cover all the requirements of human resource coordination in modern society and are supported by our experiment results. In the discussion part, we compare our model with other heuristic algorithms or linear programming methods and prove that the model proposed in this paper makes a 25.7% increase in efficiency and a 17% increase in accuracy at most. In addition, to the numerical solution of the manpower scheduling problem, this paper also studies the algorithm for scheduling task list generation and the method of displaying scheduling results. As a result, we not only provide various modifications for the basic algorithm to solve different condition problems but also propose a new algorithm that increases at least 28.91% in time efficiency by comparing with different baseline models.",An Intelligent Model for Solving Manpower Scheduling Problems,Lingyu Zhang and Tianyu Liu and Yunhai Wang,2021,Artificial Intelligence,2105.03540
"We build on abduction-based explanations for ma-chine learning and develop a method for computing local explanations for neural network models in natural language processing (NLP). Our explanations comprise a subset of the words of the in-put text that satisfies two key features: optimality w.r.t. a user-defined cost function, such as the length of explanation, and robustness, in that they ensure prediction invariance for any bounded perturbation in the embedding space of the left out words. We present two solution algorithms, respectively based on implicit hitting sets and maximum universal subsets, introducing a number of algorithmic improvements to speed up convergence of hard instances. We show how our method can be con-figured with different perturbation sets in the em-bedded space and used to detect bias in predictions by enforcing include/exclude constraints on biased terms, as well as to enhance existing heuristic-based NLP explanation frameworks such as Anchors. We evaluate our framework on three widely used sentiment analysis tasks and texts of up to100words from SST, Twitter and IMDB datasets,demonstrating the effectiveness of the derived explanations.",On Guaranteed Optimal Robust Explanations for NLP Models,"Emanuele La Malfa, Agnieszka Zbrzezny, Rhiannon Michelmore, Nicola Paoletti and Marta Kwiatkowska",2021,Artificial Intelligence,2105.03640
"Privacy protection has recently been in the spotlight of attention to both academia and industry. Society protects individual data privacy through complex legal frameworks. The increasing number of applications of data science and artificial intelligence has resulted in a higher demand for the ubiquitous application of the data. The privacy protection of the broad Data-Information-Knowledge-Wisdom (DIKW) landscape, the next generation of information organization, has taken a secondary role. In this paper, we will explore DIKW architecture through the applications of the popular swarm intelligence and differential privacy. As differential privacy proved to be an effective data privacy approach, we will look at it from a DIKW domain perspective. Swarm Intelligence can effectively optimize and reduce the number of items in DIKW used in differential privacy, thus accelerating both the effectiveness and the efficiency of differential privacy for crossing multiple modals of conceptual DIKW. The proposed approach is demonstrated through the application of personalized data that is based on the open-sourse IRIS dataset. This experiment demonstrates the efficiency of Swarm Intelligence in reducing computing complexity.",Swarm Differential Privacy for Purpose Driven Data-Information-Knowledge-Wisdom Architecture,"Yingbo Li, Yucong Duan, Zakaria Maama, Haoyang Che, Anamaria-Beatrice Spulber, Stelios Fuentes",2021,Artificial Intelligence,2105.04045
"The recent offline reinforcement learning (RL) studies have achieved much progress to make RL usable in real-world systems by learning policies from pre-collected datasets without environment interaction. Unfortunately, existing offline RL methods still face many practical challenges in real-world system control tasks, such as computational restriction during agent training and the requirement of extra control flexibility. The model-based planning framework provides an attractive alternative. However, most model-based planning algorithms are not designed for offline settings. Simply combining the ingredients of offline RL with existing methods either provides over-restrictive planning or leads to inferior performance. We propose a new light-weighted model-based offline planning framework, namely MOPP, which tackles the dilemma between the restrictions of offline learning and high-performance planning. MOPP encourages more aggressive trajectory rollout guided by the behavior policy learned from data, and prunes out problematic trajectories to avoid potential out-of-distribution samples. Experimental results show that MOPP provides competitive performance compared with existing model-based offline planning and RL approaches.",Model-Based Offline Planning with Trajectory Pruning,"Xianyuan Zhan, Xiangyu Zhu, Haoran Xu",2022,Artificial Intelligence,2105.07351
"Order effects occur when judgments about a hypothesis's probability given a sequence of information do not equal the probability of the same hypothesis when the information is reversed. Different experiments have been performed in the literature that supports evidence of order effects. We proposed a Bayesian update model for order effects where each question can be thought of as a mini-experiment where the respondents reflect on their beliefs. We showed that order effects appear, and they have a simple cognitive explanation: the respondent's prior belief that two questions are correlated. The proposed Bayesian model allows us to make several predictions: (1) we found certain conditions on the priors that limit the existence of order effects; (2) we show that, for our model, the QQ equality is not necessarily satisfied (due to symmetry assumptions); and (3) the proposed Bayesian model has the advantage of possessing fewer parameters than its quantum counterpart.",Order Effects in Bayesian Updates,Catarina Moreira and Jose Acacio de Barros,2021,Artificial Intelligence,2105.07354
"Artificial Intelligence techniques powered by deep neural nets have achieved much success in several application domains, most significantly and notably in the Computer Vision applications and Natural Language Processing tasks. Surpassing human-level performance propelled the research in the applications where different modalities amongst language, vision, sensory, text play an important role in accurate predictions and identification. Several multimodal fusion methods employing deep learning models are proposed in the literature. Despite their outstanding performance, the complex, opaque and black-box nature of the deep neural nets limits their social acceptance and usability. This has given rise to the quest for model interpretability and explainability, more so in the complex tasks involving multimodal AI methods. This paper extensively reviews the present literature to present a comprehensive survey and commentary on the explainability in multimodal deep neural nets, especially for the vision and language tasks. Several topics on multimodal AI and its applications for generic domains have been covered in this paper, including the significance, datasets, fundamental building blocks of the methods and techniques, challenges, applications, and future trends in this domain",A Review on Explainability in Multimodal Deep Neural Nets,"Gargi Joshi, Rahee Walambe, Ketan Kotecha",2021,Artificial Intelligence,2105.07878
"In the last few years, AI continues demonstrating its positive impact on society while sometimes with ethically questionable consequences. Building and maintaining public trust in AI has been identified as the key to successful and sustainable innovation. This chapter discusses the challenges related to operationalizing ethical AI principles and presents an integrated view that covers high-level ethical AI principles, the general notion of trust/trustworthiness, and product/process support in the context of responsible AI, which helps improve both trust and trustworthiness of AI for a wider set of stakeholders.",AI and Ethics -- Operationalising Responsible AI,"Liming Zhu, Xiwei Xu, Qinghua Lu, Guido Governatori, Jon Whittle",2021,Artificial Intelligence,2105.08867
"Traffic simulators act as an essential component in the operating and planning of transportation systems. Conventional traffic simulators usually employ a calibrated physical car-following model to describe vehicles' behaviors and their interactions with traffic environment. However, there is no universal physical model that can accurately predict the pattern of vehicle's behaviors in different situations. A fixed physical model tends to be less effective in a complicated environment given the non-stationary nature of traffic dynamics. In this paper, we formulate traffic simulation as an inverse reinforcement learning problem, and propose a parameter sharing adversarial inverse reinforcement learning model for dynamics-robust simulation learning. Our proposed model is able to imitate a vehicle's trajectories in the real world while simultaneously recovering the reward function that reveals the vehicle's true objective which is invariant to different dynamics. Extensive experiments on synthetic and real-world datasets show the superior performance of our approach compared to state-of-the-art methods and its robustness to variant dynamics of traffic.",Objective-aware Traffic Simulation via Inverse Reinforcement Learning,"Guanjie Zheng, Hanyang Liu, Kai Xu, Zhenhui Li",2021,Artificial Intelligence,2105.09560
"A key challenge on the path to developing agents that learn complex human-like behavior is the need to quickly and accurately quantify human-likeness. While human assessments of such behavior can be highly accurate, speed and scalability are limited. We address these limitations through a novel automated Navigation Turing Test (ANTT) that learns to predict human judgments of human-likeness. We demonstrate the effectiveness of our automated NTT on a navigation task in a complex 3D environment. We investigate six classification models to shed light on the types of architectures best suited to this task, and validate them against data collected through a human NTT. Our best models achieve high accuracy when distinguishing true human and agent behavior. At the same time, we show that predicting finer-grained human assessment of agents' progress towards human-like behavior remains unsolved. Our work takes an important step towards agents that more effectively learn complex human-like behavior.",Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation,"Sam Devlin, Raluca Georgescu, Ida Momennejad, Jaroslaw Rzepecki, Evelyn Zuniga, Gavin Costello, Guy Leroy, Ali Shaw and Katja Hofmann",2021,Artificial Intelligence,2105.09637
"Drafting, i.e., the selection of a subset of items from a larger candidate set, is a key element of many games and related problems. It encompasses team formation in sports or e-sports, as well as deck selection in many modern card games. The key difficulty of drafting is that it is typically not sufficient to simply evaluate each item in a vacuum and to select the best items. The evaluation of an item depends on the context of the set of items that were already selected earlier, as the value of a set is not just the sum of the values of its members - it must include a notion of how well items go together. In this paper, we study drafting in the context of the card game Magic: The Gathering. We propose the use of a contextual preference network, which learns to compare two possible extensions of a given deck of cards. We demonstrate that the resulting network is better able to evaluate card decks in this game than previous attempts.",Predicting Human Card Selection in Magic: The Gathering with Contextual Preference Ranking,"Timo Bertram, Johannes F\""urnkranz, Martin M\""uller",2021,Artificial Intelligence,2105.11864
"This paper discusses a new variant of the Henry Gas Solubility Optimization (HGSO) Algorithm, called Hybrid HGSO (HHGSO). Unlike its predecessor, HHGSO allows multiple clusters serving different individual meta-heuristic algorithms (i.e., with its own defined parameters and local best) to coexist within the same population. Exploiting the dynamic cluster-to-algorithm mapping via penalized and reward model with adaptive switching factor, HHGSO offers a novel approach for meta-heuristic hybridization consisting of Jaya Algorithm, Sooty Tern Optimization Algorithm, Butterfly Optimization Algorithm, and Owl Search Algorithm, respectively. The acquired results from the selected two case studies (i.e., involving team formation problem and combinatorial test suite generation) indicate that the hybridization has notably improved the performance of HGSO and gives superior performance against other competing meta-heuristic and hyper-heuristic algorithms.",Hybrid Henry Gas Solubility Optimization Algorithm with Dynamic Cluster-to-Algorithm Mapping for Search-based Software Engineering Problems,"Kamal Z. Zamli, Md. Abdul Kader, Saiful Azad, Bestoun S. Ahmed",2021,Artificial Intelligence,2105.14923
"Motivated by the abundance of uncertain event data from multiple sources including physical devices and sensors, this paper presents the task of relating a stochastic process observation to a process model that can be rendered from a dataset. In contrast to previous research that suggested to transform a stochastically known event log into a less informative uncertain log with upper and lower bounds on activity frequencies, we consider the challenge of accommodating the probabilistic knowledge into conformance checking techniques. Based on a taxonomy that captures the spectrum of conformance checking cases under stochastic process observations, we present three types of challenging cases. The first includes conformance checking of a stochastically known log with respect to a given process model. The second case extends the first to classify a stochastically known log into one of several process models. The third case extends the two previous ones into settings in which process models are only stochastically known. The suggested problem captures the increasingly growing number of applications in which sensors provide probabilistic process information.",Uncertain Process Data with Probabilistic Knowledge: Problem Characterization and Challenges,Izack Cohen and Avigdor Gal,2021,Artificial Intelligence,2106.03324
"Biological agents have meaningful interactions with their environment despite the absence of immediate reward signals. In such instances, the agent can learn preferred modes of behaviour that lead to predictable states -- necessary for survival. In this paper, we pursue the notion that this learnt behaviour can be a consequence of reward-free preference learning that ensures an appropriate trade-off between exploration and preference satisfaction. For this, we introduce a model-based Bayesian agent equipped with a preference learning mechanism (pepper) using conjugate priors. These conjugate priors are used to augment the expected free energy planner for learning preferences over states (or outcomes) across time. Importantly, our approach enables the agent to learn preferences that encourage adaptive behaviour at test time. We illustrate this in the OpenAI Gym FrozenLake and the 3D mini-world environments -- with and without volatility. Given a constant environment, these agents learn confident (i.e., precise) preferences and act to satisfy them. Conversely, in a volatile setting, perpetual preference uncertainty maintains exploratory behaviour. Our experiments suggest that learnable (reward-free) preferences entail a trade-off between exploration and preference satisfaction. Pepper offers a straightforward framework suitable for designing adaptive agents when reward functions cannot be predefined as in real environments.",Exploration and preference satisfaction trade-off in reward-free learning,"Noor Sajid, Panagiotis Tigas, Alexey Zakharov, Zafeirios Fountas and Karl Friston",2021,Artificial Intelligence,2106.04316
"We consider a problem wherein jobs arrive at random times and assume random values. Upon each job arrival, the decision-maker must decide immediately whether or not to accept the job and gain the value on offer as a reward, with the constraint that they may only accept at most $n$ jobs over some reference time period. The decision-maker only has access to $M$ independent realisations of the job arrival process. We propose an algorithm, Non-Parametric Sequential Allocation (NPSA), for solving this problem. Moreover, we prove that the expected reward returned by the NPSA algorithm converges in probability to optimality as $M$ grows large. We demonstrate the effectiveness of the algorithm empirically on synthetic data and on public fraud-detection datasets, from where the motivation for this work is derived.",Non-Parametric Stochastic Sequential Assignment With Random Arrival Times,"Danial Dervovic, Parisa Hassanzadeh, Samuel Assefa, Prashant Reddy",2021,Artificial Intelligence,2106.04944
"Planning is hard. The use of subgoals can make planning more tractable, but selecting these subgoals is computationally costly. What algorithms might enable us to reap the benefits of planning using subgoals while minimizing the computational overhead of selecting them? We propose visual scoping, a strategy that interleaves planning and acting by alternately defining a spatial region as the next subgoal and selecting actions to achieve it. We evaluated our visual scoping algorithm on a variety of physical assembly problems against two baselines: planning all subgoals in advance and planning without subgoals. We found that visual scoping achieves comparable task performance to the subgoal planner while requiring only a fraction of the total computational cost. Together, these results contribute to our understanding of how humans might make efficient use of cognitive resources to solve complex planning problems.",Visual scoping operations for physical assembly,"Felix J Binder, Marcelo M Mattar, David Kirsh, Judith E Fan",2021,Artificial Intelligence,2106.05654
"Explainable artificial intelligence has rapidly emerged since lawmakers have started requiring interpretable models for safety-critical domains. Concept-based neural networks have arisen as explainable-by-design methods as they leverage human-understandable symbols (i.e. concepts) to predict class memberships. However, most of these approaches focus on the identification of the most relevant concepts but do not provide concise, formal explanations of how such concepts are leveraged by the classifier to make predictions. In this paper, we propose a novel end-to-end differentiable approach enabling the extraction of logic explanations from neural networks using the formalism of First-Order Logic. The method relies on an entropy-based criterion which automatically identifies the most relevant concepts. We consider four different case studies to demonstrate that: (i) this entropy-based criterion enables the distillation of concise logic explanations in safety-critical domains from clinical data to computer vision; (ii) the proposed approach outperforms state-of-the-art white-box models in terms of classification accuracy and matches black box performances.",Entropy-based Logic Explanations of Neural Networks,"Pietro Barbiero, Gabriele Ciravegna, Francesco Giannini, Pietro Li\'o, Marco Gori, Stefano Melacci",2022,Artificial Intelligence,2106.06804
"Language is an interface to the outside world. In order for embodied agents to use it, language must be grounded in other, sensorimotor modalities. While there is an extended literature studying how machines can learn grounded language, the topic of how to learn spatio-temporal linguistic concepts is still largely uncharted. To make progress in this direction, we here introduce a novel spatio-temporal language grounding task where the goal is to learn the meaning of spatio-temporal descriptions of behavioral traces of an embodied agent. This is achieved by training a truth function that predicts if a description matches a given history of observations. The descriptions involve time-extended predicates in past and present tense as well as spatio-temporal references to objects in the scene. To study the role of architectural biases in this task, we train several models including multimodal Transformer architectures; the latter implement different attention computations between words and objects across space and time. We test models on two classes of generalization: 1) generalization to randomly held-out sentences; 2) generalization to grammar primitives. We observe that maintaining object identity in the attention computation of our Transformers is instrumental to achieving good performance on generalization overall, and that summarizing object traces in a single token has little influence on performance. We then discuss how this opens new perspectives for language-guided autonomous embodied agents. We also release our code under open-source license as well as pretrained models and datasets to encourage the wider community to build upon and extend our work in the future.",Grounding Spatio-Temporal Language with Transformers,"Tristan Karch, Laetitia Teodorescu, Katja Hofmann, Cl\'ement Moulin-Frier and Pierre-Yves Oudeyer",2021,Artificial Intelligence,2106.08858
"One of the main challenges in model-based reinforcement learning (RL) is to decide which aspects of the environment should be modeled. The value-equivalence (VE) principle proposes a simple answer to this question: a model should capture the aspects of the environment that are relevant for value-based planning. Technically, VE distinguishes models based on a set of policies and a set of functions: a model is said to be VE to the environment if the Bellman operators it induces for the policies yield the correct result when applied to the functions. As the number of policies and functions increase, the set of VE models shrinks, eventually collapsing to a single point corresponding to a perfect model. A fundamental question underlying the VE principle is thus how to select the smallest sets of policies and functions that are sufficient for planning. In this paper we take an important step towards answering this question. We start by generalizing the concept of VE to order-$k$ counterparts defined with respect to $k$ applications of the Bellman operator. This leads to a family of VE classes that increase in size as $k \rightarrow \infty$. In the limit, all functions become value functions, and we have a special instantiation of VE which we call proper VE or simply PVE. Unlike VE, the PVE class may contain multiple models even in the limit when all value functions are used. Crucially, all these models are sufficient for planning, meaning that they will yield an optimal policy despite the fact that they may ignore many aspects of the environment. We construct a loss function for learning PVE models and argue that popular algorithms such as MuZero can be understood as minimizing an upper bound for this loss. We leverage this connection to propose a modification to MuZero and show that it can lead to improved performance in practice.",Proper Value Equivalence,"Christopher Grimm, Andr\'e Barreto, Gregory Farquhar, David Silver, Satinder Singh",2021,Artificial Intelligence,2106.10316
"Path planning, the problem of efficiently discovering high-reward trajectories, often requires optimizing a high-dimensional and multimodal reward function. Popular approaches like CEM and CMA-ES greedily focus on promising regions of the search space and may get trapped in local maxima. DOO and VOOT balance exploration and exploitation, but use space partitioning strategies independent of the reward function to be optimized. Recently, LaMCTS empirically learns to partition the search space in a reward-sensitive manner for black-box optimization. In this paper, we develop a novel formal regret analysis for when and why such an adaptive region partitioning scheme works. We also propose a new path planning method LaP3 which improves the function value estimation within each sub-region, and uses a latent representation of the search space. Empirically, LaP3 outperforms existing path planning methods in 2D navigation tasks, especially in the presence of difficult-to-escape local optima, and shows benefits when plugged into the planning components of model-based RL such as PETS. These gains transfer to highly multimodal real-world tasks, where we outperform strong baselines in compiler phase ordering by up to 39% on average across 9 tasks, and in molecular design by up to 0.4 on properties on a 0-1 scale. Code is available at https://github.com/yangkevin2/neurips2021-lap3.",Learning Space Partitions for Path Planning,"Kevin Yang, Tianjun Zhang, Chris Cummins, Brandon Cui, Benoit Steiner, Linnan Wang, Joseph E. Gonzalez, Dan Klein, Yuandong Tian",2021,Artificial Intelligence,2106.10544
"In Silico Clinical Trials (ISTC), i.e., clinical experimental campaigns carried out by means of computer simulations, hold the promise to decrease time and cost for the safety and efficacy assessment of pharmacological treatments, reduce the need for animal and human testing, and enable precision medicine. In this paper we present methods and an algorithm that, by means of extensive computer simulation--based experimental campaigns (ISTC) guided by intelligent search, optimise a pharmacological treatment for an individual patient (precision medicine). e show the effectiveness of our approach on a case study involving a real pharmacological treatment, namely the downregulation phase of a complex clinical protocol for assisted reproduction in humans.",Optimal personalised treatment computation through in silico clinical trials on patient digital twins,"Stefano Sinisi, Vadim Alimguzhin, Toni Mancini, Enrico Tronci, Federico Mari, Brigitte Leeners",2020,Artificial Intelligence,2106.10684
"In critical infrastructures like airports, much care has to be devoted in protecting radio communication networks from external electromagnetic interference. Protection of such mission-critical radio communication networks is usually tackled by exploiting radiogoniometers: at least three suitably deployed radiogoniometers, and a gateway gathering information from them, permit to monitor and localise sources of electromagnetic emissions that are not supposed to be present in the monitored area. Typically, radiogoniometers are connected to the gateway through relay nodes. As a result, some degree of fault-tolerance for the network of relay nodes is essential in order to offer a reliable monitoring. On the other hand, deployment of relay nodes is typically quite expensive. As a result, we have two conflicting requirements: minimise costs while guaranteeing a given fault-tolerance. In this paper, we address the problem of computing a deployment for relay nodes that minimises the relay node network cost while at the same time guaranteeing proper working of the network even when some of the relay nodes (up to a given maximum number) become faulty (fault-tolerance). We show that, by means of a computation-intensive pre-processing on a HPC infrastructure, the above optimisation problem can be encoded as a 0/1 Linear Program, becoming suitable to be approached with standard Artificial Intelligence reasoners like MILP, PB-SAT, and SMT/OMT solvers. Our problem formulation enables us to present experimental results comparing the performance of these three solving technologies on a real case study of a relay node network deployment in areas of the Leonardo da Vinci Airport in Rome, Italy.","MILP, pseudo-boolean, and OMT solvers for optimal fault-tolerant placements of relay nodes in mission critical wireless networks","Quian Matteo Chen, Alberto Finzi, Toni Mancini, Igor Melatti, Enrico Tronci",2020,Artificial Intelligence,2106.10685
"Wikidata has been increasingly adopted by many communities for a wide variety of applications, which demand high-quality knowledge to deliver successful results. In this paper, we develop a framework to detect and analyze low-quality statements in Wikidata by shedding light on the current practices exercised by the community. We explore three indicators of data quality in Wikidata, based on: 1) community consensus on the currently recorded knowledge, assuming that statements that have been removed and not added back are implicitly agreed to be of low quality; 2) statements that have been deprecated; and 3) constraint violations in the data. We combine these indicators to detect low-quality statements, revealing challenges with duplicate entities, missing triples, violated type rules, and taxonomic distinctions. Our findings complement ongoing efforts by the Wikidata community to improve data quality, aiming to make it easier for users and editors to find and correct mistakes.",A Study of the Quality of Wikidata,Kartik Shenoy and Filip Ilievski and Daniel Garijo and Daniel Schwabe and Pedro Szekely,2021,Artificial Intelligence,2107.00156
"This paper applies t-SNE, a visualisation technique familiar from Deep Neural Network research to argumentation graphs by applying it to the output of graph embeddings generated using several different methods. It shows that such a visualisation approach can work for argumentation and show interesting structural properties of argumentation graphs, opening up paths for further research in the area.",Visualising Argumentation Graphs with Graph Embeddings and t-SNE,"Lars Malmqvist, Tommy Yuan, Suresh Manandhar",2020,Artificial Intelligence,2107.00528
"In this paper, we study the problem of evaluating the addition of elements to a set. This problem is difficult, because it can, in the general case, not be reduced to unconditional preferences between the choices. Therefore, we model preferences based on the context of the decision. We discuss and compare two different Siamese network architectures for this task: a twin network that compares the two sets resulting after the addition, and a triplet network that models the contribution of each candidate to the existing set. We evaluate the two settings on a real-world task; learning human card preferences for deck building in the collectible card game Magic: The Gathering. We show that the triplet approach achieves a better result than the twin network and that both outperform previous results on this task.",A Comparison of Contextual and Non-Contextual Preference Ranking for Set Addition Problems,"Timo Bertram, Johannes F\""urnkranz, Martin M\""uller",2021,Artificial Intelligence,2107.04438
"A total of 34% of AI research and development projects fails or are abandoned, according to a recent survey by Rackspace Technology of 1,870 companies. We propose a new strategic framework, aiSTROM, that empowers managers to create a successful AI strategy based on a thorough literature review. This provides a unique and integrated approach that guides managers and lead developers through the various challenges in the implementation process. In the aiSTROM framework, we start by identifying the top n potential projects (typically 3-5). For each of those, seven areas of focus are thoroughly analysed. These areas include creating a data strategy that takes into account unique cross-departmental machine learning data requirements, security, and legal requirements. aiSTROM then guides managers to think about how to put together an interdisciplinary artificial intelligence (AI) implementation team given the scarcity of AI talent. Once an AI team strategy has been established, it needs to be positioned within the organization, either cross-departmental or as a separate division. Other considerations include AI as a service (AIaas), or outsourcing development. Looking at new technologies, we have to consider challenges such as bias, legality of black-box-models, and keeping humans in the loop. Next, like any project, we need value-based key performance indicators (KPIs) to track and validate the progress. Depending on the company's risk-strategy, a SWOT analysis (strengths, weaknesses, opportunities, and threats) can help further classify the shortlisted projects. Finally, we should make sure that our strategy includes continuous education of employees to enable a culture of adoption. This unique and comprehensive framework offers a valuable, literature supported, tool for managers and lead developers.",aiSTROM -- A roadmap for developing a successful AI strategy,Dorien Herremans,2021,Artificial Intelligence,2107.06071
"Federated learning (FL) is a distributed model for deep learning that integrates client-server architecture, edge computing, and real-time intelligence. FL has the capability of revolutionizing machine learning (ML) but lacks in the practicality of implementation due to technological limitations, communication overhead, non-IID (independent and identically distributed) data, and privacy concerns. Training a ML model over heterogeneous non-IID data highly degrades the convergence rate and performance. The existing traditional and clustered FL algorithms exhibit two main limitations, including inefficient client training and static hyper-parameter utilization. To overcome these limitations, we propose a novel hybrid algorithm, namely genetic clustered FL (Genetic CFL), that clusters edge devices based on the training hyper-parameters and genetically modifies the parameters cluster-wise. Then, we introduce an algorithm that drastically increases the individual cluster accuracy by integrating the density-based clustering and genetic hyper-parameter optimization. The results are bench-marked using MNIST handwritten digit dataset and the CIFAR-10 dataset. The proposed genetic CFL shows significant improvements and works well with realistic cases of non-IID and ambiguous data.",Genetic CFL: Optimization of Hyper-Parameters in Clustered Federated Learning,"Shaashwat Agrawal, Sagnik Sarkar, Mamoun Alazab, Praveen Kumar Reddy Maddikunta, Thippa Reddy Gadekallu and Quoc-Viet Pham",2021,Artificial Intelligence,2107.07233
"We present a storytelling robot, controlled via the ACT-R cognitive architecture, able to adopt different persuasive techniques and ethical stances while conversing about some topics concerning COVID-19. The main contribution of the paper consists in the proposal of a needs-driven model that guides and evaluates, during the dialogue, the use (if any) of persuasive techniques available in the agent procedural memory. The portfolio of persuasive techniques tested in such a model ranges from the use of storytelling, to framing techniques and rhetorical-based arguments. To the best of our knowledge, this represents the first attempt of building a persuasive agent able to integrate a mix of explicitly grounded cognitive assumptions about dialogue management, storytelling and persuasive techniques as well as ethical attitudes. The paper presents the results of an exploratory evaluation of the system on 63 participants",A Storytelling Robot managing Persuasive and Ethical Stances via ACT-R: an Exploratory Study,"Agnese Augello, Giuseppe Citt\`a, Manuel Gentile, Antonio Lieto",2022,Artificial Intelligence,2107.12845
"In this paper we present a technique for procedurally generating 3D maps using a set of premade meshes which snap together based on designer-specified visual constraints. The proposed approach avoids size and layout limitations, offering the designer control over the look and feel of the generated maps, as well as immediate feedback on a given map's navigability. A prototype implementation of the method, developed in the Unity game engine, is discussed, and a number of case studies are analyzed. These include a multiplayer game where the method was used, together with a number of illustrative examples which highlight various parameterizations and piece selection methods. The technique can be used as a designer-centric map composition method and/or as a prototyping system in 3D level design, opening the door for quality map and level creation in a fraction of the time of a fully human-based approach.",Procedural Generation of 3D Maps with Snappable Meshes,"Rafael C. e Silva, Nuno Fachada, Diogo de Andrade, N\'elio C\'odices",2022,Artificial Intelligence,2108.00056
"Planning for Autonomous Unmanned Ground Vehicles (AUGV) is still a challenge, especially in difficult, off-road, critical situations. Automatic planning can be used to reach mission objectives, to perform navigation or maneuvers. Most of the time, the problem consists in finding a path from a source to a destination, while satisfying some operational constraints. In a graph without negative cycles, the computation of the single-pair shortest path from a start node to an end node is solved in polynomial time. Additional constraints on the solution path can however make the problem harder to solve. This becomes the case when we need the path to pass through a few mandatory nodes without requiring a specific order of visit. The complexity grows exponentially with the number of mandatory nodes to visit. In this paper, we focus on shortest path search with mandatory nodes on a given connected graph. We propose a hybrid model that combines a constraint-based solver and a graph convolutional neural network to improve search performance. Promising results are obtained on realistic scenarios.",Constrained Shortest Path Search with Graph Convolutional Neural Networks,"Kevin Osanlou, Christophe Guettier, Andrei Bursuc, Tristan Cazenave, Eric Jacopin",2018,Artificial Intelligence,2108.00978
"Uncertain information is being taken into account in an increasing number of application fields. In the meantime, abduction has been proved a powerful tool for handling hypothetical reasoning and incomplete knowledge. Probabilistic logical models are a suitable framework to handle uncertain information, and in the last decade many probabilistic logical languages have been proposed, as well as inference and learning systems for them. In the realm of Abductive Logic Programming (ALP), a variety of proof procedures have been defined as well. In this paper, we consider a richer logic language, coping with probabilistic abduction with variables. In particular, we consider an ALP program enriched with integrity constraints `a la IFF, possibly annotated with a probability value. We first present the overall abductive language, and its semantics according to the Distribution Semantics. We then introduce a proof procedure, obtained by extending one previously presented, and prove its soundness and completeness.",Nonground Abductive Logic Programming with Probabilistic Integrity Constraints,"Elena Bellodi, Marco Gavanelli, Riccardo Zese, Evelina Lamma, Fabrizio Riguzzi",2021,Artificial Intelligence,2108.03033
"Dealing with context dependent knowledge has led to different formalizations of the notion of context. Among them is the Contextualized Knowledge Repository (CKR) framework, which is rooted in description logics but links on the reasoning side strongly to logic programs and Answer Set Programming (ASP) in particular. The CKR framework caters for reasoning with defeasible axioms and exceptions in contexts, which was extended to knowledge inheritance across contexts in a coverage (specificity) hierarchy. However, the approach supports only this single type of contextual relation and the reasoning procedures work only for restricted hierarchies, due to non-trivial issues with model preference under exceptions. In this paper, we overcome these limitations and present a generalization of CKR hierarchies to multiple contextual relations, along with their interpretation of defeasible axioms and preference. To support reasoning, we use ASP with algebraic measures, which is a recent extension of ASP with weighted formulas over semirings that allows one to associate quantities with interpretations depending on the truth values of propositional atoms. Notably, we show that for a relevant fragment of CKR hierarchies with multiple contextual relations, query answering can be realized with the popular asprin framework. The algebraic measures approach is more powerful and enables e.g. reasoning with epistemic queries over CKRs, which opens interesting perspectives for the use of quantitative ASP extensions in other applications.",Reasoning on Multi-Relational Contextual Hierarchies via Answer Set Programming with Algebraic Measures,"Loris Bozzato, Thomas Eiter, Rafael Kiesel",2021,Artificial Intelligence,2108.03100
"The traditional production paradigm of large batch production does not offer flexibility towards satisfying the requirements of individual customers. A new generation of smart factories is expected to support new multi-variety and small-batch customized production modes. For that, Artificial Intelligence (AI) is enabling higher value-added manufacturing by accelerating the integration of manufacturing and information communication technologies, including computing, communication, and control. The characteristics of a customized smart factory are to include self-perception, operations optimization, dynamic reconfiguration, and intelligent decision-making. The AI technologies will allow manufacturing systems to perceive the environment, adapt to the external needs, and extract the process knowledge, including business models, such as intelligent production, networked collaboration, and extended service models. This paper focuses on the implementation of AI in customized manufacturing (CM). The architecture of an AI-driven customized smart factory is presented. Details of intelligent manufacturing devices, intelligent information interaction, and construction of a flexible manufacturing line are showcased. The state-of-the-art AI technologies of potential use in CM, i.e., machine learning, multi-agent systems, Internet of Things, big data, and cloud-edge computing are surveyed. The AI-enabled technologies in a customized smart factory are validated with a case study of customized packaging. The experimental results have demonstrated that the AI-assisted CM offers the possibility of higher production flexibility and efficiency. Challenges and solutions related to AI in CM are also discussed.","Artificial Intelligence-Driven Customized Manufacturing Factory: Key Technologies, Applications, and Challenges","Jiafu Wan, Xiaomin Li, Hong-Ning Dai, Andrew Kusiak, Miguel Mart\'inez-Garc\'ia, Di Li",2021,Artificial Intelligence,2108.03383
"Several studies have reported the inability of Transformer models to generalize compositionally, a key type of generalization in many NLP tasks such as semantic parsing. In this paper we explore the design space of Transformer models showing that the inductive biases given to the model by several design decisions significantly impact compositional generalization. Through this exploration, we identified Transformer configurations that generalize compositionally significantly better than previously reported in the literature in a diverse set of compositional tasks, and that achieve state-of-the-art results in a semantic parsing compositional generalization benchmark (COGS), and a string edit operation composition benchmark (PCFG).",Making Transformers Solve Compositional Tasks,"Santiago Onta\~n\'on, Joshua Ainslie, Vaclav Cvicek and Zachary Fisher",2022,Artificial Intelligence,2108.04378
"With the increasing demands of personalized learning, knowledge tracing has become important which traces students' knowledge states based on their historical practices. Factor analysis methods mainly use two kinds of factors which are separately related to students and questions to model students' knowledge states. These methods use the total number of attempts of students to model students' learning progress and hardly highlight the impact of the most recent relevant practices. Besides, current factor analysis methods ignore rich information contained in questions. In this paper, we propose Multi-Factors Aware Dual-Attentional model (MF-DAKT) which enriches question representations and utilizes multiple factors to model students' learning progress based on a dual-attentional mechanism. More specifically, we propose a novel student-related factor which records the most recent attempts on relevant concepts of students to highlight the impact of recent exercises. To enrich questions representations, we use a pre-training method to incorporate two kinds of question information including questions' relation and difficulty level. We also add a regularization term about questions' difficulty level to restrict pre-trained question representations to fine-tuning during the process of predicting students' performance. Moreover, we apply a dual-attentional mechanism to differentiate contributions of factors and factor interactions to final prediction in different practice records. At last, we conduct experiments on several real-world datasets and results show that MF-DAKT can outperform existing knowledge tracing methods. We also conduct several studies to validate the effects of each component of MF-DAKT.",Multi-Factors Aware Dual-Attentional Knowledge Tracing,"Moyu Zhang (1), Xinning Zhu (1), Chunhong Zhang (1), Yang Ji (1), Feng Pan (1), Changchuan Yin (1) ((1) Beijing University of Posts and Telecommunications)",2021,Artificial Intelligence,2108.04741
"Recent systems applying Machine Learning (ML) to solve the Traveling Salesman Problem (TSP) exhibit issues when they try to scale up to real case scenarios with several hundred vertices. The use of Candidate Lists (CLs) has been brought up to cope with the issues. The procedure allows to restrict the search space during solution creation, consequently reducing the solver computational burden. So far, ML were engaged to create CLs and values on the edges of these CLs expressing ML preferences at solution insertion. Although promising, these systems do not clearly restrict what the ML learns and does to create solutions, bringing with them some generalization issues. Therefore, motivated by exploratory and statistical studies, in this work we instead use a machine learning model to confirm the addition in the solution just for high probable edges. CLs of the high probable edge are employed as input, and the ML is in charge of distinguishing cases where such edges are in the optimal solution from those where they are not. . This strategy enables a better generalization and creates an efficient balance between machine learning and searching techniques. Our ML-Constructive heuristic is trained on small instances. Then, it is able to produce solutions, without losing quality, to large problems as well. We compare our results with classic constructive heuristics, showing good performances for TSPLIB instances up to 1748 cities. Although our heuristic exhibits an expensive constant time operation, we proved that the computational complexity in worst-case scenario, for the solution construction after training, is $O(n^2 \log n^2)$, being $n$ the number of vertices in the TSP instance.",A New Constructive Heuristic driven by Machine Learning for the Traveling Salesman Problem,"Umberto Junior Mele, Luca Maria Gambardella and Roberto Montemanni",2021,Artificial Intelligence,2108.10224
"Agent-based systems have the capability to fuse information from many distributed sources and create better plans faster. This feature makes agent-based systems naturally suitable to address the challenges in Supply Chain Management (SCM). Although agent-based supply chains systems have been proposed since early 2000; industrial uptake of them has been lagging. The reasons quoted include the immaturity of the technology, a lack of interoperability with supply chain information systems, and a lack of trust in Artificial Intelligence (AI). In this paper, we revisit the agent-based supply chain and review the state of the art. We find that agent-based technology has matured, and other supporting technologies that are penetrating supply chains; are filling in gaps, leaving the concept applicable to a wider range of functions. For example, the ubiquity of IoT technology helps agents ""sense"" the state of affairs in a supply chain and opens up new possibilities for automation. Digital ledgers help securely transfer data between third parties, making agent-based information sharing possible, without the need to integrate Enterprise Resource Planning (ERP) systems. Learning functionality in agents enables agents to move beyond automation and towards autonomy. We note this convergence effect through conceptualising an agent-based supply chain framework, reviewing its components, and highlighting research challenges that need to be addressed in moving forward.",Will bots take over the supply chain? Revisiting Agent-based supply chain automation,"Liming Xu, Stephen Mak and Alexandra Brintrup",2021,Artificial Intelligence,2109.01703
"Recent breakthroughs in AI have shown the remarkable power of deep learning and deep reinforcement learning. These developments, however, have been tied to specific tasks, and progress in out-of-distribution generalization has been limited. While it is assumed that these limitations can be overcome by incorporating suitable inductive biases, the notion of inductive biases itself is often left vague and does not provide meaningful guidance. In the paper, I articulate a different learning approach where representations do not emerge from biases in a neural architecture but are learned over a given target language with a known semantics. The basic ideas are implicit in mainstream AI where representations have been encoded in languages ranging from fragments of first-order logic to probabilistic structural causal models. The challenge is to learn from data the representations that have traditionally been crafted by hand. Generalization is then a result of the semantics of the language. The goals of this paper are to make these ideas explicit, to place them in a broader context where the design of the target language is crucial, and to illustrate them in the context of learning to act and plan. For this, after a general discussion, I consider learning representations of actions, general policies, and subgoals (""intrinsic rewards""). In these cases, learning is formulated as a combinatorial problem but nothing prevents the use of deep learning techniques instead. Indeed, learning representations over languages with a known semantics provides an account of what is to be learned, while learning representations with neural nets provides a complementary account of how representations can be learned. The challenge and the opportunity is to bring the two together.",Target Languages (vs. Inductive Biases) for Learning to Act and Plan,Hector Geffner,2022,Artificial Intelligence,2109.07195
"We present an historical overview about the connections between the analysis of risk and the control of autonomous systems. We offer two main contributions. Our first contribution is to propose three overlapping paradigms to classify the vast body of literature: the worst-case, risk-neutral, and risk-averse paradigms. We consider an appropriate assessment for the risk of an autonomous system to depend on the application at hand. In contrast, it is typical to assess risk using an expectation, variance, or probability alone. Our second contribution is to unify the concepts of risk and autonomous systems. We achieve this by connecting approaches for quantifying and optimizing the risk that arises from a system's behaviour across academic fields. The survey is highly multidisciplinary. We include research from the communities of reinforcement learning, stochastic and robust control theory, operations research, and formal verification. We describe both model-based and model-free methods, with emphasis on the former. Lastly, we highlight fruitful areas for further research. A key direction is to blend risk-averse model-based and model-free methods to enhance the real-time adaptive capabilities of systems to improve human and environmental welfare.",Risk-averse autonomous systems: A brief history and recent developments from the perspective of optimal control,Yuheng Wang and Margaret P. Chapman,2022,Artificial Intelligence,2109.08947
"Recently, a boxology (graphical language) with design patterns for hybrid AI was proposed, combining symbolic and sub-symbolic learning and reasoning. In this paper, we extend this boxology with actors and their interactions. The main contributions of this paper are: 1) an extension of the taxonomy to describe distributed hybrid AI systems with actors and interactions; and 2) showing examples using a few design patterns relevant in multi-agent systems and human-agent interaction.",Modular Design Patterns for Hybrid Actors,"Andr\'e Meyer-Vitali, Wico Mulder, Maaike H.T. de Boer",2021,Artificial Intelligence,2109.09331
"While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer it to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first dataset for this task, and we propose a revised loss function that enables the PLM to learn how to predict precise probabilities for the task. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the fine-tuned model, yielding state-of-the-art results on external datasets.",RuleBert: Teaching Soft Rules to Pre-trained Language Models,"Mohammed Saeed, Naser Ahmadi, Preslav Nakov, Paolo Papotti",2021,Artificial Intelligence,2109.13006
"On playing video games, different players usually have their own playstyles. Recently, there have been great improvements for the video game AIs on the playing strength. However, past researches for analyzing the behaviors of players still used heuristic rules or the behavior features with the game-environment support, thus being exhausted for the developers to define the features of discriminating various playstyles. In this paper, we propose the first metric for video game playstyles directly from the game observations and actions, without any prior specification on the playstyle in the target game. Our proposed method is built upon a novel scheme of learning discrete representations that can map game observations into latent discrete states, such that playstyles can be exhibited from these discrete states. Namely, we measure the playstyle distance based on game observations aligned to the same states. We demonstrate high playstyle accuracy of our metric in experiments on some video game platforms, including TORCS, RGSK, and seven Atari games, and for different agents including rule-based AI bots, learning-based AI bots, and human players.",An Unsupervised Video Game Playstyle Metric via State Discretization,"Chiu-Chou Lin, Wei-Chen Chiu and I-Chen Wu",2021,Artificial Intelligence,2110.00950
"In Mixed Integer Linear Programming (MIP), a (strong) backdoor is a ""small"" subset of an instance's integer variables with the following property: in a branch-and-bound procedure, the instance can be solved to global optimality by branching only on the variables in the backdoor. Constructing datasets of pre-computed backdoors for widely used MIP benchmark sets or particular problem families can enable new questions around novel structural properties of a MIP, or explain why a problem that is hard in theory can be solved efficiently in practice. Existing algorithms for finding backdoors rely on sampling candidate variable subsets in various ways, an approach which has demonstrated the existence of backdoors for some instances from MIPLIB2003 and MIPLIB2010. However, these algorithms fall short of consistently succeeding at the task due to an imbalance between exploration and exploitation. We propose BaMCTS, a Monte Carlo Tree Search framework for finding backdoors to MIPs. Extensive algorithmic engineering, hybridization with traditional MIP concepts, and close integration with the CPLEX solver have enabled our method to outperform baselines on MIPLIB2017 instances, finding backdoors more frequently and more efficiently.",Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework,"Elias B. Khalil, Pashootan Vaezipoor, Bistra Dilkina",2022,Artificial Intelligence,2110.08423
"Fact-checking on the Web has become the main mechanism through which we detect the credibility of the news or information. Existing fact-checkers verify the authenticity of the information (support or refute the claim) based on secondary sources of information. However, existing approaches do not consider the problem of model updates due to constantly increasing training data due to user feedback. It is therefore important to conduct user studies to correct models' inference biases and improve the model in a life-long learning manner in the future according to the user feedback. In this paper, we present FaxPlainAC, a tool that gathers user feedback on the output of explainable fact-checking models. FaxPlainAC outputs both the model decision, i.e., whether the input fact is true or not, along with the supporting/refuting evidence considered by the model. Additionally, FaxPlainAC allows for accepting user feedback both on the prediction and explanation. Developed in Python, FaxPlainAC is designed as a modular and easily deployable tool. It can be integrated with other downstream tasks and allowing for fact-checking human annotation gathering and life-long learning.",FaxPlainAC: A Fact-Checking Tool Based on EXPLAINable Models with HumAn Correction in the Loop,"Zijian Zhang, Koustav Rudra, Avishek Anand",2021,Artificial Intelligence,2110.10144
"Gray-box graph attacks aim at disrupting the performance of the victim model by using inconspicuous attacks with limited knowledge of the victim model. The parameters of the victim model and the labels of the test nodes are invisible to the attacker. To obtain the gradient on the node attributes or graph structure, the attacker constructs an imaginary surrogate model trained under supervision. However, there is a lack of discussion on the training of surrogate models and the robustness of provided gradient information. The general node classification model loses the topology of the nodes on the graph, which is, in fact, an exploitable prior for the attacker. This paper investigates the effect of representation learning of surrogate models on the transferability of gray-box graph adversarial attacks. To reserve the topology in the surrogate embedding, we propose Surrogate Representation Learning with Isometric Mapping (SRLIM). By using Isometric mapping method, our proposed SRLIM can constrain the topological structure of nodes from the input layer to the embedding space, that is, to maintain the similarity of nodes in the propagation process. Experiments prove the effectiveness of our approach through the improvement in the performance of the adversarial attacks generated by the gradient-based attacker in untargeted poisoning gray-box setups.",Surrogate Representation Learning with Isometric Mapping for Gray-box Graph Adversarial Attacks,"Zihan Liu, Yun Luo, Zelin Zang, Stan Z. Li",2022,Artificial Intelligence,2110.10482
"We derive nearly tight and non-asymptotic convergence bounds for solutions of entropic semi-discrete optimal transport. These bounds quantify the stability of the dual solutions of the regularized problem (sometimes called Sinkhorn potentials) w.r.t. the regularization parameter, for which we ensure a better than Lipschitz dependence. Such facts may be a first step towards a mathematical justification of annealing or $\varepsilon$-scaling heuristics for the numerical resolution of regularized semi-discrete optimal transport. Our results also entail a non-asymptotic and tight expansion of the difference between the entropic and the unregularized costs.",Nearly Tight Convergence Bounds for Semi-discrete Entropic Optimal Transport,"Alex Delalande (LMO, DATASHAPE)",2022,Artificial Intelligence,2110.12678
"This two-part comprehensive survey is devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Notable models in the HDC/VSA family are Tensor Product Representations, Holographic Reduced Representations, Multiply-Add-Permute, Binary Spatter Codes, and Sparse Binary Distributed Representations but there are other models too. HDC/VSA is a highly interdisciplinary area with connections to computer science, electrical engineering, artificial intelligence, mathematics, and cognitive science. This fact makes it challenging to create a thorough overview of the area. However, due to a surge of new researchers joining the area in recent years, the necessity for a comprehensive survey of the area has become extremely important. Therefore, amongst other aspects of the area, this Part I surveys important aspects such as: known computational models of HDC/VSA and transformations of various input data types to high-dimensional distributed representations. Part II of this survey is devoted to applications, cognitive computing and architectures, as well as directions for future work. The survey is written to be useful for both newcomers and practitioners.","A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part I: Models and Data Transformations","Denis Kleyko, Dmitri A. Rachkovskij, Evgeny Osipov, Abbas Rahimi",2022,Artificial Intelligence,2111.06077
"We introduce the partially observable history process (POHP) formalism for reinforcement learning. POHP centers around the actions and observations of a single agent and abstracts away the presence of other players without reducing them to stochastic processes. Our formalism provides a streamlined interface for designing algorithms that defy categorization as exclusively single or multi-agent, and for developing theory that applies across these domains. We show how the POHP formalism unifies traditional models including the Markov decision process, the Markov game, the extensive-form game, and their partially observable extensions, without introducing burdensome technical machinery or violating the philosophical underpinnings of reinforcement learning. We illustrate the utility of our formalism by concisely exploring observable sequential rationality, examining some theoretical properties of general immediate regret minimization, and generalizing the extensive-form regret minimization (EFR) algorithm.",The Partially Observable History Process,"Dustin Morrill, Amy R. Greenwald, Michael Bowling",2022,Artificial Intelligence,2111.08102
"False alerts due to misconfigured/ compromised IDS in ICS networks can lead to severe economic and operational damage. To solve this problem, research has focused on leveraging deep learning techniques that help reduce false alerts. However, a shortcoming is that these works often require or implicitly assume the physical and cyber sensors to be trustworthy. Implicit trust of data is a major problem with using artificial intelligence or machine learning for CPS security, because during critical attack detection time they are more at risk, with greater likelihood and impact, of also being compromised. To address this shortcoming, the problem is reframed on how to make good decisions given uncertainty. Then, the decision is detection, and the uncertainty includes whether the data used for ML-based IDS is compromised. Thus, this work presents an approach for reducing false alerts in CPS power systems by dealing uncertainty without the knowledge of prior distribution of alerts. Specifically, an evidence theoretic based approach leveraging Dempster Shafer combination rules are proposed for reducing false alerts. A multi-hypothesis mass function model is designed that leverages probability scores obtained from various supervised-learning classifiers. Using this model, a location-cum-domain based fusion framework is proposed and evaluated with different combination rules, that fuse multiple evidence from inter-domain and intra-domain sensors. The approach is demonstrated in a cyber-physical power system testbed with Man-In-The-Middle attack emulation in a large-scale synthetic electric grid. For evaluating the performance, plausibility, belief, pignistic, etc. metrics as decision functions are considered. To improve the performance, a multi-objective based genetic algorithm is proposed for feature selection considering the decision metrics as the fitness function.",Inter-Domain Fusion for Enhanced Intrusion Detection in Power Systems: An Evidence Theoretic and Meta-Heuristic Approach,Abhijeet Sahu and Katherine Davis,2022,Artificial Intelligence,2111.10484
"PDDL+ is an extension of PDDL2.1 which incorporates fully-featured autonomous processes and allows for better modelling of mixed discrete-continuous domains. Unlike PDDL2.1, PDDL+ lacks a logical semantics, relying instead on state-transitional semantics enriched with hybrid automata semantics for the continuous states. This complex semantics makes analysis and comparisons to other action formalisms difficult. In this paper, we propose a natural extension of Reiter's situation calculus theories inspired by hybrid automata. The kinship between PDDL+ and hybrid automata allows us to develop a direct mapping between PDDL+ and situation calculus, thereby supplying PDDL+ with a logical semantics and the situation calculus with a modern way of representing autonomous processes. We outline the potential benefits of the mapping by suggesting a new approach to effective planning in PDDL+.",A Logical Semantics for PDDL+,"Vitaliy Batusov, Mikhail Soutchanski",2019,Artificial Intelligence,2111.11588
"Wikidata is the largest general-interest knowledge base that is openly available. It is collaboratively edited by thousands of volunteer editors and has thus evolved considerably since its inception in 2012. In this paper, we present Wikidated 1.0, a dataset of Wikidata's full revision history, which encodes changes between Wikidata revisions as sets of deletions and additions of RDF triples. To the best of our knowledge, it constitutes the first large dataset of an evolving knowledge graph, a recently emerging research subject in the Semantic Web community. We introduce the methodology for generating Wikidated 1.0 from dumps of Wikidata, discuss its implementation and limitations, and present statistical characteristics of the dataset.",Wikidated 1.0: An Evolving Knowledge Graph Dataset of Wikidata's Revision History,"Lukas Schmelzeisen, Corina Dima, Steffen Staab",2021,Artificial Intelligence,2112.05003
"Agent-based models have emerged as a promising paradigm for addressing ever increasing complexity of information systems. In its initial days in the 1990s when object-oriented modeling was at its peak, an agent was treated as a special kind of ""object"" that had a persistent state and its own independent thread of execution. Since then, agent-based models have diversified enormously to even open new conceptual insights about the nature of systems in general. This paper presents a perspective on the disparate ways in which our understanding of agency, as well as computational models of agency have evolved. Advances in hardware like GPUs, that brought neural networks back to life, may also similarly infuse new life into agent-based models, as well as pave the way for advancements in research on Artificial General Intelligence (AGI).",Paradigms of Computational Agency,Srinath Srinivasa and Jayati Deshmukh,2020,Artificial Intelligence,2112.05575
"Metaheuristic and self-organizing criticality (SOC) could contribute to robust computation under perturbed environments. Implementing a logic gate in a computing system in a critical state is one of the intriguing ways to study the role of metaheuristics and SOCs. Here, we study the behavior of cellular automaton, game of life (GL), in asynchronous updating and implement probabilistic logic gates by using asynchronous GL. We find that asynchronous GL shows a phase transition, that the density of the state of 1 decays with the power law at the critical point, and that systems at the critical point have the most computability in asynchronous GL. We implement AND and OR gates in asynchronous GL with criticality, which shows good performance. Since tuning perturbations play an essential role in operating logic gates, our study reveals the interference between manipulation and perturbation in probabilistic logic gates.",Probabilistic Logic Gate in Asynchronous Game of Life with Critical Property,"Yukio-Pegio Gunji, Yoshihiko Ohzawa and Terutaka Tanaka",2021,Artificial Intelligence,2112.07846
We propose neural-symbolic integration for abstract concept explanation and interactive learning. Neural-symbolic integration and explanation allow users and domain-experts to learn about the data-driven decision making process of large neural models. The models are queried using a symbolic logic language. Interaction with the user then confirms or rejects a revision of the neural model using logic-based constraints that can be distilled into the model architecture. The approach is illustrated using the Logic Tensor Network framework alongside Concept Activation Vectors and applied to a Convolutional Neural Network.,Neural-Symbolic Integration for Interactive Learning and Conceptual Grounding,"Benedikt Wagner, Artur d'Avila Garcez",2021,Artificial Intelligence,2112.11805
"Automated Deduction in Geometry (ADG) is a forum to exchange ideas and views, to present research results and progress, and to demonstrate software tools at the intersection between geometry and automated deduction. Relevant topics include (but are not limited to): polynomial algebra, invariant and coordinate-free methods; probabilistic, synthetic, and logic approaches, techniques for automated geometric reasoning from discrete mathematics, combinatorics, and numerics; interactive theorem proving in geometry; symbolic and numeric methods for geometric computation, geometric constraint solving, automated generation/reasoning and manipulation with diagrams; design and implementation of geometry software, automated theorem provers, special-purpose tools, experimental studies; applications of ADG in mechanics, geometric modelling, CAGD/CAD, computer vision, robotics and education. Traditionally, the ADG conference is held every two years. The previous editions of ADG were held in Nanning in 2018, Strasbourg in 2016, Coimbra in 2014, Edinburgh in 2012, Munich in 2010, Shanghai in 2008, Pontevedra in 2006, Gainesville in 2004, Hagenberg in 2002, Zurich in 2000, Beijing in 1998, and Toulouse in 1996. The 13th edition of ADG was supposed to be held in 2020 in Hagenberg, Austria, but due to the COVID-19 pandemic, it was postponed for 2021, and held online (still hosted by RISC Institute, Hagenberg, Austria), September 15-17, 2021 (https://www.risc.jku.at/conferences/adg2021).",Proceedings of the 13th International Conference on Automated Deduction in Geometry,"Predrag Jani\v{c}i\'c, Zolt\'an Kov\'acs",2021,Artificial Intelligence,2112.14770
"Online reinforcement learning (RL) algorithms are often difficult to deploy in complex human-facing applications as they may learn slowly and have poor early performance. To address this, we introduce a practical algorithm for incorporating human insight to speed learning. Our algorithm, Constraint Sampling Reinforcement Learning (CSRL), incorporates prior domain knowledge as constraints/restrictions on the RL policy. It takes in multiple potential policy constraints to maintain robustness to misspecification of individual constraints while leveraging helpful ones to learn quickly. Given a base RL learning algorithm (ex. UCRL, DQN, Rainbow) we propose an upper confidence with elimination scheme that leverages the relationship between the constraints, and their observed performance, to adaptively switch among them. We instantiate our algorithm with DQN-type algorithms and UCRL as base algorithms, and evaluate our algorithm in four environments, including three simulators based on real data: recommendations, educational activity sequencing, and HIV treatment sequencing. In all cases, CSRL learns a good policy faster than baselines.",Constraint Sampling Reinforcement Learning: Incorporating Expertise For Faster Learning,"Tong Mu, Georgios Theocharous, David Arbour, Emma Brunskill",2022,Artificial Intelligence,2112.15221
"This is Part II of the two-part comprehensive survey devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Holographic Reduced Representations is an influential HDC/VSA model that is well-known in the machine learning domain and often used to refer to the whole family. However, for the sake of consistency, we use HDC/VSA to refer to the area. Part I of this survey covered foundational aspects of the area, such as historical context leading to the development of HDC/VSA, key elements of any HDC/VSA model, known HDC/VSA models, and transforming input data of various types into high-dimensional vectors suitable for HDC/VSA. This second part surveys existing applications, the role of HDC/VSA in cognitive computing and architectures, as well as directions for future work. Most of the applications lie within the machine learning/artificial intelligence domain, however we also cover other applications to provide a thorough picture. The survey is written to be useful for both newcomers and practitioners.","A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part II: Applications, Cognitive Models, and Challenges","Denis Kleyko, Dmitri A. Rachkovskij, Evgeny Osipov, Abbas Rahimi",2022,Artificial Intelligence,2112.15424
"Humans use causality and hypothetical retrospection in their daily decision-making, planning, and understanding of life events. The human mind, while retrospecting a given situation, think about questions such as ""What was the cause of the given situation?"", ""What would be the effect of my action?"", or ""Which action led to this effect?"". It develops a causal model of the world, which learns with fewer data points, makes inferences, and contemplates counterfactual scenarios. The unseen, unknown, scenarios are known as counterfactuals. AI algorithms use a representation based on knowledge graphs (KG) to represent the concepts of time, space, and facts. A KG is a graphical data model which captures the semantic relationships between entities such as events, objects, or concepts. The existing KGs represent causal relationships extracted from texts based on linguistic patterns of noun phrases for causes and effects as in ConceptNet and WordNet. The current causality representation in KGs makes it challenging to support counterfactual reasoning. A richer representation of causality in AI systems using a KG-based approach is needed for better explainability, and support for intervention and counterfactuals reasoning, leading to improved understanding of AI systems by humans. The causality representation requires a higher representation framework to define the context, the causal information, and the causal effects. The proposed Causal Knowledge Graph (CausalKG) framework, leverages recent progress of causality and KG towards explainability. CausalKG intends to address the lack of a domain adaptable causal model and represent the complex causal relations using the hyper-relational graph representation in the KG. We show that the CausalKG's interventional and counterfactual reasoning can be used by the AI system for the domain explainability.",CausalKG: Causal Knowledge Graph Explainability using interventional and counterfactual reasoning,"Utkarshani Jaimini, Amit Sheth",2022,Artificial Intelligence,2201.03647
"This paper describes the evolution of our research from video analytics to a global security system with focus on the video surveillance component. Indeed video surveillance has evolved from a commodity security tool up to the most efficient way of tracking perpetrators when terrorism hits our modern urban centers. As number of cameras soars, one could expect the system to leverage the huge amount of data carried through the video streams to provide fast access to video evidences, actionable intelligence for monitoring real-time events and enabling predictive capacities to assist operators in their surveillance tasks. This research explores a hybrid platform for video intelligence capture, automated data extraction, supervised Machine Learning for intelligently assisted urban video surveillance; Extension to other components of a global security system are discussed. Applying Knowledge Management principles in this research helps with deep problem understanding and facilitates the implementation of efficient information and experience sharing decision support systems providing assistance to people on the field as well as in operations centers. The originality of this work is also the creation of ""common"" human-machine and machine to machine language and a security ontology.",Video Intelligence as a component of a Global Security system,"Dominique Verdejo, Eunika Mercier-Laurent (CRESTIC)",2019,Artificial Intelligence,2201.04349
"It is commonly acknowledged that the availability of the huge amount of (training) data is one of the most important factors for many recent advances in Artificial Intelligence (AI). However, datasets are often designed for specific tasks in narrow AI sub areas and there is no unified way to manage and access them. This not only creates unnecessary overheads when training or deploying Machine Learning models but also limits the understanding of the data, which is very important for data-centric AI. In this paper, we present our vision about a unified framework for different datasets so that they can be integrated and queried easily, e.g., using standard query languages. We demonstrate this in our ongoing work to create a framework for datasets in Computer Vision and show its advantages in different scenarios. Our demonstration is available at https://vision.semkg.org.",Fantastic Data and How to Query Them,"Trung-Kien Tran, Anh Le-Tuan, Manh Nguyen-Duc, Jicheng Yuan, Danh Le-Phuoc",2021,Artificial Intelligence,2201.05026
"Online advertising is a major source of income for many online companies. One common approach is to sell online advertisements via waterfall auctions, through which a publisher makes sequential price offers to ad networks. The publisher controls the order and prices of the waterfall in an attempt to maximize his revenue. In this work, we propose a methodology to learn a waterfall strategy from historical data by wisely searching in the space of possible waterfalls and selecting the one leading to the highest revenues. The contribution of this work is twofold; First, we propose a novel method to estimate the valuation distribution of each user, with respect to each ad network. Second, we utilize the valuation matrix to score our candidate waterfalls as part of a procedure that iteratively searches in local neighborhoods. Our framework guarantees that the waterfall revenue improves between iterations ultimately converging into a local optimum. Real-world demonstrations are provided to show that the proposed method improves the total revenue of real-world waterfalls, as compared to manual expert optimization. Finally, the code and the data are available here.",Search and Score-based Waterfall Auction Optimization,"Dan Halbersberg, Matan Halevi, Moshe Salhov",2022,Artificial Intelligence,2201.06409
"Due to extensive spread of fake news on social and news media it became an emerging research topic now a days that gained attention. In the news media and social media the information is spread highspeed but without accuracy and hence detection mechanism should be able to predict news fast enough to tackle the dissemination of fake news. It has the potential for negative impacts on individuals and society. Therefore, detecting fake news on social media is important and also a technically challenging problem these days. We knew that Machine learning is helpful for building Artificial intelligence systems based on tacit knowledge because it can help us to solve complex problems due to real word data. On the other side we knew that Knowledge engineering is helpful for representing experts knowledge which people aware of that knowledge. Due to this we proposed that integration of Machine learning and knowledge engineering can be helpful in detection of fake news. In this paper we present what is fake news, importance of fake news, overall impact of fake news on different areas, different ways to detect fake news on social media, existing detections algorithms that can help us to overcome the issue, similar application areas and at the end we proposed combination of data driven and engineered knowledge to combat fake news. We studied and compared three different modules text classifiers, stance detection applications and fact checking existing techniques that can help to detect fake news. Furthermore, we investigated the impact of fake news on society. Experimental evaluation of publically available datasets and our proposed fake news detection combination can serve better in detection of fake news.",Combining Machine Learning with Knowledge Engineering to detect Fake News in Social Networks-a survey,"Sajjad Ahmed, Knut Hinkelmann, Flavio Corradini",2019,Artificial Intelligence,2201.08032
"Stock Market can be easily seen as one of the most attractive places for investors, but it is also very complex in terms of making trading decisions. Predicting the market is a risky venture because of the uncertainties and nonlinear nature of the market. Deciding on the right time to trade is key to every successful trader as it can lead to either a huge gain of money or totally a loss in investment that will be recorded as a careless trade. The aim of this research is to develop a prediction system for stock market using Fuzzy Logic Type2 which will handle these uncertainties and complexities of human behaviour in general when it comes to buy, hold or sell decision making in stock trading. The proposed system was developed using VB.NET programming language as frontend and Microsoft SQL Server as backend. A total of four different technical indicators were selected for this research. The selected indicators are the Relative Strength Index, William Average, Moving Average Convergence and Divergence, and Stochastic Oscillator. These indicators serve as input variable to the Fuzzy System. The MACD and SO are deployed as primary indicators, while the RSI and WA are used as secondary indicators. Fibonacci retracement ratio was adopted for the secondary indicators to determine their support and resistance level in terms of making trading decisions. The input variables to the Fuzzy System is fuzzified to Low, Medium, and High using the Triangular and Gaussian Membership Function. The Mamdani Type Fuzzy Inference rules were used for combining the trading rules for each input variable to the fuzzy system. The developed system was tested using sample data collected from ten different companies listed on the Nigerian Stock Exchange for a total of fifty two periods. The dataset collected are Opening, High, Low, and Closing prices of each security.",Implementation of a Type-2 Fuzzy Logic Based Prediction System for the Nigerian Stock Exchange,"Isobo Nelson Davies, Donald Ene, Ibiere Boma Cookey, Godwin Fred Lenu",2022,Artificial Intelligence,2202.02107
"We compare four different `game-spaces' in terms of their usefulness in characterising multi-player tabletop games, with a particular interest in any underlying change to a game's characteristics as the number of players changes. In each case we take a 16-dimensional feature space, and reduce it to a 2-dimensional visualizable landscape. We find that a space obtained from optimization of parameters in Monte Carlo Tree Search (MCTS) is the most directly interpretable to characterise our set of games in terms of the relative importance of imperfect information, adversarial opponents and reward sparsity. These results do not correlate with a space defined using attributes of the game-tree. This dimensionality reduction does not show any general effect as the number of players. We therefore consider the question using the original features to classify the games into two sets; those for which the characteristics of the game changes significantly as the number of players changes, and those for which there is no such effect.",Visualising Multiplayer Game Spaces,"James Goodman, Diego Perez-Liebana, Simon Lucas",2021,Artificial Intelligence,2202.05773
"Development of new algorithms in the area of machine learning, especially clustering, comparative studies of such algorithms as well as testing according to software engineering principles requires availability of labeled data sets. While standard benchmarks are made available, a broader range of such data sets is necessary in order to avoid the problem of overfitting. In this context, theoretical works on axiomatization of clustering algorithms, especially axioms on clustering preserving transformations are quite a cheap way to produce labeled data sets from existing ones. However, the frequently cited axiomatic system of Kleinberg:2002, as we show in this paper, is not applicable for finite dimensional Euclidean spaces, in which many algorithms like $k$-means, operate. In particular, the so-called outer-consistency axiom fails upon making small changes in datapoint positions and inner-consistency axiom is valid only for identity transformation in general settings. Hence we propose an alternative axiomatic system, in which Kleinberg's inner consistency axiom is replaced by a centric consistency axiom and outer consistency axiom is replaced by motion consistency axiom. We demonstrate that the new system is satisfiable for a hierarchical version of $k$-means with auto-adjusted $k$, hence it is not contradictory. Additionally, as $k$-means creates convex clusters only, we demonstrate that it is possible to create a version detecting concave clusters and still the axiomatic system can be satisfied. The practical application area of such an axiomatic system may be the generation of new labeled test data from existent ones for clustering algorithm testing. %We propose the gravitational consistency as a replacement which does not have this deficiency.",Towards Continuous Consistency Axiom,Mieczyslaw A. Klopotek and Robert A. Klopotek,2022,Artificial Intelligence,2202.06015
"Edward C. Tolman found reinforcement learning unsatisfactory for explaining intelligence and proposed a clear distinction between learning and behavior. Tolman's ideas on latent learning and cognitive maps eventually led to what is now known as conceptual space, a geometric representation where concepts and ideas can form points or shapes.Active navigation between ideas - reasoning - can be expressed directly as purposive navigation in conceptual space. Assimilating the theory of conceptual space from modern neuroscience, we propose autonomous navigation as a valid approach for emulated cognition. However, achieving autonomous navigation in high-dimensional Euclidean spaces is not trivial in technology. In this work, we explore whether neoRL navigation is up for the task; adopting Kaelbling's concerns for efficient robot navigation, we test whether the neoRL approach is general across navigational modalities, compositional across considerations of experience, and effective when learning in multiple Euclidean dimensions. We find neoRL learning to be more resemblant of biological learning than of RL in AI, and propose neoRL navigation of conceptual space as a plausible new path toward emulated cognition.",Navigating Conceptual Space; A new take on Artificial General Intelligence,Per R. Leikanger,2021,Artificial Intelligence,2202.09646
"Knowledge graphs (KGs) are an important source repository for a wide range of applications and rule mining from KGs recently attracts wide research interest in the KG-related research community. Many solutions have been proposed for the rule mining from large-scale KGs, which however are limited in the inefficiency of rule generation and ineffectiveness of rule evaluation. To solve these problems, in this paper we propose a generation-then-evaluation rule mining approach guided by reinforcement learning. Specifically, a two-phased framework is designed. The first phase aims to train a reinforcement learning agent for rule generation from KGs, and the second is to utilize the value function of the agent to guide the step-by-step rule generation. We conduct extensive experiments on several datasets and the results prove that our rule mining solution achieves state-of-the-art performance in terms of efficiency and effectiveness.",Rule Mining over Knowledge Graphs via Reinforcement Learning,"Lihan Chen, Sihang Jiang, Jingping Liu, Chao Wang, Sheng Zhang, Chenhao Xie, Jiaqing Liang, Yanghua Xiao and Rui Song",2022,Artificial Intelligence,2202.10381
"Advocates for Neuro-Symbolic Artificial Intelligence (NeSy) assert that combining deep learning with symbolic reasoning will lead to stronger AI than either paradigm on its own. As successful as deep learning has been, it is generally accepted that even our best deep learning systems are not very good at abstract reasoning. And since reasoning is inextricably linked to language, it makes intuitive sense that Natural Language Processing (NLP), would be a particularly well-suited candidate for NeSy. We conduct a structured review of studies implementing NeSy for NLP, with the aim of answering the question of whether NeSy is indeed meeting its promises: reasoning, out-of-distribution generalization, interpretability, learning and reasoning from small data, and transferability to new domains. We examine the impact of knowledge representation, such as rules and semantic networks, language structure and relational structure, and whether implicit or explicit reasoning contributes to higher promise scores. We find that systems where logic is compiled into the neural network lead to the most NeSy goals being satisfied, while other factors such as knowledge representation, or type of neural architecture do not exhibit a clear correlation with goals being met. We find many discrepancies in how reasoning is defined, specifically in relation to human level reasoning, which impact decisions about model architectures and drive conclusions which are not always consistent across studies. Hence we advocate for a more methodical approach to the application of theories of human reasoning as well as the development of appropriate benchmarks, which we hope can lead to a better understanding of progress in the field. We make our data and code available on github for further analysis.",Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review,"Kyle Hamilton, Aparna Nayak, Bojan Bo\v{z}i\'c, Luca Longo",2022,Artificial Intelligence,2202.12205
"Language-guided Embodied AI benchmarks requiring an agent to navigate an environment and manipulate objects typically allow one-way communication: the human user gives a natural language command to the agent, and the agent can only follow the command passively. We present DialFRED, a dialogue-enabled embodied instruction following benchmark based on the ALFRED benchmark. DialFRED allows an agent to actively ask questions to the human user; the additional information in the user's response is used by the agent to better complete its task. We release a human-annotated dataset with 53K task-relevant questions and answers and an oracle to answer questions. To solve DialFRED, we propose a questioner-performer framework wherein the questioner is pre-trained with the human-annotated data and fine-tuned with reinforcement learning. We make DialFRED publicly available and encourage researchers to propose and evaluate their solutions to building dialog-enabled embodied agents.",DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following,"Xiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, Govind Thattai, Gaurav S. Sukhatme",2022,Artificial Intelligence,2202.13330
"Clinical Pathways (CP) are medical management plans developed to standardize patient treatment activities, optimize resource usage, reduce expenses, and improve the quality of healthcare services. Most CPs currently in use are paper-based documents (i.e., not computerized). CP computerization has been an active research topic since the inception of CP use in hospitals. This literature review research aims to examine studies that focused on CP computerization and offers recommendations for future research in this important research area. Some critical research suggestions include centralizing computerized CPs in Healthcare Information Systems (HIS), CP term standardization using international medical terminology systems, developing a global CP-specific digital coding system, creating a unified CP meta-ontology, developing independent Clinical Pathway Management Systems (CPMS), and supporting CPMSs with machine learning sub-systems.",Computerization of Clinical Pathways: A Literature Review and Directions for Future Research,Ayman Alahmar and Ola Alkhatib,2022,Artificial Intelligence,2203.00815
"The finetuning of pretrained transformer-based language generation models are typically conducted in an end-to-end manner, where the model learns to attend to relevant parts of the input by itself. However, there does not exist a mechanism to directly control the model's focus. This work aims to develop a control mechanism by which a user can select spans of context as ""highlights"" for the model to focus on, and generate relevant output. To achieve this goal, we augment a pretrained model with trainable ""focus vectors"" that are directly applied to the model's embeddings, while the model itself is kept fixed. These vectors, trained on automatic annotations derived from attribution methods, act as indicators for context importance. We test our approach on two core generation tasks: dialogue response generation and abstractive summarization. We also collect evaluation data where the highlight-generation pairs are annotated by humans. Our experiments show that the trained focus vectors are effective in steering the model to generate outputs that are relevant to user-selected highlights.",Controlling the Focus of Pretrained Language Generation Models,"Jiabao Ji, Yoon Kim, James Glass, Tianxing He",2022,Artificial Intelligence,2203.01146
"For communication to happen successfully, a common language is required between agents to understand information communicated by one another. Inducing the emergence of a common language has been a difficult challenge to multi-agent learning systems. In this work, we introduce an alternative perspective to the communicative messages sent between agents, considering them as different incomplete views of the environment state. Based on this perspective, we propose a simple approach to induce the emergence of a common language by maximizing the mutual information between messages of a given trajectory in a self-supervised manner. By evaluating our method in communication-essential environments, we empirically show how our method leads to better learning performance and speed, and learns a more consistent common language than existing methods, without introducing additional learning parameters.",Learning to Ground Decentralized Multi-Agent Communication with Contrastive Learning,Yat Long Lo and Biswa Sengupta,2022,Artificial Intelligence,2203.03344
"`gym-saturation` is an OpenAI Gym environment for reinforcement learning (RL) agents capable of proving theorems. Currently, only theorems written in a formal language of the Thousands of Problems for Theorem Provers (TPTP) library in clausal normal form (CNF) are supported. `gym-saturation` implements the 'given clause' algorithm (similar to the one used in Vampire and E Prover). Being written in Python, `gym-saturation` was inspired by PyRes. In contrast to the monolithic architecture of a typical Automated Theorem Prover (ATP), `gym-saturation` gives different agents opportunities to select clauses themselves and train from their experience. Combined with a particular agent, `gym-saturation` can work as an ATP. Even with a non trained agent based on heuristics, `gym-saturation` can find refutations for 688 (of 8257) CNF problems from TPTP v7.5.0.",Gym-saturation: an OpenAI Gym environment for saturation provers,Boris Shminke,2022,Artificial Intelligence,2203.04699
"This paper focuses on a dynamic aspect of responsible autonomy, namely, to make intelligent agents be responsible at run time. That is, it considers settings where decision making by agents impinges upon the outcomes perceived by other agents. For an agent to act responsibly, it must accommodate the desires and other attitudes of its users and, through other agents, of their users. The contribution of this paper is twofold. First, it provides a conceptual analysis of consent, its benefits and misuses, and how understanding consent can help achieve responsible autonomy. Second, it outlines challenges for AI (in particular, for agents and multiagent systems) that merit investigation to form as a basis for modeling consent in multiagent systems and applying consent to achieve responsible autonomy.",Consent as a Foundation for Responsible Autonomy,Munindar P. Singh,2022,Artificial Intelligence,2203.11420
"The symbolism, connectionism and behaviorism approaches of artificial intelligence have achieved a lot of successes in various tasks, while we still do not have a clear definition of ""intelligence"" with enough consensus in the community (although there are over 70 different ""versions"" of definitions). The nature of intelligence is still in darkness. In this work we do not take any of these three traditional approaches, instead we try to identify certain fundamental aspects of the nature of intelligence, and construct a mathematical model to represent and potentially reproduce these fundamental aspects. We first stress the importance of defining the scope of discussion and granularity of investigation. We carefully compare human and artificial intelligence, and qualitatively demonstrate an information abstraction process, which we propose to be the key to connect perception and cognition. We then present the broader idea of ""concept"", separate the idea of self model out of the world model, and construct a new model called world-self model (WSM). We show the mechanisms of creating and connecting concepts, and the flow of how the WSM receives, processes and outputs information with respect to an arbitrary type of problem to solve. We also consider and discuss the potential computer implementation issues of the proposed theoretical framework, and finally we propose a unified general framework of intelligence based on WSM.",A World-Self Model Towards Understanding Intelligence,Yutao Yue,2022,Artificial Intelligence,2203.13762
"Planning under uncertainty is an area of interest in artificial intelligence. We present a novel approach based on tree search and graph machine learning for the scheduling problem known as Disjunctive Temporal Networks with Uncertainty (DTNU). Dynamic Controllability (DC) of DTNUs seeks a reactive scheduling strategy to satisfy temporal constraints in response to uncontrollable action durations. We introduce new semantics for reactive scheduling: Time-based Dynamic Controllability (TDC) and a restricted subset of TDC, R-TDC. We design a tree search algorithm to determine whether or not a DTNU is R-TDC. Moreover, we leverage a graph neural network as a heuristic for tree search guidance. Finally, we conduct experiments on a known benchmark on which we show R-TDC to retain significant completeness with regard to DC, while being faster to prove. This results in the tree search processing fifty percent more DTNU problems in R-TDC than the state-of-the-art DC solver does in DC with the same time budget. We also observe that graph neural network search guidance leads to substantial performance gains on benchmarks of more complex DTNUs, with up to eleven times more problems solved than the baseline tree search.",Solving Disjunctive Temporal Networks with Uncertainty under Restricted Time-Based Controllability using Tree Search and Graph Neural Networks,"Kevin Osanlou, Jeremy Frank, Andrei Bursuc, Tristan Cazenave, Eric Jacopin, Christophe Guettier and J. Benton",2022,Artificial Intelligence,2203.15030
"Wind energy has emerged as a highly promising source of renewable energy in recent times. However, wind turbines regularly suffer from operational inconsistencies, leading to significant costs and challenges in operations and maintenance (O&M). Condition-based monitoring (CBM) and performance assessment/analysis of turbines are vital aspects for ensuring efficient O&M planning and cost minimisation. Data-driven decision making techniques have witnessed rapid evolution in the wind industry for such O&M tasks during the last decade, from applying signal processing methods in early 2010 to artificial intelligence (AI) techniques, especially deep learning in 2020. In this article, we utilise statistical computing to present a scientometric review of the conceptual and thematic evolution of AI in the wind energy sector, providing evidence-based insights into present strengths and limitations of data-driven decision making in the wind industry. We provide a perspective into the future and on current key challenges in data availability and quality, lack of transparency in black box-natured AI models, and prevailing issues in deploying models for real-time decision support, along with possible strategies to overcome these problems. We hope that a systematic analysis of the past, present and future of CBM and performance assessment can encourage more organisations to adopt data-driven decision making techniques in O&M towards making wind energy sources more reliable, contributing to the global efforts of tackling climate change.","Scientometric Review of Artificial Intelligence for Operations & Maintenance of Wind Turbines: The Past, Present and Future","Joyjit Chatterjee, Nina Dethlefs",2021,Artificial Intelligence,2204.02360
"Existing approaches to learning to prove theorems focus on particular logics and datasets. In this work, we propose Monte-Carlo simulations guided by reinforcement learning that can work in an arbitrarily specified logic, without any human knowledge or set of problems. Since the algorithm does not need any training dataset, it is able to learn to work with any logical foundation, even when there is no body of proofs or even conjectures available. We practically demonstrate the feasibility of the approach in multiple logical systems. The approach is stronger than training on randomly generated data but weaker than the approaches trained on tailored axiom and conjecture sets. It however allows us to apply machine learning to automated theorem proving for many logics, where no such attempts have been tried to date, such as intuitionistic logic or linear logic.",Adversarial Learning to Reason in an Arbitrary Logic,Stanis{\l}aw J. Purga{\l} and Cezary Kaliszyk,2022,Artificial Intelligence,2204.02737
"Accurate estimation of post-click conversion rate is critical for building recommender systems, which has long been confronted with sample selection bias and data sparsity issues. Methods in the Entire Space Multi-task Model (ESMM) family leverage the sequential pattern of user actions, i.e. $impression\rightarrow click \rightarrow conversion$ to address data sparsity issue. However, they still fail to ensure the unbiasedness of CVR estimates. In this paper, we theoretically demonstrate that ESMM suffers from the following two problems: (1) Inherent Estimation Bias (IEB), where the estimated CVR of ESMM is inherently higher than the ground truth; (2) Potential Independence Priority (PIP) for CTCVR estimation, where there is a risk that the ESMM overlooks the causality from click to conversion. To this end, we devise a principled approach named Entire Space Counterfactual Multi-task Modelling (ESCM$^2$), which employs a counterfactual risk miminizer as a regularizer in ESMM to address both IEB and PIP issues simultaneously. Extensive experiments on offline datasets and online environments demonstrate that our proposed ESCM$^2$ can largely mitigate the inherent IEB and PIP issues and achieve better performance than baseline models.",ESCM$^2$: Entire Space Counterfactual Multi-Task Model for Post-Click Conversion Rate Estimation,"Hao Wang, Tai-Wei Chang, Tianqiao Liu, Jianmin Huang, Zhichao Chen, Chao Yu, Ruopeng Li, Wei Chu",2022,Artificial Intelligence,2204.05125
"Creative Problem Solving (CPS) is a sub-area within Artificial Intelligence (AI) that focuses on methods for solving off-nominal, or anomalous problems in autonomous systems. Despite many advancements in planning and learning, resolving novel problems or adapting existing knowledge to a new context, especially in cases where the environment may change in unpredictable ways post deployment, remains a limiting factor in the safe and useful integration of intelligent systems. The emergence of increasingly autonomous systems dictates the necessity for AI agents to deal with environmental uncertainty through creativity. To stimulate further research in CPS, we present a definition and a framework of CPS, which we adopt to categorize existing AI methods in this field. Our framework consists of four main components of a CPS problem, namely, 1) problem formulation, 2) knowledge representation, 3) method of knowledge manipulation, and 4) method of evaluation. We conclude our survey with open research questions, and suggested directions for the future.",Creative Problem Solving in Artificially Intelligent Agents: A Survey and Framework,"Evana Gizzi, Lakshmi Nair, Sonia Chernova, Jivko Sinapov",2022,Artificial Intelligence,2204.10358
"Counterfactual (CF) explanations have been employed as one of the modes of explainability in explainable AI-both to increase the transparency of AI systems and to provide recourse. Cognitive science and psychology, however, have pointed out that people regularly use CFs to express causal relationships. Most AI systems are only able to capture associations or correlations in data so interpreting them as casual would not be justified. In this paper, we present two experiment (total N = 364) exploring the effects of CF explanations of AI system's predictions on lay people's causal beliefs about the real world. In Experiment 1 we found that providing CF explanations of an AI system's predictions does indeed (unjustifiably) affect people's causal beliefs regarding factors/features the AI uses and that people are more likely to view them as causal factors in the real world. Inspired by the literature on misinformation and health warning messaging, Experiment 2 tested whether we can correct for the unjustified change in causal beliefs. We found that pointing out that AI systems capture correlations and not necessarily causal relationships can attenuate the effects of CF explanations on people's causal beliefs.","Can counterfactual explanations of AI systems' predictions skew lay users' causal intuitions about the world? If so, can we correct for that?","Marko Tesic, Ulrike Hahn",2022,Artificial Intelligence,2205.06241
"In this paper, we revisit the solving bias when evaluating models on current Math Word Problem (MWP) benchmarks. However, current solvers exist solving bias which consists of data bias and learning bias due to biased dataset and improper training strategy. Our experiments verify MWP solvers are easy to be biased by the biased training datasets which do not cover diverse questions for each problem narrative of all MWPs, thus a solver can only learn shallow heuristics rather than deep semantics for understanding problems. Besides, an MWP can be naturally solved by multiple equivalent equations while current datasets take only one of the equivalent equations as ground truth, forcing the model to match the labeled ground truth and ignoring other equivalent equations. Here, we first introduce a novel MWP dataset named UnbiasedMWP which is constructed by varying the grounded expressions in our collected data and annotating them with corresponding multiple new questions manually. Then, to further mitigate learning bias, we propose a Dynamic Target Selection (DTS) Strategy to dynamically select more suitable target expressions according to the longest prefix match between the current model output and candidate equivalent equations which are obtained by applying commutative law during training. The results show that our UnbiasedMWP has significantly fewer biases than its original data and other datasets, posing a promising benchmark for fairly evaluating the solvers' reasoning skills rather than matching nearest neighbors. And the solvers trained with our DTS achieve higher accuracies on multiple MWP benchmarks. The source code is available at https://github.com/yangzhch6/UnbiasedMWP.",Unbiased Math Word Problems Benchmark for Mitigating Solving Bias,"Zhicheng Yang, Jinghui Qin, Jiaqi Chen, and Xiaodan Liang",2022,Artificial Intelligence,2205.08108
"Recently, deep learning models have made great progress in MWP solving on answer accuracy. However, they are uninterpretable since they mainly rely on shallow heuristics to achieve high performance without understanding and reasoning the grounded math logic. To address this issue and make a step towards interpretable MWP solving, we first construct a high-quality MWP dataset named InterMWP which consists of 11,495 MWPs and annotates interpretable logical formulas based on algebraic knowledge as the grounded linguistic logic of each solution equation. Different from existing MWP datasets, our InterMWP benchmark asks for a solver to not only output the solution expressions but also predict the corresponding logical formulas. We further propose a novel approach with logical prompt and interpretation generation, called LogicSolver. For each MWP, our LogicSolver first retrieves some highly-correlated algebraic knowledge and then passes them to the backbone model as prompts to improve the semantic representations of MWPs. With these improved semantic representations, our LogicSolver generates corresponding solution expressions and interpretable knowledge formulas in accord with the generated solution expressions, simultaneously. Experimental results show that our LogicSolver has stronger logical formula-based interpretability than baselines while achieving higher answer accuracy with the help of logical prompts, simultaneously. The source code and dataset is available at https://github.com/yangzhch6/InterMWP.",LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning,"Zhicheng Yang, Jinghui Qin, Jiaqi Chen, Liang Lin and Xiaodan Liang",2022,Artificial Intelligence,2205.08232
"This paper presents the power network reconfiguration algorithm HATSGA with a ""R"" modeling approach and evaluates its behavior in computing new reconfiguration topologies for the power network in the Smart Grid context. The modeling of the power distribution network with the language ""R"" is used to represent the network and support the computation of distinct algorithm configurations towards the evaluation of new reconfiguration topologies. The HATSGA algorithm adopts a hybrid Tabu Search and Genetic Algorithm strategy and can be configured in different ways to compute network reconfiguration solutions. The evaluation of power loss with HATSGA uses the IEEE 14-Bus topology as the power test scenario. The evaluation of reconfiguration topologies with minimum power loss with HATSGA indicates that an efficient solution can be reached with a feasible computational time. This suggests that HATSGA can be potentially used for computing reconfiguration network topologies and, beyond that, it can be used for autonomic self-healing management approaches where a feasible computational time is required.",On Evaluating Power Loss with HATSGA Algorithm for Power Network Reconfiguration in the Smart Grid,"Flavio Galvao Calhau, Alysson Pezzutti and Joberto S. B. Martins",2017,Artificial Intelligence,2205.10126
"Optimal experimental design seeks to determine the most informative allocation of experiments to infer an unknown statistical quantity. In this work, we investigate the optimal design of experiments for {\em estimation of linear functionals in reproducing kernel Hilbert spaces (RKHSs)}. This problem has been extensively studied in the linear regression setting under an estimability condition, which allows estimating parameters without bias. We generalize this framework to RKHSs, and allow for the linear functional to be only approximately inferred, i.e., with a fixed bias. This scenario captures many important modern applications, such as estimation of gradient maps, integrals, and solutions to differential equations. We provide algorithms for constructing bias-aware designs for linear functionals. We derive non-asymptotic confidence sets for fixed and adaptive designs under sub-Gaussian noise, enabling us to certify estimation with bounded error with high probability.",Experimental Design for Linear Functionals in Reproducing Kernel Hilbert Spaces,Mojm\'ir Mutn\'y and Andreas Krause,2022,Artificial Intelligence,2205.13627
"Static authentication methods, like passwords, grow increasingly weak with advancements in technology and attack strategies. Continuous authentication has been proposed as a solution, in which users who have gained access to an account are still monitored in order to continuously verify that the user is not an imposter who had access to the user credentials. Mouse dynamics is the behavior of a users mouse movements and is a biometric that has shown great promise for continuous authentication schemes. This article builds upon our previous published work by evaluating our dataset of 40 users using three machine learning and deep learning algorithms. Two evaluation scenarios are considered: binary classifiers are used for user authentication, with the top performer being a 1-dimensional convolutional neural network with a peak average test accuracy of 85.73% across the top 10 users. Multi class classification is also examined using an artificial neural network which reaches an astounding peak accuracy of 92.48% the highest accuracy we have seen for any classifier on this dataset.",Machine and Deep Learning Applications to Mouse Dynamics for Continuous User Authentication,"Nyle Siddiqui, Rushit Dave, Naeem Seliya, Mounika Vanamala",2022,Artificial Intelligence,2205.13646
"Artificial Intelligence in higher education opens new possibilities for improving the lecturing process, such as enriching didactic materials, helping in assessing students' works or even providing directions to the teachers on how to enhance the lectures. We follow this research path, and in this work, we explore how an academic lecture can be assessed automatically by quantitative features. First, we prepare a set of qualitative features based on teaching practices and then annotate the dataset of academic lecture videos collected for this purpose. We then show how these features could be detected automatically using machine learning and computer vision techniques. Our results show the potential usefulness of our work.",A Deep Learning Approach for Automatic Detection of Qualitative Features of Lecturing,"Anna Wroblewska, Jozef Jasek, Bogdan Jastrzebski, Stanislaw Pawlak, Anna Grzywacz, Cheong Siew Ann, Tan Seng Chee, Tomasz Trzcinski, Janusz Holyst",2022,Artificial Intelligence,2205.14919
"With Artificial intelligence (AI) to aid or automate decision-making advancing rapidly, a particular concern is its fairness. In order to create reliable, safe and trustworthy systems through human-centred artificial intelligence (HCAI) design, recent efforts have produced user interfaces (UIs) for AI experts to investigate the fairness of AI models. In this work, we provide a design space exploration that supports not only data scientists but also domain experts to investigate AI fairness. Using loan applications as an example, we held a series of workshops with loan officers and data scientists to elicit their requirements. We instantiated these requirements into FairHIL, a UI to support human-in-the-loop fairness investigations, and describe how this UI could be generalized to other use cases. We evaluated FairHIL through a think-aloud user study. Our work contributes better designs to investigate an AI model's fairness-and move closer towards responsible AI.",Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness,Yuri Nakao and Lorenzo Strappelli and Simone Stumpf and Aisha Naseer and Daniele Regoli and Giulia Del Gamba,2022,Artificial Intelligence,2206.00474
"We formalise some aspects of the neural-symbol design patterns of van Bekkum et al., such that we can formally define notions of refinement of patterns, as well as modular combination of larger patterns from smaller building blocks. These formal notions are being implemented in the heterogeneous tool set (Hets), such that patterns and refinements can be checked for well-formedness, and combinations can be computed.",Modular design patterns for neural-symbolic integration: refinement and combination,Till Mossakowski,2022,Artificial Intelligence,2206.04724
"The Hierarchical Task Network ({\sf HTN}) formalism is very expressive and used to express a wide variety of planning problems. In contrast to the classical {\sf STRIPS} formalism in which only the action model needs to be specified, the {\sf HTN} formalism requires to specify, in addition, the tasks of the problem and their decomposition into subtasks, called {\sf HTN} methods. For this reason, hand-encoding {\sf HTN} problems is considered more difficult and more error-prone by experts than classical planning problem. To tackle this problem, we propose a new approach (HierAMLSI) based on grammar induction to acquire {\sf HTN} planning domain knowledge, by learning action models and {\sf HTN} methods with their preconditions. Unlike other approaches, HierAMLSI is able to learn both actions and methods with noisy and partial inputs observation with a high level or accuracy.",An Accurate HDDL Domain Learning Algorithm from Partial and Noisy Observations,"M. Grand, H. Fiorino and D. Pellier",2022,Artificial Intelligence,2206.06882
"The Hierarchical Task Network (HTN) formalism is used to express a wide variety of planning problems in terms of decompositions of tasks into subtaks. Many techniques have been proposed to solve such hierarchical planning problems. A particular technique is to encode hierarchical planning problems as classical STRIPS planning problems. One advantage of this technique is to benefit directly from the constant improvements made by STRIPS planners. However, there are still few effective and expressive encodings. In this paper, we present a new HTN to STRIPS encoding allowing to generate concurrent plans. We show experimentally that this encoding outperforms previous approaches on hierarchical IPC benchmarks.",An Efficient HTN to STRIPS Encoding for Concurrent Plans,"N. Cavrel, D. Pellier, H. Fiorino",2022,Artificial Intelligence,2206.07084
"A large number of people suffer from life-threatening cardiac abnormalities, and electrocardiogram (ECG) analysis is beneficial to determining whether an individual is at risk of such abnormalities. Automatic ECG classification methods, especially the deep learning based ones, have been proposed to detect cardiac abnormalities using ECG records, showing good potential to improve clinical diagnosis and help early prevention of cardiovascular diseases. However, the predictions of the known neural networks still do not satisfactorily meet the needs of clinicians, and this phenomenon suggests that some information used in clinical diagnosis may not be well captured and utilized by these methods. In this paper, we introduce some rules into convolutional neural networks, which help present clinical knowledge to deep learning based ECG analysis, in order to improve automated ECG diagnosis performance. Specifically, we propose a Handcrafted-Rule-enhanced Neural Network (called HRNN) for ECG classification with standard 12-lead ECG input, which consists of a rule inference module and a deep learning module. Experiments on two large-scale public ECG datasets show that our new approach considerably outperforms existing state-of-the-art methods. Further, our proposed approach not only can improve the diagnosis performance, but also can assist in detecting mislabelled ECG samples. Our codes are available at https://github.com/alwaysbyx/ecg_processing.",Identifying Electrocardiogram Abnormalities Using a Handcrafted-Rule-Enhanced Neural Network,"Yuexin Bian, Jintai Chen, Xiaojun Chen, Xiaoxian Yang, Danny Z. Chen, JIan Wu",2022,Artificial Intelligence,2206.10592
"Object-centric process mining is a new paradigm with more realistic assumptions about underlying data by considering several case notions, e.g., an order handling process can be analyzed based on order, item, package, and route case notions. Including many case notions can result in a very complex model. To cope with such complexity, this paper introduces a new approach to cluster similar case notions based on Markov Directly-Follow Multigraph, which is an extended version of the well-known Directly-Follow Graph supported by many industrial and academic process mining tools. This graph is used to calculate a similarity matrix for discovering clusters of similar case notions based on a threshold. A threshold tuning algorithm is also defined to identify sets of different clusters that can be discovered based on different levels of similarity. Thus, the cluster discovery will not rely on merely analysts' assumptions. The approach is implemented and released as a part of a python library, called processmining, and it is evaluated through a Purchase to Pay (P2P) object-centric event log file. Some discovered clusters are evaluated by discovering Directly Follow-Multigraph by flattening the log based on the clusters. The similarity between identified clusters is also evaluated by calculating the similarity between the behavior of the process models discovered for each case notion using inductive miner based on footprints conformance checking.",Object Type Clustering using Markov Directly-Follow Multigraph in Object-Centric Process Mining,Amin Jalali,2022,Artificial Intelligence,2206.11017
"""Unless and until our society recognizes cyber bullying for what it is, the suffering of thousands of silent victims will continue."" ~ Anna Maria Chavez. There had been series of research on cyber bullying which are unable to provide reliable solution to cyber bullying. In this research work, we were able to provide a permanent solution to this by developing a model capable of detecting and intercepting bullying incoming and outgoing messages with 92% accuracy. We also developed a chatbot automation messaging system to test our model leading to the development of Artificial Intelligence powered anti-cyber bullying system using machine learning algorithm of Multinomial Naive Bayes (MNB) and optimized linear Support Vector Machine (SVM). Our model is able to detect and intercept bullying outgoing and incoming bullying messages and take immediate action.",AI Powered Anti-Cyber Bullying System using Machine Learning Algorithm of Multinomial Naive Bayes and Optimized Linear Support Vector Machine,"Tosin Ige, Sikiru Adewale",2022,Artificial Intelligence,2207.11897
"Current AI systems are designed to solve close-world problems with the assumption that the underlying world is remaining more or less the same. However, when dealing with real-world problems such assumptions can be invalid as sudden and unexpected changes can occur. To effectively deploy AI-powered systems in the real world, AI systems should be able to deal with open-world novelty quickly. Inevitably, dealing with open-world novelty raises an important question of novelty difficulty. Knowing whether one novelty is harder to deal with than another, can help researchers to train their systems systematically. In addition, it can also serve as a measurement of the performance of novelty robust AI systems. In this paper, we propose to define the novelty reaction difficulty as a relative difficulty of performing the known task after the introduction of the novelty. We propose a universal method that can be applied to approximate the difficulty. We present the approximations of the difficulty using our method and show how it aligns with the results of the evaluation of AI agents designed to deal with novelty.",Measuring Difficulty of Novelty Reaction,"Ekaterina Nikonova, Cheng Xue, Vimukthini Pinto, Chathura Gamage, Peng Zhang, Jochen Renz",2022,Artificial Intelligence,2207.13857
"In this work, we consider the problem of procedural content generation for video game levels. Prior approaches have relied on evolutionary search (ES) methods capable of generating diverse levels, but this generation procedure is slow, which is problematic in real-time settings. Reinforcement learning (RL) has also been proposed to tackle the same problem, and while level generation is fast, training time can be prohibitively expensive. We propose a framework to tackle the procedural content generation problem that combines the best of ES and RL. In particular, our approach first uses ES to generate a sequence of levels evolved over time, and then uses behaviour cloning to distil these levels into a policy, which can then be queried to produce new levels quickly. We apply our approach to a maze game and Super Mario Bros, with our results indicating that our approach does in fact decrease the time required for level generation, especially when an increasing number of valid levels are required.",Combining Evolutionary Search with Behaviour Cloning for Procedurally Generated Content,"Nicholas Muir, Steven James",2022,Artificial Intelligence,2207.14772
"A ''technology lottery'' describes a research idea or technology succeeding over others because it is suited to the available software and hardware, not necessarily because it is superior to alternative directions--examples abound, from the synergies of deep learning and GPUs to the disconnect of urban design and autonomous vehicles. The nascent field of Self-Driving Laboratories (SDL), particularly those implemented as Materials Acceleration Platforms (MAPs), is at risk of an analogous pitfall: the next logical step for building MAPs is to take existing lab equipment and workflows and mix in some AI and automation. In this whitepaper, we argue that the same simulation and AI tools that will accelerate the search for new materials, as part of the MAPs research program, also make possible the design of fundamentally new computing mediums. We need not be constrained by existing biases in science, mechatronics, and general-purpose computing, but rather we can pursue new vectors of engineering physics with advances in cyber-physical learning and closed-loop, self-optimizing systems. Here we outline a simulation-based MAP program to design computers that use physics itself to solve optimization problems. Such systems mitigate the hardware-software-substrate-user information losses present in every other class of MAPs and they perfect alignment between computing problems and computing mediums eliminating any technology lottery. We offer concrete steps toward early ''Physical Computing (PC) -MAP'' advances and the longer term cyber-physical R&D which we expect to introduce a new era of innovative collaboration between materials researchers and computer scientists.",Physical Computing for Materials Acceleration Platforms,"Erik Peterson, Alexander Lavin",2022,Artificial Intelligence,2208.08566
"We describe the first steps in the development of an artificial agent focused on the Brazilian maritime territory, a large region within the South Atlantic also known as the Blue Amazon. The ""BLue Amazon Brain"" (BLAB) integrates a number of services aimed at disseminating information about this region and its importance, functioning as a tool for environmental awareness. The main service provided by BLAB is a conversational facility that deals with complex questions about the Blue Amazon, called BLAB-Chat; its central component is a controller that manages several task-oriented natural language processing modules (e.g., question answering and summarizer systems). These modules have access to an internal data lake as well as to third-party databases. A news reporter (BLAB-Reporter) and a purposely-developed wiki (BLAB-Wiki) are also part of the BLAB service architecture. In this paper, we describe our current version of BLAB's architecture (interface, backend, web services, NLP modules, and resources) and comment on the challenges we have faced so far, such as the lack of training data and the scattered state of domain information. Solving these issues presents a considerable challenge in the development of artificial intelligence for technical domains.",The BLue Amazon Brain (BLAB): A Modular Architecture of Services about the Brazilian Maritime Territory,"Paulo Pirozelli, Ais B. R. Castro, Ana Luiza C. de Oliveira, Andr\'e S. Oliveira, Fl\'avio N. Ca\c{c}\~ao, Igor C. Silveira, Jo\~ao G. M. Campos, Laura C. Motheo, Leticia F. Figueiredo, Lucas F. A. O. Pellicer, Marcelo A. Jos\'e, Marcos M. Jos\'e, Pedro de M. Ligabue, Ricardo S. Grava, Rodrigo M. Tavares, Vin\'icius B. Matos, Yan V. Sym, Anna H. R. Costa, Anarosa A. F. Brand\~ao, Denis D. Mau\'a, Fabio G. Cozman, Sarajane M. Peres",2022,Artificial Intelligence,2209.07928
"Reasoning is a fundamental problem for computers and deeply studied in Artificial Intelligence. In this paper, we specifically focus on answering multi-hop logical queries on Knowledge Graphs (KGs). This is a complicated task because, in real-world scenarios, the graphs tend to be large and incomplete. Most previous works have been unable to create models that accept full First-Order Logical (FOL) queries, which include negative queries, and have only been able to process a limited set of query structures. Additionally, most methods present logic operators that can only perform the logical operation they are made for. We introduce a set of models that use Neural Networks to create one-point vector embeddings to answer the queries. The versatility of neural networks allows the framework to handle FOL queries with Conjunction ($\wedge$), Disjunction ($\vee$) and Negation ($\neg$) operators. We demonstrate experimentally the performance of our model through extensive experimentation on well-known benchmarking datasets. Besides having more versatile operators, the models achieve a 10\% relative increase over the best performing state of the art and more than 30\% over the original method based on single-point vector embeddings.",Neural Methods for Logical Reasoning Over Knowledge Graphs,"Alfonso Amayuelas, Shuai Zhang, Susie Xi Rao, Ce Zhang",2022,Artificial Intelligence,2209.14464
"We consider graph modeling for a knowledge graph for vehicle development, with a focus on crash safety. An organized schema that incorporates information from various structured and unstructured data sources is provided, which includes relevant concepts within the domain. In particular, we propose semantics for crash computer aided engineering (CAE) data, which enables searchability, filtering, recommendation, and prediction for crash CAE data during the development process. This graph modeling considers the CAE data in the context of the R\&D development process and vehicle safety. Consequently, we connect CAE data to the protocols that are used to assess vehicle safety performances. The R\&D process includes CAD engineering and safety attributes, with a focus on multidisciplinary problem-solving. We describe previous efforts in graph modeling in comparison to our proposal, discuss its strengths and limitations, and identify areas for future work.",Graph Modeling in Computer Assisted Automotive Development,"Anahita Pakiman, Jochen Garcke",2022,Artificial Intelligence,2209.14910
"Knowledge graph embedding aims to predict the missing relations between entities in knowledge graphs. Tensor-decomposition-based models, such as ComplEx, provide a good trade-off between efficiency and expressiveness, that is crucial because of the large size of real world knowledge graphs. The recent multi-partition embedding interaction (MEI) model subsumes these models by using the block term tensor format and provides a systematic solution for the trade-off. However, MEI has several drawbacks, some of which carried from its subsumed tensor-decomposition-based models. In this paper, we address these drawbacks and introduce the Multi-partition Embedding Interaction iMproved beyond block term format (MEIM) model, with independent core tensor for ensemble effects and soft orthogonality for max-rank mapping, in addition to multi-partition embedding. MEIM improves expressiveness while still being highly efficient, helping it to outperform strong baselines and achieve state-of-the-art results on difficult link prediction benchmarks using fairly small embedding sizes. The source code is released at https://github.com/tranhungnghiep/MEIM-KGE.",MEIM: Multi-partition Embedding Interaction Beyond Block Term Format for Efficient and Expressive Link Prediction,"Hung Nghiep Tran, Atsuhiro Takasu",2022,Artificial Intelligence,2209.15597
"Surrogate modeling has brought about a revolution in computation in the branches of science and engineering. Backed by Artificial Intelligence, a surrogate model can present highly accurate results with a significant reduction in computation time than computer simulation of actual models. Surrogate modeling techniques have found their use in numerous branches of science and engineering, energy system modeling being one of them. Since the idea of hybrid and sustainable energy systems is spreading rapidly in the modern world for the paradigm of the smart energy shift, researchers are exploring the future application of artificial intelligence-based surrogate modeling in analyzing and optimizing hybrid energy systems. One of the promising technologies for assessing applicability for the energy system is the digital twin, which can leverage surrogate modeling. This work presents a comprehensive framework/review on Artificial Intelligence-driven surrogate modeling and its applications with a focus on the digital twin framework and energy systems. The role of machine learning and artificial intelligence in constructing an effective surrogate model is explained. After that, different surrogate models developed for different sustainable energy sources are presented. Finally, digital twin surrogate models and associated uncertainties are described.",Digital Twin and Artificial Intelligence Incorporated With Surrogate Modeling for Hybrid and Sustainable Energy Systems,"Abid Hossain Khan, Salauddin Omar, Nadia Mushtary, Richa Verma, Dinesh Kumar, Syed Alam",2022,Artificial Intelligence,2210.00073
"The ability to reuse previous policies is an important aspect of human intelligence. To achieve efficient policy reuse, a Deep Reinforcement Learning (DRL) agent needs to decide when to reuse and which source policies to reuse. Previous methods solve this problem by introducing extra components to the underlying algorithm, such as hierarchical high-level policies over source policies, or estimations of source policies' value functions on the target task. However, training these components induces either optimization non-stationarity or heavy sampling cost, significantly impairing the effectiveness of transfer. To tackle this problem, we propose a novel policy reuse algorithm called Critic-gUided Policy reuse (CUP), which avoids training any extra components and efficiently reuses source policies. CUP utilizes the critic, a common component in actor-critic methods, to evaluate and choose source policies. At each state, CUP chooses the source policy that has the largest one-step improvement over the current target policy, and forms a guidance policy. The guidance policy is theoretically guaranteed to be a monotonic improvement over the current target policy. Then the target policy is regularized to imitate the guidance policy to perform efficient policy search. Empirical results demonstrate that CUP achieves efficient transfer and significantly outperforms baseline algorithms.",CUP: Critic-Guided Policy Reuse,"Jin Zhang, Siyuan Li, Chongjie Zhang",2022,Artificial Intelligence,2210.08153
"Ontology development methodologies emphasise knowledge gathering from domain experts and documentary resources, and knowledge representation using an ontology language such as OWL or FOL. However, working ontologists are often surprised by how challenging and slow it can be to develop ontologies. Here, with a particular emphasis on the sorts of ontologies that are content-heavy and intended to be shared across a community of users (reference ontologies), we propose that a significant and heretofore under-emphasised contributor of challenges during ontology development is the need to create, or bring about, consensus in the face of disagreement. For this reason reference ontology development cannot be automated, at least within the limitations of existing AI approaches. Further, for the same reason ontologists are required to have specific social-negotiating skills which are currently lacking in most technical curricula.","Ontology Development is Consensus Creation, Not (Merely) Representation",Fabian Neuhaus and Janna Hastings,2022,Artificial Intelligence,2210.12026
"In this paper, we propose a comprehensive benchmark to investigate models' logical reasoning capabilities in complex real-life scenarios. Current explanation datasets often employ synthetic data with simple reasoning structures. Therefore, it cannot express more complex reasoning processes, such as the rebuttal to a reasoning step and the degree of certainty of the evidence. To this end, we propose a comprehensive logical reasoning explanation form. Based on the multi-hop chain of reasoning, the explanation form includes three main components: (1) The condition of rebuttal that the reasoning node can be challenged; (2) Logical formulae that uncover the internal texture of reasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The fine-grained structure conforms to the real logical reasoning scenario, better fitting the human cognitive process but, simultaneously, is more challenging for the current models. We evaluate the current best models' performance on this new explanation form. The experimental results show that generating reasoning graphs remains a challenging task for current models, even with the help of giant pre-trained language models.",MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure,"Yinya Huang, Hongming Zhang, Ruixin Hong, Xiaodan Liang, Changshui Zhang and Dong Yu",2022,Artificial Intelligence,2210.12487
"Explainable Artificial Intelligence (XAI) is a paradigm that delivers transparent models and decisions, which are easy to understand, analyze, and augment by a non-technical audience. Fuzzy Logic Systems (FLS) based XAI can provide an explainable framework, while also modeling uncertainties present in real-world environments, which renders it suitable for applications where explainability is a requirement. However, most real-life processes are not characterized by high levels of uncertainties alone; they are inherently time-dependent as well, i.e., the processes change with time. In this work, we present novel Temporal Type-2 FLS Based Approach for time-dependent XAI (TXAI) systems, which can account for the likelihood of a measurement's occurrence in the time domain using (the measurement's) frequency of occurrence. In Temporal Type-2 Fuzzy Sets (TT2FSs), a four-dimensional (4D) time-dependent membership function is developed where relations are used to construct the inter-relations between the elements of the universe of discourse and its frequency of occurrence. The TXAI system manifested better classification prowess, with 10-fold test datasets, with a mean recall of 95.40\% than a standard XAI system (based on non-temporal general type-2 (GT2) fuzzy sets) that had a mean recall of 87.04\%. TXAI also performed significantly better than most non-explainable AI systems between 3.95\%, to 19.04\% improvement gain in mean recall. In addition, TXAI can also outline the most likely time-dependent trajectories using the frequency of occurrence values embedded in the TXAI model; viz. given a rule at a determined time interval, what will be the next most likely rule at a subsequent time interval. In this regard, the proposed TXAI system can have profound implications for delineating the evolution of real-life time-dependent processes, such as behavioural or biological processes.",A Temporal Type-2 Fuzzy System for Time-dependent Explainable Artificial Intelligence,"Mehrin Kiani, Javier Andreu-Perez, Hani Hagras",2022,Artificial Intelligence,2210.12571
"One of the most important AI research questions is to trade off computation versus performance since ``perfect rationality"" exists in theory but is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant performance improvement in various challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that spends more search time on harder states and less search time on simpler states adaptively. We give theoretical bounds of the proposed method and evaluate the performance and computations on $9 \times 9$ Go board games and Atari games. Experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\%$ search time on average. We believe that this approach is a viable alternative for tasks under limited time and resources. The code is available at \url{https://github.com/YeWR/V-MCTS.git}.",Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions,"Weirui Ye, Pieter Abbeel, Yang Gao",2022,Artificial Intelligence,2210.12628
"Sparsity of formal knowledge and roughness of non-ontological construction make sparsity problem particularly prominent in Open Knowledge Graphs (OpenKGs). Due to sparse links, learning effective representation for few-shot entities becomes difficult. We hypothesize that by introducing negative samples, a contrastive learning (CL) formulation could be beneficial in such scenarios. However, existing CL methods model KG triplets as binary objects of entities ignoring the relation-guided ternary propagation patterns and they are too generic, i.e., they ignore zero-shot, few-shot and synonymity problems that appear in OpenKGs. To address this, we propose TernaryCL, a CL framework based on ternary propagation patterns among head, relation and tail. TernaryCL designs Contrastive Entity and Contrastive Relation to mine ternary discriminative features with both negative entities and relations, introduces Contrastive Self to help zero- and few-shot entities learn discriminative features, Contrastive Synonym to model synonymous entities, and Contrastive Fusion to aggregate graph features from multiple paths. Extensive experiments on benchmarks demonstrate the superiority of TernaryCL over state-of-the-art models.",Alleviating Sparsity of Open Knowledge Graphs with Ternary Contrastive Learning,"Qian Li, Shafiq Joty, Daling Wang, Shi Feng and Yifei Zhang",2022,Artificial Intelligence,2211.03950
"In this paper, we propose a new method for knowledge base completion (KBC): instance-based learning (IBL). For example, to answer (Jill Biden, lived city,? ), instead of going directly to Washington D.C., our goal is to find Joe Biden, who has the same lived city as Jill Biden. Through prototype entities, IBL provides interpretability. We develop theories for modeling prototypes and combining IBL with translational models. Experiments on various tasks confirmed the IBL model's effectiveness and interpretability. In addition, IBL shed light on the mechanism of rule-based KBC models. Previous research has generally agreed that rule-based models provide rules with semantically compatible premises and hypotheses. We challenge this view. We begin by demonstrating that some logical rules represent {\it instance-based equivalence} (i.e. prototypes) rather than semantic compatibility. These are denoted as {\it IBL rules}. Surprisingly, despite occupying only a small portion of the rule space, IBL rules outperform non-IBL rules in all four benchmarks. We use a variety of experiments to demonstrate that rule-based models work because they have the ability to represent instance-based equivalence via IBL rules. The findings provide new insights of how rule-based models work and how to interpret their rules.",Instance-based Learning for Knowledge Base Completion,"Wanyun Cui, Xingran Chen",2022,Artificial Intelligence,2211.06807
"The present paper comes across the main steps that laid from Zadeh's fuzziness ana Atanassov's intuitionistic fuzzy sets to Smarandache's indeterminacy and to Molodstov's soft sets. Two hybrid methods for assessment and decision making respectively under fuzzy conditions are also presented through suitable examples that use soft sets and real intervals as tools. The decision making method improves an earlier method of Maji et al. Further, it is described how the concept of topological space, the most general category of mathematical spaces, can be extended to fuzzy structures and how to generalize the fundamental mathematical concepts of limit, continuity compactness and Hausdorff space within such kind of structures. In particular, fuzzy and soft topological spaces are defined and examples are given to illustrate these generalizations.","Fuzziness, Indeterminacy and Soft Sets: Frontiers and Perspectives",Michael Gr. Voskoglou,2022,Artificial Intelligence,2211.15408
"In recent years, unmanned aerial vehicle (UAV) related technology has expanded knowledge in the area, bringing to light new problems and challenges that require solutions. Furthermore, because the technology allows processes usually carried out by people to be automated, it is in great demand in industrial sectors. The automation of these vehicles has been addressed in the literature, applying different machine learning strategies. Reinforcement learning (RL) is an automation framework that is frequently used to train autonomous agents. RL is a machine learning paradigm wherein an agent interacts with an environment to solve a given task. However, learning autonomously can be time consuming, computationally expensive, and may not be practical in highly-complex scenarios. Interactive reinforcement learning allows an external trainer to provide advice to an agent while it is learning a task. In this study, we set out to teach an RL agent to control a drone using reward-shaping and policy-shaping techniques simultaneously. Two simulated scenarios were proposed for the training; one without obstacles and one with obstacles. We also studied the influence of each technique. The results show that an agent trained simultaneously with both techniques obtains a lower reward than an agent trained using only a policy-based approach. Nevertheless, the agent achieves lower execution times and less dispersion during training.",Reinforcement Learning for UAV control with Policy and Reward Shaping,"Cristian Mill\'an-Arias, Ruben Contreras, Francisco Cruz and Bruno Fernandes",2022,Artificial Intelligence,2212.03828
"We propose a hierarchical framework for collaborative intelligent systems. This framework organizes research challenges based on the nature of the collaborative activity and the information that must be shared, with each level building on capabilities provided by lower levels. We review research paradigms at each level, with a description of classical engineering-based approaches and modern alternatives based on machine learning, illustrated with a running example using a hypothetical personal service robot. We discuss cross-cutting issues that occur at all levels, focusing on the problem of communicating and sharing comprehension, the role of explanation and the social nature of collaboration. We conclude with a summary of research challenges and a discussion of the potential for economic and societal impact provided by technologies that enhance human abilities and empower people and society through collaboration with Intelligent Systems.",A Hierarchical Framework for Collaborative Artificial Intelligence,"James L. Crowley (LIG, UGA, MIAI@UGA, Grenoble INP ), Jo\""elle L Coutaz (UGA), Jasmin Grosinger, Javier V\'azquez-Salceda (UPC), Cecilio Angulo (UPC), Alberto Sanfeliu (UPC), Luca Iocchi (Sapienza University of Rome), Anthony G. Cohn",2022,Artificial Intelligence,2212.08659
"Bayesian networks (BNs) are a probabilistic graphical model widely used for representing expert knowledge and reasoning under uncertainty. Traditionally, they are based on directed acyclic graphs that capture dependencies between random variables. However, directed cycles can naturally arise when cross-dependencies between random variables exist, e.g., for modeling feedback loops. Existing methods to deal with such cross-dependencies usually rely on reductions to BNs without cycles. These approaches are fragile to generalize, since their justifications are intermingled with additional knowledge about the application context. In this paper, we present a foundational study regarding semantics for cyclic BNs that are generic and conservatively extend the cycle-free setting. First, we propose constraint-based semantics that specify requirements for full joint distributions over a BN to be consistent with the local conditional probabilities and independencies. Second, two kinds of limit semantics that formalize infinite unfolding approaches are introduced and shown to be computable by a Markov chain construction.",On the Foundations of Cycles in Bayesian Networks,"Christel Baier and Clemens Dubslaff and Holger Hermanns and Nikolai K\""afer",2022,Artificial Intelligence,2301.08608
